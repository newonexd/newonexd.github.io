{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/fluid/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/icon.jpg","path":"img/icon.jpg","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/lazyload.js","path":"js/lazyload.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/local-search.js","path":"js/local-search.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/post.js","path":"js/post.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/wechat.png","path":"img/wechat.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/anchor/anchor.min.js","path":"lib/anchor/anchor.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/fancybox/jquery.fancybox.min.css","path":"lib/fancybox/jquery.fancybox.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/github-markdown/github-markdown.min.css","path":"lib/github-markdown/github-markdown.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/katex/katex.min.css","path":"lib/katex/katex.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/prettify/tomorrow-night-eighties.min.css","path":"lib/prettify/tomorrow-night-eighties.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/popper/popper.min.js","path":"lib/popper/popper.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/prettify/github-v2.min.css","path":"lib/prettify/github-v2.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/prettify/prettify.min.js","path":"lib/prettify/prettify.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/prettify/tomorrow.min.css","path":"lib/prettify/tomorrow.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/prettify/tomorrow-night.min.css","path":"lib/prettify/tomorrow-night.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/smooth-scroll/smooth-scroll.min.js","path":"lib/smooth-scroll/smooth-scroll.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/tocbot/tocbot.min.js","path":"lib/tocbot/tocbot.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/typed/typed.min.js","path":"lib/typed/typed.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/fancybox/jquery.fancybox.min.js","path":"lib/fancybox/jquery.fancybox.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/jquery/jquery.min.js","path":"lib/jquery/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/bloom/2.png","path":"img/blog/bloom/2.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/bloom/1.png","path":"img/blog/bloom/1.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/bloom/3.png","path":"img/blog/bloom/3.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/bloom/4.png","path":"img/blog/bloom/4.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/bloom/6.png","path":"img/blog/bloom/6.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/bloom/5.png","path":"img/blog/bloom/5.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/11.png","path":"img/blog/raft/11.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/10.png","path":"img/blog/raft/10.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/110.png","path":"img/blog/raft/110.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/5.png","path":"img/blog/raft/5.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.eot","path":"lib/font-awesome/webfonts/fa-regular-400.eot","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.ttf","path":"lib/font-awesome/webfonts/fa-regular-400.ttf","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.woff","path":"lib/font-awesome/webfonts/fa-regular-400.woff","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/pbft/1.png","path":"img/blog/pbft/1.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/1.png","path":"img/blog/raft/1.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/12.png","path":"img/blog/raft/12.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/4.png","path":"img/blog/raft/4.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/6.png","path":"img/blog/raft/6.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/7.png","path":"img/blog/raft/7.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/8.png","path":"img/blog/raft/8.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/raft/9.png","path":"img/blog/raft/9.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/bootstrap/js/bootstrap.min.js","path":"lib/bootstrap/js/bootstrap.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.woff","path":"lib/font-awesome/webfonts/fa-brands-400.woff","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.woff","path":"lib/font-awesome/webfonts/fa-solid-900.woff","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.eot","path":"lib/font-awesome/webfonts/fa-brands-400.eot","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.ttf","path":"lib/font-awesome/webfonts/fa-brands-400.ttf","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Bold.eot","path":"lib/mdbootstrap/font/roboto/Roboto-Bold.eot","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Bold.woff","path":"lib/mdbootstrap/font/roboto/Roboto-Bold.woff","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Bold.woff2","path":"lib/mdbootstrap/font/roboto/Roboto-Bold.woff2","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Light.eot","path":"lib/mdbootstrap/font/roboto/Roboto-Light.eot","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Medium.eot","path":"lib/mdbootstrap/font/roboto/Roboto-Medium.eot","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Medium.woff","path":"lib/mdbootstrap/font/roboto/Roboto-Medium.woff","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Medium.woff2","path":"lib/mdbootstrap/font/roboto/Roboto-Medium.woff2","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Regular.woff","path":"lib/mdbootstrap/font/roboto/Roboto-Regular.woff","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Thin.eot","path":"lib/mdbootstrap/font/roboto/Roboto-Thin.eot","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Regular.eot","path":"lib/mdbootstrap/font/roboto/Roboto-Regular.eot","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Regular.woff2","path":"lib/mdbootstrap/font/roboto/Roboto-Regular.woff2","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Thin.woff2","path":"lib/mdbootstrap/font/roboto/Roboto-Thin.woff2","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Thin.woff","path":"lib/mdbootstrap/font/roboto/Roboto-Thin.woff","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/arth.png","path":"img/blog/arth.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.svg","path":"lib/font-awesome/webfonts/fa-regular-400.svg","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Light.woff2","path":"lib/mdbootstrap/font/roboto/Roboto-Light.woff2","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Light.woff","path":"lib/mdbootstrap/font/roboto/Roboto-Light.woff","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Regular.ttf","path":"lib/mdbootstrap/font/roboto/Roboto-Regular.ttf","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/bootstrap/css/bootstrap.min.css","path":"lib/bootstrap/css/bootstrap.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.ttf","path":"lib/font-awesome/webfonts/fa-solid-900.ttf","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Bold.ttf","path":"lib/mdbootstrap/font/roboto/Roboto-Bold.ttf","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Medium.ttf","path":"lib/mdbootstrap/font/roboto/Roboto-Medium.ttf","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/css/mdb.min.css","path":"lib/mdbootstrap/css/mdb.min.css","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/blog/zfb.png","path":"img/blog/zfb.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Light.ttf","path":"lib/mdbootstrap/font/roboto/Roboto-Light.ttf","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/js/mdb.min.js","path":"lib/mdbootstrap/js/mdb.min.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.eot","path":"lib/font-awesome/webfonts/fa-solid-900.eot","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Thin.ttf","path":"lib/mdbootstrap/font/roboto/Roboto-Thin.ttf","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.svg","path":"lib/font-awesome/webfonts/fa-brands-400.svg","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.svg","path":"lib/font-awesome/webfonts/fa-solid-900.svg","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/4.jpg","path":"img/4.jpg","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/5.jpg","path":"img/5.jpg","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/0.jpg","path":"img/0.jpg","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/3.jpg","path":"img/3.jpg","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/2.jpg","path":"img/2.jpg","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/404.jpg","path":"img/404.jpg","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/1.jpg","path":"img/1.jpg","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"0af0c5178806ec375cfa2b4f16d0409b2ce7ccc3","modified":1589168677643},{"_id":"themes/fluid/_static_prefix.yml","hash":"9e67fdce54ba9ce37ef751de332fc80f79d60888","modified":1589168677690},{"_id":"themes/fluid/.DS_Store","hash":"200f64ed661b8aea69d4e6b2000f295e46a90966","modified":1589168677674},{"_id":"themes/fluid/_config.yml","hash":"703d5f1115f5c98112168be18fced1663ed0fecc","modified":1589169568124},{"_id":"themes/fluid/layout/page.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1589168677690},{"_id":"themes/fluid/languages/en.yml","hash":"fa13692b11c26d2ac9b804864c604a32f69705d6","modified":1589168677690},{"_id":"themes/fluid/languages/zh-CN.yml","hash":"1361e44677dac839d5dbabfb639aaf67cd24d7f0","modified":1589168677690},{"_id":"themes/fluid/layout/404.ejs","hash":"729313a9fa99eabca73fc81d3c009f8c9522ef5f","modified":1589168677690},{"_id":"themes/fluid/layout/about.ejs","hash":"44d93cc1e0fe69b03ecaa2731648964b54db6613","modified":1589168677690},{"_id":"themes/fluid/layout/categories.ejs","hash":"e189227680e35a8f4943039bbb22a99f807ba4ef","modified":1589168677690},{"_id":"themes/fluid/layout/archive.ejs","hash":"22e4f8b08c60fa824e1f997f0d3cdd4f9b2a6922","modified":1589168677690},{"_id":"themes/fluid/layout/category.ejs","hash":"1f01378f5113cfca21038a0db85959656e131145","modified":1589168677690},{"_id":"themes/fluid/layout/index.ejs","hash":"0b12a9ca99a6616d0b14f96b85df79d50927f41f","modified":1589168677690},{"_id":"themes/fluid/layout/layout.ejs","hash":"b4e73a9ecadecfa0ed4a2e005571eab83617fd73","modified":1589168677690},{"_id":"themes/fluid/layout/post.ejs","hash":"3a0827478dbd8adbe5f44d944516619c281f67c1","modified":1589168677690},{"_id":"themes/fluid/layout/tag.ejs","hash":"e18e1b90451f6909e060247bedf7e4fd2903f5d0","modified":1589168677690},{"_id":"themes/fluid/layout/tags.ejs","hash":"9115bafe3c2b4eac0e79e37c7537cda9468a7cf8","modified":1589168677690},{"_id":"themes/fluid/pages/about.md","hash":"5d3199150b82efc54a1231a4815737d710057563","modified":1589168677690},{"_id":"themes/fluid/pages/local-search.xml","hash":"14b8940c68632eaf91aef6688d12535bfc0719e9","modified":1589168677690},{"_id":"themes/fluid/scripts/helpers.js","hash":"1990df1124034fc36cb1c98828fbe416762e6764","modified":1589168677690},{"_id":"themes/fluid/scripts/lazyload.js","hash":"6589bd1c528cae60355fcdce10c82830e11b4ca5","modified":1589168677690},{"_id":"themes/fluid/scripts/local-search.js","hash":"22b142c321ab5cc05a0317d54e3fb61bc32d62ed","modified":1589168677690},{"_id":"themes/fluid/scripts/merge-configs.js","hash":"a82460f50dc46d643c9166c81a64bed3a12cc7c1","modified":1589168677690},{"_id":"themes/fluid/scripts/pages.js","hash":"1d3f0af98ac41288614d971bd80a7ea0a240f9e3","modified":1589168677690},{"_id":"themes/fluid/scripts/wordcount.js","hash":"0adcee7effcfdf49e90ed3fdf1c82ea4a2887b2e","modified":1589168677690},{"_id":"themes/fluid/source/.DS_Store","hash":"70dee1b5a06b2d4f680acbb733f401fc9ee3e875","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/beian.ejs","hash":"ea66bf9fa3a059b9a21cb94ec4d641dfa49fb3d0","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/busuanzi.ejs","hash":"a80167aee1a2bbd75e79d17ad57c967fc4fae003","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/css.ejs","hash":"39b1e8c3ae936c20d318e71e1f5c7f1ed087eb98","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/footer.ejs","hash":"21bb6252846049c6c7292cdac22d5593f9bc28b4","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/head.ejs","hash":"ea6f30084906efeb17aaef41a631b93303f0a21c","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/nav.ejs","hash":"4b58ba8839038b89f4981e7d8b488192d82b5349","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/paginator.ejs","hash":"9c1bb3a6b42a8516f31432ee1d8749c2700bf0fb","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/scripts.ejs","hash":"0155436333032f98a3015d5c833c272151c6566f","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/search.ejs","hash":"bea21f1b5de61badd6c068080315c201fc80bc36","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/toc.ejs","hash":"124a57b782b7a13302ce6726b7773dfc40084e0c","modified":1589168677690},{"_id":"themes/fluid/scripts/utils/join-path.js","hash":"2cc9ff68919447ba6ea433c43db62d0d85e756b5","modified":1589168677690},{"_id":"themes/fluid/source/css/main.styl","hash":"33a5dac12a89f0b6f343f4b7f639f8cb2c29e330","modified":1589168677705},{"_id":"themes/fluid/source/img/icon.jpg","hash":"2010cb1c865036d445877d43bcb8aafe4cc16e1b","modified":1589168677877},{"_id":"themes/fluid/source/js/lazyload.js","hash":"9672dc0bab99ce11dd1eb5a0d8eba4eba5d7a324","modified":1589168677877},{"_id":"themes/fluid/source/js/local-search.js","hash":"bf09d2d9a1d5965255b4ec50ab4a8e276962365a","modified":1589168677877},{"_id":"themes/fluid/source/js/main.js","hash":"a7073e60c875df0666ffbec8c70d71c9601e4588","modified":1589168677877},{"_id":"themes/fluid/source/js/post.js","hash":"7418f861b29a78201c5fcffdcd1f15d92df2aa2d","modified":1589168677877},{"_id":"themes/fluid/source/img/wechat.png","hash":"a612ba6ac7a62ea4ced229b7d11006e675ca8aac","modified":1589168677877},{"_id":"themes/fluid/source/css/_functions/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1589168677705},{"_id":"themes/fluid/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1589168677705},{"_id":"themes/fluid/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1589168677705},{"_id":"source/_posts/blog/blockchain/Jchain1.md","hash":"dba9c0234ca6705b264a78fb53d8e81499384dce","modified":1589871996754},{"_id":"source/_posts/blog/blockchain/Jchain2.md","hash":"20da37f6e11ab259925839c9f4dd54d8c8798d77","modified":1589871737343},{"_id":"source/_posts/blog/blockchain/Jchain3.md","hash":"7613277e1460b4d83a1cc2f8b0ef1fceb115b7cf","modified":1589871781304},{"_id":"source/_posts/blog/blockchain/Jchain4.md","hash":"928e087962115e54ef0d7f76007ace5db0a74129","modified":1589871811619},{"_id":"source/_posts/blog/consensus/paxos.md","hash":"7bb60e77aa724ca46aa037a5f84e562e4483d9c9","modified":1589168677643},{"_id":"source/_posts/blog/consensus/pbft-three_phase.md","hash":"d077d049cd58d69e717c70f68bed81ca34056601","modified":1589168677643},{"_id":"source/_posts/blog/consensus/pbft-view.md","hash":"f97fd55760a51753065d60008d2df78430efa99c","modified":1589168677643},{"_id":"source/_posts/blog/consensus/raft-log.md","hash":"fbc4d886f3ead249d5ed07a0f99715d702989d65","modified":1589168677643},{"_id":"source/_posts/blog/consensus/raft-election.md","hash":"1b374e49b6bf85435cf6c02d504351a3fcfba9ad","modified":1589168677643},{"_id":"source/_posts/blog/consensus/raft-snapshot.md","hash":"580cf6868c6cd9c43ab8ddae46930e2b4fe92dc3","modified":1589168677643},{"_id":"source/_posts/blog/consensus/raft-relationship.md","hash":"8aa4b9b4c029f42ebadb09c761207dd252ded17f","modified":1589168677643},{"_id":"source/_posts/blog/couchDB/CouchDB基本操作.md","hash":"07a83a3ac6cc8c9d88b72f74a9d01bb570bdf846","modified":1589168677643},{"_id":"source/_posts/blog/consensus/raft.md","hash":"a80d9c02e92a39b7cb551508431744281d38f986","modified":1589168677643},{"_id":"source/_posts/blog/couchDB/CouchDB学习-API.md","hash":"db4cadc84837ea2314773b8b3c7f519d1cee9d89","modified":1589168677659},{"_id":"source/_posts/blog/couchDB/CouchDB学习-维护.md","hash":"9464753838b1860ee9f6409cb00455828a14b352","modified":1589168677659},{"_id":"source/_posts/blog/couchDB/CouchDB学习-集群管理.md","hash":"1e3634ca76bd58712608680ae58fc15d934833c5","modified":1589168677659},{"_id":"source/_posts/blog/couchDB/CouchDB学习-介绍.md","hash":"7aa9e0080f138815daa565e83b91aa66994f1a9a","modified":1589168677659},{"_id":"source/_posts/blog/couchDB/CouchDB学习一.md","hash":"c254ac1172c2e761e8ecc57c28ccacba5d4644b9","modified":1589168677659},{"_id":"source/_posts/blog/etcd/ETCD配置参数.md","hash":"06418c2a16c7fae7574d616d6b485cfd764802bf","modified":1589169287958},{"_id":"source/_posts/blog/etcd/HTTP_JSON_API通过gRPC网关.md","hash":"27c9b5a76abf37d2f75147866499d04fec7a8e9b","modified":1589168677659},{"_id":"source/_posts/blog/etcd/etcd网关.md","hash":"36ff6ef946bbeea3111ac512539631b2600cc99e","modified":1589168677659},{"_id":"source/_posts/blog/etcd/TLS.md","hash":"d35ec3a61aebc35cb758f5efd3bdf8aa7dfa54eb","modified":1589168677659},{"_id":"source/_posts/blog/etcd/gRPC代理.md","hash":"527f34e2ddfee9fda521c497bffb7812814e2cab","modified":1589168677659},{"_id":"source/_posts/blog/etcd/gRPC命名与发现.md","hash":"b683129d5fea2a3bde2271168a2e6f35113ba85b","modified":1589168677659},{"_id":"source/_posts/blog/etcd/单机集群.md","hash":"7b27de68722cb2b4ec734287ccdc5bc18e6e07b9","modified":1589168677659},{"_id":"source/_posts/blog/etcd/与etcd进行交互.md","hash":"7064d34119523b27b6583f473d640da13e9e028a","modified":1589168677659},{"_id":"source/_posts/blog/etcd/在容器内运行etcd集群.md","hash":"8443b2e0acfca0ca9ebeaac10daaacd08e51a671","modified":1589168677659},{"_id":"source/_posts/blog/etcd/基于角色的访问控制.md","hash":"147c18e1526b4a76046bdc82ba14b09424c08bf4","modified":1589168677659},{"_id":"source/_posts/blog/etcd/多机集群.md","hash":"085f47344282c62e4b545f98d98e4b8e37b48fd4","modified":1589168677659},{"_id":"source/_posts/blog/etcd/实验特性和APIs.md","hash":"4052cc750d0bad24bbfeab6b9b2220cffd8b1d87","modified":1589168677659},{"_id":"source/_posts/blog/etcd/客户端v3.md","hash":"589ed8b379eff00c62736db5603422fea556a5c1","modified":1589168677659},{"_id":"source/_posts/blog/etcd/文档.md","hash":"72f1b8197c1c15fc7607e1401dc7e3f2e6d29b56","modified":1589168677659},{"_id":"source/_posts/blog/etcd/系统限制.md","hash":"615ff2bda765f9e26d61fed721ea95978af49907","modified":1589168677659},{"_id":"source/_posts/blog/etcd/词汇表.md","hash":"81ab40de7b4941debe085601b73ce62f66c4f9dc","modified":1589168677659},{"_id":"source/_posts/blog/etcd/运行时重新配置.md","hash":"3d354e80f0b185361372d08b770199b7d22a4279","modified":1589168677659},{"_id":"source/_posts/blog/etcd/运行时重新配置设计.md","hash":"070ccebdca8f214be04e2804c6d56e573b927245","modified":1589168677659},{"_id":"source/_posts/blog/fabric/Fabric1.4多机部署.md","hash":"ca7ca3c590545419a8814522e88e1ae453b466ed","modified":1589168677659},{"_id":"source/_posts/blog/fabric/Fabric_index.md","hash":"d1eee25203ff4021fff387ba7d10224d2d892f71","modified":1589168677659},{"_id":"source/_posts/blog/fabric/Fabric1.4源码解析之链码容器启动过程.md","hash":"969251775b39fafd1a0284604410f311bcd37c53","modified":1589168677659},{"_id":"source/_posts/blog/fabric/Fabric环境搭建.md","hash":"ed2a6acdf1f2b11c12aac7b8df5ed770fa592b67","modified":1589168677659},{"_id":"source/_posts/blog/fabric/Hyperledger_Fabric动态添加组织到网络中.md","hash":"b2a3ec4ca948980ae0776157b3de7dc9111183db","modified":1589168677674},{"_id":"source/_posts/blog/fabric/Hyperledger_Fabric手动生成CA证书搭建Fabric网络.md","hash":"51c9fc29667b07cb52b5b8ef13b36332c6d415bf","modified":1589168677674},{"_id":"source/_posts/blog/fabric/TLS_SDK调用.md","hash":"ee2e38d32502437127d35543d27249e6f26fe011","modified":1589168677674},{"_id":"source/_posts/blog/fabric/使用硬件安全模块.md","hash":"5917e739cc485e7a87528b05fcfcf55d663b915b","modified":1589168677674},{"_id":"source/_posts/blog/fabric/Hyperledger_Fabric_CA.md","hash":"7e52a5bc12148a13ffabd89402f95dcd95387aea","modified":1589168677674},{"_id":"source/_posts/blog/fabric/动态配置Raft节点.md","hash":"4fe05f8ef5848c8bba385c5d910acb78a8d1e75d","modified":1589168677674},{"_id":"source/_posts/blog/fabric/外部链码构建和运行.md","hash":"40f813239039fe9f293a95b1e62ed61e67f7c7fd","modified":1589168677674},{"_id":"source/_posts/blog/fabric/深入解析Fabric搭建的全过程.md","hash":"1c3d57cb1a6ca22eb85f4f99d3138536af78587e","modified":1589358972347},{"_id":"source/_posts/blog/fabric/私有数据.md","hash":"bac3a5dad06f499963cf27d60d3f6ab9a8f3e610","modified":1589168677674},{"_id":"source/_posts/blog/fabric/链码作为外部服务.md","hash":"e14847301e160822904ff237e3bff5f67bf2e842","modified":1589168677674},{"_id":"source/_posts/blog/fabric/链码测试.md","hash":"f8f15dfb187cbe52c1b0d2caef8baa9759fe8dd5","modified":1589168677674},{"_id":"source/_posts/blog/ipfs/IPFS学习-DNS链接.md","hash":"4499790e28acc0da0b34ea68d2635d3cd75a45bb","modified":1589168677674},{"_id":"source/_posts/blog/ipfs/IPFS学习-IPNS.md","hash":"33d1aa2a647b0e3feb1201d065029c05551c1bf1","modified":1589168677674},{"_id":"source/_posts/blog/ipfs/IPFS学习-内容标识符CID.md","hash":"d5c10a9765910eff156332d9543ea5379f56debb","modified":1589168677674},{"_id":"source/_posts/blog/ipfs/IPFS学习-哈希.md","hash":"f5b2e0f715c9960ee4f84ccbf1485d2168131f76","modified":1589168677674},{"_id":"source/_posts/blog/ipfs/IPFS学习-分布式哈希表DHT.md","hash":"37df8830ed3088e68461cafe5f30e6a797ff3260","modified":1589168677674},{"_id":"source/_posts/blog/linux/正则表达式.md","hash":"b87c8cf1dfeb81309a24d0a3ae3b75d17df3a116","modified":1589168677674},{"_id":"source/_posts/blog/other/CURL命令学习三.md","hash":"cbf8b03a7826616242374174162f4ecca790da66","modified":1589168677674},{"_id":"source/_posts/blog/linux/Linux命令-grep_sed_awk.md","hash":"a15689e6dce5cad78ab42c933ec230f9745da880","modified":1589168677674},{"_id":"source/_posts/blog/other/CURL命令学习一.md","hash":"fe9adf5f1bb4510a819a191abad332e3b15f4153","modified":1589168677674},{"_id":"source/_posts/blog/other/CURL学习二.md","hash":"8bd25e7ef2bcabc944e09e6c22ee84dcc058228a","modified":1589168677674},{"_id":"themes/fluid/layout/_partial/comments/utterances.ejs","hash":"c0ab32d00014f8aa0cef9de5bf3e5d3999922843","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/comments/disqus.ejs","hash":"fe05b763fee002f69a3e2c6d3acb5db6f4cc71f9","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/comments/gitalk.ejs","hash":"744b40e9326b12059a28063ecf0de6e06f522793","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/comments/valine.ejs","hash":"f5d95b1e387231b0dd825edbf419dc2f7df79994","modified":1589168677690},{"_id":"source/_posts/blog/other/bloom.md","hash":"5b944ae4af0f5585817e81960553a18fe6a23259","modified":1589169240567},{"_id":"themes/fluid/layout/_partial/plugins/anchor.ejs","hash":"f536fd62eec76fc2a7251ec39ba100e56ee93fa3","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/plugins/analytics.ejs","hash":"696ec04c57dffca0d432593bae3cd5d3d9319663","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/plugins/fancybox.ejs","hash":"08f4aaa883b556460f6a0b0a07dbc16247d1385e","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/plugins/local-search.ejs","hash":"ede7449f17e3b7a53178db31ba705dbd65c2d3d7","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/plugins/math.ejs","hash":"267b2512c9afffc9b90d4e692bdda9768ee9511c","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/plugins/prettify.ejs","hash":"95a02713ac0f77820490c8292269887952a5014b","modified":1589168677690},{"_id":"themes/fluid/layout/_partial/plugins/typed.ejs","hash":"85b80f022a037f9b19ae7c9852c1dbf24dfda231","modified":1589168677690},{"_id":"themes/fluid/source/css/_custom/custom.styl","hash":"3626d8439c27c9bfde008de8a7ba5124bbc7c397","modified":1589168677705},{"_id":"themes/fluid/source/css/_functions/base.styl","hash":"1342ac3f52d44349c8f324a70967acf24055f473","modified":1589168677705},{"_id":"themes/fluid/source/css/_mixins/base.styl","hash":"68caf7cc73fa6211f9802b69293a2a38d7322de7","modified":1589168677705},{"_id":"themes/fluid/source/css/_pages/pages.styl","hash":"03703ece80dac645607d4731dff3fdd923777d2a","modified":1589168677705},{"_id":"themes/fluid/source/css/_variables/base.styl","hash":"08946cf375bfee06c9c210bacfd75fc25bd8f915","modified":1589168677705},{"_id":"themes/fluid/source/lib/anchor/anchor.min.js","hash":"8c7218d1fca9ba81eef21718b2dc360a4eb256c7","modified":1589168677877},{"_id":"themes/fluid/source/lib/fancybox/jquery.fancybox.min.css","hash":"1fc31bc44ee1156d5715f74ae58fa93b583eaafc","modified":1589168677877},{"_id":"themes/fluid/source/lib/github-markdown/github-markdown.min.css","hash":"6345bb8a9d0b11620722cb47b6cda4810ff43126","modified":1589168677908},{"_id":"themes/fluid/source/lib/katex/katex.min.css","hash":"7f82be27229c54a3ef27b31e8b5ac10ba93cb09f","modified":1589168677924},{"_id":"themes/fluid/source/lib/prettify/tomorrow-night-eighties.min.css","hash":"164bf021bcc2d38513771f397d80e6da5a7b796f","modified":1589168677940},{"_id":"themes/fluid/source/lib/popper/popper.min.js","hash":"90baeeac13ea91ceeaf311d831a4722a0d40f0e2","modified":1589168677940},{"_id":"themes/fluid/source/lib/prettify/github-v2.min.css","hash":"024d3a04332541d00768fb968f3f2d0c88e7f08c","modified":1589168677940},{"_id":"themes/fluid/source/lib/prettify/prettify.min.js","hash":"d09d2922c4cbb439d6322a5a7e867ac9cf35fa25","modified":1589168677940},{"_id":"themes/fluid/source/lib/prettify/tomorrow.min.css","hash":"aa28b0e08aa4b2444e534b33eae45a1072aefecc","modified":1589168677940},{"_id":"themes/fluid/source/lib/prettify/tomorrow-night.min.css","hash":"bcd7d7dcd860db497f090c470f4a8b7f28824ed5","modified":1589168677940},{"_id":"themes/fluid/source/lib/smooth-scroll/smooth-scroll.min.js","hash":"38de3473ba737a2162f57bb8cf0e2eec9d572305","modified":1589168677940},{"_id":"themes/fluid/source/lib/tocbot/tocbot.min.js","hash":"bae97e8a24a05a99335f8e725641c8ca9c50502a","modified":1589168677940},{"_id":"themes/fluid/source/lib/typed/typed.min.js","hash":"5dc41c20ea1d0228323022c277c8a2814ee52058","modified":1589168677940},{"_id":"themes/fluid/source/css/_pages/_archive/archive.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1589168677705},{"_id":"themes/fluid/source/css/_pages/_category/category.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1589168677705},{"_id":"themes/fluid/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1589168677705},{"_id":"themes/fluid/source/lib/fancybox/jquery.fancybox.min.js","hash":"978434640f142d5765f2cd6b390303d882f69879","modified":1589168677877},{"_id":"themes/fluid/source/lib/jquery/jquery.min.js","hash":"b15f7cfa79519756dff1ad22553fd0ed09024343","modified":1589168677908},{"_id":"themes/fluid/source/css/_pages/_about/about.styl","hash":"122d6ba9ec51577022d33cd12483f5d4ad3cc8d8","modified":1589168677705},{"_id":"themes/fluid/source/css/_pages/_base/base.styl","hash":"2f2e47ea23062ade2ed1d32ed7c8ea11f138b188","modified":1589168677705},{"_id":"themes/fluid/source/css/_pages/_category/categories.styl","hash":"66169a7d6cdd0c30337c9b9852abd73078d9b7d7","modified":1589168677705},{"_id":"themes/fluid/source/css/_pages/_index/index.styl","hash":"5f6b648e8f56c1719597802053ca51ab3a0b4771","modified":1589168677705},{"_id":"themes/fluid/source/css/_pages/_post/post.styl","hash":"71d7144a226de4509294f1bce668b69c1c565fe8","modified":1589168677705},{"_id":"themes/fluid/source/css/_pages/_tag/tags.styl","hash":"87c952d03d208e93e942b803b807cff08a8cc216","modified":1589168677705},{"_id":"themes/fluid/source/img/blog/bloom/2.png","hash":"bc8d73648bc07117b5d71f4e328e3c95aa031e32","modified":1589166867886},{"_id":"themes/fluid/source/img/blog/bloom/1.png","hash":"f05c34f141c27e2625ca7863bfaced2bacbc31ca","modified":1589166859348},{"_id":"themes/fluid/source/img/blog/bloom/3.png","hash":"65438b25bdf362dff41c205cd5fc1a28773e7a80","modified":1589166876969},{"_id":"themes/fluid/source/img/blog/bloom/4.png","hash":"6067ad2fc30860f9706280343a862b318ebafaf2","modified":1589166885973},{"_id":"themes/fluid/source/img/blog/bloom/6.png","hash":"18c67938b37cfca0e4b3d3be1aa68321204451d8","modified":1589166925583},{"_id":"themes/fluid/source/img/blog/bloom/5.png","hash":"45d072db173ad6cfa7300b614c1479b076972898","modified":1589166893955},{"_id":"themes/fluid/source/img/blog/raft/11.png","hash":"f66093b0a61374955e72349cf47eb56d0f9f6b80","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/10.png","hash":"2ada3e13ad9c0e3f1773ad2a78fd1fcf1ce70a0a","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/110.png","hash":"0c80df8bdc04fa0f6cfdea28de0ca92dc4bdbfbc","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/5.png","hash":"3693d2d84813a8aa94c647ab9a1ba1b63b11312a","modified":1589168677862},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.eot","hash":"2e97930b520222ec3c2e4188ce07cc1904beba48","modified":1589168677893},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.ttf","hash":"5eb58f4263f87c543388bf66dec7d1f0b7c5b32c","modified":1589168677893},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"8b356dc021032d9380af47f7608a6b62a9b6f363","modified":1589168677893},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.woff","hash":"38e6bd17442bb34e0e13a2c9bcbc5299f68be173","modified":1589168677893},{"_id":"themes/fluid/source/img/blog/pbft/1.png","hash":"141d159706c234f2ca908cc534890649555aafde","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/1.png","hash":"62c7588b88842c671956e359886e2ba657e72196","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/12.png","hash":"22f625c2574d38ba3b1c05d0c750e925d88a72ce","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/4.png","hash":"5c38b1cc152460d9bb7344b792aabe15b4b0774f","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/6.png","hash":"c328b267fc513b4cb202c9c2d8192f3deacfdb89","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/7.png","hash":"42a698a1a32447447eeddfd24c7a03ee7c2bb88b","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/8.png","hash":"101b22dd693d62804032b355e66494c0859480d6","modified":1589168677862},{"_id":"themes/fluid/source/img/blog/raft/9.png","hash":"df22e4f6cbd26397e7775241a2433f4b8599899e","modified":1589168677862},{"_id":"themes/fluid/source/lib/bootstrap/js/bootstrap.min.js","hash":"228e91568d10d686548c752dc4f6db96923e06d8","modified":1589168677877},{"_id":"themes/fluid/source/lib/font-awesome/css/all.min.css","hash":"fe44f70dba801d5171e4f8837b1ce0c52783e038","modified":1589168677877},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"9465c5894ca2f93655fa5767b820b762aff6b518","modified":1589168677893},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.woff","hash":"2417fe03c7330a5160f070d6ab747a2bc4bbd41b","modified":1589168677893},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"d378644ff0f7549fa6f217a08dfd2566a770638e","modified":1589168677908},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.woff","hash":"5d5d1448b199c38f1b39a49b1e9b3f1381a26cad","modified":1589168677908},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.eot","hash":"453e71a65f2958480b74fdb75a53d41068699dbf","modified":1589168677877},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.ttf","hash":"3fbe9822118e91350912f51f3080ce4aa9b3ec38","modified":1589168677893},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1589168677940},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Thin.eot","hash":"0790a51a848dbe7292c98f9d0459218bf1a8ffdd","modified":1589168677940},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1589168677940},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Thin.woff2","hash":"2449e3dac5ddb7c3da8bb07450493b62d052758c","modified":1589168677940},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Thin.woff","hash":"fbc3e71d456c96667d8082ab910e3946ef89240b","modified":1589168677940},{"_id":"themes/fluid/source/img/blog/arth.png","hash":"6b1d95ecf580c284778854e37dc852da589f6113","modified":1589168677846},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-regular-400.svg","hash":"3c4ae85dc2835e2ac7d10e1acb6f12ce8ab3d8cb","modified":1589168677893},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1589168677940},{"_id":"themes/fluid/source/lib/bootstrap/css/bootstrap.min.css","hash":"83f46f78c4c6a713108a228dd3e0e83f0a2bcf52","modified":1589168677877},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.ttf","hash":"7b280debee5800806092e35a6bc2c6fd9c51cf63","modified":1589168677908},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/css/mdb.min.css","hash":"7fd027bbc70695aaa0081b5f9a5a0f664a8708da","modified":1589168677924},{"_id":"themes/fluid/source/img/blog/zfb.png","hash":"2256222c00de963e56dd29e1cb5e5e7da49a3d2b","modified":1589168677877},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1589168677924},{"_id":"themes/fluid/source/lib/mdbootstrap/js/mdb.min.js","hash":"fef8d611bbc14ad31ca9ec9e2990bfde4d873bb1","modified":1589168677940},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.eot","hash":"2b0ebea58a0bc895400dffe8c5e434c8b12338e3","modified":1589168677908},{"_id":"themes/fluid/source/lib/mdbootstrap/font/roboto/Roboto-Thin.ttf","hash":"173ed64528b4d010a76d8d38deb1d7e7eed58eda","modified":1589168677940},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-brands-400.svg","hash":"a8e867908941916f4ccbecf0fbcbedfb9e8e6927","modified":1589168677893},{"_id":"themes/fluid/source/lib/font-awesome/webfonts/fa-solid-900.svg","hash":"432a16b3c6ab238a4735891c6bb8f80321101992","modified":1589168677908},{"_id":"themes/fluid/source/img/4.jpg","hash":"58bd0d4a2a92d39752081e56b7dc32c919f8c25d","modified":1589168677815},{"_id":"themes/fluid/source/img/5.jpg","hash":"cc9918d0732ef02c227353427923a60bfb504a31","modified":1589168677846},{"_id":"themes/fluid/source/img/0.jpg","hash":"088da12f57f324f87cf688e99b780705a0e54a93","modified":1589168677721},{"_id":"themes/fluid/source/img/3.jpg","hash":"ecae551dd8913897ad7bd042892ae98e9a49860b","modified":1589168677799},{"_id":"themes/fluid/source/img/2.jpg","hash":"5346f4941b26440f457d8fb2cd95789c15f3003e","modified":1589168677799},{"_id":"themes/fluid/source/img/404.jpg","hash":"906b2caf9a4b2ba3c86820acac4cae1426cf6940","modified":1589168677846},{"_id":"themes/fluid/source/img/1.jpg","hash":"203c61bfed8e1d7f8b02b9349248fe1548b896df","modified":1589168677768},{"_id":"public/sitemap.xml","hash":"8b02a95d6df722d56a107ecb1781d9c167ac9cef","modified":1589872013717},{"_id":"public/baidusitemap.xml","hash":"18a755630b709a3c796831bb504df9a2412cd74a","modified":1589872013717},{"_id":"public/local-search.xml","hash":"9dd85ff3f9e8497ff2545d4813062486166d2c2c","modified":1589872013717},{"_id":"public/2019/12/21/blog/other/CURL命令学习三/index.html","hash":"ae6e96127f1873809b78974fc7b122b977be9c7e","modified":1589872013717},{"_id":"public/2019/12/18/blog/ipfs/IPFS学习-IPNS/index.html","hash":"c27a6235191aac40ae66178070655952154af5b9","modified":1589872013717},{"_id":"public/2019/11/25/blog/etcd/系统限制/index.html","hash":"f8d2789884247881310f102e3e97dc8934e0509c","modified":1589872013717},{"_id":"public/2019/11/24/blog/etcd/实验特性和APIs/index.html","hash":"0c276b1bfd5e37d5d7aea6dedd5c811e3cda5548","modified":1589872013717},{"_id":"public/archives/index.html","hash":"d0964032d8668316c6b48d549cff89fa5c72d076","modified":1589872013717},{"_id":"public/archives/tags/2/index.html","hash":"662d11889d343183ec0db6b0f04fe141e7768563","modified":1589872013717},{"_id":"public/archives/tags/3/index.html","hash":"b8679b002935e9ae398cc9417bbf26cbe3061bde","modified":1589872013717},{"_id":"public/archives/tags/4/index.html","hash":"811694003ed5b3912d50c69e42cc82baa1021f6e","modified":1589872013717},{"_id":"public/archives/tags/5/index.html","hash":"51cbaea2937c6b5e33bd04188db79e7e8ac1b65e","modified":1589872013717},{"_id":"public/archives/tags/6/index.html","hash":"141d4b2bdb6d6984878cd6b033867eefa1bcc5c1","modified":1589872013717},{"_id":"public/archives/tags/7/index.html","hash":"80f94ad41af3d6a2536be2c9d0ebe4601d805e32","modified":1589872013717},{"_id":"public/archives/2019/index.html","hash":"13a2e7e95936d08794de720f2c6aa10860a22c80","modified":1589872013717},{"_id":"public/archives/2019/tags/2/index.html","hash":"a4a8d8511c9c5fcf0edecb504336c1d700633d5e","modified":1589872013717},{"_id":"public/archives/2019/tags/3/index.html","hash":"c6ea87c726124fe1a1bcbe9f53ce88fbb8bb7b84","modified":1589872013717},{"_id":"public/archives/2019/tags/4/index.html","hash":"0d8e7f0d62dc39266311a8de7b97e3646345e48e","modified":1589872013717},{"_id":"public/archives/2019/tags/5/index.html","hash":"5a76e004774e33c5cbff803a688963d032d17e89","modified":1589872013717},{"_id":"public/archives/2019/11/index.html","hash":"0ca91ca30392d3f67e7673570b8ab2855d981924","modified":1589872013717},{"_id":"public/archives/2019/11/tags/2/index.html","hash":"7d7dc1260526c1d4fb5f1145a81abd18b9259090","modified":1589872013717},{"_id":"public/archives/2019/11/tags/3/index.html","hash":"dba17f5ba1751f9cf53a00d1997c601dd57f26c6","modified":1589872013717},{"_id":"public/archives/2019/12/index.html","hash":"463a34b9344564634e9c39356bd75e3fa573a287","modified":1589872013717},{"_id":"public/archives/2019/12/tags/2/index.html","hash":"e40bb28c57c24b2083d9e933af9fc0d0a057da28","modified":1589872013717},{"_id":"public/archives/2019/12/tags/3/index.html","hash":"e8d8c68ebc6504bd7a746baf1e93cf5b7834d640","modified":1589872013717},{"_id":"public/archives/2020/index.html","hash":"88fd52c8928b9d677964afd4d7fae4a7f4f6d79f","modified":1589872013717},{"_id":"public/archives/2020/tags/2/index.html","hash":"71a4c58ea4a3f4a3a746ab8657cb3bf1c02d2b9b","modified":1589872013717},{"_id":"public/archives/2020/01/index.html","hash":"94914cd0522d45eab542d54103c46608796fda2b","modified":1589872013717},{"_id":"public/archives/2020/05/index.html","hash":"118dc8b544c755f1252f49222d59e8c0e8c897b3","modified":1589872013717},{"_id":"public/categories/algorithm/index.html","hash":"63204ea57b8d88328706549b86343ceeb556979d","modified":1589872013717},{"_id":"public/categories/CouchDb应用/index.html","hash":"d2b7403a404f269abc514f3afd4ba24d354990ef","modified":1589872013717},{"_id":"public/categories/CouchDb学习/index.html","hash":"41390279c5eda8c846e1d2c5c481042e3d74c632","modified":1589872013717},{"_id":"public/categories/etcd文档翻译/index.html","hash":"ea7f9b534c643b1bb5d39ab8e977e0bc59e5cfd9","modified":1589872013717},{"_id":"public/categories/etcd文档翻译/tags/2/index.html","hash":"4fb9abcb5125ab8ce00403c483d9f72cb33bd40c","modified":1589872013717},{"_id":"public/categories/fabric应用/index.html","hash":"cd48d5fa783740f1f0f12199822e8380833362ad","modified":1589872013717},{"_id":"public/categories/fabric应用/tags/2/index.html","hash":"3a198aca1dae3ea3999e57c600c0360e07518ff0","modified":1589872013717},{"_id":"public/categories/fabric-ca应用/index.html","hash":"f27500792c567cbfc27a5babb0836fb84378c7af","modified":1589872013717},{"_id":"public/categories/fabric源码解析/index.html","hash":"364fbadcf01d4b33b39ca8672bda75803b920ab2","modified":1589872013717},{"_id":"public/categories/IPFS学习/index.html","hash":"d70eac78192febe07d97acc6933b5fc9eb747be8","modified":1589872013717},{"_id":"public/categories/curl/index.html","hash":"bc3c0c232e1eba1322f57e4f6a45cf36943db980","modified":1589872013717},{"_id":"public/categories/Linux命令/index.html","hash":"3c0efefad2a05843e7bd23c083e1406586353d29","modified":1589872013717},{"_id":"public/categories/other/index.html","hash":"057e2cc69b91801aa05fd52df13675448cc53358","modified":1589872013717},{"_id":"public/tags/7/index.html","hash":"2acdc661ada28b650f505986a533b8dc5ed3f611","modified":1589872013717},{"_id":"public/tags/blockchain/index.html","hash":"1de0213a9b6aaed0eff98ad9d1ded88e37929a66","modified":1589872013717},{"_id":"public/tags/algorithm/index.html","hash":"8979c75a03d1a13ded27b70545d6f35d91dae9e2","modified":1589872013717},{"_id":"public/tags/Pbft/index.html","hash":"c1ec1e6fc233edcb0a5b2aa2cefb43661e1152f3","modified":1589872013717},{"_id":"public/tags/Raft/index.html","hash":"78f34dcfe90a21ca239b26f5ed901c76768ee402","modified":1589872013717},{"_id":"public/tags/CouchDb/index.html","hash":"18389f14b3dc8376967a90dcbc5504b3ec792354","modified":1589872013717},{"_id":"public/tags/etcd/index.html","hash":"79975292ae05102cb13ed56e8e14cf1d3a562890","modified":1589872013717},{"_id":"public/tags/etcd/tags/2/index.html","hash":"f5829755e891f7ff63e4540e2e6a840e8390db18","modified":1589872013717},{"_id":"public/tags/fabric/index.html","hash":"877aa13e3447a87772d58be2f9cd47244363860b","modified":1589872013717},{"_id":"public/tags/fabric/tags/2/index.html","hash":"160738852141f1eb20e2eadcd15a67da315afd58","modified":1589872013717},{"_id":"public/tags/fabric-ca/index.html","hash":"999a23b7180f4cca294928419fe1a53ad771c755","modified":1589872013717},{"_id":"public/tags/IPFS/index.html","hash":"4425fcb9032b7b2cbcf71deb85fa2aa666656c50","modified":1589872013717},{"_id":"public/tags/正则表达式/index.html","hash":"f6cd2f3c9ef25d1f5273aef21e3609705be310b0","modified":1589872013717},{"_id":"public/tags/curl学习/index.html","hash":"e88086d08a21a3a03666ef03ff899b8614082822","modified":1589872013717},{"_id":"public/tags/Linux/index.html","hash":"136a413afd00d2ac8902369a8fbd845e5d721917","modified":1589872013717},{"_id":"public/tags/bloom/index.html","hash":"9f79ec6a9ef8fefa4769bdf8dfabd12b3060ca15","modified":1589872013717},{"_id":"public/404.html","hash":"50c918ffd8051a9c5172ebab507917c94e530806","modified":1589872013717},{"_id":"public/tags/index.html","hash":"80a08c6361ecdceea86c1c70a4745e26816998be","modified":1589872013717},{"_id":"public/categories/index.html","hash":"3f569123325ea931017bec269671167c4470d3e5","modified":1589872013717},{"_id":"public/about/index.html","hash":"89021f58d2041dec41c49ff29776e444a822c859","modified":1589872013717},{"_id":"public/2020/05/19/blog/blockchain/Jchain4/index.html","hash":"5fc71e16e514d7a894cad582db6b31f44523ba47","modified":1589872013717},{"_id":"public/2020/05/19/blog/blockchain/Jchain3/index.html","hash":"f957a4ff6d6a462f90f79ebb10a4bd668b3e0510","modified":1589872013717},{"_id":"public/2020/05/19/blog/blockchain/Jchain2/index.html","hash":"f83510b1fb511d441e28c9e6b9a0c91ec1bb0234","modified":1589872013717},{"_id":"public/2020/05/19/blog/blockchain/Jchain1/index.html","hash":"7ccf98ee21217efc9bd5a6edc4a3f991f4d31dc0","modified":1589872013717},{"_id":"public/2020/05/11/blog/other/bloom/index.html","hash":"79fa67ef1b7e81c5ac487aa2220f033e3ef52609","modified":1589872013717},{"_id":"public/2020/01/10/blog/consensus/pbft-view/index.html","hash":"ccb47c271a87ca1e1850d1f1259d3fb59e90d69e","modified":1589872013717},{"_id":"public/2020/01/09/blog/consensus/pbft-three_phase/index.html","hash":"64de5b0d4c777c227568297b8599bed29bcf4fd2","modified":1589872013717},{"_id":"public/2020/01/09/blog/fabric/Fabric_index/index.html","hash":"ac041b8a30f74f023b4eca2677daf702dd0e7df7","modified":1589872013717},{"_id":"public/2020/01/07/blog/consensus/raft-snapshot/index.html","hash":"f31052c4bf3b9914d56ec61bd0d687bdc3682d09","modified":1589872013717},{"_id":"public/2020/01/06/blog/consensus/raft-relationship/index.html","hash":"1b91243a20e1c7569bf51516868d24560aa111d1","modified":1589872013717},{"_id":"public/2020/01/05/blog/consensus/raft-log/index.html","hash":"34c92e7bc3ca9190e5c51c2ad260cddf892be5a5","modified":1589872013717},{"_id":"public/2020/01/04/blog/consensus/raft-election/index.html","hash":"629943f656080c5698495c4dd586b586f008d3c3","modified":1589872013717},{"_id":"public/2020/01/04/blog/consensus/raft/index.html","hash":"2533064e5333007b39b4c872bc14baa0df503bcc","modified":1589872013717},{"_id":"public/2019/12/31/blog/fabric/动态配置Raft节点/index.html","hash":"fe1f9e849e4b5d4f333d653ad71317bcaa12a665","modified":1589872013717},{"_id":"public/2019/12/28/blog/fabric/TLS_SDK调用/index.html","hash":"e270de7266a7f4afea857ea32662bad16ec9488d","modified":1589872013717},{"_id":"public/2019/12/27/blog/fabric/链码作为外部服务/index.html","hash":"572d64be8b61e18f83b7a4e9c9f9ea93c3393a2b","modified":1589872013717},{"_id":"public/2019/12/26/blog/couchDB/CouchDB基本操作/index.html","hash":"4f85c1c9510fd6b0f1e89948682bbec7852ecb9f","modified":1589872013717},{"_id":"public/2019/12/24/blog/fabric/外部链码构建和运行/index.html","hash":"64587f21345a4eb73373d58bf3255ed021bbddbe","modified":1589872013717},{"_id":"public/2019/12/24/blog/fabric/使用硬件安全模块/index.html","hash":"f4dc90855312fee9b5bae69c6de466b99362e515","modified":1589872013717},{"_id":"public/2019/12/24/blog/couchDB/CouchDB学习-API/index.html","hash":"2db8c81b93c18f92e79741efedab729d8ba32ec4","modified":1589872013717},{"_id":"public/2019/12/23/blog/consensus/paxos/index.html","hash":"1ace78a26ffe3f394f8bbd91713046f8e3989b9c","modified":1589872013717},{"_id":"public/2019/12/22/blog/linux/Linux命令-grep_sed_awk/index.html","hash":"45a4eb282d1097f8b10501c15f100deffc1eb072","modified":1589872013717},{"_id":"public/2019/12/22/blog/linux/正则表达式/index.html","hash":"2412e4ed1b12b8ea9204fbb2afdad129cc27cfc5","modified":1589872013717},{"_id":"public/2019/12/22/blog/couchDB/CouchDB学习-维护/index.html","hash":"058aceaec18cba06f2dcfd8595d1c82bbf90dd93","modified":1589872013717},{"_id":"public/2019/12/21/blog/couchDB/CouchDB学习-集群管理/index.html","hash":"a338f87cb55c0803f7b1e6d3f6db9ab45af7d1e3","modified":1589872013717},{"_id":"public/2019/12/21/blog/couchDB/CouchDB学习-介绍/index.html","hash":"b9122634f3d8b856143a293f2d9364a32daf737d","modified":1589872013717},{"_id":"public/2019/12/19/blog/fabric/Fabric1.4源码解析之链码容器启动过程/index.html","hash":"6928b60576d011489cab994c4092f44785c14938","modified":1589872013717},{"_id":"public/2019/12/19/blog/other/CURL学习二/index.html","hash":"a977409c9d5dcc7532f20ac10aae2d762aa8baec","modified":1589872013717},{"_id":"public/2019/12/18/blog/ipfs/IPFS学习-哈希/index.html","hash":"aae9682d6e9d20625ea393d890ed4d47e5509f73","modified":1589872013717},{"_id":"public/2019/12/18/blog/ipfs/IPFS学习-DNS链接/index.html","hash":"53270b6b608c45ad008d8c8175f20b1b1e5cf5f0","modified":1589872013717},{"_id":"public/2019/12/18/blog/ipfs/IPFS学习-内容标识符CID/index.html","hash":"4c11cd161f5128806817f7e600482bd79d3e92f1","modified":1589872013717},{"_id":"public/2019/12/18/blog/ipfs/IPFS学习-分布式哈希表DHT/index.html","hash":"ab285f8b530c7a0e8ae68c4ef6cdb2a6114300f5","modified":1589872013717},{"_id":"public/2019/12/17/blog/other/CURL命令学习一/index.html","hash":"a5af07626e832a069eaa3bb368dca116ba243f3a","modified":1589872013717},{"_id":"public/2019/12/08/blog/fabric/Hyperledger_Fabric_CA/index.html","hash":"5ba96a96590f5b3b9ca9c9eed3b9133b38b2fa0b","modified":1589872013717},{"_id":"public/2019/12/08/blog/fabric/Hyperledger_Fabric动态添加组织到网络中/index.html","hash":"879a7a4a855c05338edd66c18a1f629da932eb30","modified":1589872013717},{"_id":"public/2019/12/08/blog/fabric/Hyperledger_Fabric手动生成CA证书搭建Fabric网络/index.html","hash":"c9cf997034691096e0632b92205a493aee8759ad","modified":1589872013717},{"_id":"public/2019/12/04/blog/fabric/私有数据/index.html","hash":"65fbd871a34353193f117ca7c322d3f29cbcca2b","modified":1589872013717},{"_id":"public/2019/11/27/blog/fabric/链码测试/index.html","hash":"8abbfb1ed65208e048cc6eedebf6dc56d34e1a55","modified":1589872013717},{"_id":"public/2019/11/25/blog/etcd/基于角色的访问控制/index.html","hash":"4016ec035618c8a739f0b3dd6fa0320f46b2d1dd","modified":1589872013717},{"_id":"public/2019/11/25/blog/etcd/TLS/index.html","hash":"d9837af9757372f160f9cfd5bd95167c7a5e36a2","modified":1589872013717},{"_id":"public/2019/11/24/blog/etcd/词汇表/index.html","hash":"070dfbf8b81df340ff7b078f54ba304fe1c3f10a","modified":1589872013717},{"_id":"public/2019/11/24/blog/etcd/在容器内运行etcd集群/index.html","hash":"444adad14940ec2b531d0f7d088ebf21176a7b41","modified":1589872013717},{"_id":"public/2019/11/24/blog/couchDB/CouchDB学习一/index.html","hash":"c57abacd6cca5f306aebc1c719077721e7c2613d","modified":1589872013717},{"_id":"public/2019/11/24/blog/etcd/etcd网关/index.html","hash":"cf203c5c07a023aa735b2bebf2362899c9577321","modified":1589872013717},{"_id":"public/2019/11/24/blog/etcd/gRPC代理/index.html","hash":"9f19a93733978cc02c0da04e3c3e55db18abde97","modified":1589872013717},{"_id":"public/2019/11/24/blog/etcd/ETCD配置参数/index.html","hash":"7ef087317194e9211ef527a606f4a12a4f244b1e","modified":1589872013717},{"_id":"public/2019/11/23/blog/fabric/深入解析Fabric搭建的全过程/index.html","hash":"e924f7c31db43b7729b99efd0db5ddaefa5caaaa","modified":1589872013717},{"_id":"public/2019/11/23/blog/fabric/Fabric环境搭建/index.html","hash":"29cc8ed4fc95614e29e6c54a976ef783c47d38e7","modified":1589872013717},{"_id":"public/2019/11/23/blog/fabric/Fabric1.4多机部署/index.html","hash":"2c8a09619730fa3d622fcf22ac243c321ee70b3c","modified":1589872013717},{"_id":"public/2019/11/23/blog/etcd/运行时重新配置/index.html","hash":"d0034300b5a58f36de58201fad032e731a7eb81f","modified":1589872013717},{"_id":"public/2019/11/23/blog/etcd/HTTP_JSON_API通过gRPC网关/index.html","hash":"f20b11bd8d2b8bee24c5a94d69a58e57441a7bad","modified":1589872013717},{"_id":"public/2019/11/23/blog/etcd/gRPC命名与发现/index.html","hash":"6fe19f7ce3abcb0627ccacb0f0f22f5f79af0fa3","modified":1589872013717},{"_id":"public/2019/11/23/blog/etcd/运行时重新配置设计/index.html","hash":"e32f7083b27f5db79ebeed148f37c0833ebd39a1","modified":1589872013717},{"_id":"public/2019/11/23/blog/etcd/与etcd进行交互/index.html","hash":"5e9f08f44cb2ccc2b59141310bd6d27fcc23d662","modified":1589872013717},{"_id":"public/2019/11/23/blog/etcd/客户端v3/index.html","hash":"fa2bb6f78c6415ea7047fa19800cb97ccec9f8c6","modified":1589872013717},{"_id":"public/2019/11/23/blog/etcd/多机集群/index.html","hash":"c797a0708b36ed7aa394d2e17aac4005f9407fec","modified":1589872013717},{"_id":"public/2019/11/23/blog/etcd/单机集群/index.html","hash":"362a1133d66e53136eafe7a36837bb989c012e20","modified":1589872013717},{"_id":"public/2019/11/23/blog/etcd/文档/index.html","hash":"fe7606bffcd9cd5039d6b1daca8f18326e5a84cf","modified":1589872013717},{"_id":"public/index.html","hash":"4b04216a0bb16c8b0e46a4799cd88028373ad86d","modified":1589872013717},{"_id":"public/tags/2/index.html","hash":"4a90f2eb68b176cf967fa11799cb53e8e7798f75","modified":1589872013717},{"_id":"public/tags/3/index.html","hash":"9f9500cee28fbd745f0362ce671358c6a417e803","modified":1589872013717},{"_id":"public/tags/4/index.html","hash":"bbf3d7d7de5330c523e78d4dbcafa3facbf2f0c4","modified":1589872013717},{"_id":"public/tags/5/index.html","hash":"3c3012ff8bc791d5839b460f15864061e4de5d17","modified":1589872013717},{"_id":"public/tags/6/index.html","hash":"668128790e276b88bcf8e84ab7c9b86a36973ccd","modified":1589872013717},{"_id":"public/CNAME","hash":"0af0c5178806ec375cfa2b4f16d0409b2ce7ccc3","modified":1589872013717},{"_id":"public/img/icon.jpg","hash":"2010cb1c865036d445877d43bcb8aafe4cc16e1b","modified":1589872013717},{"_id":"public/img/wechat.png","hash":"a612ba6ac7a62ea4ced229b7d11006e675ca8aac","modified":1589872013717},{"_id":"public/img/blog/bloom/2.png","hash":"bc8d73648bc07117b5d71f4e328e3c95aa031e32","modified":1589872013717},{"_id":"public/img/blog/bloom/1.png","hash":"f05c34f141c27e2625ca7863bfaced2bacbc31ca","modified":1589872013717},{"_id":"public/img/blog/bloom/6.png","hash":"18c67938b37cfca0e4b3d3be1aa68321204451d8","modified":1589872013717},{"_id":"public/img/blog/bloom/3.png","hash":"65438b25bdf362dff41c205cd5fc1a28773e7a80","modified":1589872013717},{"_id":"public/img/blog/bloom/4.png","hash":"6067ad2fc30860f9706280343a862b318ebafaf2","modified":1589872013717},{"_id":"public/img/blog/raft/11.png","hash":"f66093b0a61374955e72349cf47eb56d0f9f6b80","modified":1589872013717},{"_id":"public/img/blog/bloom/5.png","hash":"45d072db173ad6cfa7300b614c1479b076972898","modified":1589872013717},{"_id":"public/img/blog/raft/10.png","hash":"2ada3e13ad9c0e3f1773ad2a78fd1fcf1ce70a0a","modified":1589872013717},{"_id":"public/img/blog/raft/110.png","hash":"0c80df8bdc04fa0f6cfdea28de0ca92dc4bdbfbc","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.ttf","hash":"5eb58f4263f87c543388bf66dec7d1f0b7c5b32c","modified":1589872013717},{"_id":"public/img/blog/raft/5.png","hash":"3693d2d84813a8aa94c647ab9a1ba1b63b11312a","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.eot","hash":"2e97930b520222ec3c2e4188ce07cc1904beba48","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"8b356dc021032d9380af47f7608a6b62a9b6f363","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff","hash":"38e6bd17442bb34e0e13a2c9bcbc5299f68be173","modified":1589872013717},{"_id":"public/img/blog/raft/9.png","hash":"df22e4f6cbd26397e7775241a2433f4b8599899e","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Thin.eot","hash":"0790a51a848dbe7292c98f9d0459218bf1a8ffdd","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Thin.woff2","hash":"2449e3dac5ddb7c3da8bb07450493b62d052758c","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Thin.woff","hash":"fbc3e71d456c96667d8082ab910e3946ef89240b","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1589872013717},{"_id":"public/img/blog/raft/12.png","hash":"22f625c2574d38ba3b1c05d0c750e925d88a72ce","modified":1589872013717},{"_id":"public/img/blog/pbft/1.png","hash":"141d159706c234f2ca908cc534890649555aafde","modified":1589872013717},{"_id":"public/img/blog/raft/1.png","hash":"62c7588b88842c671956e359886e2ba657e72196","modified":1589872013717},{"_id":"public/img/blog/raft/7.png","hash":"42a698a1a32447447eeddfd24c7a03ee7c2bb88b","modified":1589872013717},{"_id":"public/img/blog/raft/4.png","hash":"5c38b1cc152460d9bb7344b792aabe15b4b0774f","modified":1589872013717},{"_id":"public/img/blog/raft/6.png","hash":"c328b267fc513b4cb202c9c2d8192f3deacfdb89","modified":1589872013717},{"_id":"public/img/blog/raft/8.png","hash":"101b22dd693d62804032b355e66494c0859480d6","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"9465c5894ca2f93655fa5767b820b762aff6b518","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff","hash":"2417fe03c7330a5160f070d6ab747a2bc4bbd41b","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"d378644ff0f7549fa6f217a08dfd2566a770638e","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff","hash":"5d5d1448b199c38f1b39a49b1e9b3f1381a26cad","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.eot","hash":"453e71a65f2958480b74fdb75a53d41068699dbf","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.ttf","hash":"3fbe9822118e91350912f51f3080ce4aa9b3ec38","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1589872013717},{"_id":"public/lib/mdbootstrap/font/roboto/Roboto-Thin.ttf","hash":"173ed64528b4d010a76d8d38deb1d7e7eed58eda","modified":1589872013717},{"_id":"public/css/main.css","hash":"61ab3e5ccd01c29c0ddc2d5dfdc40fee6b31f6da","modified":1589872013717},{"_id":"public/js/main.js","hash":"21ebed6b8c52fe92a7677ebda729f195bf97c9c6","modified":1589872013717},{"_id":"public/js/local-search.js","hash":"d0fd6bcdd70bed752e9ef4152a36e7bcabcbe11a","modified":1589872013717},{"_id":"public/js/lazyload.js","hash":"e0c37c12adae99ace86ef514f0c3f49cdc4eea79","modified":1589872013717},{"_id":"public/lib/fancybox/jquery.fancybox.min.css","hash":"bfa13de0fab7defa3a25c3197e90b600c4897c34","modified":1589872013717},{"_id":"public/lib/anchor/anchor.min.js","hash":"0996588202bd062dad6f592615cb4791e1f8be91","modified":1589872013717},{"_id":"public/lib/github-markdown/github-markdown.min.css","hash":"23ec6f05c5b69aa8ffb12c59c9bf1325ee5a26d1","modified":1589872013717},{"_id":"public/js/post.js","hash":"3c4894c77b4f28d61ab017dd61548054ed781dbe","modified":1589872013717},{"_id":"public/lib/prettify/tomorrow-night-eighties.min.css","hash":"a5f2102fc148359a92435b170f3bfb25e1221837","modified":1589872013717},{"_id":"public/lib/prettify/prettify.min.js","hash":"03044b62cdb1c300537c14dcf424333fcf4c9110","modified":1589872013717},{"_id":"public/lib/prettify/github-v2.min.css","hash":"da1b8e6d4df1f044d12f461880e677d65dbbf2d3","modified":1589872013717},{"_id":"public/lib/prettify/tomorrow.min.css","hash":"ea61879c64ca73a5ea233b1315faf7f2fdfebca9","modified":1589872013717},{"_id":"public/lib/prettify/tomorrow-night.min.css","hash":"535256d676d247d3282e9a8ae2777c6f7df4fdc6","modified":1589872013717},{"_id":"public/lib/tocbot/tocbot.min.js","hash":"bae97e8a24a05a99335f8e725641c8ca9c50502a","modified":1589872013717},{"_id":"public/lib/typed/typed.min.js","hash":"38b792348023d55caabd7f888ae477ee143e6abe","modified":1589872013717},{"_id":"public/lib/smooth-scroll/smooth-scroll.min.js","hash":"ee5dea9ea4c5edb110f30a277e5fca7993f948b5","modified":1589872013717},{"_id":"public/lib/katex/katex.min.css","hash":"f29c27f5b804ec30acdebb8ec0488fc4a9e1538a","modified":1589872013717},{"_id":"public/lib/popper/popper.min.js","hash":"27d61a7e89d12ce0744f34fa804230eeb13ff128","modified":1589872013717},{"_id":"public/lib/fancybox/jquery.fancybox.min.js","hash":"211f4852cbb5662e11b9688bef8415ca328a88ef","modified":1589872013717},{"_id":"public/lib/jquery/jquery.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1589872013717},{"_id":"public/lib/bootstrap/js/bootstrap.min.js","hash":"8260ff4bf54350c075bc10d18e349d158e1a4af1","modified":1589872013717},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"6f4095f66e56d39ef0adefbe85a1dcfc13bd133b","modified":1589872013717},{"_id":"public/lib/bootstrap/css/bootstrap.min.css","hash":"3665a5389b7a20dd3b2fe9cb0ed3d80bec1cf2a3","modified":1589872013717},{"_id":"public/lib/mdbootstrap/css/mdb.min.css","hash":"62818e7755b098a1c3b503425356570a2c7474d9","modified":1589872013717},{"_id":"public/lib/mdbootstrap/js/mdb.min.js","hash":"fef8d611bbc14ad31ca9ec9e2990bfde4d873bb1","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.svg","hash":"3c4ae85dc2835e2ac7d10e1acb6f12ce8ab3d8cb","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.eot","hash":"2b0ebea58a0bc895400dffe8c5e434c8b12338e3","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.ttf","hash":"7b280debee5800806092e35a6bc2c6fd9c51cf63","modified":1589872013717},{"_id":"public/img/blog/arth.png","hash":"6b1d95ecf580c284778854e37dc852da589f6113","modified":1589872013717},{"_id":"public/img/blog/zfb.png","hash":"2256222c00de963e56dd29e1cb5e5e7da49a3d2b","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.svg","hash":"a8e867908941916f4ccbecf0fbcbedfb9e8e6927","modified":1589872013717},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.svg","hash":"432a16b3c6ab238a4735891c6bb8f80321101992","modified":1589872013717},{"_id":"public/img/4.jpg","hash":"58bd0d4a2a92d39752081e56b7dc32c919f8c25d","modified":1589872013717},{"_id":"public/img/5.jpg","hash":"cc9918d0732ef02c227353427923a60bfb504a31","modified":1589872013717},{"_id":"public/img/0.jpg","hash":"088da12f57f324f87cf688e99b780705a0e54a93","modified":1589872013717},{"_id":"public/img/3.jpg","hash":"ecae551dd8913897ad7bd042892ae98e9a49860b","modified":1589872013717},{"_id":"public/img/2.jpg","hash":"5346f4941b26440f457d8fb2cd95789c15f3003e","modified":1589872013717},{"_id":"public/img/404.jpg","hash":"906b2caf9a4b2ba3c86820acac4cae1426cf6940","modified":1589872013717},{"_id":"public/img/1.jpg","hash":"203c61bfed8e1d7f8b02b9349248fe1548b896df","modified":1589872013717}],"Category":[{"name":"algorithm","_id":"ckadkqyfa0007k0vqeh4n49n7"},{"name":"CouchDb应用","_id":"ckadkqygk000uk0vq517yfwnk"},{"name":"CouchDb学习","_id":"ckadkqygx0011k0vq78zi3t9p"},{"name":"etcd文档翻译","_id":"ckadkqyhe001nk0vqauecaub5"},{"name":"fabric应用","_id":"ckadkqyix0035k0vqaw5dcznr"},{"name":"fabric-ca应用","_id":"ckadkqyjo003sk0vq0p359xfp"},{"name":"fabric源码解析","_id":"ckadkqyjt0040k0vqfxvl8k7g"},{"name":"IPFS学习","_id":"ckadkqyk5004kk0vq54jr5mf3"},{"name":"curl","_id":"ckadkqykt0056k0vq72t22qex"},{"name":"Linux命令","_id":"ckadkqyl1005ek0vqcubm4tm4"},{"name":"other","_id":"ckadkqyl4005ok0vq6w5o3xte"}],"Data":[],"Page":[],"Post":[{"title":"搭建你的第一个区块链网络(一)","date":"2020-05-19T07:00:28.000Z","_content":"\n写一个系列文章，由简入深搭建一个区块链网络，也是从零开始开发一个开源项目。\n不再介绍区块链的基础知识了，所以希望读者提前了解区块链的基础知识，项目是使用SpringBoot(暂时用不到各项配置，为了以后方便)+JAVA开发，所以也需要读者了解JAVA语言。本文为第一篇。\n\n## 区块\n\n### 区块属性定义\n第一步首先是区块信息的定义，暂时不考虑那么复杂，这里只定义一些最基础的属性:\n* **区块号：** 就是区块的序号。 \n* **当前区块哈希值：** 保证区块唯一，同时后一个区块链通过保留这一属性链接该区块。\n* **前一区块哈希值：** 用于链接上一个区块。\n* **时间戳：** 记录该区块产出时间。\n* **数据：** 区块中存储的数据，为了简单化，这里使用字符串代替。\n\n目前暂时只定义这些属性，后面开发如果需要其他属性再进行迭代。接下来是构造方法了:\n\n### 构造方法定义\n\n构建一个新的区块需要传入被构建区块的区块号，区块数据以及前一个区块的哈希值。\n\n```\n#Block.java\n\n@Getter  # lombok工具包，加上这两个注释就不用写get(),set()方法了\n@Setter   #工具包在下面\npublic class Block {\n    // 区块号\n    public int blkNum;\n    // 当前区块哈希值\n    public String curBlockHash;\n    // 前一个区块的哈希值\n    public String prevBlockHash;\n    // 生成当前区块的时间，用时间戳表示\n    public String timeStamp;\n    // 当前区块中的Transaction,使用字符串简单代替\n    public String data;\n    public Block(int blkNum,String data, String prevBlockHash){\n        this.blkNum = blkNum;\n        this.data = data;\n        this.prevBlockHash = prevBlockHash;\n        this.timeStamp = Util.getTimeStamp();\n    }\n    @Override\n    public String toString(){\n        return JSONObject.toJSONString(this);\n    }\n}\n```\n\n其中涉及到获取时间戳的``getTimeStamp()``方法以及计算哈希值的``getSHA256()``方法。\n\n```\n#Util.java\npublic final class Util{\n    public static String getSHA256(String data) {\n        byte[] b = {};\n        try{\n            MessageDigest md = MessageDigest.getInstance(\"SHA-256\");\n            b = md.digest(data.getBytes());\n        }catch(NoSuchAlgorithmException e){\n        }\n        return Hex.encodeHexString(b);\n    }\n    public static String getTimeStamp(){\n        SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");//设置日期格式\n        String date = df.format(new java.util.Date());\n        return date;\n    }\n}\n```\n\n这里使用到了几个工具包，可以在Maven的``pom.xml``文件``<dependencies>``标签中添加如下字段:\n\n```\n#pom.xml\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n            <optional>true</optional>\n        </dependency>\n         <dependency>\n            <groupId>commons-codec</groupId>\n            <artifactId>commons-codec</artifactId>\n            <version>1.12</version>\n        </dependency>\n        <!-- https://mvnrepository.com/artifact/com.alibaba/fastjson -->\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>fastjson</artifactId>\n            <version>1.2.62</version>\n        </dependency>\n```\n\n### 区块链\n我们需要一个区块链实例，用于记录所产出的区块。定义的区块链属性如下:\n\n* **区块链实例:** 需要静态的(或者使用单例模式)，保证只存在一个区块链实例。\n* **区块集合：** 定义一个集合用于保存产出的区块。\n\n```\n#Blockchain.java\n    private static Blockchain BC;\n    public ArrayList<Block> block;\n    private Blockchain(){}\n    public static Blockchain getInstance(){\n        if(BC==null){\n            synchronized(Blockchain.class){\n                if(BC==null){\n                    BC = new Blockchain();\n                    BC.block = new ArrayList<Block>();\n                }\n            }\n        }\n        return BC;\n    }\n```\n\n### 创建第一个区块\n\n目前没有其他属性需要，接下来是定义方法。区块链实例定义完成，接下来可以创建区块链中的区块了，由于创世区块与后续区块稍微不同，所以定义一个构建创世区块的方法:\n\n```\n    public Block CrtGenesisBlock(){\n        Block block = new Block(1,\"Genesis Block\",\"00000000000000000\");\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        block.setCurBlockHash(hash);\n        this.block.add(block);\n        return this.block.get(0);\n    }\n```\n\n由于创世区块是第一个区块，因此不存在前一个区块哈希值，直接定义为字符串\"00000000000000000\"。\n\n### 添加区块\n\n接下来是第二个方法，创建创世区块的后续区块方法:\n\n```\n    #传入的参数为需要区块中存储的数据\n    public Block addBlock(String data){\n         #获取区块集合的大小，即获取当前已经产出几个区块\n        int num = this.block.size()；\n        Block block = new Block(\n             # 区块号为当前区块集合大小+1\n            num+1,data, this.block.get(num-1).curBlockHash); \n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash());\n        block.setCurBlockHash(hash);\n        #将创建的区块添加到区块链中\n        this.block.add(block);\n        return this.block.get(num);\n}\n```\n\nOK,一个简单的区块链已经完成，测试一下:\n\n```\n#Test.java\npublic class Test {\n    public static void main(String[] args){\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(\"Block 2\").toString());\n    }\n}\n```\n\n看起来没什么问题:\n\n```\n{\"blkNum\":1,\"curBlockHash\":\"db27dd5c1e51197f6e9580613d9dbd5198a053b8c92da6560538579834e83159\",\"data\":\"Genesis Block\",\"prevBlockHash\":\"00000000000000000\",\"timeStamp\":\"2020-05-16 17:09:10\"}\n{\"blkNum\":2,\"curBlockHash\":\"bc6a75cf858241503a64ebdc5cf21ddcb187e6759c016b906f288cdac26ccae2\",\"data\":\"Block 2\",\"prevBlockHash\":\"db27dd5c1e51197f6e9580613d9dbd5198a053b8c92da6560538579834e83159\",\"timeStamp\":\"2020-05-16 17:09:10\"}\n```\n\n后一篇文章: [搭建你的第一个区块链网络(二)](https://www.cnblogs.com/cbkj-xd/p/12904660.html)\n\n#### Github仓库地址在这里，随时保持更新中.....\nGithub地址：[Jchain](https://github.com/newonexd/Jchain)","source":"_posts/blog/blockchain/Jchain1.md","raw":"---\ntitle: 搭建你的第一个区块链网络(一)\ndate: 2020-05-19 15:00:28\ntags: blockchain\n---\n\n写一个系列文章，由简入深搭建一个区块链网络，也是从零开始开发一个开源项目。\n不再介绍区块链的基础知识了，所以希望读者提前了解区块链的基础知识，项目是使用SpringBoot(暂时用不到各项配置，为了以后方便)+JAVA开发，所以也需要读者了解JAVA语言。本文为第一篇。\n\n## 区块\n\n### 区块属性定义\n第一步首先是区块信息的定义，暂时不考虑那么复杂，这里只定义一些最基础的属性:\n* **区块号：** 就是区块的序号。 \n* **当前区块哈希值：** 保证区块唯一，同时后一个区块链通过保留这一属性链接该区块。\n* **前一区块哈希值：** 用于链接上一个区块。\n* **时间戳：** 记录该区块产出时间。\n* **数据：** 区块中存储的数据，为了简单化，这里使用字符串代替。\n\n目前暂时只定义这些属性，后面开发如果需要其他属性再进行迭代。接下来是构造方法了:\n\n### 构造方法定义\n\n构建一个新的区块需要传入被构建区块的区块号，区块数据以及前一个区块的哈希值。\n\n```\n#Block.java\n\n@Getter  # lombok工具包，加上这两个注释就不用写get(),set()方法了\n@Setter   #工具包在下面\npublic class Block {\n    // 区块号\n    public int blkNum;\n    // 当前区块哈希值\n    public String curBlockHash;\n    // 前一个区块的哈希值\n    public String prevBlockHash;\n    // 生成当前区块的时间，用时间戳表示\n    public String timeStamp;\n    // 当前区块中的Transaction,使用字符串简单代替\n    public String data;\n    public Block(int blkNum,String data, String prevBlockHash){\n        this.blkNum = blkNum;\n        this.data = data;\n        this.prevBlockHash = prevBlockHash;\n        this.timeStamp = Util.getTimeStamp();\n    }\n    @Override\n    public String toString(){\n        return JSONObject.toJSONString(this);\n    }\n}\n```\n\n其中涉及到获取时间戳的``getTimeStamp()``方法以及计算哈希值的``getSHA256()``方法。\n\n```\n#Util.java\npublic final class Util{\n    public static String getSHA256(String data) {\n        byte[] b = {};\n        try{\n            MessageDigest md = MessageDigest.getInstance(\"SHA-256\");\n            b = md.digest(data.getBytes());\n        }catch(NoSuchAlgorithmException e){\n        }\n        return Hex.encodeHexString(b);\n    }\n    public static String getTimeStamp(){\n        SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");//设置日期格式\n        String date = df.format(new java.util.Date());\n        return date;\n    }\n}\n```\n\n这里使用到了几个工具包，可以在Maven的``pom.xml``文件``<dependencies>``标签中添加如下字段:\n\n```\n#pom.xml\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n            <optional>true</optional>\n        </dependency>\n         <dependency>\n            <groupId>commons-codec</groupId>\n            <artifactId>commons-codec</artifactId>\n            <version>1.12</version>\n        </dependency>\n        <!-- https://mvnrepository.com/artifact/com.alibaba/fastjson -->\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>fastjson</artifactId>\n            <version>1.2.62</version>\n        </dependency>\n```\n\n### 区块链\n我们需要一个区块链实例，用于记录所产出的区块。定义的区块链属性如下:\n\n* **区块链实例:** 需要静态的(或者使用单例模式)，保证只存在一个区块链实例。\n* **区块集合：** 定义一个集合用于保存产出的区块。\n\n```\n#Blockchain.java\n    private static Blockchain BC;\n    public ArrayList<Block> block;\n    private Blockchain(){}\n    public static Blockchain getInstance(){\n        if(BC==null){\n            synchronized(Blockchain.class){\n                if(BC==null){\n                    BC = new Blockchain();\n                    BC.block = new ArrayList<Block>();\n                }\n            }\n        }\n        return BC;\n    }\n```\n\n### 创建第一个区块\n\n目前没有其他属性需要，接下来是定义方法。区块链实例定义完成，接下来可以创建区块链中的区块了，由于创世区块与后续区块稍微不同，所以定义一个构建创世区块的方法:\n\n```\n    public Block CrtGenesisBlock(){\n        Block block = new Block(1,\"Genesis Block\",\"00000000000000000\");\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        block.setCurBlockHash(hash);\n        this.block.add(block);\n        return this.block.get(0);\n    }\n```\n\n由于创世区块是第一个区块，因此不存在前一个区块哈希值，直接定义为字符串\"00000000000000000\"。\n\n### 添加区块\n\n接下来是第二个方法，创建创世区块的后续区块方法:\n\n```\n    #传入的参数为需要区块中存储的数据\n    public Block addBlock(String data){\n         #获取区块集合的大小，即获取当前已经产出几个区块\n        int num = this.block.size()；\n        Block block = new Block(\n             # 区块号为当前区块集合大小+1\n            num+1,data, this.block.get(num-1).curBlockHash); \n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash());\n        block.setCurBlockHash(hash);\n        #将创建的区块添加到区块链中\n        this.block.add(block);\n        return this.block.get(num);\n}\n```\n\nOK,一个简单的区块链已经完成，测试一下:\n\n```\n#Test.java\npublic class Test {\n    public static void main(String[] args){\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(\"Block 2\").toString());\n    }\n}\n```\n\n看起来没什么问题:\n\n```\n{\"blkNum\":1,\"curBlockHash\":\"db27dd5c1e51197f6e9580613d9dbd5198a053b8c92da6560538579834e83159\",\"data\":\"Genesis Block\",\"prevBlockHash\":\"00000000000000000\",\"timeStamp\":\"2020-05-16 17:09:10\"}\n{\"blkNum\":2,\"curBlockHash\":\"bc6a75cf858241503a64ebdc5cf21ddcb187e6759c016b906f288cdac26ccae2\",\"data\":\"Block 2\",\"prevBlockHash\":\"db27dd5c1e51197f6e9580613d9dbd5198a053b8c92da6560538579834e83159\",\"timeStamp\":\"2020-05-16 17:09:10\"}\n```\n\n后一篇文章: [搭建你的第一个区块链网络(二)](https://www.cnblogs.com/cbkj-xd/p/12904660.html)\n\n#### Github仓库地址在这里，随时保持更新中.....\nGithub地址：[Jchain](https://github.com/newonexd/Jchain)","slug":"blog/blockchain/Jchain1","published":1,"updated":"2020-05-19T07:06:36.754Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqye80000k0vqfhxyeyrb","content":"<p>写一个系列文章，由简入深搭建一个区块链网络，也是从零开始开发一个开源项目。<br>不再介绍区块链的基础知识了，所以希望读者提前了解区块链的基础知识，项目是使用SpringBoot(暂时用不到各项配置，为了以后方便)+JAVA开发，所以也需要读者了解JAVA语言。本文为第一篇。</p>\n<h2 id=\"区块\"><a href=\"#区块\" class=\"headerlink\" title=\"区块\"></a>区块</h2><h3 id=\"区块属性定义\"><a href=\"#区块属性定义\" class=\"headerlink\" title=\"区块属性定义\"></a>区块属性定义</h3><p>第一步首先是区块信息的定义，暂时不考虑那么复杂，这里只定义一些最基础的属性:</p>\n<ul>\n<li><strong>区块号：</strong> 就是区块的序号。 </li>\n<li><strong>当前区块哈希值：</strong> 保证区块唯一，同时后一个区块链通过保留这一属性链接该区块。</li>\n<li><strong>前一区块哈希值：</strong> 用于链接上一个区块。</li>\n<li><strong>时间戳：</strong> 记录该区块产出时间。</li>\n<li><strong>数据：</strong> 区块中存储的数据，为了简单化，这里使用字符串代替。</li>\n</ul>\n<p>目前暂时只定义这些属性，后面开发如果需要其他属性再进行迭代。接下来是构造方法了:</p>\n<h3 id=\"构造方法定义\"><a href=\"#构造方法定义\" class=\"headerlink\" title=\"构造方法定义\"></a>构造方法定义</h3><p>构建一个新的区块需要传入被构建区块的区块号，区块数据以及前一个区块的哈希值。</p>\n<pre><code>#Block.java\n\n@Getter  # lombok工具包，加上这两个注释就不用写get(),set()方法了\n@Setter   #工具包在下面\npublic class Block {\n    // 区块号\n    public int blkNum;\n    // 当前区块哈希值\n    public String curBlockHash;\n    // 前一个区块的哈希值\n    public String prevBlockHash;\n    // 生成当前区块的时间，用时间戳表示\n    public String timeStamp;\n    // 当前区块中的Transaction,使用字符串简单代替\n    public String data;\n    public Block(int blkNum,String data, String prevBlockHash){\n        this.blkNum = blkNum;\n        this.data = data;\n        this.prevBlockHash = prevBlockHash;\n        this.timeStamp = Util.getTimeStamp();\n    }\n    @Override\n    public String toString(){\n        return JSONObject.toJSONString(this);\n    }\n}</code></pre><p>其中涉及到获取时间戳的<code>getTimeStamp()</code>方法以及计算哈希值的<code>getSHA256()</code>方法。</p>\n<pre><code>#Util.java\npublic final class Util{\n    public static String getSHA256(String data) {\n        byte[] b = {};\n        try{\n            MessageDigest md = MessageDigest.getInstance(&quot;SHA-256&quot;);\n            b = md.digest(data.getBytes());\n        }catch(NoSuchAlgorithmException e){\n        }\n        return Hex.encodeHexString(b);\n    }\n    public static String getTimeStamp(){\n        SimpleDateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);//设置日期格式\n        String date = df.format(new java.util.Date());\n        return date;\n    }\n}</code></pre><p>这里使用到了几个工具包，可以在Maven的<code>pom.xml</code>文件<code>&lt;dependencies&gt;</code>标签中添加如下字段:</p>\n<pre><code>#pom.xml\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\n            &lt;artifactId&gt;lombok&lt;/artifactId&gt;\n            &lt;optional&gt;true&lt;/optional&gt;\n        &lt;/dependency&gt;\n         &lt;dependency&gt;\n            &lt;groupId&gt;commons-codec&lt;/groupId&gt;\n            &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;\n            &lt;version&gt;1.12&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;\n            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;\n            &lt;version&gt;1.2.62&lt;/version&gt;\n        &lt;/dependency&gt;</code></pre><h3 id=\"区块链\"><a href=\"#区块链\" class=\"headerlink\" title=\"区块链\"></a>区块链</h3><p>我们需要一个区块链实例，用于记录所产出的区块。定义的区块链属性如下:</p>\n<ul>\n<li><strong>区块链实例:</strong> 需要静态的(或者使用单例模式)，保证只存在一个区块链实例。</li>\n<li><strong>区块集合：</strong> 定义一个集合用于保存产出的区块。</li>\n</ul>\n<pre><code>#Blockchain.java\n    private static Blockchain BC;\n    public ArrayList&lt;Block&gt; block;\n    private Blockchain(){}\n    public static Blockchain getInstance(){\n        if(BC==null){\n            synchronized(Blockchain.class){\n                if(BC==null){\n                    BC = new Blockchain();\n                    BC.block = new ArrayList&lt;Block&gt;();\n                }\n            }\n        }\n        return BC;\n    }</code></pre><h3 id=\"创建第一个区块\"><a href=\"#创建第一个区块\" class=\"headerlink\" title=\"创建第一个区块\"></a>创建第一个区块</h3><p>目前没有其他属性需要，接下来是定义方法。区块链实例定义完成，接下来可以创建区块链中的区块了，由于创世区块与后续区块稍微不同，所以定义一个构建创世区块的方法:</p>\n<pre><code>    public Block CrtGenesisBlock(){\n        Block block = new Block(1,&quot;Genesis Block&quot;,&quot;00000000000000000&quot;);\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        block.setCurBlockHash(hash);\n        this.block.add(block);\n        return this.block.get(0);\n    }</code></pre><p>由于创世区块是第一个区块，因此不存在前一个区块哈希值，直接定义为字符串”00000000000000000”。</p>\n<h3 id=\"添加区块\"><a href=\"#添加区块\" class=\"headerlink\" title=\"添加区块\"></a>添加区块</h3><p>接下来是第二个方法，创建创世区块的后续区块方法:</p>\n<pre><code>    #传入的参数为需要区块中存储的数据\n    public Block addBlock(String data){\n         #获取区块集合的大小，即获取当前已经产出几个区块\n        int num = this.block.size()；\n        Block block = new Block(\n             # 区块号为当前区块集合大小+1\n            num+1,data, this.block.get(num-1).curBlockHash); \n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash());\n        block.setCurBlockHash(hash);\n        #将创建的区块添加到区块链中\n        this.block.add(block);\n        return this.block.get(num);\n}</code></pre><p>OK,一个简单的区块链已经完成，测试一下:</p>\n<pre><code>#Test.java\npublic class Test {\n    public static void main(String[] args){\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(&quot;Block 2&quot;).toString());\n    }\n}</code></pre><p>看起来没什么问题:</p>\n<pre><code>{&quot;blkNum&quot;:1,&quot;curBlockHash&quot;:&quot;db27dd5c1e51197f6e9580613d9dbd5198a053b8c92da6560538579834e83159&quot;,&quot;data&quot;:&quot;Genesis Block&quot;,&quot;prevBlockHash&quot;:&quot;00000000000000000&quot;,&quot;timeStamp&quot;:&quot;2020-05-16 17:09:10&quot;}\n{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;bc6a75cf858241503a64ebdc5cf21ddcb187e6759c016b906f288cdac26ccae2&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;prevBlockHash&quot;:&quot;db27dd5c1e51197f6e9580613d9dbd5198a053b8c92da6560538579834e83159&quot;,&quot;timeStamp&quot;:&quot;2020-05-16 17:09:10&quot;}</code></pre><p>后一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12904660.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(二)</a></p>\n<h4 id=\"Github仓库地址在这里，随时保持更新中…\"><a href=\"#Github仓库地址在这里，随时保持更新中…\" class=\"headerlink\" title=\"Github仓库地址在这里，随时保持更新中…..\"></a>Github仓库地址在这里，随时保持更新中…..</h4><p>Github地址：<a href=\"https://github.com/newonexd/Jchain\" target=\"_blank\" rel=\"noopener\">Jchain</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>写一个系列文章，由简入深搭建一个区块链网络，也是从零开始开发一个开源项目。<br>不再介绍区块链的基础知识了，所以希望读者提前了解区块链的基础知识，项目是使用SpringBoot(暂时用不到各项配置，为了以后方便)+JAVA开发，所以也需要读者了解JAVA语言。本文为第一篇。</p>\n<h2 id=\"区块\"><a href=\"#区块\" class=\"headerlink\" title=\"区块\"></a>区块</h2><h3 id=\"区块属性定义\"><a href=\"#区块属性定义\" class=\"headerlink\" title=\"区块属性定义\"></a>区块属性定义</h3><p>第一步首先是区块信息的定义，暂时不考虑那么复杂，这里只定义一些最基础的属性:</p>\n<ul>\n<li><strong>区块号：</strong> 就是区块的序号。 </li>\n<li><strong>当前区块哈希值：</strong> 保证区块唯一，同时后一个区块链通过保留这一属性链接该区块。</li>\n<li><strong>前一区块哈希值：</strong> 用于链接上一个区块。</li>\n<li><strong>时间戳：</strong> 记录该区块产出时间。</li>\n<li><strong>数据：</strong> 区块中存储的数据，为了简单化，这里使用字符串代替。</li>\n</ul>\n<p>目前暂时只定义这些属性，后面开发如果需要其他属性再进行迭代。接下来是构造方法了:</p>\n<h3 id=\"构造方法定义\"><a href=\"#构造方法定义\" class=\"headerlink\" title=\"构造方法定义\"></a>构造方法定义</h3><p>构建一个新的区块需要传入被构建区块的区块号，区块数据以及前一个区块的哈希值。</p>\n<pre><code>#Block.java\n\n@Getter  # lombok工具包，加上这两个注释就不用写get(),set()方法了\n@Setter   #工具包在下面\npublic class Block {\n    // 区块号\n    public int blkNum;\n    // 当前区块哈希值\n    public String curBlockHash;\n    // 前一个区块的哈希值\n    public String prevBlockHash;\n    // 生成当前区块的时间，用时间戳表示\n    public String timeStamp;\n    // 当前区块中的Transaction,使用字符串简单代替\n    public String data;\n    public Block(int blkNum,String data, String prevBlockHash){\n        this.blkNum = blkNum;\n        this.data = data;\n        this.prevBlockHash = prevBlockHash;\n        this.timeStamp = Util.getTimeStamp();\n    }\n    @Override\n    public String toString(){\n        return JSONObject.toJSONString(this);\n    }\n}</code></pre><p>其中涉及到获取时间戳的<code>getTimeStamp()</code>方法以及计算哈希值的<code>getSHA256()</code>方法。</p>\n<pre><code>#Util.java\npublic final class Util{\n    public static String getSHA256(String data) {\n        byte[] b = {};\n        try{\n            MessageDigest md = MessageDigest.getInstance(&quot;SHA-256&quot;);\n            b = md.digest(data.getBytes());\n        }catch(NoSuchAlgorithmException e){\n        }\n        return Hex.encodeHexString(b);\n    }\n    public static String getTimeStamp(){\n        SimpleDateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);//设置日期格式\n        String date = df.format(new java.util.Date());\n        return date;\n    }\n}</code></pre><p>这里使用到了几个工具包，可以在Maven的<code>pom.xml</code>文件<code>&lt;dependencies&gt;</code>标签中添加如下字段:</p>\n<pre><code>#pom.xml\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\n            &lt;artifactId&gt;lombok&lt;/artifactId&gt;\n            &lt;optional&gt;true&lt;/optional&gt;\n        &lt;/dependency&gt;\n         &lt;dependency&gt;\n            &lt;groupId&gt;commons-codec&lt;/groupId&gt;\n            &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;\n            &lt;version&gt;1.12&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;\n            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;\n            &lt;version&gt;1.2.62&lt;/version&gt;\n        &lt;/dependency&gt;</code></pre><h3 id=\"区块链\"><a href=\"#区块链\" class=\"headerlink\" title=\"区块链\"></a>区块链</h3><p>我们需要一个区块链实例，用于记录所产出的区块。定义的区块链属性如下:</p>\n<ul>\n<li><strong>区块链实例:</strong> 需要静态的(或者使用单例模式)，保证只存在一个区块链实例。</li>\n<li><strong>区块集合：</strong> 定义一个集合用于保存产出的区块。</li>\n</ul>\n<pre><code>#Blockchain.java\n    private static Blockchain BC;\n    public ArrayList&lt;Block&gt; block;\n    private Blockchain(){}\n    public static Blockchain getInstance(){\n        if(BC==null){\n            synchronized(Blockchain.class){\n                if(BC==null){\n                    BC = new Blockchain();\n                    BC.block = new ArrayList&lt;Block&gt;();\n                }\n            }\n        }\n        return BC;\n    }</code></pre><h3 id=\"创建第一个区块\"><a href=\"#创建第一个区块\" class=\"headerlink\" title=\"创建第一个区块\"></a>创建第一个区块</h3><p>目前没有其他属性需要，接下来是定义方法。区块链实例定义完成，接下来可以创建区块链中的区块了，由于创世区块与后续区块稍微不同，所以定义一个构建创世区块的方法:</p>\n<pre><code>    public Block CrtGenesisBlock(){\n        Block block = new Block(1,&quot;Genesis Block&quot;,&quot;00000000000000000&quot;);\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        block.setCurBlockHash(hash);\n        this.block.add(block);\n        return this.block.get(0);\n    }</code></pre><p>由于创世区块是第一个区块，因此不存在前一个区块哈希值，直接定义为字符串”00000000000000000”。</p>\n<h3 id=\"添加区块\"><a href=\"#添加区块\" class=\"headerlink\" title=\"添加区块\"></a>添加区块</h3><p>接下来是第二个方法，创建创世区块的后续区块方法:</p>\n<pre><code>    #传入的参数为需要区块中存储的数据\n    public Block addBlock(String data){\n         #获取区块集合的大小，即获取当前已经产出几个区块\n        int num = this.block.size()；\n        Block block = new Block(\n             # 区块号为当前区块集合大小+1\n            num+1,data, this.block.get(num-1).curBlockHash); \n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash());\n        block.setCurBlockHash(hash);\n        #将创建的区块添加到区块链中\n        this.block.add(block);\n        return this.block.get(num);\n}</code></pre><p>OK,一个简单的区块链已经完成，测试一下:</p>\n<pre><code>#Test.java\npublic class Test {\n    public static void main(String[] args){\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(&quot;Block 2&quot;).toString());\n    }\n}</code></pre><p>看起来没什么问题:</p>\n<pre><code>{&quot;blkNum&quot;:1,&quot;curBlockHash&quot;:&quot;db27dd5c1e51197f6e9580613d9dbd5198a053b8c92da6560538579834e83159&quot;,&quot;data&quot;:&quot;Genesis Block&quot;,&quot;prevBlockHash&quot;:&quot;00000000000000000&quot;,&quot;timeStamp&quot;:&quot;2020-05-16 17:09:10&quot;}\n{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;bc6a75cf858241503a64ebdc5cf21ddcb187e6759c016b906f288cdac26ccae2&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;prevBlockHash&quot;:&quot;db27dd5c1e51197f6e9580613d9dbd5198a053b8c92da6560538579834e83159&quot;,&quot;timeStamp&quot;:&quot;2020-05-16 17:09:10&quot;}</code></pre><p>后一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12904660.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(二)</a></p>\n<h4 id=\"Github仓库地址在这里，随时保持更新中…\"><a href=\"#Github仓库地址在这里，随时保持更新中…\" class=\"headerlink\" title=\"Github仓库地址在这里，随时保持更新中…..\"></a>Github仓库地址在这里，随时保持更新中…..</h4><p>Github地址：<a href=\"https://github.com/newonexd/Jchain\" target=\"_blank\" rel=\"noopener\">Jchain</a></p>\n"},{"title":"搭建你的第一个区块链网络(二)","date":"2020-05-19T07:00:49.000Z","_content":"\n前一篇文章: [搭建你的第一个区块链网络(一)](https://www.cnblogs.com/cbkj-xd/p/12901312.html)\n\n\n## 共识与本地化\n### POW共识\n\n共识机制也是区块链系统中不可缺少的一部分，在比特币网络中，使用的是POW共识，概念相对比较简单，所以我们在该项目中使用POW共识机制(后期如果可以的话修改为可插拔的共识机制)。\n\n#### POW原理\nPOW原理是通过解决一个数学难题，其实就是通过计算一个哈希值，如果计算出来的哈希值的前缀有足够多个\"0\"，就说明成功解决了该数学难题。通常哈希值中\"0\"的个数越多难度越大。难度值是通过之前生成的区块所消耗的时间动态调整的。而生成哈希值的原数据实际上就是区块信息，另外再加一个``nonce``属性，用于调整难度值。\n在比特币中，平均每10分钟产出一个区块，如果新区块的产出只消耗了9分钟，那么难度值将会增加。如果算力不发生变化的话，下一次产出区块将会消耗更多的时间。同理，如果新区块的产出消耗了11分钟，那么难度值则会相应地降低。动态调整难度值维持区块产出时间平均为10分钟。实际上比特币中的POW更加复杂，难度值的调整是通过过去的2016个区块产出的时间与20160分钟进行比较的。\n在这里，不设置那么麻烦，难度值不再动态调整，暂时将哈希值中\"0\"的数量固定保证每次生成区块的难度是相同的。同时也要设置一个最大难度值，防止无限循环计算。\n\n```\n#Pow.java\npublic class Pow {\n    //固定的难度值\n    private static final String DIFFICULT = \"0000\";\n    //最大难度值 防止计算难度值变为无限循环\n    private static final int MAX_VALUE = Integer.MAX_VALUE;\n    public static int calc(Block block){\n        //nonce从0开始\n        int nonce = 0;\n        //如果nonce小于最大难度值\n        while(nonce<MAX_VALUE){\n            //计算哈希值\n            if(Util.getSHA256(block.toString()+nonce)\n                    //如果计算出的哈希值前缀满足条件，退出循环\n                    .startsWith(DIFFICULT))\n                break;\n            //不满足条件，nonce+1，重新计算哈希值\n            nonce++;\n        }\n        return nonce;\n    }\n}\n```\n\n#### 更新属性\n一个简单的POW共识完成了，接下来需要更新一下区块的属性，添加``nonce``属性:\n\n```\n#Block.java\n    //产出该区块的难度\n    public int nonce;\n```\n\n还要修改生成区块的方法，每次生成区块时需要进行POW共识计算:\n\n```\n    public Block CrtGenesisBlock(){\n        Block block = new Block(1,\"Genesis Block\",\"00000000000000000\");\n        block.setNonce(\n            Pow.calc(block));\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        ...\n    }\n    public Block addBlock(String data){\n        ...\n        Block block = new Block(\n            num+1,data, this.block.curBlockHash);\n        //每次将区块添加进区块链之前需要计算难度值\n        block.setNonce(\n            Pow.calc(block));\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        ...\n    }\n```\n\n#### 测试POW共识\nOK了，还是之前的测试方法，测试一下:\n\n```\n#Test.java\npublic class Test {\n    public static void main(String[] args){\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(\"Block 2\").toString());\n    }\n}\n```\n可以看到区块号为2的区块``nonce``属性有了具体的值，并且每次测试``curBlockHash``的值前缀都是以\"0000\"开头的。\n```\n{\"blkNum\":1,\"curBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"data\":\"Genesis Block\",\"nonce\":37846,\"prevBlockHash\":\"00000000000000000\",\"timeStamp\":\"2020-05-17 10:49:48\"}\n{\"blkNum\":2,\"curBlockHash\":\"00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9\",\"data\":\"Block 2\",\"nonce\":15318,\"prevBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"timeStamp\":\"2020-05-17 10:49:48\"}\n```\n\n### 本地化\n此外，每次重新启动程序都需要从创世区块重新开始生成，所以需要将区块信息序列化到本地。保证每次启动程序都可以从本地读取数据不再重新生成创世区块。\n\n方便起见，暂时不使用数据库存储区块信息，只简单序列化到本地文件中来。\n首先需要修改区块的信息，继承``Serializable``接口才能进行序列化。\n\n```\n#Block.java\npublic class Block implements Serializable{\n    private static final long serialVersionUID = 1L;\n    ...\n}\n```\n\n#### 序列化与反序列化\n接下来是序列化与反序列化的方法,在这里我们将每一个区块都保存为一个名字为区块号，后缀为``.block``的文件，同样从本地反序列化到程序中也只需要通过区块号来取。\n```\n#Storage.java\npublic final class Storage {\n     //序列化区块信息\n     public static void Serialize(Block block) throws IOException {\n        File file = new File(\"src/main/resources/blocks/\"+block.getBlkNum()+\".block\");\n        if(!file.exists()) file.createNewFile();\n        FileOutputStream fos = new FileOutputStream(file);\n        ObjectOutputStream oos = new ObjectOutputStream(fos);\n        \n        oos.writeObject(block);\n        oos.close();\n        fos.close();\n    }\n    /**\n     * 反序列化区块\n     */\n    public static Block Deserialize(int num) throws FileNotFoundException, IOException, ClassNotFoundException {\n        File file = new File(\"src/main/resources/blocks/\"+num+\".block\");\n        if(!file.exists()) return null;\n        ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file));\n        \n        Block block = (Block)ois.readObject();\n        ois.close();\n        return block;\n    }\n}\n```\n\n然后是区块链的属性，之前我们使用``ArrayList``存储区块信息，而现在我们直接将区块序列化到本地，需要哪一个区块直接到本地来取，因此不再需要``ArrayList``保存区块数据。对于区块链来讲，仅仅需要记录最新区块数据即可。\n\n```\n\npublic final class Blockchain {\n    ...\n    //Arraylist<Block> block修改为 Block block;\n    public Block block;\n    ...\n    public static Blockchain getInstance() {\n        if (BC == null) {\n            synchronized (Blockchain.class) {\n                if (BC == null) {\n                    BC = new Blockchain();\n                    //删除创建ArrayList\n                }\n            }\n        }\n        return BC;\n    }\n\n    public Block CrtGenesisBlock() throws IOException {\n        ...\n        block.setCurBlockHash(hash);\n        //序列化\n        Storage.Serialize(block);\n        this.block=block;\n        return this.block;\n    }\n    public Block addBlock(String data) throws IOException {\n        int num = this.block.getBlkNum();\n        ...\n        block.setCurBlockHash(hash);\n        //序列化\n        Storage.Serialize(block);\n        this.block = block;\n        return this.block;\n    }\n}\n```\n\n测试一下:\n```\npublic class Test {\n    public static void main(String[] args) throws IOException {\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(\"Block 2\").toString());\n    }\n}\n```\n存储是没有问题的，在``resources/blocks/``文件下成功生成了``1.block,2.block``两个文件。\n\n#### 反序列化\n\n但是还没有完成从本地取数据的操作，接下来的流程是这样子的:\n启动程序后，首先实例化``Blockchain``的实例，然后从本地读取数据，如果本地存在区块数据，直接反序列化区块号最大的区块，如果本地没有数据，则进行创始区块的创建。\n\n```\n#Blockchain.java\npublic Block getLastBlock() throws FileNotFoundException, ClassNotFoundException, IOException {\n        File file = new File(\"src/main/resources/blocks\");\n        String[] files = file.list();\n        if(files.length!=0){\n            int MaxFileNum = 1;\n            //遍历存储区块数据的文件夹，查找区块号最大的区块\n            for(String s:files){\n                int num = Integer.valueOf(s.substring(0, 1));\n                if(num>=MaxFileNum)\n                    MaxFileNum = num;\n            }\n            //反序列化最大区块号的区块\n           return Storage.Deserialize(MaxFileNum);\n        }\n        return null;\n    }\n```\n然后是``Blockchain``的实例方法，在获取实例时候判断是否需要创建创世区块:\n```\n#Blockchain.java\n    public static Blockchain getInstance() throws FileNotFoundException, ClassNotFoundException, IOException {\n        if (BC == null) {\n            synchronized (Blockchain.class) {\n                if (BC == null) {\n                    BC = new Blockchain();\n                }\n            }\n        }\n        //获取到Blockchain实例后，判断是否存在区块\n        if(BC.block==null){\n            //如果不存在则尝试获取本地区块号最大的区块\n            //如果存在则直接赋值到Blockchain的属性然后返回\n            Block block = BC.getLastBlock();\n            BC.block = block;\n            if(block==null){\n                //如果不存在则生成创世区块\n                BC.CrtGenesisBlock();\n            }\n        }\n        return BC;\n    }\n    \n    //因此创建创世区块的方法可以修改为私有的\n    private Block CrtGenesisBlock() throws IOException {\n        ...\n    }\n```\n\n接下来可以测试了:\n\n```\npublic class Test {\n    public static void main(String[] args) throws IOException, ClassNotFoundException {\n        System.out.println(Blockchain.getInstance().block.toString());\n        System.out.println(Blockchain.getInstance().addBlock(\"Block 2\").toString());\n    }\n}\n```\n测试多次可以发现区块并没有重新从创世区块开始生成，而是根据先前生成的区块号继续增长。\n```\n{\"blkNum\":1,\"curBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"data\":\"Genesis Block\",\"nonce\":37846,\"prevBlockHash\":\"00000000000000000\",\"timeStamp\":\"2020-05-17 11:51:37\"}\n{\"blkNum\":2,\"curBlockHash\":\"00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9\",\"data\":\"Block 2\",\"nonce\":15318,\"prevBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"timeStamp\":\"2020-05-17 11:51:37\"}\n\nCurrent Last Block num is:2\n{\"blkNum\":2,\"curBlockHash\":\"00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9\",\"data\":\"Block 2\",\"nonce\":15318,\"prevBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"timeStamp\":\"2020-05-17 11:51:37\"}\n{\"blkNum\":3,\"curBlockHash\":\"0000d350c1199eb51c2d43194653f5b44444665e40373d5883edd3567c60cd68\",\"data\":\"Block 2\",\"nonce\":23695,\"prevBlockHash\":\"00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9\",\"timeStamp\":\"2020-05-17 11:51:44\"}\n```\n\n大致工作已完成，接下来添加几个额外的方法:\n\n```\n#Block.java\n       /**\n     * 是否存在前一个区块\n     */\n    public boolean hasPrevBlock(){\n        if(this.getBlkNum()!=1){\n            return true;\n        }\n        return false;\n    }\n    @Transient\n    @JsonIgnore\n    /**\n     * 获取前一个区块\n     */\n    public Block getPrevBlock() throws FileNotFoundException, ClassNotFoundException, IOException {\n        if(this.hasPrevBlock())\n            return Storage.Deserialize(this.getBlkNum()-1);\n        return null;          \n    }\n```\n\n后一篇文章: [搭建你的第一个区块链网络(三)](https://www.cnblogs.com/cbkj-xd/p/12905706.html)\n\n#### Github仓库地址在这里，随时保持更新中.....\nGithub地址：[Jchain](https://github.com/newonexd/Jchain)","source":"_posts/blog/blockchain/Jchain2.md","raw":"---\ntitle: 搭建你的第一个区块链网络(二)\ndate: 2020-05-19 15:00:49\ntags: blockchain\n---\n\n前一篇文章: [搭建你的第一个区块链网络(一)](https://www.cnblogs.com/cbkj-xd/p/12901312.html)\n\n\n## 共识与本地化\n### POW共识\n\n共识机制也是区块链系统中不可缺少的一部分，在比特币网络中，使用的是POW共识，概念相对比较简单，所以我们在该项目中使用POW共识机制(后期如果可以的话修改为可插拔的共识机制)。\n\n#### POW原理\nPOW原理是通过解决一个数学难题，其实就是通过计算一个哈希值，如果计算出来的哈希值的前缀有足够多个\"0\"，就说明成功解决了该数学难题。通常哈希值中\"0\"的个数越多难度越大。难度值是通过之前生成的区块所消耗的时间动态调整的。而生成哈希值的原数据实际上就是区块信息，另外再加一个``nonce``属性，用于调整难度值。\n在比特币中，平均每10分钟产出一个区块，如果新区块的产出只消耗了9分钟，那么难度值将会增加。如果算力不发生变化的话，下一次产出区块将会消耗更多的时间。同理，如果新区块的产出消耗了11分钟，那么难度值则会相应地降低。动态调整难度值维持区块产出时间平均为10分钟。实际上比特币中的POW更加复杂，难度值的调整是通过过去的2016个区块产出的时间与20160分钟进行比较的。\n在这里，不设置那么麻烦，难度值不再动态调整，暂时将哈希值中\"0\"的数量固定保证每次生成区块的难度是相同的。同时也要设置一个最大难度值，防止无限循环计算。\n\n```\n#Pow.java\npublic class Pow {\n    //固定的难度值\n    private static final String DIFFICULT = \"0000\";\n    //最大难度值 防止计算难度值变为无限循环\n    private static final int MAX_VALUE = Integer.MAX_VALUE;\n    public static int calc(Block block){\n        //nonce从0开始\n        int nonce = 0;\n        //如果nonce小于最大难度值\n        while(nonce<MAX_VALUE){\n            //计算哈希值\n            if(Util.getSHA256(block.toString()+nonce)\n                    //如果计算出的哈希值前缀满足条件，退出循环\n                    .startsWith(DIFFICULT))\n                break;\n            //不满足条件，nonce+1，重新计算哈希值\n            nonce++;\n        }\n        return nonce;\n    }\n}\n```\n\n#### 更新属性\n一个简单的POW共识完成了，接下来需要更新一下区块的属性，添加``nonce``属性:\n\n```\n#Block.java\n    //产出该区块的难度\n    public int nonce;\n```\n\n还要修改生成区块的方法，每次生成区块时需要进行POW共识计算:\n\n```\n    public Block CrtGenesisBlock(){\n        Block block = new Block(1,\"Genesis Block\",\"00000000000000000\");\n        block.setNonce(\n            Pow.calc(block));\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        ...\n    }\n    public Block addBlock(String data){\n        ...\n        Block block = new Block(\n            num+1,data, this.block.curBlockHash);\n        //每次将区块添加进区块链之前需要计算难度值\n        block.setNonce(\n            Pow.calc(block));\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        ...\n    }\n```\n\n#### 测试POW共识\nOK了，还是之前的测试方法，测试一下:\n\n```\n#Test.java\npublic class Test {\n    public static void main(String[] args){\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(\"Block 2\").toString());\n    }\n}\n```\n可以看到区块号为2的区块``nonce``属性有了具体的值，并且每次测试``curBlockHash``的值前缀都是以\"0000\"开头的。\n```\n{\"blkNum\":1,\"curBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"data\":\"Genesis Block\",\"nonce\":37846,\"prevBlockHash\":\"00000000000000000\",\"timeStamp\":\"2020-05-17 10:49:48\"}\n{\"blkNum\":2,\"curBlockHash\":\"00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9\",\"data\":\"Block 2\",\"nonce\":15318,\"prevBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"timeStamp\":\"2020-05-17 10:49:48\"}\n```\n\n### 本地化\n此外，每次重新启动程序都需要从创世区块重新开始生成，所以需要将区块信息序列化到本地。保证每次启动程序都可以从本地读取数据不再重新生成创世区块。\n\n方便起见，暂时不使用数据库存储区块信息，只简单序列化到本地文件中来。\n首先需要修改区块的信息，继承``Serializable``接口才能进行序列化。\n\n```\n#Block.java\npublic class Block implements Serializable{\n    private static final long serialVersionUID = 1L;\n    ...\n}\n```\n\n#### 序列化与反序列化\n接下来是序列化与反序列化的方法,在这里我们将每一个区块都保存为一个名字为区块号，后缀为``.block``的文件，同样从本地反序列化到程序中也只需要通过区块号来取。\n```\n#Storage.java\npublic final class Storage {\n     //序列化区块信息\n     public static void Serialize(Block block) throws IOException {\n        File file = new File(\"src/main/resources/blocks/\"+block.getBlkNum()+\".block\");\n        if(!file.exists()) file.createNewFile();\n        FileOutputStream fos = new FileOutputStream(file);\n        ObjectOutputStream oos = new ObjectOutputStream(fos);\n        \n        oos.writeObject(block);\n        oos.close();\n        fos.close();\n    }\n    /**\n     * 反序列化区块\n     */\n    public static Block Deserialize(int num) throws FileNotFoundException, IOException, ClassNotFoundException {\n        File file = new File(\"src/main/resources/blocks/\"+num+\".block\");\n        if(!file.exists()) return null;\n        ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file));\n        \n        Block block = (Block)ois.readObject();\n        ois.close();\n        return block;\n    }\n}\n```\n\n然后是区块链的属性，之前我们使用``ArrayList``存储区块信息，而现在我们直接将区块序列化到本地，需要哪一个区块直接到本地来取，因此不再需要``ArrayList``保存区块数据。对于区块链来讲，仅仅需要记录最新区块数据即可。\n\n```\n\npublic final class Blockchain {\n    ...\n    //Arraylist<Block> block修改为 Block block;\n    public Block block;\n    ...\n    public static Blockchain getInstance() {\n        if (BC == null) {\n            synchronized (Blockchain.class) {\n                if (BC == null) {\n                    BC = new Blockchain();\n                    //删除创建ArrayList\n                }\n            }\n        }\n        return BC;\n    }\n\n    public Block CrtGenesisBlock() throws IOException {\n        ...\n        block.setCurBlockHash(hash);\n        //序列化\n        Storage.Serialize(block);\n        this.block=block;\n        return this.block;\n    }\n    public Block addBlock(String data) throws IOException {\n        int num = this.block.getBlkNum();\n        ...\n        block.setCurBlockHash(hash);\n        //序列化\n        Storage.Serialize(block);\n        this.block = block;\n        return this.block;\n    }\n}\n```\n\n测试一下:\n```\npublic class Test {\n    public static void main(String[] args) throws IOException {\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(\"Block 2\").toString());\n    }\n}\n```\n存储是没有问题的，在``resources/blocks/``文件下成功生成了``1.block,2.block``两个文件。\n\n#### 反序列化\n\n但是还没有完成从本地取数据的操作，接下来的流程是这样子的:\n启动程序后，首先实例化``Blockchain``的实例，然后从本地读取数据，如果本地存在区块数据，直接反序列化区块号最大的区块，如果本地没有数据，则进行创始区块的创建。\n\n```\n#Blockchain.java\npublic Block getLastBlock() throws FileNotFoundException, ClassNotFoundException, IOException {\n        File file = new File(\"src/main/resources/blocks\");\n        String[] files = file.list();\n        if(files.length!=0){\n            int MaxFileNum = 1;\n            //遍历存储区块数据的文件夹，查找区块号最大的区块\n            for(String s:files){\n                int num = Integer.valueOf(s.substring(0, 1));\n                if(num>=MaxFileNum)\n                    MaxFileNum = num;\n            }\n            //反序列化最大区块号的区块\n           return Storage.Deserialize(MaxFileNum);\n        }\n        return null;\n    }\n```\n然后是``Blockchain``的实例方法，在获取实例时候判断是否需要创建创世区块:\n```\n#Blockchain.java\n    public static Blockchain getInstance() throws FileNotFoundException, ClassNotFoundException, IOException {\n        if (BC == null) {\n            synchronized (Blockchain.class) {\n                if (BC == null) {\n                    BC = new Blockchain();\n                }\n            }\n        }\n        //获取到Blockchain实例后，判断是否存在区块\n        if(BC.block==null){\n            //如果不存在则尝试获取本地区块号最大的区块\n            //如果存在则直接赋值到Blockchain的属性然后返回\n            Block block = BC.getLastBlock();\n            BC.block = block;\n            if(block==null){\n                //如果不存在则生成创世区块\n                BC.CrtGenesisBlock();\n            }\n        }\n        return BC;\n    }\n    \n    //因此创建创世区块的方法可以修改为私有的\n    private Block CrtGenesisBlock() throws IOException {\n        ...\n    }\n```\n\n接下来可以测试了:\n\n```\npublic class Test {\n    public static void main(String[] args) throws IOException, ClassNotFoundException {\n        System.out.println(Blockchain.getInstance().block.toString());\n        System.out.println(Blockchain.getInstance().addBlock(\"Block 2\").toString());\n    }\n}\n```\n测试多次可以发现区块并没有重新从创世区块开始生成，而是根据先前生成的区块号继续增长。\n```\n{\"blkNum\":1,\"curBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"data\":\"Genesis Block\",\"nonce\":37846,\"prevBlockHash\":\"00000000000000000\",\"timeStamp\":\"2020-05-17 11:51:37\"}\n{\"blkNum\":2,\"curBlockHash\":\"00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9\",\"data\":\"Block 2\",\"nonce\":15318,\"prevBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"timeStamp\":\"2020-05-17 11:51:37\"}\n\nCurrent Last Block num is:2\n{\"blkNum\":2,\"curBlockHash\":\"00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9\",\"data\":\"Block 2\",\"nonce\":15318,\"prevBlockHash\":\"000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1\",\"timeStamp\":\"2020-05-17 11:51:37\"}\n{\"blkNum\":3,\"curBlockHash\":\"0000d350c1199eb51c2d43194653f5b44444665e40373d5883edd3567c60cd68\",\"data\":\"Block 2\",\"nonce\":23695,\"prevBlockHash\":\"00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9\",\"timeStamp\":\"2020-05-17 11:51:44\"}\n```\n\n大致工作已完成，接下来添加几个额外的方法:\n\n```\n#Block.java\n       /**\n     * 是否存在前一个区块\n     */\n    public boolean hasPrevBlock(){\n        if(this.getBlkNum()!=1){\n            return true;\n        }\n        return false;\n    }\n    @Transient\n    @JsonIgnore\n    /**\n     * 获取前一个区块\n     */\n    public Block getPrevBlock() throws FileNotFoundException, ClassNotFoundException, IOException {\n        if(this.hasPrevBlock())\n            return Storage.Deserialize(this.getBlkNum()-1);\n        return null;          \n    }\n```\n\n后一篇文章: [搭建你的第一个区块链网络(三)](https://www.cnblogs.com/cbkj-xd/p/12905706.html)\n\n#### Github仓库地址在这里，随时保持更新中.....\nGithub地址：[Jchain](https://github.com/newonexd/Jchain)","slug":"blog/blockchain/Jchain2","published":1,"updated":"2020-05-19T07:02:17.343Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyem0001k0vq90rvgwhy","content":"<p>前一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12901312.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(一)</a></p>\n<h2 id=\"共识与本地化\"><a href=\"#共识与本地化\" class=\"headerlink\" title=\"共识与本地化\"></a>共识与本地化</h2><h3 id=\"POW共识\"><a href=\"#POW共识\" class=\"headerlink\" title=\"POW共识\"></a>POW共识</h3><p>共识机制也是区块链系统中不可缺少的一部分，在比特币网络中，使用的是POW共识，概念相对比较简单，所以我们在该项目中使用POW共识机制(后期如果可以的话修改为可插拔的共识机制)。</p>\n<h4 id=\"POW原理\"><a href=\"#POW原理\" class=\"headerlink\" title=\"POW原理\"></a>POW原理</h4><p>POW原理是通过解决一个数学难题，其实就是通过计算一个哈希值，如果计算出来的哈希值的前缀有足够多个”0”，就说明成功解决了该数学难题。通常哈希值中”0”的个数越多难度越大。难度值是通过之前生成的区块所消耗的时间动态调整的。而生成哈希值的原数据实际上就是区块信息，另外再加一个<code>nonce</code>属性，用于调整难度值。<br>在比特币中，平均每10分钟产出一个区块，如果新区块的产出只消耗了9分钟，那么难度值将会增加。如果算力不发生变化的话，下一次产出区块将会消耗更多的时间。同理，如果新区块的产出消耗了11分钟，那么难度值则会相应地降低。动态调整难度值维持区块产出时间平均为10分钟。实际上比特币中的POW更加复杂，难度值的调整是通过过去的2016个区块产出的时间与20160分钟进行比较的。<br>在这里，不设置那么麻烦，难度值不再动态调整，暂时将哈希值中”0”的数量固定保证每次生成区块的难度是相同的。同时也要设置一个最大难度值，防止无限循环计算。</p>\n<pre><code>#Pow.java\npublic class Pow {\n    //固定的难度值\n    private static final String DIFFICULT = &quot;0000&quot;;\n    //最大难度值 防止计算难度值变为无限循环\n    private static final int MAX_VALUE = Integer.MAX_VALUE;\n    public static int calc(Block block){\n        //nonce从0开始\n        int nonce = 0;\n        //如果nonce小于最大难度值\n        while(nonce&lt;MAX_VALUE){\n            //计算哈希值\n            if(Util.getSHA256(block.toString()+nonce)\n                    //如果计算出的哈希值前缀满足条件，退出循环\n                    .startsWith(DIFFICULT))\n                break;\n            //不满足条件，nonce+1，重新计算哈希值\n            nonce++;\n        }\n        return nonce;\n    }\n}</code></pre><h4 id=\"更新属性\"><a href=\"#更新属性\" class=\"headerlink\" title=\"更新属性\"></a>更新属性</h4><p>一个简单的POW共识完成了，接下来需要更新一下区块的属性，添加<code>nonce</code>属性:</p>\n<pre><code>#Block.java\n    //产出该区块的难度\n    public int nonce;</code></pre><p>还要修改生成区块的方法，每次生成区块时需要进行POW共识计算:</p>\n<pre><code>    public Block CrtGenesisBlock(){\n        Block block = new Block(1,&quot;Genesis Block&quot;,&quot;00000000000000000&quot;);\n        block.setNonce(\n            Pow.calc(block));\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        ...\n    }\n    public Block addBlock(String data){\n        ...\n        Block block = new Block(\n            num+1,data, this.block.curBlockHash);\n        //每次将区块添加进区块链之前需要计算难度值\n        block.setNonce(\n            Pow.calc(block));\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        ...\n    }</code></pre><h4 id=\"测试POW共识\"><a href=\"#测试POW共识\" class=\"headerlink\" title=\"测试POW共识\"></a>测试POW共识</h4><p>OK了，还是之前的测试方法，测试一下:</p>\n<pre><code>#Test.java\npublic class Test {\n    public static void main(String[] args){\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(&quot;Block 2&quot;).toString());\n    }\n}</code></pre><p>可以看到区块号为2的区块<code>nonce</code>属性有了具体的值，并且每次测试<code>curBlockHash</code>的值前缀都是以”0000”开头的。</p>\n<pre><code>{&quot;blkNum&quot;:1,&quot;curBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;data&quot;:&quot;Genesis Block&quot;,&quot;nonce&quot;:37846,&quot;prevBlockHash&quot;:&quot;00000000000000000&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 10:49:48&quot;}\n{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;nonce&quot;:15318,&quot;prevBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 10:49:48&quot;}</code></pre><h3 id=\"本地化\"><a href=\"#本地化\" class=\"headerlink\" title=\"本地化\"></a>本地化</h3><p>此外，每次重新启动程序都需要从创世区块重新开始生成，所以需要将区块信息序列化到本地。保证每次启动程序都可以从本地读取数据不再重新生成创世区块。</p>\n<p>方便起见，暂时不使用数据库存储区块信息，只简单序列化到本地文件中来。<br>首先需要修改区块的信息，继承<code>Serializable</code>接口才能进行序列化。</p>\n<pre><code>#Block.java\npublic class Block implements Serializable{\n    private static final long serialVersionUID = 1L;\n    ...\n}</code></pre><h4 id=\"序列化与反序列化\"><a href=\"#序列化与反序列化\" class=\"headerlink\" title=\"序列化与反序列化\"></a>序列化与反序列化</h4><p>接下来是序列化与反序列化的方法,在这里我们将每一个区块都保存为一个名字为区块号，后缀为<code>.block</code>的文件，同样从本地反序列化到程序中也只需要通过区块号来取。</p>\n<pre><code>#Storage.java\npublic final class Storage {\n     //序列化区块信息\n     public static void Serialize(Block block) throws IOException {\n        File file = new File(&quot;src/main/resources/blocks/&quot;+block.getBlkNum()+&quot;.block&quot;);\n        if(!file.exists()) file.createNewFile();\n        FileOutputStream fos = new FileOutputStream(file);\n        ObjectOutputStream oos = new ObjectOutputStream(fos);\n\n        oos.writeObject(block);\n        oos.close();\n        fos.close();\n    }\n    /**\n     * 反序列化区块\n     */\n    public static Block Deserialize(int num) throws FileNotFoundException, IOException, ClassNotFoundException {\n        File file = new File(&quot;src/main/resources/blocks/&quot;+num+&quot;.block&quot;);\n        if(!file.exists()) return null;\n        ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file));\n\n        Block block = (Block)ois.readObject();\n        ois.close();\n        return block;\n    }\n}</code></pre><p>然后是区块链的属性，之前我们使用<code>ArrayList</code>存储区块信息，而现在我们直接将区块序列化到本地，需要哪一个区块直接到本地来取，因此不再需要<code>ArrayList</code>保存区块数据。对于区块链来讲，仅仅需要记录最新区块数据即可。</p>\n<pre><code>\npublic final class Blockchain {\n    ...\n    //Arraylist&lt;Block&gt; block修改为 Block block;\n    public Block block;\n    ...\n    public static Blockchain getInstance() {\n        if (BC == null) {\n            synchronized (Blockchain.class) {\n                if (BC == null) {\n                    BC = new Blockchain();\n                    //删除创建ArrayList\n                }\n            }\n        }\n        return BC;\n    }\n\n    public Block CrtGenesisBlock() throws IOException {\n        ...\n        block.setCurBlockHash(hash);\n        //序列化\n        Storage.Serialize(block);\n        this.block=block;\n        return this.block;\n    }\n    public Block addBlock(String data) throws IOException {\n        int num = this.block.getBlkNum();\n        ...\n        block.setCurBlockHash(hash);\n        //序列化\n        Storage.Serialize(block);\n        this.block = block;\n        return this.block;\n    }\n}</code></pre><p>测试一下:</p>\n<pre><code>public class Test {\n    public static void main(String[] args) throws IOException {\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(&quot;Block 2&quot;).toString());\n    }\n}</code></pre><p>存储是没有问题的，在<code>resources/blocks/</code>文件下成功生成了<code>1.block,2.block</code>两个文件。</p>\n<h4 id=\"反序列化\"><a href=\"#反序列化\" class=\"headerlink\" title=\"反序列化\"></a>反序列化</h4><p>但是还没有完成从本地取数据的操作，接下来的流程是这样子的:<br>启动程序后，首先实例化<code>Blockchain</code>的实例，然后从本地读取数据，如果本地存在区块数据，直接反序列化区块号最大的区块，如果本地没有数据，则进行创始区块的创建。</p>\n<pre><code>#Blockchain.java\npublic Block getLastBlock() throws FileNotFoundException, ClassNotFoundException, IOException {\n        File file = new File(&quot;src/main/resources/blocks&quot;);\n        String[] files = file.list();\n        if(files.length!=0){\n            int MaxFileNum = 1;\n            //遍历存储区块数据的文件夹，查找区块号最大的区块\n            for(String s:files){\n                int num = Integer.valueOf(s.substring(0, 1));\n                if(num&gt;=MaxFileNum)\n                    MaxFileNum = num;\n            }\n            //反序列化最大区块号的区块\n           return Storage.Deserialize(MaxFileNum);\n        }\n        return null;\n    }</code></pre><p>然后是<code>Blockchain</code>的实例方法，在获取实例时候判断是否需要创建创世区块:</p>\n<pre><code>#Blockchain.java\n    public static Blockchain getInstance() throws FileNotFoundException, ClassNotFoundException, IOException {\n        if (BC == null) {\n            synchronized (Blockchain.class) {\n                if (BC == null) {\n                    BC = new Blockchain();\n                }\n            }\n        }\n        //获取到Blockchain实例后，判断是否存在区块\n        if(BC.block==null){\n            //如果不存在则尝试获取本地区块号最大的区块\n            //如果存在则直接赋值到Blockchain的属性然后返回\n            Block block = BC.getLastBlock();\n            BC.block = block;\n            if(block==null){\n                //如果不存在则生成创世区块\n                BC.CrtGenesisBlock();\n            }\n        }\n        return BC;\n    }\n\n    //因此创建创世区块的方法可以修改为私有的\n    private Block CrtGenesisBlock() throws IOException {\n        ...\n    }</code></pre><p>接下来可以测试了:</p>\n<pre><code>public class Test {\n    public static void main(String[] args) throws IOException, ClassNotFoundException {\n        System.out.println(Blockchain.getInstance().block.toString());\n        System.out.println(Blockchain.getInstance().addBlock(&quot;Block 2&quot;).toString());\n    }\n}</code></pre><p>测试多次可以发现区块并没有重新从创世区块开始生成，而是根据先前生成的区块号继续增长。</p>\n<pre><code>{&quot;blkNum&quot;:1,&quot;curBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;data&quot;:&quot;Genesis Block&quot;,&quot;nonce&quot;:37846,&quot;prevBlockHash&quot;:&quot;00000000000000000&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 11:51:37&quot;}\n{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;nonce&quot;:15318,&quot;prevBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 11:51:37&quot;}\n\nCurrent Last Block num is:2\n{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;nonce&quot;:15318,&quot;prevBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 11:51:37&quot;}\n{&quot;blkNum&quot;:3,&quot;curBlockHash&quot;:&quot;0000d350c1199eb51c2d43194653f5b44444665e40373d5883edd3567c60cd68&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;nonce&quot;:23695,&quot;prevBlockHash&quot;:&quot;00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 11:51:44&quot;}</code></pre><p>大致工作已完成，接下来添加几个额外的方法:</p>\n<pre><code>#Block.java\n       /**\n     * 是否存在前一个区块\n     */\n    public boolean hasPrevBlock(){\n        if(this.getBlkNum()!=1){\n            return true;\n        }\n        return false;\n    }\n    @Transient\n    @JsonIgnore\n    /**\n     * 获取前一个区块\n     */\n    public Block getPrevBlock() throws FileNotFoundException, ClassNotFoundException, IOException {\n        if(this.hasPrevBlock())\n            return Storage.Deserialize(this.getBlkNum()-1);\n        return null;          \n    }</code></pre><p>后一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12905706.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(三)</a></p>\n<h4 id=\"Github仓库地址在这里，随时保持更新中…\"><a href=\"#Github仓库地址在这里，随时保持更新中…\" class=\"headerlink\" title=\"Github仓库地址在这里，随时保持更新中…..\"></a>Github仓库地址在这里，随时保持更新中…..</h4><p>Github地址：<a href=\"https://github.com/newonexd/Jchain\" target=\"_blank\" rel=\"noopener\">Jchain</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>前一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12901312.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(一)</a></p>\n<h2 id=\"共识与本地化\"><a href=\"#共识与本地化\" class=\"headerlink\" title=\"共识与本地化\"></a>共识与本地化</h2><h3 id=\"POW共识\"><a href=\"#POW共识\" class=\"headerlink\" title=\"POW共识\"></a>POW共识</h3><p>共识机制也是区块链系统中不可缺少的一部分，在比特币网络中，使用的是POW共识，概念相对比较简单，所以我们在该项目中使用POW共识机制(后期如果可以的话修改为可插拔的共识机制)。</p>\n<h4 id=\"POW原理\"><a href=\"#POW原理\" class=\"headerlink\" title=\"POW原理\"></a>POW原理</h4><p>POW原理是通过解决一个数学难题，其实就是通过计算一个哈希值，如果计算出来的哈希值的前缀有足够多个”0”，就说明成功解决了该数学难题。通常哈希值中”0”的个数越多难度越大。难度值是通过之前生成的区块所消耗的时间动态调整的。而生成哈希值的原数据实际上就是区块信息，另外再加一个<code>nonce</code>属性，用于调整难度值。<br>在比特币中，平均每10分钟产出一个区块，如果新区块的产出只消耗了9分钟，那么难度值将会增加。如果算力不发生变化的话，下一次产出区块将会消耗更多的时间。同理，如果新区块的产出消耗了11分钟，那么难度值则会相应地降低。动态调整难度值维持区块产出时间平均为10分钟。实际上比特币中的POW更加复杂，难度值的调整是通过过去的2016个区块产出的时间与20160分钟进行比较的。<br>在这里，不设置那么麻烦，难度值不再动态调整，暂时将哈希值中”0”的数量固定保证每次生成区块的难度是相同的。同时也要设置一个最大难度值，防止无限循环计算。</p>\n<pre><code>#Pow.java\npublic class Pow {\n    //固定的难度值\n    private static final String DIFFICULT = &quot;0000&quot;;\n    //最大难度值 防止计算难度值变为无限循环\n    private static final int MAX_VALUE = Integer.MAX_VALUE;\n    public static int calc(Block block){\n        //nonce从0开始\n        int nonce = 0;\n        //如果nonce小于最大难度值\n        while(nonce&lt;MAX_VALUE){\n            //计算哈希值\n            if(Util.getSHA256(block.toString()+nonce)\n                    //如果计算出的哈希值前缀满足条件，退出循环\n                    .startsWith(DIFFICULT))\n                break;\n            //不满足条件，nonce+1，重新计算哈希值\n            nonce++;\n        }\n        return nonce;\n    }\n}</code></pre><h4 id=\"更新属性\"><a href=\"#更新属性\" class=\"headerlink\" title=\"更新属性\"></a>更新属性</h4><p>一个简单的POW共识完成了，接下来需要更新一下区块的属性，添加<code>nonce</code>属性:</p>\n<pre><code>#Block.java\n    //产出该区块的难度\n    public int nonce;</code></pre><p>还要修改生成区块的方法，每次生成区块时需要进行POW共识计算:</p>\n<pre><code>    public Block CrtGenesisBlock(){\n        Block block = new Block(1,&quot;Genesis Block&quot;,&quot;00000000000000000&quot;);\n        block.setNonce(\n            Pow.calc(block));\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        ...\n    }\n    public Block addBlock(String data){\n        ...\n        Block block = new Block(\n            num+1,data, this.block.curBlockHash);\n        //每次将区块添加进区块链之前需要计算难度值\n        block.setNonce(\n            Pow.calc(block));\n        //计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum()+block.getData()+block.getPrevBlockHash()+block.getPrevBlockHash()+block.getNonce());\n        ...\n    }</code></pre><h4 id=\"测试POW共识\"><a href=\"#测试POW共识\" class=\"headerlink\" title=\"测试POW共识\"></a>测试POW共识</h4><p>OK了，还是之前的测试方法，测试一下:</p>\n<pre><code>#Test.java\npublic class Test {\n    public static void main(String[] args){\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(&quot;Block 2&quot;).toString());\n    }\n}</code></pre><p>可以看到区块号为2的区块<code>nonce</code>属性有了具体的值，并且每次测试<code>curBlockHash</code>的值前缀都是以”0000”开头的。</p>\n<pre><code>{&quot;blkNum&quot;:1,&quot;curBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;data&quot;:&quot;Genesis Block&quot;,&quot;nonce&quot;:37846,&quot;prevBlockHash&quot;:&quot;00000000000000000&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 10:49:48&quot;}\n{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;nonce&quot;:15318,&quot;prevBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 10:49:48&quot;}</code></pre><h3 id=\"本地化\"><a href=\"#本地化\" class=\"headerlink\" title=\"本地化\"></a>本地化</h3><p>此外，每次重新启动程序都需要从创世区块重新开始生成，所以需要将区块信息序列化到本地。保证每次启动程序都可以从本地读取数据不再重新生成创世区块。</p>\n<p>方便起见，暂时不使用数据库存储区块信息，只简单序列化到本地文件中来。<br>首先需要修改区块的信息，继承<code>Serializable</code>接口才能进行序列化。</p>\n<pre><code>#Block.java\npublic class Block implements Serializable{\n    private static final long serialVersionUID = 1L;\n    ...\n}</code></pre><h4 id=\"序列化与反序列化\"><a href=\"#序列化与反序列化\" class=\"headerlink\" title=\"序列化与反序列化\"></a>序列化与反序列化</h4><p>接下来是序列化与反序列化的方法,在这里我们将每一个区块都保存为一个名字为区块号，后缀为<code>.block</code>的文件，同样从本地反序列化到程序中也只需要通过区块号来取。</p>\n<pre><code>#Storage.java\npublic final class Storage {\n     //序列化区块信息\n     public static void Serialize(Block block) throws IOException {\n        File file = new File(&quot;src/main/resources/blocks/&quot;+block.getBlkNum()+&quot;.block&quot;);\n        if(!file.exists()) file.createNewFile();\n        FileOutputStream fos = new FileOutputStream(file);\n        ObjectOutputStream oos = new ObjectOutputStream(fos);\n\n        oos.writeObject(block);\n        oos.close();\n        fos.close();\n    }\n    /**\n     * 反序列化区块\n     */\n    public static Block Deserialize(int num) throws FileNotFoundException, IOException, ClassNotFoundException {\n        File file = new File(&quot;src/main/resources/blocks/&quot;+num+&quot;.block&quot;);\n        if(!file.exists()) return null;\n        ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file));\n\n        Block block = (Block)ois.readObject();\n        ois.close();\n        return block;\n    }\n}</code></pre><p>然后是区块链的属性，之前我们使用<code>ArrayList</code>存储区块信息，而现在我们直接将区块序列化到本地，需要哪一个区块直接到本地来取，因此不再需要<code>ArrayList</code>保存区块数据。对于区块链来讲，仅仅需要记录最新区块数据即可。</p>\n<pre><code>\npublic final class Blockchain {\n    ...\n    //Arraylist&lt;Block&gt; block修改为 Block block;\n    public Block block;\n    ...\n    public static Blockchain getInstance() {\n        if (BC == null) {\n            synchronized (Blockchain.class) {\n                if (BC == null) {\n                    BC = new Blockchain();\n                    //删除创建ArrayList\n                }\n            }\n        }\n        return BC;\n    }\n\n    public Block CrtGenesisBlock() throws IOException {\n        ...\n        block.setCurBlockHash(hash);\n        //序列化\n        Storage.Serialize(block);\n        this.block=block;\n        return this.block;\n    }\n    public Block addBlock(String data) throws IOException {\n        int num = this.block.getBlkNum();\n        ...\n        block.setCurBlockHash(hash);\n        //序列化\n        Storage.Serialize(block);\n        this.block = block;\n        return this.block;\n    }\n}</code></pre><p>测试一下:</p>\n<pre><code>public class Test {\n    public static void main(String[] args) throws IOException {\n        System.out.println(Blockchain.getInstance().CrtGenesisBlock().toString());\n        System.out.println(Blockchain.getInstance().addBlock(&quot;Block 2&quot;).toString());\n    }\n}</code></pre><p>存储是没有问题的，在<code>resources/blocks/</code>文件下成功生成了<code>1.block,2.block</code>两个文件。</p>\n<h4 id=\"反序列化\"><a href=\"#反序列化\" class=\"headerlink\" title=\"反序列化\"></a>反序列化</h4><p>但是还没有完成从本地取数据的操作，接下来的流程是这样子的:<br>启动程序后，首先实例化<code>Blockchain</code>的实例，然后从本地读取数据，如果本地存在区块数据，直接反序列化区块号最大的区块，如果本地没有数据，则进行创始区块的创建。</p>\n<pre><code>#Blockchain.java\npublic Block getLastBlock() throws FileNotFoundException, ClassNotFoundException, IOException {\n        File file = new File(&quot;src/main/resources/blocks&quot;);\n        String[] files = file.list();\n        if(files.length!=0){\n            int MaxFileNum = 1;\n            //遍历存储区块数据的文件夹，查找区块号最大的区块\n            for(String s:files){\n                int num = Integer.valueOf(s.substring(0, 1));\n                if(num&gt;=MaxFileNum)\n                    MaxFileNum = num;\n            }\n            //反序列化最大区块号的区块\n           return Storage.Deserialize(MaxFileNum);\n        }\n        return null;\n    }</code></pre><p>然后是<code>Blockchain</code>的实例方法，在获取实例时候判断是否需要创建创世区块:</p>\n<pre><code>#Blockchain.java\n    public static Blockchain getInstance() throws FileNotFoundException, ClassNotFoundException, IOException {\n        if (BC == null) {\n            synchronized (Blockchain.class) {\n                if (BC == null) {\n                    BC = new Blockchain();\n                }\n            }\n        }\n        //获取到Blockchain实例后，判断是否存在区块\n        if(BC.block==null){\n            //如果不存在则尝试获取本地区块号最大的区块\n            //如果存在则直接赋值到Blockchain的属性然后返回\n            Block block = BC.getLastBlock();\n            BC.block = block;\n            if(block==null){\n                //如果不存在则生成创世区块\n                BC.CrtGenesisBlock();\n            }\n        }\n        return BC;\n    }\n\n    //因此创建创世区块的方法可以修改为私有的\n    private Block CrtGenesisBlock() throws IOException {\n        ...\n    }</code></pre><p>接下来可以测试了:</p>\n<pre><code>public class Test {\n    public static void main(String[] args) throws IOException, ClassNotFoundException {\n        System.out.println(Blockchain.getInstance().block.toString());\n        System.out.println(Blockchain.getInstance().addBlock(&quot;Block 2&quot;).toString());\n    }\n}</code></pre><p>测试多次可以发现区块并没有重新从创世区块开始生成，而是根据先前生成的区块号继续增长。</p>\n<pre><code>{&quot;blkNum&quot;:1,&quot;curBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;data&quot;:&quot;Genesis Block&quot;,&quot;nonce&quot;:37846,&quot;prevBlockHash&quot;:&quot;00000000000000000&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 11:51:37&quot;}\n{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;nonce&quot;:15318,&quot;prevBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 11:51:37&quot;}\n\nCurrent Last Block num is:2\n{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;nonce&quot;:15318,&quot;prevBlockHash&quot;:&quot;000002278a13f6caefda04c77d35e14128aafbc287578b86e1f2079c0e6747b1&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 11:51:37&quot;}\n{&quot;blkNum&quot;:3,&quot;curBlockHash&quot;:&quot;0000d350c1199eb51c2d43194653f5b44444665e40373d5883edd3567c60cd68&quot;,&quot;data&quot;:&quot;Block 2&quot;,&quot;nonce&quot;:23695,&quot;prevBlockHash&quot;:&quot;00002654109d8eb6092da686d66e70cdb1e26cf4a87e453e3d8e2ff7508f11f9&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 11:51:44&quot;}</code></pre><p>大致工作已完成，接下来添加几个额外的方法:</p>\n<pre><code>#Block.java\n       /**\n     * 是否存在前一个区块\n     */\n    public boolean hasPrevBlock(){\n        if(this.getBlkNum()!=1){\n            return true;\n        }\n        return false;\n    }\n    @Transient\n    @JsonIgnore\n    /**\n     * 获取前一个区块\n     */\n    public Block getPrevBlock() throws FileNotFoundException, ClassNotFoundException, IOException {\n        if(this.hasPrevBlock())\n            return Storage.Deserialize(this.getBlkNum()-1);\n        return null;          \n    }</code></pre><p>后一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12905706.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(三)</a></p>\n<h4 id=\"Github仓库地址在这里，随时保持更新中…\"><a href=\"#Github仓库地址在这里，随时保持更新中…\" class=\"headerlink\" title=\"Github仓库地址在这里，随时保持更新中…..\"></a>Github仓库地址在这里，随时保持更新中…..</h4><p>Github地址：<a href=\"https://github.com/newonexd/Jchain\" target=\"_blank\" rel=\"noopener\">Jchain</a></p>\n"},{"title":"搭建你的第一个区块链网络(三)","date":"2020-05-19T07:01:06.000Z","_content":"前一篇文章: [搭建你的第一个区块链网络(二)](https://www.cnblogs.com/cbkj-xd/p/12904660.html)\n\n\n## 钱包与CLI\n\n### 钱包\n对于区块链系统来说，密码学是必不可少的，因此加密与解密也是核心操作，而密钥通常使用钱包进行保存，这一节我们完成钱包的设计。这一节比较简单。\n在比特币网络中，使用的是非对称加密算法，密钥是通过椭圆曲线算法实现的，而本文中，暂且使用RSA算法进行实现，后期再对椭圆曲线算法进行添加。\n首先是RSA算法的工具类，参考[这里](https://www.jianshu.com/p/aff5492d64f0).整理成以下方法:\n\n```\n#RSAKey.java\n@Getter\n@Setter\npublic final class RSAKey {\n     //非对称密钥算法\n     public static final String KEY_ALGORITHM = \"RSA\";\n     /**\n      * 密钥长度必须是64的倍数，在512到65536位之间\n      */\n     private static final int KEY_SIZE = 512;\n     //公钥\n     private static final String PUBLIC_KEY = \"RSAPublicKey\";\n \n     //私钥\n     private static final String PRIVATE_KEY = \"RSAPrivateKey\";\n    private byte[] privateKey;\n    private byte[] publicKey;\n    private String address;\n    private RSAKey() {\n    }\n    /**\n     * 生成密钥\n     */\n    public static RSAKey GenerateKeyPair() throws Exception {\n        RSAKey key = new RSAKey();\n        Map<String, Object> keyPair = key.initKey();\n        Key pk = (Key) keyPair.get(PRIVATE_KEY);\n        key.setPrivateKey(pk.getEncoded());\n        pk = (Key)keyPair.get(PUBLIC_KEY);\n        key.setPublicKey(pk.getEncoded());\n        return key;\n    }\n    private Map<String, Object> initKey() throws Exception {\n        //实例化密钥生成器\n        KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(KEY_ALGORITHM);\n        //初始化密钥生成器\n        keyPairGenerator.initialize(KEY_SIZE);\n        //生成密钥对\n        KeyPair keyPair = keyPairGenerator.generateKeyPair();\n        //公钥\n        RSAPublicKey publicKey = (RSAPublicKey) keyPair.getPublic();\n        //私钥\n        RSAPrivateKey privateKey = (RSAPrivateKey) keyPair.getPrivate();\n        //将密钥存储在map中\n        Map<String, Object> keyMap = new HashMap<String, Object>();\n        keyMap.put(PUBLIC_KEY, publicKey);\n        keyMap.put(PRIVATE_KEY, privateKey);\n        return keyMap;\n    }\n    /**\n     * 私钥加密\n     *\n     * @param data 待加密数据\n     * @param key       密钥\n     * @return byte[] 加密数据\n     */\n    public static byte[] encryptByPrivateKey(byte[] data,byte[] pk) throws Exception {\n        //取得私钥\n        PKCS8EncodedKeySpec pkcs8KeySpec = new PKCS8EncodedKeySpec(pk);\n        KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM);\n        //生成私钥\n        PrivateKey privateKey = keyFactory.generatePrivate(pkcs8KeySpec);\n        //数据加密\n        Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm());\n        cipher.init(Cipher.ENCRYPT_MODE, privateKey);\n        return cipher.doFinal(data);\n    }\n   /**\n    * 公钥解密\n    *\n    * @param data 待解密数据\n    * @param key  密钥\n    * @return byte[] 解密数据\n    */\n    public static byte[] decryptByPublicKey(byte[] data,byte[] pk) throws Exception {\n        //实例化密钥工厂\n        KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM);\n        //初始化公钥\n        //密钥材料转换\n        X509EncodedKeySpec x509KeySpec = new X509EncodedKeySpec(pk);\n        //产生公钥\n        PublicKey pubKey = keyFactory.generatePublic(x509KeySpec);\n        //数据解密\n        Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm());\n        cipher.init(Cipher.DECRYPT_MODE, pubKey);\n        return cipher.doFinal(data);\n    }\n}\n```\n#### 密钥\n接下来创建钱包实例，对于用户的钱包，需要一下属性:\n\n* **私钥：** 用于加密数据，签名数据。\n* **公钥：** 用于解密数据，验证签名。\n* **地址：** 钱包的地址。\n\n目前，暂时将钱包设置为单例模式，同时一个钱包中只含有一对密钥对:\n\n```\n#Wallet.java\n\n@Getter\n@Setter\npublic class Wallet {\n    //单例模式的Wallet\n    private static Wallet wallet;\n    //私钥\n    private byte[] privateKey;\n    //公钥\n    private byte[] publicKey;\n    //地址\n    private String address;\n    private Wallet() throws Exception {\n        RSAKey key  = RSAKey.GenerateKeyPair();\n        this.privateKey = key.getPrivateKey();\n        this.publicKey = key.getPublicKey();\n        this.address = generateAddress();\n    }\n    public static Wallet getInstance() throws Exception {\n        if (wallet == null) {\n            synchronized (Wallet.class) {\n                if (wallet == null) {\n                    wallet = new Wallet();\n                }\n            }\n        }\n        return wallet;\n    }\n }\n```\n\n接下来是加密与解密操作，只需要包装一下工具类即可:\n```\n#Wallet.java\n    /**\n     * 加密数据\n     */\n    public String encrypt(byte[] data) throws Exception {\n        byte[] encry = RSAKey.encryptByPrivateKey(data,this.privateKey);\n        return Hex.encodeHexString(encry);\n    }\n    /**\n     *  解密数据\n     */\n    public byte[] decrypt(String encry) throws DecoderException, Exception {\n        return RSAKey.decryptByPublicKey(Hex.decodeHex(encry),this.publicKey);\n    }\n```\n还有重要的签名与验证签名的操作:\n签名的本质实际上就是哈希操作+加密操作。\n\n1. 对数据原文进行哈希: **哈希(数据原文)**\n2. 使用私钥对哈希值进行加密: **加密(哈希(数据原文))**\n3. 将数据原文与签名数据进行组合\n数字签名的组成部分为：**数据原文+加密(哈希(数据原文))**\n\n```\n#Wallet.java\n    /**\n     * 签名数据\n     */\n    public String sign(byte[] data) throws Exception {\n        //原文首先进行哈希\n        String hash = Util.getSHA256(\n            Hex.encodeHexString(data));\n        //哈希值进行加密\n        String sign = encrypt(\n            Hex.decodeHex(hash));\n        //原文+encry(hash(原文))\n        return Hex.encodeHexString(data)+\"%%%\"+sign;\n    }\n```\n在这里，我们简单使用``%%%``将两部分数据进行组合。\n至于数字签名的验证则可以分析了：\n\n1. 首先将数字签名拆分为**数据原文  加密(哈希(数据原文))**\n2. 将数据原文进行哈希操作: **哈希(数据原文)**\n3. 使用公钥解密签名信息： **解密->加密(哈希(数据原文))->哈希(数据原文)**\n4. 对两部分哈希值进行对比，相同则说明数字签名是有效的。\n\n```\n#Wallet.java\n    /**\n     * 验证签名\n     */\n    public boolean verify(String data) throws DecoderException, Exception {\n        String[] str = data.split(\"%%%\");\n        // 原文     encry(hash(原文))\n        if(str.length!=2){\n            return false;\n        }\n        String hash = Util.getSHA256(str[0]);\n        String hash2 = Hex.encodeHexString(this.decrypt(str[1]));\n        if(hash.equals(hash2)){\n            return true;\n        }\n        return false;\n    }\n```\n\n#### 地址\n对于钱包地址，比特币是有自己的一套生成地址的算法，步骤相对比较繁杂，在本文中，简单使用哈希操作模拟地址的生成。\n\n```\n#Wallet.java\n    /**\n     * 根据密钥生成地址\n     */\n    private String generateAddress() throws NoSuchAlgorithmException {\n        String pk = Hex.encodeHexString(this.publicKey);\n        this.address = \"R\" + Util.getSHA256(pk) + Util.getSHA256(Util.getSHA256(pk));\n        return this.address;\n    }\n    /**\n     * 获取地址\n     * @return\n     * @throws NoSuchAlgorithmException\n     */\n    public String getAddress() throws NoSuchAlgorithmException {\n        return this.generateAddress();\n    }\n```\n\n所有的一切都已经完成，测试一下:\n\n```\n#KeyTest.java\npublic class KeyTest {\n    public static void main(String[] args) throws DecoderException, Exception {\n        Wallet wallet  = Wallet.getInstance();\n        System.out.println(\"private Key:  \"+Hex.encodeHexString(wallet.getPrivateKey()));\n        System.out.println();\n        System.out.println(\"public Key:  \"+Hex.encodeHexString(wallet.getPublicKey()));\n        System.out.println(\n            wallet.verify(\n                wallet.sign(\"test\".getBytes())));\n        System.out.println(\"address: \"+wallet.getAddress());\n    }\n}\n```\n\n看起来一切都没有问题:\n\n```\nprivate Key:  30820156020100300d06092a864886f70d0101010500048201403082013c020100024100b204075a20a86a8773681a2bee6574a68d1028516577c80f22d1f693dbc1c70cca59d95a74b8c7a55c3e02801ebdb025272f1df18ca862701b640a6bc444b7e50203010001024066a67a12d7a8261dcb47a967d1c5813995384ef778da546b9df993057a0048a5b9e2f3986bef45bbcffc13baff6a93b31b054ecd6f23ad9c23a088597bc169b5022100e210191df6e5661b7fbe239866110bc54ace03e22d9e242d199b0f95d42c3e7f022100c9970dbe3640ad34633cb1a3defa5fc4be1dd9881eb65ff19d53d0ae2c569f9b022100c46a544872b2926b262ca064d399cfee55b6762d589164c142d435506b0f1e25022100a65a09543aaeda7f4d98eb3a4029ba57bf4f20904c4fd112aff25755336f741b022100dbe2e256464346e26c134395aada2bd669f72700b146b494920e9c75df12403f\n\npublic Key:  305c300d06092a864886f70d0101010500034b003048024100b204075a20a86a8773681a2bee6574a68d1028516577c80f22d1f693dbc1c70cca59d95a74b8c7a55c3e02801ebdb025272f1df18ca862701b640a6bc444b7e50203010001\ntrue\naddress: R92439f4d205def0794e23f626cf61013d04ccf1fdf9106ff78ca3ec30f7bc7cad4cdc346ee44501831c67085a54463e4ffd774654a2bd9328a382652de663f1a\n```\n\n### Cli\n\n到此为止我们开发了区块链系统的部分功能，距离完成还有很长一段距离。不过，先完成一个比较小的功能好了，即与用户进行交互的操作。之前考虑过使用``curl``，不过看到了``apache``的``cli``工具，所以决定使用这个了。其实就是一种命令行解析工具，根据输入的命令解析后执行对应的功能。参考[这里](https://www.ibm.com/developerworks/cn/java/j-lo-commonscli/)\n所以需要在``pom.xml``添加一下字段导包:\n\n```\n        <dependency>\n            <groupId>commons-cli</groupId>\n            <artifactId>commons-cli</artifactId>\n            <version>1.2</version>\n        </dependency>\n```\n\n整个步骤分为三个阶段: **定义，解析，询问**。\n\n#### 定义\n首先需要定义一些参数信息，作为应用程序的接口。\n\n```\n        //创建Options对象\n        Options options = new Options();\n        //添加-h参数。 h为参数简单形式，help为参数复杂形式，false定义该参数不需要额外的输入，Print help为参数的介绍\n        Option opt = new Option(\"h\", \"help\", false, \"Print help\");\n        //定义该参数是否为必须的\n        opt.setRequired(false);\n        options.addOption(opt);\n```\n\n#### 解析\n由CLi对用户输入的命令行进行解析。\n\n```\n        HelpFormatter hf = new HelpFormatter();\n        hf.setWidth(110);\n        CommandLine commandLine = null;\n        CommandLineParser parser = new PosixParser();\n        try {\n            commandLine = parser.parse(options, args);\n            //如果含有参数h，则打印帮助信息\n            if (commandLine.hasOption('h')) {\n                // 打印使用帮助\n                hf.printHelp(\"Jchain\", options, true);\n            }\n            ...\n        }catch(Exception e){\n        }\n```\n\n``CommandLineParser`` 类中定义的 ``parse`` 方法将用 CLI 定义阶段中产生的 ``Options`` 实例和一组字符串作为输入，并返回解析后生成的 ``CommandLine``。\nCLI 解析阶段的目标结果就是创建`` CommandLine`` 实例。\n\n#### 询问\n\n该阶段是根据输入的参数决定进入哪一个逻辑分支中。\n\n```\n            Option[] opts = commandLine.getOptions();\n            if (opts != null) {\n                for (Option opt1 : opts) {\n                    //name为参数名称\n                    String name = opt1.getLongOpt();\n                    //如果有额外的参数则传入value中\n                    String value = commandLine.getOptionValue(name);\n                    //...根据name指定具体的逻辑分支\n                }\n            }\n```\n\n分析完了，然后制定需要的参数好了:\n这里指定了三个参数:``s,a,w``，分别为获取区块链实例，添加区块以及获取钱包实例的功能。\n```\n        opt = new Option(\"s\", \"start\", false, \"start blockchain\");\n        opt.setRequired(false);\n        options.addOption(opt);\n        opt = new Option(\"a\", \"add\", true, \"add block\");\n        opt.setRequired(false);\n        options.addOption(opt);\n        opt = new Option(\"w\", \"wallet\", false, \"init wallet\");\n        opt.setRequired(false);\n        options.addOption(opt);\n```\n\n然后是具体的逻辑分支:\n\n```\n        if (name.equals(\"s\") || name.equals(\"start\")) {\n            System.out.println(Blockchain.getInstance().block.toString());\n        }\n        if(name.equals(\"a\")||name.equals(\"add\")&&value!=\"\"){\n            System.out.println(Blockchain.getInstance().addBlock(value).toString());\n        } \n        if(name.equals(\"w\")||name.equals(\"wallet\")){\n            Wallet wallet = Wallet.getInstance();\n            System.out.println(\"private Key:  \"+Hex.encodeHexString(wallet.getPrivateKey()));\n            System.out.println();\n            System.out.println(\"public Key:  \"+Hex.encodeHexString(wallet.getPublicKey()));\n        } \n```\n简单测试一下是否正常工作:\n\n```\n#CliTest.java\npublic class CliTest {\n    public static void main(String[] args){\n        String[] str = {\"-s\",\"-w\",\"-a\",\"block\"};\n        // String[] str = {\"-h\"};\n        Cli.Start(str);\n    }\n}\n```\n\n看起来没有问题，获得了高度为4的区块链实例。也成功创建了钱包打印出公私钥。\n最后生成了高度为5的区块 区块信息为我们输入的\"block\"。\n```\nCurrent Last Block num is:4\n{\"blkNum\":4,\"curBlockHash\":\"0000287895ae8f4e4fc781137adee2b2fd0da4d7be3abb68c04507979157eb70\",\"data\":\"block\",\"nonce\":121263,\"prevBlockHash\":\"00003ae4ca11f2dd6262d9218ffe6a98416b4e9e2ad789b39aef74b383cc96a6\",\"timeStamp\":\"2020-05-17 14:28:16\"}\nprivate Key:  30820154020100300d06092a864886f70d01010105000482013e3082013a02010002410082a46b7f68d835b5e8047b8794cfd4d5daf4b6f3d8258a8a78f670052f35bab562f52aa7a6eeeb69bc14e03f0d8019db5b754d68e8d0918aa6f2f4b636ba0f070203010001024009de8ffc6d18405e80abae055d19a253919a012444c4f94562c4034c70f79726372e85f8853b9093e984b2fee8d828cf6078b2b66239e5871e299985a9b85ea1022100bed5f1748ffeabd423e7518c2c9840d9299f5190f3e482b0ec50ae203cd25b17022100af409e3b74a06c4937f8ca3bc12776ff21217750a4b5e1d02de71d1a7ceda19102202085d3959ae8bb1df75477d85ccd41d800b8ef2cb5f40eb5da4051bc9ac0fad702206342871c87b6e0fe2b6c872687051239f88adae85b12051f0310b6842d23ee710221008ebb60975fc2ee07e94242da08e0cb81478b7c57091e20e2177aa325883e4714\n\npublic Key:  305c300d06092a864886f70d0101010500034b00304802410082a46b7f68d835b5e8047b8794cfd4d5daf4b6f3d8258a8a78f670052f35bab562f52aa7a6eeeb69bc14e03f0d8019db5b754d68e8d0918aa6f2f4b636ba0f070203010001\n{\"blkNum\":5,\"curBlockHash\":\"000047e8fc13a5fe0404aeca104f8624581738361f12f2a8c07c4f172dde62cc\",\"data\":\"block\",\"nonce\":67701,\"prevBlockHash\":\"0000287895ae8f4e4fc781137adee2b2fd0da4d7be3abb68c04507979157eb70\",\"timeStamp\":\"2020-05-17 16:27:29\"}\n```\n\n后一篇文章: [搭建你的第一个区块链网络(二)](https://www.cnblogs.com/cbkj-xd/p/12910299.html)\n\n#### Github仓库地址在这里，随时保持更新中.....\nGithub地址：[Jchain](https://github.com/newonexd/Jchain)","source":"_posts/blog/blockchain/Jchain3.md","raw":"---\ntitle: 搭建你的第一个区块链网络(三)\ndate: 2020-05-19 15:01:06\ntags: blockchain\n---\n前一篇文章: [搭建你的第一个区块链网络(二)](https://www.cnblogs.com/cbkj-xd/p/12904660.html)\n\n\n## 钱包与CLI\n\n### 钱包\n对于区块链系统来说，密码学是必不可少的，因此加密与解密也是核心操作，而密钥通常使用钱包进行保存，这一节我们完成钱包的设计。这一节比较简单。\n在比特币网络中，使用的是非对称加密算法，密钥是通过椭圆曲线算法实现的，而本文中，暂且使用RSA算法进行实现，后期再对椭圆曲线算法进行添加。\n首先是RSA算法的工具类，参考[这里](https://www.jianshu.com/p/aff5492d64f0).整理成以下方法:\n\n```\n#RSAKey.java\n@Getter\n@Setter\npublic final class RSAKey {\n     //非对称密钥算法\n     public static final String KEY_ALGORITHM = \"RSA\";\n     /**\n      * 密钥长度必须是64的倍数，在512到65536位之间\n      */\n     private static final int KEY_SIZE = 512;\n     //公钥\n     private static final String PUBLIC_KEY = \"RSAPublicKey\";\n \n     //私钥\n     private static final String PRIVATE_KEY = \"RSAPrivateKey\";\n    private byte[] privateKey;\n    private byte[] publicKey;\n    private String address;\n    private RSAKey() {\n    }\n    /**\n     * 生成密钥\n     */\n    public static RSAKey GenerateKeyPair() throws Exception {\n        RSAKey key = new RSAKey();\n        Map<String, Object> keyPair = key.initKey();\n        Key pk = (Key) keyPair.get(PRIVATE_KEY);\n        key.setPrivateKey(pk.getEncoded());\n        pk = (Key)keyPair.get(PUBLIC_KEY);\n        key.setPublicKey(pk.getEncoded());\n        return key;\n    }\n    private Map<String, Object> initKey() throws Exception {\n        //实例化密钥生成器\n        KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(KEY_ALGORITHM);\n        //初始化密钥生成器\n        keyPairGenerator.initialize(KEY_SIZE);\n        //生成密钥对\n        KeyPair keyPair = keyPairGenerator.generateKeyPair();\n        //公钥\n        RSAPublicKey publicKey = (RSAPublicKey) keyPair.getPublic();\n        //私钥\n        RSAPrivateKey privateKey = (RSAPrivateKey) keyPair.getPrivate();\n        //将密钥存储在map中\n        Map<String, Object> keyMap = new HashMap<String, Object>();\n        keyMap.put(PUBLIC_KEY, publicKey);\n        keyMap.put(PRIVATE_KEY, privateKey);\n        return keyMap;\n    }\n    /**\n     * 私钥加密\n     *\n     * @param data 待加密数据\n     * @param key       密钥\n     * @return byte[] 加密数据\n     */\n    public static byte[] encryptByPrivateKey(byte[] data,byte[] pk) throws Exception {\n        //取得私钥\n        PKCS8EncodedKeySpec pkcs8KeySpec = new PKCS8EncodedKeySpec(pk);\n        KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM);\n        //生成私钥\n        PrivateKey privateKey = keyFactory.generatePrivate(pkcs8KeySpec);\n        //数据加密\n        Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm());\n        cipher.init(Cipher.ENCRYPT_MODE, privateKey);\n        return cipher.doFinal(data);\n    }\n   /**\n    * 公钥解密\n    *\n    * @param data 待解密数据\n    * @param key  密钥\n    * @return byte[] 解密数据\n    */\n    public static byte[] decryptByPublicKey(byte[] data,byte[] pk) throws Exception {\n        //实例化密钥工厂\n        KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM);\n        //初始化公钥\n        //密钥材料转换\n        X509EncodedKeySpec x509KeySpec = new X509EncodedKeySpec(pk);\n        //产生公钥\n        PublicKey pubKey = keyFactory.generatePublic(x509KeySpec);\n        //数据解密\n        Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm());\n        cipher.init(Cipher.DECRYPT_MODE, pubKey);\n        return cipher.doFinal(data);\n    }\n}\n```\n#### 密钥\n接下来创建钱包实例，对于用户的钱包，需要一下属性:\n\n* **私钥：** 用于加密数据，签名数据。\n* **公钥：** 用于解密数据，验证签名。\n* **地址：** 钱包的地址。\n\n目前，暂时将钱包设置为单例模式，同时一个钱包中只含有一对密钥对:\n\n```\n#Wallet.java\n\n@Getter\n@Setter\npublic class Wallet {\n    //单例模式的Wallet\n    private static Wallet wallet;\n    //私钥\n    private byte[] privateKey;\n    //公钥\n    private byte[] publicKey;\n    //地址\n    private String address;\n    private Wallet() throws Exception {\n        RSAKey key  = RSAKey.GenerateKeyPair();\n        this.privateKey = key.getPrivateKey();\n        this.publicKey = key.getPublicKey();\n        this.address = generateAddress();\n    }\n    public static Wallet getInstance() throws Exception {\n        if (wallet == null) {\n            synchronized (Wallet.class) {\n                if (wallet == null) {\n                    wallet = new Wallet();\n                }\n            }\n        }\n        return wallet;\n    }\n }\n```\n\n接下来是加密与解密操作，只需要包装一下工具类即可:\n```\n#Wallet.java\n    /**\n     * 加密数据\n     */\n    public String encrypt(byte[] data) throws Exception {\n        byte[] encry = RSAKey.encryptByPrivateKey(data,this.privateKey);\n        return Hex.encodeHexString(encry);\n    }\n    /**\n     *  解密数据\n     */\n    public byte[] decrypt(String encry) throws DecoderException, Exception {\n        return RSAKey.decryptByPublicKey(Hex.decodeHex(encry),this.publicKey);\n    }\n```\n还有重要的签名与验证签名的操作:\n签名的本质实际上就是哈希操作+加密操作。\n\n1. 对数据原文进行哈希: **哈希(数据原文)**\n2. 使用私钥对哈希值进行加密: **加密(哈希(数据原文))**\n3. 将数据原文与签名数据进行组合\n数字签名的组成部分为：**数据原文+加密(哈希(数据原文))**\n\n```\n#Wallet.java\n    /**\n     * 签名数据\n     */\n    public String sign(byte[] data) throws Exception {\n        //原文首先进行哈希\n        String hash = Util.getSHA256(\n            Hex.encodeHexString(data));\n        //哈希值进行加密\n        String sign = encrypt(\n            Hex.decodeHex(hash));\n        //原文+encry(hash(原文))\n        return Hex.encodeHexString(data)+\"%%%\"+sign;\n    }\n```\n在这里，我们简单使用``%%%``将两部分数据进行组合。\n至于数字签名的验证则可以分析了：\n\n1. 首先将数字签名拆分为**数据原文  加密(哈希(数据原文))**\n2. 将数据原文进行哈希操作: **哈希(数据原文)**\n3. 使用公钥解密签名信息： **解密->加密(哈希(数据原文))->哈希(数据原文)**\n4. 对两部分哈希值进行对比，相同则说明数字签名是有效的。\n\n```\n#Wallet.java\n    /**\n     * 验证签名\n     */\n    public boolean verify(String data) throws DecoderException, Exception {\n        String[] str = data.split(\"%%%\");\n        // 原文     encry(hash(原文))\n        if(str.length!=2){\n            return false;\n        }\n        String hash = Util.getSHA256(str[0]);\n        String hash2 = Hex.encodeHexString(this.decrypt(str[1]));\n        if(hash.equals(hash2)){\n            return true;\n        }\n        return false;\n    }\n```\n\n#### 地址\n对于钱包地址，比特币是有自己的一套生成地址的算法，步骤相对比较繁杂，在本文中，简单使用哈希操作模拟地址的生成。\n\n```\n#Wallet.java\n    /**\n     * 根据密钥生成地址\n     */\n    private String generateAddress() throws NoSuchAlgorithmException {\n        String pk = Hex.encodeHexString(this.publicKey);\n        this.address = \"R\" + Util.getSHA256(pk) + Util.getSHA256(Util.getSHA256(pk));\n        return this.address;\n    }\n    /**\n     * 获取地址\n     * @return\n     * @throws NoSuchAlgorithmException\n     */\n    public String getAddress() throws NoSuchAlgorithmException {\n        return this.generateAddress();\n    }\n```\n\n所有的一切都已经完成，测试一下:\n\n```\n#KeyTest.java\npublic class KeyTest {\n    public static void main(String[] args) throws DecoderException, Exception {\n        Wallet wallet  = Wallet.getInstance();\n        System.out.println(\"private Key:  \"+Hex.encodeHexString(wallet.getPrivateKey()));\n        System.out.println();\n        System.out.println(\"public Key:  \"+Hex.encodeHexString(wallet.getPublicKey()));\n        System.out.println(\n            wallet.verify(\n                wallet.sign(\"test\".getBytes())));\n        System.out.println(\"address: \"+wallet.getAddress());\n    }\n}\n```\n\n看起来一切都没有问题:\n\n```\nprivate Key:  30820156020100300d06092a864886f70d0101010500048201403082013c020100024100b204075a20a86a8773681a2bee6574a68d1028516577c80f22d1f693dbc1c70cca59d95a74b8c7a55c3e02801ebdb025272f1df18ca862701b640a6bc444b7e50203010001024066a67a12d7a8261dcb47a967d1c5813995384ef778da546b9df993057a0048a5b9e2f3986bef45bbcffc13baff6a93b31b054ecd6f23ad9c23a088597bc169b5022100e210191df6e5661b7fbe239866110bc54ace03e22d9e242d199b0f95d42c3e7f022100c9970dbe3640ad34633cb1a3defa5fc4be1dd9881eb65ff19d53d0ae2c569f9b022100c46a544872b2926b262ca064d399cfee55b6762d589164c142d435506b0f1e25022100a65a09543aaeda7f4d98eb3a4029ba57bf4f20904c4fd112aff25755336f741b022100dbe2e256464346e26c134395aada2bd669f72700b146b494920e9c75df12403f\n\npublic Key:  305c300d06092a864886f70d0101010500034b003048024100b204075a20a86a8773681a2bee6574a68d1028516577c80f22d1f693dbc1c70cca59d95a74b8c7a55c3e02801ebdb025272f1df18ca862701b640a6bc444b7e50203010001\ntrue\naddress: R92439f4d205def0794e23f626cf61013d04ccf1fdf9106ff78ca3ec30f7bc7cad4cdc346ee44501831c67085a54463e4ffd774654a2bd9328a382652de663f1a\n```\n\n### Cli\n\n到此为止我们开发了区块链系统的部分功能，距离完成还有很长一段距离。不过，先完成一个比较小的功能好了，即与用户进行交互的操作。之前考虑过使用``curl``，不过看到了``apache``的``cli``工具，所以决定使用这个了。其实就是一种命令行解析工具，根据输入的命令解析后执行对应的功能。参考[这里](https://www.ibm.com/developerworks/cn/java/j-lo-commonscli/)\n所以需要在``pom.xml``添加一下字段导包:\n\n```\n        <dependency>\n            <groupId>commons-cli</groupId>\n            <artifactId>commons-cli</artifactId>\n            <version>1.2</version>\n        </dependency>\n```\n\n整个步骤分为三个阶段: **定义，解析，询问**。\n\n#### 定义\n首先需要定义一些参数信息，作为应用程序的接口。\n\n```\n        //创建Options对象\n        Options options = new Options();\n        //添加-h参数。 h为参数简单形式，help为参数复杂形式，false定义该参数不需要额外的输入，Print help为参数的介绍\n        Option opt = new Option(\"h\", \"help\", false, \"Print help\");\n        //定义该参数是否为必须的\n        opt.setRequired(false);\n        options.addOption(opt);\n```\n\n#### 解析\n由CLi对用户输入的命令行进行解析。\n\n```\n        HelpFormatter hf = new HelpFormatter();\n        hf.setWidth(110);\n        CommandLine commandLine = null;\n        CommandLineParser parser = new PosixParser();\n        try {\n            commandLine = parser.parse(options, args);\n            //如果含有参数h，则打印帮助信息\n            if (commandLine.hasOption('h')) {\n                // 打印使用帮助\n                hf.printHelp(\"Jchain\", options, true);\n            }\n            ...\n        }catch(Exception e){\n        }\n```\n\n``CommandLineParser`` 类中定义的 ``parse`` 方法将用 CLI 定义阶段中产生的 ``Options`` 实例和一组字符串作为输入，并返回解析后生成的 ``CommandLine``。\nCLI 解析阶段的目标结果就是创建`` CommandLine`` 实例。\n\n#### 询问\n\n该阶段是根据输入的参数决定进入哪一个逻辑分支中。\n\n```\n            Option[] opts = commandLine.getOptions();\n            if (opts != null) {\n                for (Option opt1 : opts) {\n                    //name为参数名称\n                    String name = opt1.getLongOpt();\n                    //如果有额外的参数则传入value中\n                    String value = commandLine.getOptionValue(name);\n                    //...根据name指定具体的逻辑分支\n                }\n            }\n```\n\n分析完了，然后制定需要的参数好了:\n这里指定了三个参数:``s,a,w``，分别为获取区块链实例，添加区块以及获取钱包实例的功能。\n```\n        opt = new Option(\"s\", \"start\", false, \"start blockchain\");\n        opt.setRequired(false);\n        options.addOption(opt);\n        opt = new Option(\"a\", \"add\", true, \"add block\");\n        opt.setRequired(false);\n        options.addOption(opt);\n        opt = new Option(\"w\", \"wallet\", false, \"init wallet\");\n        opt.setRequired(false);\n        options.addOption(opt);\n```\n\n然后是具体的逻辑分支:\n\n```\n        if (name.equals(\"s\") || name.equals(\"start\")) {\n            System.out.println(Blockchain.getInstance().block.toString());\n        }\n        if(name.equals(\"a\")||name.equals(\"add\")&&value!=\"\"){\n            System.out.println(Blockchain.getInstance().addBlock(value).toString());\n        } \n        if(name.equals(\"w\")||name.equals(\"wallet\")){\n            Wallet wallet = Wallet.getInstance();\n            System.out.println(\"private Key:  \"+Hex.encodeHexString(wallet.getPrivateKey()));\n            System.out.println();\n            System.out.println(\"public Key:  \"+Hex.encodeHexString(wallet.getPublicKey()));\n        } \n```\n简单测试一下是否正常工作:\n\n```\n#CliTest.java\npublic class CliTest {\n    public static void main(String[] args){\n        String[] str = {\"-s\",\"-w\",\"-a\",\"block\"};\n        // String[] str = {\"-h\"};\n        Cli.Start(str);\n    }\n}\n```\n\n看起来没有问题，获得了高度为4的区块链实例。也成功创建了钱包打印出公私钥。\n最后生成了高度为5的区块 区块信息为我们输入的\"block\"。\n```\nCurrent Last Block num is:4\n{\"blkNum\":4,\"curBlockHash\":\"0000287895ae8f4e4fc781137adee2b2fd0da4d7be3abb68c04507979157eb70\",\"data\":\"block\",\"nonce\":121263,\"prevBlockHash\":\"00003ae4ca11f2dd6262d9218ffe6a98416b4e9e2ad789b39aef74b383cc96a6\",\"timeStamp\":\"2020-05-17 14:28:16\"}\nprivate Key:  30820154020100300d06092a864886f70d01010105000482013e3082013a02010002410082a46b7f68d835b5e8047b8794cfd4d5daf4b6f3d8258a8a78f670052f35bab562f52aa7a6eeeb69bc14e03f0d8019db5b754d68e8d0918aa6f2f4b636ba0f070203010001024009de8ffc6d18405e80abae055d19a253919a012444c4f94562c4034c70f79726372e85f8853b9093e984b2fee8d828cf6078b2b66239e5871e299985a9b85ea1022100bed5f1748ffeabd423e7518c2c9840d9299f5190f3e482b0ec50ae203cd25b17022100af409e3b74a06c4937f8ca3bc12776ff21217750a4b5e1d02de71d1a7ceda19102202085d3959ae8bb1df75477d85ccd41d800b8ef2cb5f40eb5da4051bc9ac0fad702206342871c87b6e0fe2b6c872687051239f88adae85b12051f0310b6842d23ee710221008ebb60975fc2ee07e94242da08e0cb81478b7c57091e20e2177aa325883e4714\n\npublic Key:  305c300d06092a864886f70d0101010500034b00304802410082a46b7f68d835b5e8047b8794cfd4d5daf4b6f3d8258a8a78f670052f35bab562f52aa7a6eeeb69bc14e03f0d8019db5b754d68e8d0918aa6f2f4b636ba0f070203010001\n{\"blkNum\":5,\"curBlockHash\":\"000047e8fc13a5fe0404aeca104f8624581738361f12f2a8c07c4f172dde62cc\",\"data\":\"block\",\"nonce\":67701,\"prevBlockHash\":\"0000287895ae8f4e4fc781137adee2b2fd0da4d7be3abb68c04507979157eb70\",\"timeStamp\":\"2020-05-17 16:27:29\"}\n```\n\n后一篇文章: [搭建你的第一个区块链网络(二)](https://www.cnblogs.com/cbkj-xd/p/12910299.html)\n\n#### Github仓库地址在这里，随时保持更新中.....\nGithub地址：[Jchain](https://github.com/newonexd/Jchain)","slug":"blog/blockchain/Jchain3","published":1,"updated":"2020-05-19T07:03:01.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyeu0003k0vqhcaf6aaa","content":"<p>前一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12904660.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(二)</a></p>\n<h2 id=\"钱包与CLI\"><a href=\"#钱包与CLI\" class=\"headerlink\" title=\"钱包与CLI\"></a>钱包与CLI</h2><h3 id=\"钱包\"><a href=\"#钱包\" class=\"headerlink\" title=\"钱包\"></a>钱包</h3><p>对于区块链系统来说，密码学是必不可少的，因此加密与解密也是核心操作，而密钥通常使用钱包进行保存，这一节我们完成钱包的设计。这一节比较简单。<br>在比特币网络中，使用的是非对称加密算法，密钥是通过椭圆曲线算法实现的，而本文中，暂且使用RSA算法进行实现，后期再对椭圆曲线算法进行添加。<br>首先是RSA算法的工具类，参考<a href=\"https://www.jianshu.com/p/aff5492d64f0\" target=\"_blank\" rel=\"noopener\">这里</a>.整理成以下方法:</p>\n<pre><code>#RSAKey.java\n@Getter\n@Setter\npublic final class RSAKey {\n     //非对称密钥算法\n     public static final String KEY_ALGORITHM = &quot;RSA&quot;;\n     /**\n      * 密钥长度必须是64的倍数，在512到65536位之间\n      */\n     private static final int KEY_SIZE = 512;\n     //公钥\n     private static final String PUBLIC_KEY = &quot;RSAPublicKey&quot;;\n\n     //私钥\n     private static final String PRIVATE_KEY = &quot;RSAPrivateKey&quot;;\n    private byte[] privateKey;\n    private byte[] publicKey;\n    private String address;\n    private RSAKey() {\n    }\n    /**\n     * 生成密钥\n     */\n    public static RSAKey GenerateKeyPair() throws Exception {\n        RSAKey key = new RSAKey();\n        Map&lt;String, Object&gt; keyPair = key.initKey();\n        Key pk = (Key) keyPair.get(PRIVATE_KEY);\n        key.setPrivateKey(pk.getEncoded());\n        pk = (Key)keyPair.get(PUBLIC_KEY);\n        key.setPublicKey(pk.getEncoded());\n        return key;\n    }\n    private Map&lt;String, Object&gt; initKey() throws Exception {\n        //实例化密钥生成器\n        KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(KEY_ALGORITHM);\n        //初始化密钥生成器\n        keyPairGenerator.initialize(KEY_SIZE);\n        //生成密钥对\n        KeyPair keyPair = keyPairGenerator.generateKeyPair();\n        //公钥\n        RSAPublicKey publicKey = (RSAPublicKey) keyPair.getPublic();\n        //私钥\n        RSAPrivateKey privateKey = (RSAPrivateKey) keyPair.getPrivate();\n        //将密钥存储在map中\n        Map&lt;String, Object&gt; keyMap = new HashMap&lt;String, Object&gt;();\n        keyMap.put(PUBLIC_KEY, publicKey);\n        keyMap.put(PRIVATE_KEY, privateKey);\n        return keyMap;\n    }\n    /**\n     * 私钥加密\n     *\n     * @param data 待加密数据\n     * @param key       密钥\n     * @return byte[] 加密数据\n     */\n    public static byte[] encryptByPrivateKey(byte[] data,byte[] pk) throws Exception {\n        //取得私钥\n        PKCS8EncodedKeySpec pkcs8KeySpec = new PKCS8EncodedKeySpec(pk);\n        KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM);\n        //生成私钥\n        PrivateKey privateKey = keyFactory.generatePrivate(pkcs8KeySpec);\n        //数据加密\n        Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm());\n        cipher.init(Cipher.ENCRYPT_MODE, privateKey);\n        return cipher.doFinal(data);\n    }\n   /**\n    * 公钥解密\n    *\n    * @param data 待解密数据\n    * @param key  密钥\n    * @return byte[] 解密数据\n    */\n    public static byte[] decryptByPublicKey(byte[] data,byte[] pk) throws Exception {\n        //实例化密钥工厂\n        KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM);\n        //初始化公钥\n        //密钥材料转换\n        X509EncodedKeySpec x509KeySpec = new X509EncodedKeySpec(pk);\n        //产生公钥\n        PublicKey pubKey = keyFactory.generatePublic(x509KeySpec);\n        //数据解密\n        Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm());\n        cipher.init(Cipher.DECRYPT_MODE, pubKey);\n        return cipher.doFinal(data);\n    }\n}</code></pre><h4 id=\"密钥\"><a href=\"#密钥\" class=\"headerlink\" title=\"密钥\"></a>密钥</h4><p>接下来创建钱包实例，对于用户的钱包，需要一下属性:</p>\n<ul>\n<li><strong>私钥：</strong> 用于加密数据，签名数据。</li>\n<li><strong>公钥：</strong> 用于解密数据，验证签名。</li>\n<li><strong>地址：</strong> 钱包的地址。</li>\n</ul>\n<p>目前，暂时将钱包设置为单例模式，同时一个钱包中只含有一对密钥对:</p>\n<pre><code>#Wallet.java\n\n@Getter\n@Setter\npublic class Wallet {\n    //单例模式的Wallet\n    private static Wallet wallet;\n    //私钥\n    private byte[] privateKey;\n    //公钥\n    private byte[] publicKey;\n    //地址\n    private String address;\n    private Wallet() throws Exception {\n        RSAKey key  = RSAKey.GenerateKeyPair();\n        this.privateKey = key.getPrivateKey();\n        this.publicKey = key.getPublicKey();\n        this.address = generateAddress();\n    }\n    public static Wallet getInstance() throws Exception {\n        if (wallet == null) {\n            synchronized (Wallet.class) {\n                if (wallet == null) {\n                    wallet = new Wallet();\n                }\n            }\n        }\n        return wallet;\n    }\n }</code></pre><p>接下来是加密与解密操作，只需要包装一下工具类即可:</p>\n<pre><code>#Wallet.java\n    /**\n     * 加密数据\n     */\n    public String encrypt(byte[] data) throws Exception {\n        byte[] encry = RSAKey.encryptByPrivateKey(data,this.privateKey);\n        return Hex.encodeHexString(encry);\n    }\n    /**\n     *  解密数据\n     */\n    public byte[] decrypt(String encry) throws DecoderException, Exception {\n        return RSAKey.decryptByPublicKey(Hex.decodeHex(encry),this.publicKey);\n    }</code></pre><p>还有重要的签名与验证签名的操作:<br>签名的本质实际上就是哈希操作+加密操作。</p>\n<ol>\n<li>对数据原文进行哈希: <strong>哈希(数据原文)</strong></li>\n<li>使用私钥对哈希值进行加密: <strong>加密(哈希(数据原文))</strong></li>\n<li>将数据原文与签名数据进行组合<br>数字签名的组成部分为：<strong>数据原文+加密(哈希(数据原文))</strong></li>\n</ol>\n<pre><code>#Wallet.java\n    /**\n     * 签名数据\n     */\n    public String sign(byte[] data) throws Exception {\n        //原文首先进行哈希\n        String hash = Util.getSHA256(\n            Hex.encodeHexString(data));\n        //哈希值进行加密\n        String sign = encrypt(\n            Hex.decodeHex(hash));\n        //原文+encry(hash(原文))\n        return Hex.encodeHexString(data)+&quot;%%%&quot;+sign;\n    }</code></pre><p>在这里，我们简单使用<code>%%%</code>将两部分数据进行组合。<br>至于数字签名的验证则可以分析了：</p>\n<ol>\n<li>首先将数字签名拆分为<strong>数据原文  加密(哈希(数据原文))</strong></li>\n<li>将数据原文进行哈希操作: <strong>哈希(数据原文)</strong></li>\n<li>使用公钥解密签名信息： <strong>解密-&gt;加密(哈希(数据原文))-&gt;哈希(数据原文)</strong></li>\n<li>对两部分哈希值进行对比，相同则说明数字签名是有效的。</li>\n</ol>\n<pre><code>#Wallet.java\n    /**\n     * 验证签名\n     */\n    public boolean verify(String data) throws DecoderException, Exception {\n        String[] str = data.split(&quot;%%%&quot;);\n        // 原文     encry(hash(原文))\n        if(str.length!=2){\n            return false;\n        }\n        String hash = Util.getSHA256(str[0]);\n        String hash2 = Hex.encodeHexString(this.decrypt(str[1]));\n        if(hash.equals(hash2)){\n            return true;\n        }\n        return false;\n    }</code></pre><h4 id=\"地址\"><a href=\"#地址\" class=\"headerlink\" title=\"地址\"></a>地址</h4><p>对于钱包地址，比特币是有自己的一套生成地址的算法，步骤相对比较繁杂，在本文中，简单使用哈希操作模拟地址的生成。</p>\n<pre><code>#Wallet.java\n    /**\n     * 根据密钥生成地址\n     */\n    private String generateAddress() throws NoSuchAlgorithmException {\n        String pk = Hex.encodeHexString(this.publicKey);\n        this.address = &quot;R&quot; + Util.getSHA256(pk) + Util.getSHA256(Util.getSHA256(pk));\n        return this.address;\n    }\n    /**\n     * 获取地址\n     * @return\n     * @throws NoSuchAlgorithmException\n     */\n    public String getAddress() throws NoSuchAlgorithmException {\n        return this.generateAddress();\n    }</code></pre><p>所有的一切都已经完成，测试一下:</p>\n<pre><code>#KeyTest.java\npublic class KeyTest {\n    public static void main(String[] args) throws DecoderException, Exception {\n        Wallet wallet  = Wallet.getInstance();\n        System.out.println(&quot;private Key:  &quot;+Hex.encodeHexString(wallet.getPrivateKey()));\n        System.out.println();\n        System.out.println(&quot;public Key:  &quot;+Hex.encodeHexString(wallet.getPublicKey()));\n        System.out.println(\n            wallet.verify(\n                wallet.sign(&quot;test&quot;.getBytes())));\n        System.out.println(&quot;address: &quot;+wallet.getAddress());\n    }\n}</code></pre><p>看起来一切都没有问题:</p>\n<pre><code>private Key:  30820156020100300d06092a864886f70d0101010500048201403082013c020100024100b204075a20a86a8773681a2bee6574a68d1028516577c80f22d1f693dbc1c70cca59d95a74b8c7a55c3e02801ebdb025272f1df18ca862701b640a6bc444b7e50203010001024066a67a12d7a8261dcb47a967d1c5813995384ef778da546b9df993057a0048a5b9e2f3986bef45bbcffc13baff6a93b31b054ecd6f23ad9c23a088597bc169b5022100e210191df6e5661b7fbe239866110bc54ace03e22d9e242d199b0f95d42c3e7f022100c9970dbe3640ad34633cb1a3defa5fc4be1dd9881eb65ff19d53d0ae2c569f9b022100c46a544872b2926b262ca064d399cfee55b6762d589164c142d435506b0f1e25022100a65a09543aaeda7f4d98eb3a4029ba57bf4f20904c4fd112aff25755336f741b022100dbe2e256464346e26c134395aada2bd669f72700b146b494920e9c75df12403f\n\npublic Key:  305c300d06092a864886f70d0101010500034b003048024100b204075a20a86a8773681a2bee6574a68d1028516577c80f22d1f693dbc1c70cca59d95a74b8c7a55c3e02801ebdb025272f1df18ca862701b640a6bc444b7e50203010001\ntrue\naddress: R92439f4d205def0794e23f626cf61013d04ccf1fdf9106ff78ca3ec30f7bc7cad4cdc346ee44501831c67085a54463e4ffd774654a2bd9328a382652de663f1a</code></pre><h3 id=\"Cli\"><a href=\"#Cli\" class=\"headerlink\" title=\"Cli\"></a>Cli</h3><p>到此为止我们开发了区块链系统的部分功能，距离完成还有很长一段距离。不过，先完成一个比较小的功能好了，即与用户进行交互的操作。之前考虑过使用<code>curl</code>，不过看到了<code>apache</code>的<code>cli</code>工具，所以决定使用这个了。其实就是一种命令行解析工具，根据输入的命令解析后执行对应的功能。参考<a href=\"https://www.ibm.com/developerworks/cn/java/j-lo-commonscli/\" target=\"_blank\" rel=\"noopener\">这里</a><br>所以需要在<code>pom.xml</code>添加一下字段导包:</p>\n<pre><code>        &lt;dependency&gt;\n            &lt;groupId&gt;commons-cli&lt;/groupId&gt;\n            &lt;artifactId&gt;commons-cli&lt;/artifactId&gt;\n            &lt;version&gt;1.2&lt;/version&gt;\n        &lt;/dependency&gt;</code></pre><p>整个步骤分为三个阶段: <strong>定义，解析，询问</strong>。</p>\n<h4 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h4><p>首先需要定义一些参数信息，作为应用程序的接口。</p>\n<pre><code>        //创建Options对象\n        Options options = new Options();\n        //添加-h参数。 h为参数简单形式，help为参数复杂形式，false定义该参数不需要额外的输入，Print help为参数的介绍\n        Option opt = new Option(&quot;h&quot;, &quot;help&quot;, false, &quot;Print help&quot;);\n        //定义该参数是否为必须的\n        opt.setRequired(false);\n        options.addOption(opt);</code></pre><h4 id=\"解析\"><a href=\"#解析\" class=\"headerlink\" title=\"解析\"></a>解析</h4><p>由CLi对用户输入的命令行进行解析。</p>\n<pre><code>        HelpFormatter hf = new HelpFormatter();\n        hf.setWidth(110);\n        CommandLine commandLine = null;\n        CommandLineParser parser = new PosixParser();\n        try {\n            commandLine = parser.parse(options, args);\n            //如果含有参数h，则打印帮助信息\n            if (commandLine.hasOption(&#39;h&#39;)) {\n                // 打印使用帮助\n                hf.printHelp(&quot;Jchain&quot;, options, true);\n            }\n            ...\n        }catch(Exception e){\n        }</code></pre><p><code>CommandLineParser</code> 类中定义的 <code>parse</code> 方法将用 CLI 定义阶段中产生的 <code>Options</code> 实例和一组字符串作为输入，并返回解析后生成的 <code>CommandLine</code>。<br>CLI 解析阶段的目标结果就是创建<code>CommandLine</code> 实例。</p>\n<h4 id=\"询问\"><a href=\"#询问\" class=\"headerlink\" title=\"询问\"></a>询问</h4><p>该阶段是根据输入的参数决定进入哪一个逻辑分支中。</p>\n<pre><code>            Option[] opts = commandLine.getOptions();\n            if (opts != null) {\n                for (Option opt1 : opts) {\n                    //name为参数名称\n                    String name = opt1.getLongOpt();\n                    //如果有额外的参数则传入value中\n                    String value = commandLine.getOptionValue(name);\n                    //...根据name指定具体的逻辑分支\n                }\n            }</code></pre><p>分析完了，然后制定需要的参数好了:<br>这里指定了三个参数:<code>s,a,w</code>，分别为获取区块链实例，添加区块以及获取钱包实例的功能。</p>\n<pre><code>        opt = new Option(&quot;s&quot;, &quot;start&quot;, false, &quot;start blockchain&quot;);\n        opt.setRequired(false);\n        options.addOption(opt);\n        opt = new Option(&quot;a&quot;, &quot;add&quot;, true, &quot;add block&quot;);\n        opt.setRequired(false);\n        options.addOption(opt);\n        opt = new Option(&quot;w&quot;, &quot;wallet&quot;, false, &quot;init wallet&quot;);\n        opt.setRequired(false);\n        options.addOption(opt);</code></pre><p>然后是具体的逻辑分支:</p>\n<pre><code>        if (name.equals(&quot;s&quot;) || name.equals(&quot;start&quot;)) {\n            System.out.println(Blockchain.getInstance().block.toString());\n        }\n        if(name.equals(&quot;a&quot;)||name.equals(&quot;add&quot;)&amp;&amp;value!=&quot;&quot;){\n            System.out.println(Blockchain.getInstance().addBlock(value).toString());\n        } \n        if(name.equals(&quot;w&quot;)||name.equals(&quot;wallet&quot;)){\n            Wallet wallet = Wallet.getInstance();\n            System.out.println(&quot;private Key:  &quot;+Hex.encodeHexString(wallet.getPrivateKey()));\n            System.out.println();\n            System.out.println(&quot;public Key:  &quot;+Hex.encodeHexString(wallet.getPublicKey()));\n        } </code></pre><p>简单测试一下是否正常工作:</p>\n<pre><code>#CliTest.java\npublic class CliTest {\n    public static void main(String[] args){\n        String[] str = {&quot;-s&quot;,&quot;-w&quot;,&quot;-a&quot;,&quot;block&quot;};\n        // String[] str = {&quot;-h&quot;};\n        Cli.Start(str);\n    }\n}</code></pre><p>看起来没有问题，获得了高度为4的区块链实例。也成功创建了钱包打印出公私钥。<br>最后生成了高度为5的区块 区块信息为我们输入的”block”。</p>\n<pre><code>Current Last Block num is:4\n{&quot;blkNum&quot;:4,&quot;curBlockHash&quot;:&quot;0000287895ae8f4e4fc781137adee2b2fd0da4d7be3abb68c04507979157eb70&quot;,&quot;data&quot;:&quot;block&quot;,&quot;nonce&quot;:121263,&quot;prevBlockHash&quot;:&quot;00003ae4ca11f2dd6262d9218ffe6a98416b4e9e2ad789b39aef74b383cc96a6&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 14:28:16&quot;}\nprivate Key:  30820154020100300d06092a864886f70d01010105000482013e3082013a02010002410082a46b7f68d835b5e8047b8794cfd4d5daf4b6f3d8258a8a78f670052f35bab562f52aa7a6eeeb69bc14e03f0d8019db5b754d68e8d0918aa6f2f4b636ba0f070203010001024009de8ffc6d18405e80abae055d19a253919a012444c4f94562c4034c70f79726372e85f8853b9093e984b2fee8d828cf6078b2b66239e5871e299985a9b85ea1022100bed5f1748ffeabd423e7518c2c9840d9299f5190f3e482b0ec50ae203cd25b17022100af409e3b74a06c4937f8ca3bc12776ff21217750a4b5e1d02de71d1a7ceda19102202085d3959ae8bb1df75477d85ccd41d800b8ef2cb5f40eb5da4051bc9ac0fad702206342871c87b6e0fe2b6c872687051239f88adae85b12051f0310b6842d23ee710221008ebb60975fc2ee07e94242da08e0cb81478b7c57091e20e2177aa325883e4714\n\npublic Key:  305c300d06092a864886f70d0101010500034b00304802410082a46b7f68d835b5e8047b8794cfd4d5daf4b6f3d8258a8a78f670052f35bab562f52aa7a6eeeb69bc14e03f0d8019db5b754d68e8d0918aa6f2f4b636ba0f070203010001\n{&quot;blkNum&quot;:5,&quot;curBlockHash&quot;:&quot;000047e8fc13a5fe0404aeca104f8624581738361f12f2a8c07c4f172dde62cc&quot;,&quot;data&quot;:&quot;block&quot;,&quot;nonce&quot;:67701,&quot;prevBlockHash&quot;:&quot;0000287895ae8f4e4fc781137adee2b2fd0da4d7be3abb68c04507979157eb70&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 16:27:29&quot;}</code></pre><p>后一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12910299.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(二)</a></p>\n<h4 id=\"Github仓库地址在这里，随时保持更新中…\"><a href=\"#Github仓库地址在这里，随时保持更新中…\" class=\"headerlink\" title=\"Github仓库地址在这里，随时保持更新中…..\"></a>Github仓库地址在这里，随时保持更新中…..</h4><p>Github地址：<a href=\"https://github.com/newonexd/Jchain\" target=\"_blank\" rel=\"noopener\">Jchain</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>前一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12904660.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(二)</a></p>\n<h2 id=\"钱包与CLI\"><a href=\"#钱包与CLI\" class=\"headerlink\" title=\"钱包与CLI\"></a>钱包与CLI</h2><h3 id=\"钱包\"><a href=\"#钱包\" class=\"headerlink\" title=\"钱包\"></a>钱包</h3><p>对于区块链系统来说，密码学是必不可少的，因此加密与解密也是核心操作，而密钥通常使用钱包进行保存，这一节我们完成钱包的设计。这一节比较简单。<br>在比特币网络中，使用的是非对称加密算法，密钥是通过椭圆曲线算法实现的，而本文中，暂且使用RSA算法进行实现，后期再对椭圆曲线算法进行添加。<br>首先是RSA算法的工具类，参考<a href=\"https://www.jianshu.com/p/aff5492d64f0\" target=\"_blank\" rel=\"noopener\">这里</a>.整理成以下方法:</p>\n<pre><code>#RSAKey.java\n@Getter\n@Setter\npublic final class RSAKey {\n     //非对称密钥算法\n     public static final String KEY_ALGORITHM = &quot;RSA&quot;;\n     /**\n      * 密钥长度必须是64的倍数，在512到65536位之间\n      */\n     private static final int KEY_SIZE = 512;\n     //公钥\n     private static final String PUBLIC_KEY = &quot;RSAPublicKey&quot;;\n\n     //私钥\n     private static final String PRIVATE_KEY = &quot;RSAPrivateKey&quot;;\n    private byte[] privateKey;\n    private byte[] publicKey;\n    private String address;\n    private RSAKey() {\n    }\n    /**\n     * 生成密钥\n     */\n    public static RSAKey GenerateKeyPair() throws Exception {\n        RSAKey key = new RSAKey();\n        Map&lt;String, Object&gt; keyPair = key.initKey();\n        Key pk = (Key) keyPair.get(PRIVATE_KEY);\n        key.setPrivateKey(pk.getEncoded());\n        pk = (Key)keyPair.get(PUBLIC_KEY);\n        key.setPublicKey(pk.getEncoded());\n        return key;\n    }\n    private Map&lt;String, Object&gt; initKey() throws Exception {\n        //实例化密钥生成器\n        KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(KEY_ALGORITHM);\n        //初始化密钥生成器\n        keyPairGenerator.initialize(KEY_SIZE);\n        //生成密钥对\n        KeyPair keyPair = keyPairGenerator.generateKeyPair();\n        //公钥\n        RSAPublicKey publicKey = (RSAPublicKey) keyPair.getPublic();\n        //私钥\n        RSAPrivateKey privateKey = (RSAPrivateKey) keyPair.getPrivate();\n        //将密钥存储在map中\n        Map&lt;String, Object&gt; keyMap = new HashMap&lt;String, Object&gt;();\n        keyMap.put(PUBLIC_KEY, publicKey);\n        keyMap.put(PRIVATE_KEY, privateKey);\n        return keyMap;\n    }\n    /**\n     * 私钥加密\n     *\n     * @param data 待加密数据\n     * @param key       密钥\n     * @return byte[] 加密数据\n     */\n    public static byte[] encryptByPrivateKey(byte[] data,byte[] pk) throws Exception {\n        //取得私钥\n        PKCS8EncodedKeySpec pkcs8KeySpec = new PKCS8EncodedKeySpec(pk);\n        KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM);\n        //生成私钥\n        PrivateKey privateKey = keyFactory.generatePrivate(pkcs8KeySpec);\n        //数据加密\n        Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm());\n        cipher.init(Cipher.ENCRYPT_MODE, privateKey);\n        return cipher.doFinal(data);\n    }\n   /**\n    * 公钥解密\n    *\n    * @param data 待解密数据\n    * @param key  密钥\n    * @return byte[] 解密数据\n    */\n    public static byte[] decryptByPublicKey(byte[] data,byte[] pk) throws Exception {\n        //实例化密钥工厂\n        KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM);\n        //初始化公钥\n        //密钥材料转换\n        X509EncodedKeySpec x509KeySpec = new X509EncodedKeySpec(pk);\n        //产生公钥\n        PublicKey pubKey = keyFactory.generatePublic(x509KeySpec);\n        //数据解密\n        Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm());\n        cipher.init(Cipher.DECRYPT_MODE, pubKey);\n        return cipher.doFinal(data);\n    }\n}</code></pre><h4 id=\"密钥\"><a href=\"#密钥\" class=\"headerlink\" title=\"密钥\"></a>密钥</h4><p>接下来创建钱包实例，对于用户的钱包，需要一下属性:</p>\n<ul>\n<li><strong>私钥：</strong> 用于加密数据，签名数据。</li>\n<li><strong>公钥：</strong> 用于解密数据，验证签名。</li>\n<li><strong>地址：</strong> 钱包的地址。</li>\n</ul>\n<p>目前，暂时将钱包设置为单例模式，同时一个钱包中只含有一对密钥对:</p>\n<pre><code>#Wallet.java\n\n@Getter\n@Setter\npublic class Wallet {\n    //单例模式的Wallet\n    private static Wallet wallet;\n    //私钥\n    private byte[] privateKey;\n    //公钥\n    private byte[] publicKey;\n    //地址\n    private String address;\n    private Wallet() throws Exception {\n        RSAKey key  = RSAKey.GenerateKeyPair();\n        this.privateKey = key.getPrivateKey();\n        this.publicKey = key.getPublicKey();\n        this.address = generateAddress();\n    }\n    public static Wallet getInstance() throws Exception {\n        if (wallet == null) {\n            synchronized (Wallet.class) {\n                if (wallet == null) {\n                    wallet = new Wallet();\n                }\n            }\n        }\n        return wallet;\n    }\n }</code></pre><p>接下来是加密与解密操作，只需要包装一下工具类即可:</p>\n<pre><code>#Wallet.java\n    /**\n     * 加密数据\n     */\n    public String encrypt(byte[] data) throws Exception {\n        byte[] encry = RSAKey.encryptByPrivateKey(data,this.privateKey);\n        return Hex.encodeHexString(encry);\n    }\n    /**\n     *  解密数据\n     */\n    public byte[] decrypt(String encry) throws DecoderException, Exception {\n        return RSAKey.decryptByPublicKey(Hex.decodeHex(encry),this.publicKey);\n    }</code></pre><p>还有重要的签名与验证签名的操作:<br>签名的本质实际上就是哈希操作+加密操作。</p>\n<ol>\n<li>对数据原文进行哈希: <strong>哈希(数据原文)</strong></li>\n<li>使用私钥对哈希值进行加密: <strong>加密(哈希(数据原文))</strong></li>\n<li>将数据原文与签名数据进行组合<br>数字签名的组成部分为：<strong>数据原文+加密(哈希(数据原文))</strong></li>\n</ol>\n<pre><code>#Wallet.java\n    /**\n     * 签名数据\n     */\n    public String sign(byte[] data) throws Exception {\n        //原文首先进行哈希\n        String hash = Util.getSHA256(\n            Hex.encodeHexString(data));\n        //哈希值进行加密\n        String sign = encrypt(\n            Hex.decodeHex(hash));\n        //原文+encry(hash(原文))\n        return Hex.encodeHexString(data)+&quot;%%%&quot;+sign;\n    }</code></pre><p>在这里，我们简单使用<code>%%%</code>将两部分数据进行组合。<br>至于数字签名的验证则可以分析了：</p>\n<ol>\n<li>首先将数字签名拆分为<strong>数据原文  加密(哈希(数据原文))</strong></li>\n<li>将数据原文进行哈希操作: <strong>哈希(数据原文)</strong></li>\n<li>使用公钥解密签名信息： <strong>解密-&gt;加密(哈希(数据原文))-&gt;哈希(数据原文)</strong></li>\n<li>对两部分哈希值进行对比，相同则说明数字签名是有效的。</li>\n</ol>\n<pre><code>#Wallet.java\n    /**\n     * 验证签名\n     */\n    public boolean verify(String data) throws DecoderException, Exception {\n        String[] str = data.split(&quot;%%%&quot;);\n        // 原文     encry(hash(原文))\n        if(str.length!=2){\n            return false;\n        }\n        String hash = Util.getSHA256(str[0]);\n        String hash2 = Hex.encodeHexString(this.decrypt(str[1]));\n        if(hash.equals(hash2)){\n            return true;\n        }\n        return false;\n    }</code></pre><h4 id=\"地址\"><a href=\"#地址\" class=\"headerlink\" title=\"地址\"></a>地址</h4><p>对于钱包地址，比特币是有自己的一套生成地址的算法，步骤相对比较繁杂，在本文中，简单使用哈希操作模拟地址的生成。</p>\n<pre><code>#Wallet.java\n    /**\n     * 根据密钥生成地址\n     */\n    private String generateAddress() throws NoSuchAlgorithmException {\n        String pk = Hex.encodeHexString(this.publicKey);\n        this.address = &quot;R&quot; + Util.getSHA256(pk) + Util.getSHA256(Util.getSHA256(pk));\n        return this.address;\n    }\n    /**\n     * 获取地址\n     * @return\n     * @throws NoSuchAlgorithmException\n     */\n    public String getAddress() throws NoSuchAlgorithmException {\n        return this.generateAddress();\n    }</code></pre><p>所有的一切都已经完成，测试一下:</p>\n<pre><code>#KeyTest.java\npublic class KeyTest {\n    public static void main(String[] args) throws DecoderException, Exception {\n        Wallet wallet  = Wallet.getInstance();\n        System.out.println(&quot;private Key:  &quot;+Hex.encodeHexString(wallet.getPrivateKey()));\n        System.out.println();\n        System.out.println(&quot;public Key:  &quot;+Hex.encodeHexString(wallet.getPublicKey()));\n        System.out.println(\n            wallet.verify(\n                wallet.sign(&quot;test&quot;.getBytes())));\n        System.out.println(&quot;address: &quot;+wallet.getAddress());\n    }\n}</code></pre><p>看起来一切都没有问题:</p>\n<pre><code>private Key:  30820156020100300d06092a864886f70d0101010500048201403082013c020100024100b204075a20a86a8773681a2bee6574a68d1028516577c80f22d1f693dbc1c70cca59d95a74b8c7a55c3e02801ebdb025272f1df18ca862701b640a6bc444b7e50203010001024066a67a12d7a8261dcb47a967d1c5813995384ef778da546b9df993057a0048a5b9e2f3986bef45bbcffc13baff6a93b31b054ecd6f23ad9c23a088597bc169b5022100e210191df6e5661b7fbe239866110bc54ace03e22d9e242d199b0f95d42c3e7f022100c9970dbe3640ad34633cb1a3defa5fc4be1dd9881eb65ff19d53d0ae2c569f9b022100c46a544872b2926b262ca064d399cfee55b6762d589164c142d435506b0f1e25022100a65a09543aaeda7f4d98eb3a4029ba57bf4f20904c4fd112aff25755336f741b022100dbe2e256464346e26c134395aada2bd669f72700b146b494920e9c75df12403f\n\npublic Key:  305c300d06092a864886f70d0101010500034b003048024100b204075a20a86a8773681a2bee6574a68d1028516577c80f22d1f693dbc1c70cca59d95a74b8c7a55c3e02801ebdb025272f1df18ca862701b640a6bc444b7e50203010001\ntrue\naddress: R92439f4d205def0794e23f626cf61013d04ccf1fdf9106ff78ca3ec30f7bc7cad4cdc346ee44501831c67085a54463e4ffd774654a2bd9328a382652de663f1a</code></pre><h3 id=\"Cli\"><a href=\"#Cli\" class=\"headerlink\" title=\"Cli\"></a>Cli</h3><p>到此为止我们开发了区块链系统的部分功能，距离完成还有很长一段距离。不过，先完成一个比较小的功能好了，即与用户进行交互的操作。之前考虑过使用<code>curl</code>，不过看到了<code>apache</code>的<code>cli</code>工具，所以决定使用这个了。其实就是一种命令行解析工具，根据输入的命令解析后执行对应的功能。参考<a href=\"https://www.ibm.com/developerworks/cn/java/j-lo-commonscli/\" target=\"_blank\" rel=\"noopener\">这里</a><br>所以需要在<code>pom.xml</code>添加一下字段导包:</p>\n<pre><code>        &lt;dependency&gt;\n            &lt;groupId&gt;commons-cli&lt;/groupId&gt;\n            &lt;artifactId&gt;commons-cli&lt;/artifactId&gt;\n            &lt;version&gt;1.2&lt;/version&gt;\n        &lt;/dependency&gt;</code></pre><p>整个步骤分为三个阶段: <strong>定义，解析，询问</strong>。</p>\n<h4 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h4><p>首先需要定义一些参数信息，作为应用程序的接口。</p>\n<pre><code>        //创建Options对象\n        Options options = new Options();\n        //添加-h参数。 h为参数简单形式，help为参数复杂形式，false定义该参数不需要额外的输入，Print help为参数的介绍\n        Option opt = new Option(&quot;h&quot;, &quot;help&quot;, false, &quot;Print help&quot;);\n        //定义该参数是否为必须的\n        opt.setRequired(false);\n        options.addOption(opt);</code></pre><h4 id=\"解析\"><a href=\"#解析\" class=\"headerlink\" title=\"解析\"></a>解析</h4><p>由CLi对用户输入的命令行进行解析。</p>\n<pre><code>        HelpFormatter hf = new HelpFormatter();\n        hf.setWidth(110);\n        CommandLine commandLine = null;\n        CommandLineParser parser = new PosixParser();\n        try {\n            commandLine = parser.parse(options, args);\n            //如果含有参数h，则打印帮助信息\n            if (commandLine.hasOption(&#39;h&#39;)) {\n                // 打印使用帮助\n                hf.printHelp(&quot;Jchain&quot;, options, true);\n            }\n            ...\n        }catch(Exception e){\n        }</code></pre><p><code>CommandLineParser</code> 类中定义的 <code>parse</code> 方法将用 CLI 定义阶段中产生的 <code>Options</code> 实例和一组字符串作为输入，并返回解析后生成的 <code>CommandLine</code>。<br>CLI 解析阶段的目标结果就是创建<code>CommandLine</code> 实例。</p>\n<h4 id=\"询问\"><a href=\"#询问\" class=\"headerlink\" title=\"询问\"></a>询问</h4><p>该阶段是根据输入的参数决定进入哪一个逻辑分支中。</p>\n<pre><code>            Option[] opts = commandLine.getOptions();\n            if (opts != null) {\n                for (Option opt1 : opts) {\n                    //name为参数名称\n                    String name = opt1.getLongOpt();\n                    //如果有额外的参数则传入value中\n                    String value = commandLine.getOptionValue(name);\n                    //...根据name指定具体的逻辑分支\n                }\n            }</code></pre><p>分析完了，然后制定需要的参数好了:<br>这里指定了三个参数:<code>s,a,w</code>，分别为获取区块链实例，添加区块以及获取钱包实例的功能。</p>\n<pre><code>        opt = new Option(&quot;s&quot;, &quot;start&quot;, false, &quot;start blockchain&quot;);\n        opt.setRequired(false);\n        options.addOption(opt);\n        opt = new Option(&quot;a&quot;, &quot;add&quot;, true, &quot;add block&quot;);\n        opt.setRequired(false);\n        options.addOption(opt);\n        opt = new Option(&quot;w&quot;, &quot;wallet&quot;, false, &quot;init wallet&quot;);\n        opt.setRequired(false);\n        options.addOption(opt);</code></pre><p>然后是具体的逻辑分支:</p>\n<pre><code>        if (name.equals(&quot;s&quot;) || name.equals(&quot;start&quot;)) {\n            System.out.println(Blockchain.getInstance().block.toString());\n        }\n        if(name.equals(&quot;a&quot;)||name.equals(&quot;add&quot;)&amp;&amp;value!=&quot;&quot;){\n            System.out.println(Blockchain.getInstance().addBlock(value).toString());\n        } \n        if(name.equals(&quot;w&quot;)||name.equals(&quot;wallet&quot;)){\n            Wallet wallet = Wallet.getInstance();\n            System.out.println(&quot;private Key:  &quot;+Hex.encodeHexString(wallet.getPrivateKey()));\n            System.out.println();\n            System.out.println(&quot;public Key:  &quot;+Hex.encodeHexString(wallet.getPublicKey()));\n        } </code></pre><p>简单测试一下是否正常工作:</p>\n<pre><code>#CliTest.java\npublic class CliTest {\n    public static void main(String[] args){\n        String[] str = {&quot;-s&quot;,&quot;-w&quot;,&quot;-a&quot;,&quot;block&quot;};\n        // String[] str = {&quot;-h&quot;};\n        Cli.Start(str);\n    }\n}</code></pre><p>看起来没有问题，获得了高度为4的区块链实例。也成功创建了钱包打印出公私钥。<br>最后生成了高度为5的区块 区块信息为我们输入的”block”。</p>\n<pre><code>Current Last Block num is:4\n{&quot;blkNum&quot;:4,&quot;curBlockHash&quot;:&quot;0000287895ae8f4e4fc781137adee2b2fd0da4d7be3abb68c04507979157eb70&quot;,&quot;data&quot;:&quot;block&quot;,&quot;nonce&quot;:121263,&quot;prevBlockHash&quot;:&quot;00003ae4ca11f2dd6262d9218ffe6a98416b4e9e2ad789b39aef74b383cc96a6&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 14:28:16&quot;}\nprivate Key:  30820154020100300d06092a864886f70d01010105000482013e3082013a02010002410082a46b7f68d835b5e8047b8794cfd4d5daf4b6f3d8258a8a78f670052f35bab562f52aa7a6eeeb69bc14e03f0d8019db5b754d68e8d0918aa6f2f4b636ba0f070203010001024009de8ffc6d18405e80abae055d19a253919a012444c4f94562c4034c70f79726372e85f8853b9093e984b2fee8d828cf6078b2b66239e5871e299985a9b85ea1022100bed5f1748ffeabd423e7518c2c9840d9299f5190f3e482b0ec50ae203cd25b17022100af409e3b74a06c4937f8ca3bc12776ff21217750a4b5e1d02de71d1a7ceda19102202085d3959ae8bb1df75477d85ccd41d800b8ef2cb5f40eb5da4051bc9ac0fad702206342871c87b6e0fe2b6c872687051239f88adae85b12051f0310b6842d23ee710221008ebb60975fc2ee07e94242da08e0cb81478b7c57091e20e2177aa325883e4714\n\npublic Key:  305c300d06092a864886f70d0101010500034b00304802410082a46b7f68d835b5e8047b8794cfd4d5daf4b6f3d8258a8a78f670052f35bab562f52aa7a6eeeb69bc14e03f0d8019db5b754d68e8d0918aa6f2f4b636ba0f070203010001\n{&quot;blkNum&quot;:5,&quot;curBlockHash&quot;:&quot;000047e8fc13a5fe0404aeca104f8624581738361f12f2a8c07c4f172dde62cc&quot;,&quot;data&quot;:&quot;block&quot;,&quot;nonce&quot;:67701,&quot;prevBlockHash&quot;:&quot;0000287895ae8f4e4fc781137adee2b2fd0da4d7be3abb68c04507979157eb70&quot;,&quot;timeStamp&quot;:&quot;2020-05-17 16:27:29&quot;}</code></pre><p>后一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12910299.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(二)</a></p>\n<h4 id=\"Github仓库地址在这里，随时保持更新中…\"><a href=\"#Github仓库地址在这里，随时保持更新中…\" class=\"headerlink\" title=\"Github仓库地址在这里，随时保持更新中…..\"></a>Github仓库地址在这里，随时保持更新中…..</h4><p>Github地址：<a href=\"https://github.com/newonexd/Jchain\" target=\"_blank\" rel=\"noopener\">Jchain</a></p>\n"},{"title":"Paxos算法","date":"2019-12-23T05:26:27.000Z","_content":"# 使Paxos变简单\n\n**摘要**\nPaxos算法，用英语说明时，变得非常简单。\n\n## 1 介绍\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;人们一直认为，用于实现容错分布式系统的Paxos算法难以理解，可能是因为最初的演示文稿对许多读者来说是希腊文.事实上，它是分布式算法中最简单，最有效的方法之一。它的核心是共识算法。下一节将说明这种共识算法几乎不可避免地遵循了我们希望它满足的特性。最后一部分介绍了完整的Paxos算法，该算法是通过将共识直接应用于构建分布式系统的状态机方法而获得的，这种方法应该是众所周知的，因为它是有关分布式系统理论的最常被引用的文章的主题。\n\n## 2 共识算法\n### 2.1 问题\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设有一个可以提出值的进程的集合。共识算法确保只有一个提出的值被选中。如果没有值被提出，则没有值应该被选中。如果一个值被选中，那么所有过程应该能够`learn`被选中的值。共识需要满足以下要求：\n\n* 只有被提出的值才可以被选中\n* 被选中的只有一个值\n* 除非一个值真正地被选中，否则某个进程不会去`learn`这个值。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们不会尝试指定精确的活动要求，然而，目标是确保最终存在一个值被选定。并且当一个值被选定时，进程最终会`learn`到这个值。\n我们在共识算法中定义了三种角色：\n\n* `proposers`\n* `acceptors`\n* `learners`\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在算法的实现中，某个进程可能同时担任多个角色，但是在这里不讨论角色到进程的映射关系。\n假设角色之间通过发送消息进行通信。我们使用异步，非拜占庭模型：\n\n* 角色以任意的速度执行，可能由于停止而宕掉，可能会重启。所有的角色可能在一个值被选中之后宕掉重启。除非宕掉再重启的角色可以记住某些信息，否则等重启后无法确定被选定的值。\n* 消息可能要花很长时间才能被交付，可能会复制可能会丢失，但是都没有关系。\n\n### 2.2 选择一个值\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最简单的方式是存在单个的`acceptor`角色然后选择一个值。一个`proposer`发送一个`proposal`到`acceptor`，`acceptor`选择它接收到的第一个`proposal`的值。尽管简单，但是这种解决方案不能满足要求，因为如果`acceptor`宕掉将会使未来的步骤无法继续(单点故障)。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，让我们尝试另外一种方式选择一个值。使用多个`acceptor`角色代替单个`acceptor`。一个`proposer`发送一个`proposal`值到`acceptor`角色的集合。`acceptor`可能会接受`proposal`的值。当足够多的`acceptor`接受了该值，则说明这个值被选择了。足够多是多少呢？为了确保只有一个值被选择。我们可以让足够多数量的一组包含任何大多数角色。因为任何两个足够多数量的组都至少有一个共同的接受者，所以如果一个接受者最多可以接受一个值，则此方法有效。\n在没有失败或消息丢失的情况下，如果只有一个值由单个的`proposer`提出，我们想要这个值被选择，需要满足以下要求：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P1**:**任何`acceptor`必须接受它收到的第一个`proposal`。**\n\n但是这个要求会出现一个问题。如果在同一时间有多个不同的`proposer`提出多个值，将会导致这种状态：每一个`acceptor`将会接受到一个值，但是不存在一个被大多数成员接受的值。即使只提出了两个值。如果每一个都由一半的`acceptor`接受，当一个`acceptor`宕掉后，将无法确定哪一个值被选择。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P1**和**被大多数的`acceptor`接受的值才能被选择**，这两个要求隐性说名了每一个`acceptor`都必须可以接受多个值。我们通过为每个`proposal`分配一个（自然）编号来跟踪接受者可以接受的不同提案，那么每一个`proposal`将由一个`proposal`序号和一个值组成。为了避免冲突，我们要求不同的`proposal`所含有的`proposal`序号都是不同的。如何做到这一点取决于实现方法。现在我们只是假设。当一个`proposal`的值被大多数`acceptor`接受，那么该值说明被选择。这种情况下，我们说该`proposal`被选择。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们允许多个`proposal`被选择。但是需要保证所有被选择的`proposal`具有相同的值。通过对`proposal`编号的归纳，足以保证：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P2**：**如果一个值为*v*的`proposal`被选择，那么被选择的比该`proposal`编号大的`proposal`具有相同的值*v*。**\n\n由于数字是完全有序的，因此条件P2保证了仅选择一个值的关键安全性。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个`proposal`要被选择，建议必须至少由一个`acceptor`接受。 因此，我们可以通过满足以下条件来满足P2：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P2<sup>a</sup>**：**如果一个值为*v*的`proposal`被选择。那么由任意的`acceptor`接受的被选择的比该`proposal`编号大的`proposal`具有相同的值*v*。**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们始终保证**P1**来确保一些`proposal`被选择。因为通信是异步的。一个`proposal`可以被一些从没有接受到任何`proposal`的`acceptor`*c*选择。假设一个新的`proposer`刚刚启动就接受到一个高编号的且值不同的`proposal`。**P1**则要求*c*接受该`proposal`，因此不满足要求**P2<sup>a</sup>**.维持**P1**和**P2<sup>a</sup>**需要加强P2<sup>a</sup>为：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P2<sup>b</sup>**:**如果一个值为*v*的`proposal`被选择，那么每一个由任意的`proposer`提出的编号高的`proposal`都具有相同的*v*。**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于一个`proposal`都需要在被任意`acceptor`接受之前都由`proposer`提出，因此满足了要求**P2<sub>b</sub>**，就满足了要求**P2<sub>a</sub>**,所以也就满足了**P2**。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了发现如何满足要求**P2<sup>b</sup>**,让我们考虑一下如何证明它成立。我们假设被选择的`proposal`具有编号*m*与值*v*并且表明证明任何发布的编号为*n*>*m*的`proposal`也具有值*v*。我们可以通过对*n*进行归纳来简化证明，因此可以证明`proposal`编号*n*在值*v*的附加假设下每个提案编号都在*m*..(*n*−1)区间内并且值为*v*，其中*i..j*表示从*i*到*j*的一组数字。为了选择编号为*m*的`proposal`，必须有一些由大多数`acceptor`组成的集合*C*，以便*C*中的每个`acceptor`都接受它。将其与归纳假设相结合，选择*m*的假设就意味着：\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*C*中的每个`acceptor`都接受了一个编号为*m*..(*n-1*)的`proposal`，并且任何`acceptor`接受的每个编号为*m*..(*n-1*)的`proposal`都具有值*v*。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由大多数成员组成的集合*s*，至少包括一个*C*中的成员。我们可以通过确保以下不变式得出结论：编号为*n*的`proposal`具有值*v*.\n**P2<sup>c</sup>**:**对于任何值*v*和*n*，如果一个`proposal`具有编号*n*和值*v*，那么由主要`acceptor`组成的集合满足以下其中一个条件：**\n\n* ***S*中的`acceptor`不会接受任何编号小于*n*的`proposal`。**\n* ***S*中的`acceptor`接受的最大编号的`proposal`的值为*v*。**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此满足了**P2<sup>c</sup>**的不变式即满足了**P2<sup>b</sup>.**\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了维持**P2<sup>c</sup>**的不变式，`proposer`想要提出一个编号为*n*的`proposal`必须`learn`(如果有的话)已被或将要被大多数`acceptor`中的每个`acceptor`接受的编号小于*n*的最高编号的`proposal`。了解已经接受的`proposal`很容易；预测未来是否会被接受很难。`proposer`没有试图预测未来，而是通过提取不会有任何此类接受的承诺来控制未来。换句话说，`proposer`要求`acceptor`不接受任何其他编号小于*n*的`proposal`。 推导出以下用于发布提案的算法：\n\n* **一个`proposal`选择编号为*n*的`proposal`并发送请求到包括半数以上个`acceptor`的集合，并要求得到以下其中一个回应：**\n    1. **一个不会接受编号值小于*n*的`proposal`的承诺。**\n    2. **如果`acceptor`已经接受过`proposal`，则响应已接受的小于编号*n*的最大编号的`proposal`。**\n\n将该请求称为编号为*n*的`prepare`请求。\n\n* **如果`proposer`接受到大部分`acceptor`的请求响应，那么可以提出一个编号为*n*且值为*v*的`proposal`。这里的*v*是请求响应中编号最高的`proposal`中的值。或者如果响应中没有任何`proposal`，那么该值将由`proposer`自由选择。**\n\n`proposer`通过发送`proposal`到包括半数以上个`acceptor`集合(需要与起初的请求集合不是同一个)，并期望接受该请求。将该请求称为`accept`请求。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里描述的是关于`proposer`的算法。关于`acceptor`呢？`acceptor`可以从`proposer`接收两种类型的请求：`prepare`和`accept`请求。`acceptor`可以忽略任何请求而不会影响安全性。 因此，我们仅需说何时才允许响应请求。它总是可以响应`prepare`请求。 如果它没有答应不接受，它可以响应`accept`请求，接受`proposal`。 换一种说法：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P1<sup>a</sup>**:**如果`acceptor`没有响应编号大于*n*的`prepare`请求那么可以接收一个编号为*n*的`proposal`。**\n\n观察到**P1<sup>a</sup>**包含**P1**。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们现在拥有了一个完整的算法选择一个满足安全性要求的值-通过假设一个唯一的`proposal`编号。最终算法通过一个小的优化获得。\n假设`acceptor`收到编号为*n*的`prepare`请求.但是已经响应过一个编号值大于*n*的`prepare`请求。因此承诺不会接受任何编号为*n*的新的`proposal`请求。这样，`acceptor`就没有理由响应新的`prepare`请求，因为它不会接受`proposer`想要发出的编号为*n*的`proposal`。 因此，我们让`acceptor`忽略这样的`prepare`请求。 我们也可以忽略它已经接受的`proposal`的`prepare`请求。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过这种优化，`acceptor`只需要记住它曾经接受的编号最高的`proposal`以及它已响应的编号最高的`prepare`请求的数量。无论如何失败，**P2<sup>c</sup>**也必须保持不变，所以即使失败，接受者也必须记住该信息，然后重新启动。 请注意，只要`proposer`从不尝试发布具有相同编号的另一个`proposal`，它总是可以放弃该`proposal`并将其遗忘。\n将`proposer`和`acceptor`的动作放在一起，我们看到该算法在以下两个阶段中运行：\n\n**阶段一**：\n\n1. **`proposer`选择一个编号为*n*的`proposal`并发送编号为*n*的`prepare`请求到大多数的`acceptor`。**\n2. **如果`acceptor`接受了编号为*n*的`prepare`请求，并且编号*n*比接受到的任何`prepare`请求的编号都要大，那么将会响应它接受到的编号最大的`proposal`(如果有的话)到`proposer`并且承诺不再接受任何编号小于*n*的`proposal`。**\n\n**阶段二**：\n\n1. **如果`proposer`从大多数的`acceptor`接受到编号为*n*的请求响应。那么将会发送编号为*n*且值为*v*的`accept`请求到接受`prepare`请求的每一个`acceptor`。这里的*v*是所有`prepare`响应中编号最大的响应中的值。或者当`prepare`响应中没有`proposal`时，该值由自己选择。**\n2. **如果`acceptor`接收到编号为*n*的`accept`请求，除非它已经响应了一个编号大于*n*的`prepare`请求，否则它将接受该`proposal`。**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个`proposer`可以提出多个`proposal`，只要它遵循每个`proposal`的算法即可。 它可以随时在协议中间放弃`proposal`。(即使`proposal`的请求和/或响应可能在`proposal`被放弃很长时间后到达目的地，也可以保持正确性。)如果某个`proposer`已经开始尝试发布编号更大的`proposal`，那么放弃`proposal`可能是一个好主意。因此，如果某个`acceptor`忽略了`prepare`或者是`accept`请求，那是因为已经接受了一个编号更大的`prepare`请求。所以`acceptor`应该通知`proposer`放弃它的`proposal`。这是对性能的优化并且不会影响正确性。\n\n### 2.3 `learn`一个被选择的值\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`learner`必须发现一个被大多数`acceptor`接受的`proposal`，才能`learn`被选择的值。最明显的算法是让每个`acceptor`在接受`proposal`时对所有`learner`做出回应，向他们发送`proposal`。这使`learner`能够尽快发现所选的值，但是它要求每个`acceptor`对每个`learner`做出回应-回应的数量等于`acceptor`数量与`learner`数量的乘积。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非拜占庭式失败的假设使一个`learner`很容易从另一个`learner`那里发现一个值已经被接受。我们可以让`acceptor`以他们的接受来回应一个特别的`learner`，当选择了一个值时，后者又会通知其他`learner`。这种方法需要所有`learner`进行额外的一轮努力才能发现所选的值。它也不太可靠，因为特别的`learner`可能会失败。但是，它需要的响应数量仅等于`acceptor`数量和`learner`数量之和。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更一般而言，`acceptor`可以用他们对某些特别的`learner`的接受做出响应，然后当选择了某个值时，每个`learner`都可以通知所有`learner`。使用更多的特别的`learner`以更高的通信复杂性为代价提供更高的可靠性。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于消息丢失，一个值的被选择可能会没有`learner`会发现。`learner`可以向`acceptor`询问他们接受了哪些`proposal`，但是`acceptor`的失败可能使得无法知道大多数人是否接受了特定`proposal`。在这种情况下，`learner`只有在选择新`proposal`时才能发现什么值被选择。 如果`learner`需要知道是否一个值被选择，则可以使用上述算法让`proposer`发布`proposal`。\n\n### 2.4 流程\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很容易构造这样一个场景，在该场景中，两个`proposer`各自不断发布数量越来越多的`proposal`序列，而从未选择过一个。 `proposer`*p*完成阶段1的编号为*n1*的`proposal`。然后，另一个`proposer`*q*完成阶段1的编号*n2>n1*的`proposal`。 `proposer`*p*的第2阶段编号为*n1*的`proposal`请求被忽略，因为`acceptor`都承诺不接受任何编号少于*n2*的新`proposal`。 因此，`proposer`*p*又开始以编号为*n3>n2*的`proposal`进行阶段1，导致阶段2`proposer`*q*的请求被忽略,并一直持续下去。为了保证进度，必须选择特别的`proposer`作为尝试发布`proposal`的唯一`proposer`。 如果特别的`proposer`可以与大多数`acceptor`成功通信，并且使用的`proposal`编号大于已使用的`proposal`的编号，那么它将成功发布被接受的`proposal`。并在得知某个`proposal`具有更高`proposal`编号的请求后通过放弃`proposal`再试一次，特别的`proposer`最终将选择足够高的`proposal`编号。如果系统（`proposer`，`acceptor`和通信网络）能够正常工作，那么可以通过选举一个特别的`proposer`来实现活跃性。Fischer，Lynch和Pattererson的著名成果表明，一种可靠的选择`proposer`的算法必须使用随机性或实时性，例如使用超时。但是，无论选举成功与否，都会确保安全。\n\n### 2.5 实现\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Paxos算法假设网络的进程。在其共识算法中，每个进程都扮演着`proposer`，`acceptor`和`learner`的角色。该算法选择一个`leader`，该`leader`同时扮演特别的`proposer`与特别的`learner`。Paxos共识算法正是上述算法，其中请求和响应作为普通消息发送。（响应消息带有相应的`proposal`编号，以防止混淆。）在故障期间保留的稳定存储用于维护`acceptor`必须记住的信息。`acceptor`在实际发送响应之前将其预期的响应记录在稳定的存储中。剩下的就是描述如何保证没有两个`proposal`编号相同的机制。不同的`proposer`从不相交的数字集中选择他们的数字，因此两个不同的`proposer`从不发布具有相同编号的`proposal`。 每个`proposer`（在稳定的存储中）都会记住尝试发布的编号最高的`proposal`，并从第1阶段开始使用比其已经使用过的`proposer`编号更高的`proposer`编号。\n\n## 3 状态机的实现\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实现分布式系统的一种简单方法是作为向中央服务器发出命令的客户端的集合。 服务器可以描述为以某种顺序执行客户端命令的确定性状态机。状态机具有当前状态。它通过将命令作为输入并产生输出和新状态来执行步骤。例如，分布式银行系统的客户可能是柜员，并且状态机状态可能由所有用户的帐户余额组成。 提现将通过执行状态机命令来执行，该命令会在且仅当余额大于提款金额时才减少帐户的余额，并产生旧余额和新余额作为输出。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果单个中央服务器发生故障，则该服务器的实施将失败。因此，我们改为使用服务器的集合，每个服务器独立实现状态机。 因为状态机是确定性的，所以如果所有服务器都执行相同的命令序列，则所有服务器将产生相同的状态序列和输出。 然后，发出命令的客户端可以使用任何服务器为其生成的输出。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了确保所有服务器执行相同的状态机命令序列，我们实现了Paxos共识算法的不同实例序列，第i个实例选择的值是序列中的第i个状态机命令。每个服务器在算法的每个实例中扮演所有角色(`proposer`，`acceptor`和`learner`)。现在，假设服务器组是固定的，因此共识算法的所有实例使用相同的角色。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在正常操作中，将选择一台服务器作为`leader`，在共识算法的所有实例中均充当特别的`proposer`(尝试发布`proposal`的唯一的`proposer`)。 客户将命令发送给`leader`，`leader`决定每个命令应出现的顺序。如果`leader`确定某个客户命令应为第135个命令，则它将尝试选择该命令作为共识算法的第135个实例的值。通常会成功。 它也可能由于故障而失败，或者因为另一台服务器也认为自己是`leader`并且认为另一条客户端命令应该为第135条命令。但是共识算法确保最多可以选择一个命令作为第135个命令。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方法的效率的关键在于，在Paxos共识算法中，直到第2阶段才选择要提出的值。回想一下，在完成`proposer`算法的第1阶段之后，要么确定了要提出的值，要么提议者可以自由提出任何值。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，将描述Paxos状态机实现在正常操作期间如何工作。稍后，将讨论可能出问题的地方。 考虑当前`leader`刚刚失败并选择了新`leader`时会发生什么。(系统启动是一种特殊情况，其中尚未提出任何命令。)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新的`leader`，在共识算法的所有情况下都是`learner`，应该知道已经选择的大多数命令。 假设它知道命令1–134、138和139，即共识算法实例1–134、138和139中选择的值。(将在后面看到如何在命令序列中出现这样的间隙.)然后，它执行实例135-137和大于139的所有实例的阶段1。(在下面描述如何做到这一点.)假设这些执行确定了在实例135和140中要提出的值，但在所有其他情况下都不受约束。领导者然后对实例135和140执行阶段2，从而选择命令135和140。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`leader`以及任何其他服务器一样`learn leader`知道的所有命令，现在可以执行命令1–135。但是，它无法执行它也知道的命令138-140，因为尚未选择命令136和137。`leader`可以将客户请求的下两个命令作为命令136和137。相反，我们通过建议一个特殊的`no-op`命令作为命令136和137，使状态保持不变，从而立即填补了空白。（这是通过执行共识算法实例136和137的阶段2来完成的。）一旦选择了这些`no-op`命令，就可以执行命令138-140。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在已选择命令1–140。`leader`还已经为大于共识算法140的所有实例完成了阶段1，并且可以自由地在那些实例的阶段2中提出任何值。它将命令号141分配给客户端请求的下一个命令，并建议将其作为共识算法实例141的阶段2中的值。 它提出了接收到的下一个客户端命令作为命令142，依此类推。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`leader`可以在获悉已选择其提出的命令141之前提出命令142。 它在提议命令141中发送的所有消息都可能丢失，并且有可能在任何其他服务器了解`leader`作为命令141提出的建议之前选择命令142。当`leader`未能收到在实例141中的消息对其阶段2的预期响应时，它将重传那些消息。如果一切顺利，将选择其建议的命令。但是，它可能失败，从而在选择的命令序列中留下空白。通常，假设`leader`可以提前获得α命令-也就是说，在选择命令`1`至`i`之后，它可以提出命令`i+1`至`i+α`。最多可能会出现`α-1`命令的间隔。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于实例135-137和大于139的所有实例,新选择的`leader`在上面的方案中使用共识算法执行第1阶段的次数不限。它对所有实例使用相同的编号，可以通过向其他服务器发送一条合理的短消息来实现此目的。在阶段1中，只有在`acceptor`已经从某个`proposer`那里收到阶段2消息的情况下，接受者才用简单的OK做出响应.(仅对于实例135和140是这种情况.)因此，服务器(充当`acceptor`)可以使用单个合理的短消息对所有实例进行响应。 因此，执行这些无限多个阶段1实例不会带来任何问题。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于`leader`失败选举新`leader`的情况很少见，因此执行状态机命令的有效成本（即，对命令/值达成共识）是仅执行共识算法第二阶段的成本。可以证明，存在故障时达成共识的任何算法的最小可能成本在Paxos共识算法的第2阶段。 因此，Paxos算法本质上是最佳的。\n对系统正常运行的讨论假定只有一个`leader`，除了在当前`leader`失败和选举新`leader`之间的短暂时间之外。 在异常情况下，`leader`选举可能会失败。如果没有服务器充当`leader`，则不会提出新命令。如果多个服务器认为自己是`leader`，则它们都可以在同一共识算法实例中提出值，这可能会阻止选择任何值。但是，安全性得以保留-两个不同的服务器将永远不会在选择作为第i个状态机命令的值上发生分歧。仅选举一位`leader`才能确保取得进展。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果服务器组可以更改，则必须采用某种方法确定哪些服务器实现共识算法的哪些实例。最简单的方法是通过状态机本身。 当前服务器集可以成为状态的一部分，并且可以通过普通的状态机命令进行更改。我们可以允许`leader`提前获得`α`命令,通过让执行第`i`个状态机命令后的状态指定执行共识算法的实例`i+α`的服务器集。这允许任意复杂的重新配置算法的简单实现。","source":"_posts/blog/consensus/paxos.md","raw":"---\ntitle: Paxos算法\ndate: 2019-12-23 13:26:27\ntags: \n- algorithm\ncategories:\n- algorithm\n---\n# 使Paxos变简单\n\n**摘要**\nPaxos算法，用英语说明时，变得非常简单。\n\n## 1 介绍\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;人们一直认为，用于实现容错分布式系统的Paxos算法难以理解，可能是因为最初的演示文稿对许多读者来说是希腊文.事实上，它是分布式算法中最简单，最有效的方法之一。它的核心是共识算法。下一节将说明这种共识算法几乎不可避免地遵循了我们希望它满足的特性。最后一部分介绍了完整的Paxos算法，该算法是通过将共识直接应用于构建分布式系统的状态机方法而获得的，这种方法应该是众所周知的，因为它是有关分布式系统理论的最常被引用的文章的主题。\n\n## 2 共识算法\n### 2.1 问题\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设有一个可以提出值的进程的集合。共识算法确保只有一个提出的值被选中。如果没有值被提出，则没有值应该被选中。如果一个值被选中，那么所有过程应该能够`learn`被选中的值。共识需要满足以下要求：\n\n* 只有被提出的值才可以被选中\n* 被选中的只有一个值\n* 除非一个值真正地被选中，否则某个进程不会去`learn`这个值。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们不会尝试指定精确的活动要求，然而，目标是确保最终存在一个值被选定。并且当一个值被选定时，进程最终会`learn`到这个值。\n我们在共识算法中定义了三种角色：\n\n* `proposers`\n* `acceptors`\n* `learners`\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在算法的实现中，某个进程可能同时担任多个角色，但是在这里不讨论角色到进程的映射关系。\n假设角色之间通过发送消息进行通信。我们使用异步，非拜占庭模型：\n\n* 角色以任意的速度执行，可能由于停止而宕掉，可能会重启。所有的角色可能在一个值被选中之后宕掉重启。除非宕掉再重启的角色可以记住某些信息，否则等重启后无法确定被选定的值。\n* 消息可能要花很长时间才能被交付，可能会复制可能会丢失，但是都没有关系。\n\n### 2.2 选择一个值\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最简单的方式是存在单个的`acceptor`角色然后选择一个值。一个`proposer`发送一个`proposal`到`acceptor`，`acceptor`选择它接收到的第一个`proposal`的值。尽管简单，但是这种解决方案不能满足要求，因为如果`acceptor`宕掉将会使未来的步骤无法继续(单点故障)。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，让我们尝试另外一种方式选择一个值。使用多个`acceptor`角色代替单个`acceptor`。一个`proposer`发送一个`proposal`值到`acceptor`角色的集合。`acceptor`可能会接受`proposal`的值。当足够多的`acceptor`接受了该值，则说明这个值被选择了。足够多是多少呢？为了确保只有一个值被选择。我们可以让足够多数量的一组包含任何大多数角色。因为任何两个足够多数量的组都至少有一个共同的接受者，所以如果一个接受者最多可以接受一个值，则此方法有效。\n在没有失败或消息丢失的情况下，如果只有一个值由单个的`proposer`提出，我们想要这个值被选择，需要满足以下要求：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P1**:**任何`acceptor`必须接受它收到的第一个`proposal`。**\n\n但是这个要求会出现一个问题。如果在同一时间有多个不同的`proposer`提出多个值，将会导致这种状态：每一个`acceptor`将会接受到一个值，但是不存在一个被大多数成员接受的值。即使只提出了两个值。如果每一个都由一半的`acceptor`接受，当一个`acceptor`宕掉后，将无法确定哪一个值被选择。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P1**和**被大多数的`acceptor`接受的值才能被选择**，这两个要求隐性说名了每一个`acceptor`都必须可以接受多个值。我们通过为每个`proposal`分配一个（自然）编号来跟踪接受者可以接受的不同提案，那么每一个`proposal`将由一个`proposal`序号和一个值组成。为了避免冲突，我们要求不同的`proposal`所含有的`proposal`序号都是不同的。如何做到这一点取决于实现方法。现在我们只是假设。当一个`proposal`的值被大多数`acceptor`接受，那么该值说明被选择。这种情况下，我们说该`proposal`被选择。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们允许多个`proposal`被选择。但是需要保证所有被选择的`proposal`具有相同的值。通过对`proposal`编号的归纳，足以保证：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P2**：**如果一个值为*v*的`proposal`被选择，那么被选择的比该`proposal`编号大的`proposal`具有相同的值*v*。**\n\n由于数字是完全有序的，因此条件P2保证了仅选择一个值的关键安全性。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个`proposal`要被选择，建议必须至少由一个`acceptor`接受。 因此，我们可以通过满足以下条件来满足P2：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P2<sup>a</sup>**：**如果一个值为*v*的`proposal`被选择。那么由任意的`acceptor`接受的被选择的比该`proposal`编号大的`proposal`具有相同的值*v*。**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们始终保证**P1**来确保一些`proposal`被选择。因为通信是异步的。一个`proposal`可以被一些从没有接受到任何`proposal`的`acceptor`*c*选择。假设一个新的`proposer`刚刚启动就接受到一个高编号的且值不同的`proposal`。**P1**则要求*c*接受该`proposal`，因此不满足要求**P2<sup>a</sup>**.维持**P1**和**P2<sup>a</sup>**需要加强P2<sup>a</sup>为：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P2<sup>b</sup>**:**如果一个值为*v*的`proposal`被选择，那么每一个由任意的`proposer`提出的编号高的`proposal`都具有相同的*v*。**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于一个`proposal`都需要在被任意`acceptor`接受之前都由`proposer`提出，因此满足了要求**P2<sub>b</sub>**，就满足了要求**P2<sub>a</sub>**,所以也就满足了**P2**。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了发现如何满足要求**P2<sup>b</sup>**,让我们考虑一下如何证明它成立。我们假设被选择的`proposal`具有编号*m*与值*v*并且表明证明任何发布的编号为*n*>*m*的`proposal`也具有值*v*。我们可以通过对*n*进行归纳来简化证明，因此可以证明`proposal`编号*n*在值*v*的附加假设下每个提案编号都在*m*..(*n*−1)区间内并且值为*v*，其中*i..j*表示从*i*到*j*的一组数字。为了选择编号为*m*的`proposal`，必须有一些由大多数`acceptor`组成的集合*C*，以便*C*中的每个`acceptor`都接受它。将其与归纳假设相结合，选择*m*的假设就意味着：\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*C*中的每个`acceptor`都接受了一个编号为*m*..(*n-1*)的`proposal`，并且任何`acceptor`接受的每个编号为*m*..(*n-1*)的`proposal`都具有值*v*。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由大多数成员组成的集合*s*，至少包括一个*C*中的成员。我们可以通过确保以下不变式得出结论：编号为*n*的`proposal`具有值*v*.\n**P2<sup>c</sup>**:**对于任何值*v*和*n*，如果一个`proposal`具有编号*n*和值*v*，那么由主要`acceptor`组成的集合满足以下其中一个条件：**\n\n* ***S*中的`acceptor`不会接受任何编号小于*n*的`proposal`。**\n* ***S*中的`acceptor`接受的最大编号的`proposal`的值为*v*。**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此满足了**P2<sup>c</sup>**的不变式即满足了**P2<sup>b</sup>.**\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了维持**P2<sup>c</sup>**的不变式，`proposer`想要提出一个编号为*n*的`proposal`必须`learn`(如果有的话)已被或将要被大多数`acceptor`中的每个`acceptor`接受的编号小于*n*的最高编号的`proposal`。了解已经接受的`proposal`很容易；预测未来是否会被接受很难。`proposer`没有试图预测未来，而是通过提取不会有任何此类接受的承诺来控制未来。换句话说，`proposer`要求`acceptor`不接受任何其他编号小于*n*的`proposal`。 推导出以下用于发布提案的算法：\n\n* **一个`proposal`选择编号为*n*的`proposal`并发送请求到包括半数以上个`acceptor`的集合，并要求得到以下其中一个回应：**\n    1. **一个不会接受编号值小于*n*的`proposal`的承诺。**\n    2. **如果`acceptor`已经接受过`proposal`，则响应已接受的小于编号*n*的最大编号的`proposal`。**\n\n将该请求称为编号为*n*的`prepare`请求。\n\n* **如果`proposer`接受到大部分`acceptor`的请求响应，那么可以提出一个编号为*n*且值为*v*的`proposal`。这里的*v*是请求响应中编号最高的`proposal`中的值。或者如果响应中没有任何`proposal`，那么该值将由`proposer`自由选择。**\n\n`proposer`通过发送`proposal`到包括半数以上个`acceptor`集合(需要与起初的请求集合不是同一个)，并期望接受该请求。将该请求称为`accept`请求。\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里描述的是关于`proposer`的算法。关于`acceptor`呢？`acceptor`可以从`proposer`接收两种类型的请求：`prepare`和`accept`请求。`acceptor`可以忽略任何请求而不会影响安全性。 因此，我们仅需说何时才允许响应请求。它总是可以响应`prepare`请求。 如果它没有答应不接受，它可以响应`accept`请求，接受`proposal`。 换一种说法：\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**P1<sup>a</sup>**:**如果`acceptor`没有响应编号大于*n*的`prepare`请求那么可以接收一个编号为*n*的`proposal`。**\n\n观察到**P1<sup>a</sup>**包含**P1**。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们现在拥有了一个完整的算法选择一个满足安全性要求的值-通过假设一个唯一的`proposal`编号。最终算法通过一个小的优化获得。\n假设`acceptor`收到编号为*n*的`prepare`请求.但是已经响应过一个编号值大于*n*的`prepare`请求。因此承诺不会接受任何编号为*n*的新的`proposal`请求。这样，`acceptor`就没有理由响应新的`prepare`请求，因为它不会接受`proposer`想要发出的编号为*n*的`proposal`。 因此，我们让`acceptor`忽略这样的`prepare`请求。 我们也可以忽略它已经接受的`proposal`的`prepare`请求。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过这种优化，`acceptor`只需要记住它曾经接受的编号最高的`proposal`以及它已响应的编号最高的`prepare`请求的数量。无论如何失败，**P2<sup>c</sup>**也必须保持不变，所以即使失败，接受者也必须记住该信息，然后重新启动。 请注意，只要`proposer`从不尝试发布具有相同编号的另一个`proposal`，它总是可以放弃该`proposal`并将其遗忘。\n将`proposer`和`acceptor`的动作放在一起，我们看到该算法在以下两个阶段中运行：\n\n**阶段一**：\n\n1. **`proposer`选择一个编号为*n*的`proposal`并发送编号为*n*的`prepare`请求到大多数的`acceptor`。**\n2. **如果`acceptor`接受了编号为*n*的`prepare`请求，并且编号*n*比接受到的任何`prepare`请求的编号都要大，那么将会响应它接受到的编号最大的`proposal`(如果有的话)到`proposer`并且承诺不再接受任何编号小于*n*的`proposal`。**\n\n**阶段二**：\n\n1. **如果`proposer`从大多数的`acceptor`接受到编号为*n*的请求响应。那么将会发送编号为*n*且值为*v*的`accept`请求到接受`prepare`请求的每一个`acceptor`。这里的*v*是所有`prepare`响应中编号最大的响应中的值。或者当`prepare`响应中没有`proposal`时，该值由自己选择。**\n2. **如果`acceptor`接收到编号为*n*的`accept`请求，除非它已经响应了一个编号大于*n*的`prepare`请求，否则它将接受该`proposal`。**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个`proposer`可以提出多个`proposal`，只要它遵循每个`proposal`的算法即可。 它可以随时在协议中间放弃`proposal`。(即使`proposal`的请求和/或响应可能在`proposal`被放弃很长时间后到达目的地，也可以保持正确性。)如果某个`proposer`已经开始尝试发布编号更大的`proposal`，那么放弃`proposal`可能是一个好主意。因此，如果某个`acceptor`忽略了`prepare`或者是`accept`请求，那是因为已经接受了一个编号更大的`prepare`请求。所以`acceptor`应该通知`proposer`放弃它的`proposal`。这是对性能的优化并且不会影响正确性。\n\n### 2.3 `learn`一个被选择的值\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`learner`必须发现一个被大多数`acceptor`接受的`proposal`，才能`learn`被选择的值。最明显的算法是让每个`acceptor`在接受`proposal`时对所有`learner`做出回应，向他们发送`proposal`。这使`learner`能够尽快发现所选的值，但是它要求每个`acceptor`对每个`learner`做出回应-回应的数量等于`acceptor`数量与`learner`数量的乘积。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非拜占庭式失败的假设使一个`learner`很容易从另一个`learner`那里发现一个值已经被接受。我们可以让`acceptor`以他们的接受来回应一个特别的`learner`，当选择了一个值时，后者又会通知其他`learner`。这种方法需要所有`learner`进行额外的一轮努力才能发现所选的值。它也不太可靠，因为特别的`learner`可能会失败。但是，它需要的响应数量仅等于`acceptor`数量和`learner`数量之和。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更一般而言，`acceptor`可以用他们对某些特别的`learner`的接受做出响应，然后当选择了某个值时，每个`learner`都可以通知所有`learner`。使用更多的特别的`learner`以更高的通信复杂性为代价提供更高的可靠性。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于消息丢失，一个值的被选择可能会没有`learner`会发现。`learner`可以向`acceptor`询问他们接受了哪些`proposal`，但是`acceptor`的失败可能使得无法知道大多数人是否接受了特定`proposal`。在这种情况下，`learner`只有在选择新`proposal`时才能发现什么值被选择。 如果`learner`需要知道是否一个值被选择，则可以使用上述算法让`proposer`发布`proposal`。\n\n### 2.4 流程\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很容易构造这样一个场景，在该场景中，两个`proposer`各自不断发布数量越来越多的`proposal`序列，而从未选择过一个。 `proposer`*p*完成阶段1的编号为*n1*的`proposal`。然后，另一个`proposer`*q*完成阶段1的编号*n2>n1*的`proposal`。 `proposer`*p*的第2阶段编号为*n1*的`proposal`请求被忽略，因为`acceptor`都承诺不接受任何编号少于*n2*的新`proposal`。 因此，`proposer`*p*又开始以编号为*n3>n2*的`proposal`进行阶段1，导致阶段2`proposer`*q*的请求被忽略,并一直持续下去。为了保证进度，必须选择特别的`proposer`作为尝试发布`proposal`的唯一`proposer`。 如果特别的`proposer`可以与大多数`acceptor`成功通信，并且使用的`proposal`编号大于已使用的`proposal`的编号，那么它将成功发布被接受的`proposal`。并在得知某个`proposal`具有更高`proposal`编号的请求后通过放弃`proposal`再试一次，特别的`proposer`最终将选择足够高的`proposal`编号。如果系统（`proposer`，`acceptor`和通信网络）能够正常工作，那么可以通过选举一个特别的`proposer`来实现活跃性。Fischer，Lynch和Pattererson的著名成果表明，一种可靠的选择`proposer`的算法必须使用随机性或实时性，例如使用超时。但是，无论选举成功与否，都会确保安全。\n\n### 2.5 实现\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Paxos算法假设网络的进程。在其共识算法中，每个进程都扮演着`proposer`，`acceptor`和`learner`的角色。该算法选择一个`leader`，该`leader`同时扮演特别的`proposer`与特别的`learner`。Paxos共识算法正是上述算法，其中请求和响应作为普通消息发送。（响应消息带有相应的`proposal`编号，以防止混淆。）在故障期间保留的稳定存储用于维护`acceptor`必须记住的信息。`acceptor`在实际发送响应之前将其预期的响应记录在稳定的存储中。剩下的就是描述如何保证没有两个`proposal`编号相同的机制。不同的`proposer`从不相交的数字集中选择他们的数字，因此两个不同的`proposer`从不发布具有相同编号的`proposal`。 每个`proposer`（在稳定的存储中）都会记住尝试发布的编号最高的`proposal`，并从第1阶段开始使用比其已经使用过的`proposer`编号更高的`proposer`编号。\n\n## 3 状态机的实现\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实现分布式系统的一种简单方法是作为向中央服务器发出命令的客户端的集合。 服务器可以描述为以某种顺序执行客户端命令的确定性状态机。状态机具有当前状态。它通过将命令作为输入并产生输出和新状态来执行步骤。例如，分布式银行系统的客户可能是柜员，并且状态机状态可能由所有用户的帐户余额组成。 提现将通过执行状态机命令来执行，该命令会在且仅当余额大于提款金额时才减少帐户的余额，并产生旧余额和新余额作为输出。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果单个中央服务器发生故障，则该服务器的实施将失败。因此，我们改为使用服务器的集合，每个服务器独立实现状态机。 因为状态机是确定性的，所以如果所有服务器都执行相同的命令序列，则所有服务器将产生相同的状态序列和输出。 然后，发出命令的客户端可以使用任何服务器为其生成的输出。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了确保所有服务器执行相同的状态机命令序列，我们实现了Paxos共识算法的不同实例序列，第i个实例选择的值是序列中的第i个状态机命令。每个服务器在算法的每个实例中扮演所有角色(`proposer`，`acceptor`和`learner`)。现在，假设服务器组是固定的，因此共识算法的所有实例使用相同的角色。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在正常操作中，将选择一台服务器作为`leader`，在共识算法的所有实例中均充当特别的`proposer`(尝试发布`proposal`的唯一的`proposer`)。 客户将命令发送给`leader`，`leader`决定每个命令应出现的顺序。如果`leader`确定某个客户命令应为第135个命令，则它将尝试选择该命令作为共识算法的第135个实例的值。通常会成功。 它也可能由于故障而失败，或者因为另一台服务器也认为自己是`leader`并且认为另一条客户端命令应该为第135条命令。但是共识算法确保最多可以选择一个命令作为第135个命令。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方法的效率的关键在于，在Paxos共识算法中，直到第2阶段才选择要提出的值。回想一下，在完成`proposer`算法的第1阶段之后，要么确定了要提出的值，要么提议者可以自由提出任何值。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，将描述Paxos状态机实现在正常操作期间如何工作。稍后，将讨论可能出问题的地方。 考虑当前`leader`刚刚失败并选择了新`leader`时会发生什么。(系统启动是一种特殊情况，其中尚未提出任何命令。)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新的`leader`，在共识算法的所有情况下都是`learner`，应该知道已经选择的大多数命令。 假设它知道命令1–134、138和139，即共识算法实例1–134、138和139中选择的值。(将在后面看到如何在命令序列中出现这样的间隙.)然后，它执行实例135-137和大于139的所有实例的阶段1。(在下面描述如何做到这一点.)假设这些执行确定了在实例135和140中要提出的值，但在所有其他情况下都不受约束。领导者然后对实例135和140执行阶段2，从而选择命令135和140。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`leader`以及任何其他服务器一样`learn leader`知道的所有命令，现在可以执行命令1–135。但是，它无法执行它也知道的命令138-140，因为尚未选择命令136和137。`leader`可以将客户请求的下两个命令作为命令136和137。相反，我们通过建议一个特殊的`no-op`命令作为命令136和137，使状态保持不变，从而立即填补了空白。（这是通过执行共识算法实例136和137的阶段2来完成的。）一旦选择了这些`no-op`命令，就可以执行命令138-140。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在已选择命令1–140。`leader`还已经为大于共识算法140的所有实例完成了阶段1，并且可以自由地在那些实例的阶段2中提出任何值。它将命令号141分配给客户端请求的下一个命令，并建议将其作为共识算法实例141的阶段2中的值。 它提出了接收到的下一个客户端命令作为命令142，依此类推。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`leader`可以在获悉已选择其提出的命令141之前提出命令142。 它在提议命令141中发送的所有消息都可能丢失，并且有可能在任何其他服务器了解`leader`作为命令141提出的建议之前选择命令142。当`leader`未能收到在实例141中的消息对其阶段2的预期响应时，它将重传那些消息。如果一切顺利，将选择其建议的命令。但是，它可能失败，从而在选择的命令序列中留下空白。通常，假设`leader`可以提前获得α命令-也就是说，在选择命令`1`至`i`之后，它可以提出命令`i+1`至`i+α`。最多可能会出现`α-1`命令的间隔。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于实例135-137和大于139的所有实例,新选择的`leader`在上面的方案中使用共识算法执行第1阶段的次数不限。它对所有实例使用相同的编号，可以通过向其他服务器发送一条合理的短消息来实现此目的。在阶段1中，只有在`acceptor`已经从某个`proposer`那里收到阶段2消息的情况下，接受者才用简单的OK做出响应.(仅对于实例135和140是这种情况.)因此，服务器(充当`acceptor`)可以使用单个合理的短消息对所有实例进行响应。 因此，执行这些无限多个阶段1实例不会带来任何问题。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于`leader`失败选举新`leader`的情况很少见，因此执行状态机命令的有效成本（即，对命令/值达成共识）是仅执行共识算法第二阶段的成本。可以证明，存在故障时达成共识的任何算法的最小可能成本在Paxos共识算法的第2阶段。 因此，Paxos算法本质上是最佳的。\n对系统正常运行的讨论假定只有一个`leader`，除了在当前`leader`失败和选举新`leader`之间的短暂时间之外。 在异常情况下，`leader`选举可能会失败。如果没有服务器充当`leader`，则不会提出新命令。如果多个服务器认为自己是`leader`，则它们都可以在同一共识算法实例中提出值，这可能会阻止选择任何值。但是，安全性得以保留-两个不同的服务器将永远不会在选择作为第i个状态机命令的值上发生分歧。仅选举一位`leader`才能确保取得进展。\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果服务器组可以更改，则必须采用某种方法确定哪些服务器实现共识算法的哪些实例。最简单的方法是通过状态机本身。 当前服务器集可以成为状态的一部分，并且可以通过普通的状态机命令进行更改。我们可以允许`leader`提前获得`α`命令,通过让执行第`i`个状态机命令后的状态指定执行共识算法的实例`i+α`的服务器集。这允许任意复杂的重新配置算法的简单实现。","slug":"blog/consensus/paxos","published":1,"updated":"2020-05-11T03:44:37.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyf10004k0vqhbxi3m61","content":"<h1 id=\"使Paxos变简单\"><a href=\"#使Paxos变简单\" class=\"headerlink\" title=\"使Paxos变简单\"></a>使Paxos变简单</h1><p><strong>摘要</strong><br>Paxos算法，用英语说明时，变得非常简单。</p>\n<h2 id=\"1-介绍\"><a href=\"#1-介绍\" class=\"headerlink\" title=\"1 介绍\"></a>1 介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;人们一直认为，用于实现容错分布式系统的Paxos算法难以理解，可能是因为最初的演示文稿对许多读者来说是希腊文.事实上，它是分布式算法中最简单，最有效的方法之一。它的核心是共识算法。下一节将说明这种共识算法几乎不可避免地遵循了我们希望它满足的特性。最后一部分介绍了完整的Paxos算法，该算法是通过将共识直接应用于构建分布式系统的状态机方法而获得的，这种方法应该是众所周知的，因为它是有关分布式系统理论的最常被引用的文章的主题。</p>\n<h2 id=\"2-共识算法\"><a href=\"#2-共识算法\" class=\"headerlink\" title=\"2 共识算法\"></a>2 共识算法</h2><h3 id=\"2-1-问题\"><a href=\"#2-1-问题\" class=\"headerlink\" title=\"2.1 问题\"></a>2.1 问题</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设有一个可以提出值的进程的集合。共识算法确保只有一个提出的值被选中。如果没有值被提出，则没有值应该被选中。如果一个值被选中，那么所有过程应该能够<code>learn</code>被选中的值。共识需要满足以下要求：</p>\n<ul>\n<li>只有被提出的值才可以被选中</li>\n<li>被选中的只有一个值</li>\n<li>除非一个值真正地被选中，否则某个进程不会去<code>learn</code>这个值。</li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们不会尝试指定精确的活动要求，然而，目标是确保最终存在一个值被选定。并且当一个值被选定时，进程最终会<code>learn</code>到这个值。<br>我们在共识算法中定义了三种角色：</p>\n<ul>\n<li><code>proposers</code></li>\n<li><code>acceptors</code></li>\n<li><code>learners</code></li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在算法的实现中，某个进程可能同时担任多个角色，但是在这里不讨论角色到进程的映射关系。<br>假设角色之间通过发送消息进行通信。我们使用异步，非拜占庭模型：</p>\n<ul>\n<li>角色以任意的速度执行，可能由于停止而宕掉，可能会重启。所有的角色可能在一个值被选中之后宕掉重启。除非宕掉再重启的角色可以记住某些信息，否则等重启后无法确定被选定的值。</li>\n<li>消息可能要花很长时间才能被交付，可能会复制可能会丢失，但是都没有关系。</li>\n</ul>\n<h3 id=\"2-2-选择一个值\"><a href=\"#2-2-选择一个值\" class=\"headerlink\" title=\"2.2 选择一个值\"></a>2.2 选择一个值</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最简单的方式是存在单个的<code>acceptor</code>角色然后选择一个值。一个<code>proposer</code>发送一个<code>proposal</code>到<code>acceptor</code>，<code>acceptor</code>选择它接收到的第一个<code>proposal</code>的值。尽管简单，但是这种解决方案不能满足要求，因为如果<code>acceptor</code>宕掉将会使未来的步骤无法继续(单点故障)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，让我们尝试另外一种方式选择一个值。使用多个<code>acceptor</code>角色代替单个<code>acceptor</code>。一个<code>proposer</code>发送一个<code>proposal</code>值到<code>acceptor</code>角色的集合。<code>acceptor</code>可能会接受<code>proposal</code>的值。当足够多的<code>acceptor</code>接受了该值，则说明这个值被选择了。足够多是多少呢？为了确保只有一个值被选择。我们可以让足够多数量的一组包含任何大多数角色。因为任何两个足够多数量的组都至少有一个共同的接受者，所以如果一个接受者最多可以接受一个值，则此方法有效。<br>在没有失败或消息丢失的情况下，如果只有一个值由单个的<code>proposer</code>提出，我们想要这个值被选择，需要满足以下要求：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P1</strong>:<strong>任何<code>acceptor</code>必须接受它收到的第一个<code>proposal</code>。</strong></p>\n<p>但是这个要求会出现一个问题。如果在同一时间有多个不同的<code>proposer</code>提出多个值，将会导致这种状态：每一个<code>acceptor</code>将会接受到一个值，但是不存在一个被大多数成员接受的值。即使只提出了两个值。如果每一个都由一半的<code>acceptor</code>接受，当一个<code>acceptor</code>宕掉后，将无法确定哪一个值被选择。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P1</strong>和<strong>被大多数的<code>acceptor</code>接受的值才能被选择</strong>，这两个要求隐性说名了每一个<code>acceptor</code>都必须可以接受多个值。我们通过为每个<code>proposal</code>分配一个（自然）编号来跟踪接受者可以接受的不同提案，那么每一个<code>proposal</code>将由一个<code>proposal</code>序号和一个值组成。为了避免冲突，我们要求不同的<code>proposal</code>所含有的<code>proposal</code>序号都是不同的。如何做到这一点取决于实现方法。现在我们只是假设。当一个<code>proposal</code>的值被大多数<code>acceptor</code>接受，那么该值说明被选择。这种情况下，我们说该<code>proposal</code>被选择。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们允许多个<code>proposal</code>被选择。但是需要保证所有被选择的<code>proposal</code>具有相同的值。通过对<code>proposal</code>编号的归纳，足以保证：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P2</strong>：<strong>如果一个值为<em>v</em>的<code>proposal</code>被选择，那么被选择的比该<code>proposal</code>编号大的<code>proposal</code>具有相同的值<em>v</em>。</strong></p>\n<p>由于数字是完全有序的，因此条件P2保证了仅选择一个值的关键安全性。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个<code>proposal</code>要被选择，建议必须至少由一个<code>acceptor</code>接受。 因此，我们可以通过满足以下条件来满足P2：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P2<sup>a</sup></strong>：<strong>如果一个值为<em>v</em>的<code>proposal</code>被选择。那么由任意的<code>acceptor</code>接受的被选择的比该<code>proposal</code>编号大的<code>proposal</code>具有相同的值<em>v</em>。</strong></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们始终保证<strong>P1</strong>来确保一些<code>proposal</code>被选择。因为通信是异步的。一个<code>proposal</code>可以被一些从没有接受到任何<code>proposal</code>的<code>acceptor</code><em>c</em>选择。假设一个新的<code>proposer</code>刚刚启动就接受到一个高编号的且值不同的<code>proposal</code>。<strong>P1</strong>则要求<em>c</em>接受该<code>proposal</code>，因此不满足要求<strong>P2<sup>a</sup></strong>.维持<strong>P1</strong>和<strong>P2<sup>a</sup></strong>需要加强P2<sup>a</sup>为：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P2<sup>b</sup></strong>:<strong>如果一个值为<em>v</em>的<code>proposal</code>被选择，那么每一个由任意的<code>proposer</code>提出的编号高的<code>proposal</code>都具有相同的<em>v</em>。</strong></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于一个<code>proposal</code>都需要在被任意<code>acceptor</code>接受之前都由<code>proposer</code>提出，因此满足了要求<strong>P2<sub>b</sub></strong>，就满足了要求<strong>P2<sub>a</sub></strong>,所以也就满足了<strong>P2</strong>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了发现如何满足要求<strong>P2<sup>b</sup></strong>,让我们考虑一下如何证明它成立。我们假设被选择的<code>proposal</code>具有编号<em>m</em>与值<em>v</em>并且表明证明任何发布的编号为<em>n</em>&gt;<em>m</em>的<code>proposal</code>也具有值<em>v</em>。我们可以通过对<em>n</em>进行归纳来简化证明，因此可以证明<code>proposal</code>编号<em>n</em>在值<em>v</em>的附加假设下每个提案编号都在<em>m</em>..(<em>n</em>−1)区间内并且值为<em>v</em>，其中<em>i..j<em>表示从</em>i<em>到</em>j<em>的一组数字。为了选择编号为</em>m<em>的<code>proposal</code>，必须有一些由大多数<code>acceptor</code>组成的集合</em>C<em>，以便</em>C<em>中的每个<code>acceptor</code>都接受它。将其与归纳假设相结合，选择</em>m<em>的假设就意味着：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</em>C<em>中的每个<code>acceptor</code>都接受了一个编号为</em>m</em>..(<em>n-1</em>)的<code>proposal</code>，并且任何<code>acceptor</code>接受的每个编号为<em>m</em>..(<em>n-1</em>)的<code>proposal</code>都具有值<em>v</em>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由大多数成员组成的集合<em>s</em>，至少包括一个<em>C</em>中的成员。我们可以通过确保以下不变式得出结论：编号为<em>n</em>的<code>proposal</code>具有值<em>v</em>.<br><strong>P2<sup>c</sup></strong>:<strong>对于任何值<em>v</em>和<em>n</em>，如果一个<code>proposal</code>具有编号<em>n</em>和值<em>v</em>，那么由主要<code>acceptor</code>组成的集合满足以下其中一个条件：</strong></p>\n<ul>\n<li><strong><em>S</em>中的<code>acceptor</code>不会接受任何编号小于<em>n</em>的<code>proposal</code>。</strong></li>\n<li><strong><em>S</em>中的<code>acceptor</code>接受的最大编号的<code>proposal</code>的值为<em>v</em>。</strong></li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此满足了<strong>P2<sup>c</sup></strong>的不变式即满足了<strong>P2<sup>b</sup>.</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了维持<strong>P2<sup>c</sup></strong>的不变式，<code>proposer</code>想要提出一个编号为<em>n</em>的<code>proposal</code>必须<code>learn</code>(如果有的话)已被或将要被大多数<code>acceptor</code>中的每个<code>acceptor</code>接受的编号小于<em>n</em>的最高编号的<code>proposal</code>。了解已经接受的<code>proposal</code>很容易；预测未来是否会被接受很难。<code>proposer</code>没有试图预测未来，而是通过提取不会有任何此类接受的承诺来控制未来。换句话说，<code>proposer</code>要求<code>acceptor</code>不接受任何其他编号小于<em>n</em>的<code>proposal</code>。 推导出以下用于发布提案的算法：</p>\n<ul>\n<li><strong>一个<code>proposal</code>选择编号为<em>n</em>的<code>proposal</code>并发送请求到包括半数以上个<code>acceptor</code>的集合，并要求得到以下其中一个回应：</strong><ol>\n<li><strong>一个不会接受编号值小于<em>n</em>的<code>proposal</code>的承诺。</strong></li>\n<li><strong>如果<code>acceptor</code>已经接受过<code>proposal</code>，则响应已接受的小于编号<em>n</em>的最大编号的<code>proposal</code>。</strong></li>\n</ol>\n</li>\n</ul>\n<p>将该请求称为编号为<em>n</em>的<code>prepare</code>请求。</p>\n<ul>\n<li><strong>如果<code>proposer</code>接受到大部分<code>acceptor</code>的请求响应，那么可以提出一个编号为<em>n</em>且值为<em>v</em>的<code>proposal</code>。这里的<em>v</em>是请求响应中编号最高的<code>proposal</code>中的值。或者如果响应中没有任何<code>proposal</code>，那么该值将由<code>proposer</code>自由选择。</strong></li>\n</ul>\n<p><code>proposer</code>通过发送<code>proposal</code>到包括半数以上个<code>acceptor</code>集合(需要与起初的请求集合不是同一个)，并期望接受该请求。将该请求称为<code>accept</code>请求。</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里描述的是关于<code>proposer</code>的算法。关于<code>acceptor</code>呢？<code>acceptor</code>可以从<code>proposer</code>接收两种类型的请求：<code>prepare</code>和<code>accept</code>请求。<code>acceptor</code>可以忽略任何请求而不会影响安全性。 因此，我们仅需说何时才允许响应请求。它总是可以响应<code>prepare</code>请求。 如果它没有答应不接受，它可以响应<code>accept</code>请求，接受<code>proposal</code>。 换一种说法：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P1<sup>a</sup></strong>:<strong>如果<code>acceptor</code>没有响应编号大于<em>n</em>的<code>prepare</code>请求那么可以接收一个编号为<em>n</em>的<code>proposal</code>。</strong></p>\n<p>观察到<strong>P1<sup>a</sup></strong>包含<strong>P1</strong>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们现在拥有了一个完整的算法选择一个满足安全性要求的值-通过假设一个唯一的<code>proposal</code>编号。最终算法通过一个小的优化获得。<br>假设<code>acceptor</code>收到编号为<em>n</em>的<code>prepare</code>请求.但是已经响应过一个编号值大于<em>n</em>的<code>prepare</code>请求。因此承诺不会接受任何编号为<em>n</em>的新的<code>proposal</code>请求。这样，<code>acceptor</code>就没有理由响应新的<code>prepare</code>请求，因为它不会接受<code>proposer</code>想要发出的编号为<em>n</em>的<code>proposal</code>。 因此，我们让<code>acceptor</code>忽略这样的<code>prepare</code>请求。 我们也可以忽略它已经接受的<code>proposal</code>的<code>prepare</code>请求。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过这种优化，<code>acceptor</code>只需要记住它曾经接受的编号最高的<code>proposal</code>以及它已响应的编号最高的<code>prepare</code>请求的数量。无论如何失败，<strong>P2<sup>c</sup></strong>也必须保持不变，所以即使失败，接受者也必须记住该信息，然后重新启动。 请注意，只要<code>proposer</code>从不尝试发布具有相同编号的另一个<code>proposal</code>，它总是可以放弃该<code>proposal</code>并将其遗忘。<br>将<code>proposer</code>和<code>acceptor</code>的动作放在一起，我们看到该算法在以下两个阶段中运行：</p>\n<p><strong>阶段一</strong>：</p>\n<ol>\n<li><strong><code>proposer</code>选择一个编号为<em>n</em>的<code>proposal</code>并发送编号为<em>n</em>的<code>prepare</code>请求到大多数的<code>acceptor</code>。</strong></li>\n<li><strong>如果<code>acceptor</code>接受了编号为<em>n</em>的<code>prepare</code>请求，并且编号<em>n</em>比接受到的任何<code>prepare</code>请求的编号都要大，那么将会响应它接受到的编号最大的<code>proposal</code>(如果有的话)到<code>proposer</code>并且承诺不再接受任何编号小于<em>n</em>的<code>proposal</code>。</strong></li>\n</ol>\n<p><strong>阶段二</strong>：</p>\n<ol>\n<li><strong>如果<code>proposer</code>从大多数的<code>acceptor</code>接受到编号为<em>n</em>的请求响应。那么将会发送编号为<em>n</em>且值为<em>v</em>的<code>accept</code>请求到接受<code>prepare</code>请求的每一个<code>acceptor</code>。这里的<em>v</em>是所有<code>prepare</code>响应中编号最大的响应中的值。或者当<code>prepare</code>响应中没有<code>proposal</code>时，该值由自己选择。</strong></li>\n<li><strong>如果<code>acceptor</code>接收到编号为<em>n</em>的<code>accept</code>请求，除非它已经响应了一个编号大于<em>n</em>的<code>prepare</code>请求，否则它将接受该<code>proposal</code>。</strong></li>\n</ol>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个<code>proposer</code>可以提出多个<code>proposal</code>，只要它遵循每个<code>proposal</code>的算法即可。 它可以随时在协议中间放弃<code>proposal</code>。(即使<code>proposal</code>的请求和/或响应可能在<code>proposal</code>被放弃很长时间后到达目的地，也可以保持正确性。)如果某个<code>proposer</code>已经开始尝试发布编号更大的<code>proposal</code>，那么放弃<code>proposal</code>可能是一个好主意。因此，如果某个<code>acceptor</code>忽略了<code>prepare</code>或者是<code>accept</code>请求，那是因为已经接受了一个编号更大的<code>prepare</code>请求。所以<code>acceptor</code>应该通知<code>proposer</code>放弃它的<code>proposal</code>。这是对性能的优化并且不会影响正确性。</p>\n<h3 id=\"2-3-learn一个被选择的值\"><a href=\"#2-3-learn一个被选择的值\" class=\"headerlink\" title=\"2.3 learn一个被选择的值\"></a>2.3 <code>learn</code>一个被选择的值</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>learner</code>必须发现一个被大多数<code>acceptor</code>接受的<code>proposal</code>，才能<code>learn</code>被选择的值。最明显的算法是让每个<code>acceptor</code>在接受<code>proposal</code>时对所有<code>learner</code>做出回应，向他们发送<code>proposal</code>。这使<code>learner</code>能够尽快发现所选的值，但是它要求每个<code>acceptor</code>对每个<code>learner</code>做出回应-回应的数量等于<code>acceptor</code>数量与<code>learner</code>数量的乘积。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非拜占庭式失败的假设使一个<code>learner</code>很容易从另一个<code>learner</code>那里发现一个值已经被接受。我们可以让<code>acceptor</code>以他们的接受来回应一个特别的<code>learner</code>，当选择了一个值时，后者又会通知其他<code>learner</code>。这种方法需要所有<code>learner</code>进行额外的一轮努力才能发现所选的值。它也不太可靠，因为特别的<code>learner</code>可能会失败。但是，它需要的响应数量仅等于<code>acceptor</code>数量和<code>learner</code>数量之和。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更一般而言，<code>acceptor</code>可以用他们对某些特别的<code>learner</code>的接受做出响应，然后当选择了某个值时，每个<code>learner</code>都可以通知所有<code>learner</code>。使用更多的特别的<code>learner</code>以更高的通信复杂性为代价提供更高的可靠性。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于消息丢失，一个值的被选择可能会没有<code>learner</code>会发现。<code>learner</code>可以向<code>acceptor</code>询问他们接受了哪些<code>proposal</code>，但是<code>acceptor</code>的失败可能使得无法知道大多数人是否接受了特定<code>proposal</code>。在这种情况下，<code>learner</code>只有在选择新<code>proposal</code>时才能发现什么值被选择。 如果<code>learner</code>需要知道是否一个值被选择，则可以使用上述算法让<code>proposer</code>发布<code>proposal</code>。</p>\n<h3 id=\"2-4-流程\"><a href=\"#2-4-流程\" class=\"headerlink\" title=\"2.4 流程\"></a>2.4 流程</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很容易构造这样一个场景，在该场景中，两个<code>proposer</code>各自不断发布数量越来越多的<code>proposal</code>序列，而从未选择过一个。 <code>proposer</code><em>p</em>完成阶段1的编号为<em>n1</em>的<code>proposal</code>。然后，另一个<code>proposer</code><em>q</em>完成阶段1的编号<em>n2&gt;n1</em>的<code>proposal</code>。 <code>proposer</code><em>p</em>的第2阶段编号为<em>n1</em>的<code>proposal</code>请求被忽略，因为<code>acceptor</code>都承诺不接受任何编号少于<em>n2</em>的新<code>proposal</code>。 因此，<code>proposer</code><em>p</em>又开始以编号为<em>n3&gt;n2</em>的<code>proposal</code>进行阶段1，导致阶段2<code>proposer</code><em>q</em>的请求被忽略,并一直持续下去。为了保证进度，必须选择特别的<code>proposer</code>作为尝试发布<code>proposal</code>的唯一<code>proposer</code>。 如果特别的<code>proposer</code>可以与大多数<code>acceptor</code>成功通信，并且使用的<code>proposal</code>编号大于已使用的<code>proposal</code>的编号，那么它将成功发布被接受的<code>proposal</code>。并在得知某个<code>proposal</code>具有更高<code>proposal</code>编号的请求后通过放弃<code>proposal</code>再试一次，特别的<code>proposer</code>最终将选择足够高的<code>proposal</code>编号。如果系统（<code>proposer</code>，<code>acceptor</code>和通信网络）能够正常工作，那么可以通过选举一个特别的<code>proposer</code>来实现活跃性。Fischer，Lynch和Pattererson的著名成果表明，一种可靠的选择<code>proposer</code>的算法必须使用随机性或实时性，例如使用超时。但是，无论选举成功与否，都会确保安全。</p>\n<h3 id=\"2-5-实现\"><a href=\"#2-5-实现\" class=\"headerlink\" title=\"2.5 实现\"></a>2.5 实现</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Paxos算法假设网络的进程。在其共识算法中，每个进程都扮演着<code>proposer</code>，<code>acceptor</code>和<code>learner</code>的角色。该算法选择一个<code>leader</code>，该<code>leader</code>同时扮演特别的<code>proposer</code>与特别的<code>learner</code>。Paxos共识算法正是上述算法，其中请求和响应作为普通消息发送。（响应消息带有相应的<code>proposal</code>编号，以防止混淆。）在故障期间保留的稳定存储用于维护<code>acceptor</code>必须记住的信息。<code>acceptor</code>在实际发送响应之前将其预期的响应记录在稳定的存储中。剩下的就是描述如何保证没有两个<code>proposal</code>编号相同的机制。不同的<code>proposer</code>从不相交的数字集中选择他们的数字，因此两个不同的<code>proposer</code>从不发布具有相同编号的<code>proposal</code>。 每个<code>proposer</code>（在稳定的存储中）都会记住尝试发布的编号最高的<code>proposal</code>，并从第1阶段开始使用比其已经使用过的<code>proposer</code>编号更高的<code>proposer</code>编号。</p>\n<h2 id=\"3-状态机的实现\"><a href=\"#3-状态机的实现\" class=\"headerlink\" title=\"3 状态机的实现\"></a>3 状态机的实现</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实现分布式系统的一种简单方法是作为向中央服务器发出命令的客户端的集合。 服务器可以描述为以某种顺序执行客户端命令的确定性状态机。状态机具有当前状态。它通过将命令作为输入并产生输出和新状态来执行步骤。例如，分布式银行系统的客户可能是柜员，并且状态机状态可能由所有用户的帐户余额组成。 提现将通过执行状态机命令来执行，该命令会在且仅当余额大于提款金额时才减少帐户的余额，并产生旧余额和新余额作为输出。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果单个中央服务器发生故障，则该服务器的实施将失败。因此，我们改为使用服务器的集合，每个服务器独立实现状态机。 因为状态机是确定性的，所以如果所有服务器都执行相同的命令序列，则所有服务器将产生相同的状态序列和输出。 然后，发出命令的客户端可以使用任何服务器为其生成的输出。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了确保所有服务器执行相同的状态机命令序列，我们实现了Paxos共识算法的不同实例序列，第i个实例选择的值是序列中的第i个状态机命令。每个服务器在算法的每个实例中扮演所有角色(<code>proposer</code>，<code>acceptor</code>和<code>learner</code>)。现在，假设服务器组是固定的，因此共识算法的所有实例使用相同的角色。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在正常操作中，将选择一台服务器作为<code>leader</code>，在共识算法的所有实例中均充当特别的<code>proposer</code>(尝试发布<code>proposal</code>的唯一的<code>proposer</code>)。 客户将命令发送给<code>leader</code>，<code>leader</code>决定每个命令应出现的顺序。如果<code>leader</code>确定某个客户命令应为第135个命令，则它将尝试选择该命令作为共识算法的第135个实例的值。通常会成功。 它也可能由于故障而失败，或者因为另一台服务器也认为自己是<code>leader</code>并且认为另一条客户端命令应该为第135条命令。但是共识算法确保最多可以选择一个命令作为第135个命令。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方法的效率的关键在于，在Paxos共识算法中，直到第2阶段才选择要提出的值。回想一下，在完成<code>proposer</code>算法的第1阶段之后，要么确定了要提出的值，要么提议者可以自由提出任何值。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，将描述Paxos状态机实现在正常操作期间如何工作。稍后，将讨论可能出问题的地方。 考虑当前<code>leader</code>刚刚失败并选择了新<code>leader</code>时会发生什么。(系统启动是一种特殊情况，其中尚未提出任何命令。)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新的<code>leader</code>，在共识算法的所有情况下都是<code>learner</code>，应该知道已经选择的大多数命令。 假设它知道命令1–134、138和139，即共识算法实例1–134、138和139中选择的值。(将在后面看到如何在命令序列中出现这样的间隙.)然后，它执行实例135-137和大于139的所有实例的阶段1。(在下面描述如何做到这一点.)假设这些执行确定了在实例135和140中要提出的值，但在所有其他情况下都不受约束。领导者然后对实例135和140执行阶段2，从而选择命令135和140。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>leader</code>以及任何其他服务器一样<code>learn leader</code>知道的所有命令，现在可以执行命令1–135。但是，它无法执行它也知道的命令138-140，因为尚未选择命令136和137。<code>leader</code>可以将客户请求的下两个命令作为命令136和137。相反，我们通过建议一个特殊的<code>no-op</code>命令作为命令136和137，使状态保持不变，从而立即填补了空白。（这是通过执行共识算法实例136和137的阶段2来完成的。）一旦选择了这些<code>no-op</code>命令，就可以执行命令138-140。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在已选择命令1–140。<code>leader</code>还已经为大于共识算法140的所有实例完成了阶段1，并且可以自由地在那些实例的阶段2中提出任何值。它将命令号141分配给客户端请求的下一个命令，并建议将其作为共识算法实例141的阶段2中的值。 它提出了接收到的下一个客户端命令作为命令142，依此类推。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>leader</code>可以在获悉已选择其提出的命令141之前提出命令142。 它在提议命令141中发送的所有消息都可能丢失，并且有可能在任何其他服务器了解<code>leader</code>作为命令141提出的建议之前选择命令142。当<code>leader</code>未能收到在实例141中的消息对其阶段2的预期响应时，它将重传那些消息。如果一切顺利，将选择其建议的命令。但是，它可能失败，从而在选择的命令序列中留下空白。通常，假设<code>leader</code>可以提前获得α命令-也就是说，在选择命令<code>1</code>至<code>i</code>之后，它可以提出命令<code>i+1</code>至<code>i+α</code>。最多可能会出现<code>α-1</code>命令的间隔。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于实例135-137和大于139的所有实例,新选择的<code>leader</code>在上面的方案中使用共识算法执行第1阶段的次数不限。它对所有实例使用相同的编号，可以通过向其他服务器发送一条合理的短消息来实现此目的。在阶段1中，只有在<code>acceptor</code>已经从某个<code>proposer</code>那里收到阶段2消息的情况下，接受者才用简单的OK做出响应.(仅对于实例135和140是这种情况.)因此，服务器(充当<code>acceptor</code>)可以使用单个合理的短消息对所有实例进行响应。 因此，执行这些无限多个阶段1实例不会带来任何问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于<code>leader</code>失败选举新<code>leader</code>的情况很少见，因此执行状态机命令的有效成本（即，对命令/值达成共识）是仅执行共识算法第二阶段的成本。可以证明，存在故障时达成共识的任何算法的最小可能成本在Paxos共识算法的第2阶段。 因此，Paxos算法本质上是最佳的。<br>对系统正常运行的讨论假定只有一个<code>leader</code>，除了在当前<code>leader</code>失败和选举新<code>leader</code>之间的短暂时间之外。 在异常情况下，<code>leader</code>选举可能会失败。如果没有服务器充当<code>leader</code>，则不会提出新命令。如果多个服务器认为自己是<code>leader</code>，则它们都可以在同一共识算法实例中提出值，这可能会阻止选择任何值。但是，安全性得以保留-两个不同的服务器将永远不会在选择作为第i个状态机命令的值上发生分歧。仅选举一位<code>leader</code>才能确保取得进展。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果服务器组可以更改，则必须采用某种方法确定哪些服务器实现共识算法的哪些实例。最简单的方法是通过状态机本身。 当前服务器集可以成为状态的一部分，并且可以通过普通的状态机命令进行更改。我们可以允许<code>leader</code>提前获得<code>α</code>命令,通过让执行第<code>i</code>个状态机命令后的状态指定执行共识算法的实例<code>i+α</code>的服务器集。这允许任意复杂的重新配置算法的简单实现。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"使Paxos变简单\"><a href=\"#使Paxos变简单\" class=\"headerlink\" title=\"使Paxos变简单\"></a>使Paxos变简单</h1><p><strong>摘要</strong><br>Paxos算法，用英语说明时，变得非常简单。</p>\n<h2 id=\"1-介绍\"><a href=\"#1-介绍\" class=\"headerlink\" title=\"1 介绍\"></a>1 介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;人们一直认为，用于实现容错分布式系统的Paxos算法难以理解，可能是因为最初的演示文稿对许多读者来说是希腊文.事实上，它是分布式算法中最简单，最有效的方法之一。它的核心是共识算法。下一节将说明这种共识算法几乎不可避免地遵循了我们希望它满足的特性。最后一部分介绍了完整的Paxos算法，该算法是通过将共识直接应用于构建分布式系统的状态机方法而获得的，这种方法应该是众所周知的，因为它是有关分布式系统理论的最常被引用的文章的主题。</p>\n<h2 id=\"2-共识算法\"><a href=\"#2-共识算法\" class=\"headerlink\" title=\"2 共识算法\"></a>2 共识算法</h2><h3 id=\"2-1-问题\"><a href=\"#2-1-问题\" class=\"headerlink\" title=\"2.1 问题\"></a>2.1 问题</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设有一个可以提出值的进程的集合。共识算法确保只有一个提出的值被选中。如果没有值被提出，则没有值应该被选中。如果一个值被选中，那么所有过程应该能够<code>learn</code>被选中的值。共识需要满足以下要求：</p>\n<ul>\n<li>只有被提出的值才可以被选中</li>\n<li>被选中的只有一个值</li>\n<li>除非一个值真正地被选中，否则某个进程不会去<code>learn</code>这个值。</li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们不会尝试指定精确的活动要求，然而，目标是确保最终存在一个值被选定。并且当一个值被选定时，进程最终会<code>learn</code>到这个值。<br>我们在共识算法中定义了三种角色：</p>\n<ul>\n<li><code>proposers</code></li>\n<li><code>acceptors</code></li>\n<li><code>learners</code></li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在算法的实现中，某个进程可能同时担任多个角色，但是在这里不讨论角色到进程的映射关系。<br>假设角色之间通过发送消息进行通信。我们使用异步，非拜占庭模型：</p>\n<ul>\n<li>角色以任意的速度执行，可能由于停止而宕掉，可能会重启。所有的角色可能在一个值被选中之后宕掉重启。除非宕掉再重启的角色可以记住某些信息，否则等重启后无法确定被选定的值。</li>\n<li>消息可能要花很长时间才能被交付，可能会复制可能会丢失，但是都没有关系。</li>\n</ul>\n<h3 id=\"2-2-选择一个值\"><a href=\"#2-2-选择一个值\" class=\"headerlink\" title=\"2.2 选择一个值\"></a>2.2 选择一个值</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最简单的方式是存在单个的<code>acceptor</code>角色然后选择一个值。一个<code>proposer</code>发送一个<code>proposal</code>到<code>acceptor</code>，<code>acceptor</code>选择它接收到的第一个<code>proposal</code>的值。尽管简单，但是这种解决方案不能满足要求，因为如果<code>acceptor</code>宕掉将会使未来的步骤无法继续(单点故障)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，让我们尝试另外一种方式选择一个值。使用多个<code>acceptor</code>角色代替单个<code>acceptor</code>。一个<code>proposer</code>发送一个<code>proposal</code>值到<code>acceptor</code>角色的集合。<code>acceptor</code>可能会接受<code>proposal</code>的值。当足够多的<code>acceptor</code>接受了该值，则说明这个值被选择了。足够多是多少呢？为了确保只有一个值被选择。我们可以让足够多数量的一组包含任何大多数角色。因为任何两个足够多数量的组都至少有一个共同的接受者，所以如果一个接受者最多可以接受一个值，则此方法有效。<br>在没有失败或消息丢失的情况下，如果只有一个值由单个的<code>proposer</code>提出，我们想要这个值被选择，需要满足以下要求：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P1</strong>:<strong>任何<code>acceptor</code>必须接受它收到的第一个<code>proposal</code>。</strong></p>\n<p>但是这个要求会出现一个问题。如果在同一时间有多个不同的<code>proposer</code>提出多个值，将会导致这种状态：每一个<code>acceptor</code>将会接受到一个值，但是不存在一个被大多数成员接受的值。即使只提出了两个值。如果每一个都由一半的<code>acceptor</code>接受，当一个<code>acceptor</code>宕掉后，将无法确定哪一个值被选择。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P1</strong>和<strong>被大多数的<code>acceptor</code>接受的值才能被选择</strong>，这两个要求隐性说名了每一个<code>acceptor</code>都必须可以接受多个值。我们通过为每个<code>proposal</code>分配一个（自然）编号来跟踪接受者可以接受的不同提案，那么每一个<code>proposal</code>将由一个<code>proposal</code>序号和一个值组成。为了避免冲突，我们要求不同的<code>proposal</code>所含有的<code>proposal</code>序号都是不同的。如何做到这一点取决于实现方法。现在我们只是假设。当一个<code>proposal</code>的值被大多数<code>acceptor</code>接受，那么该值说明被选择。这种情况下，我们说该<code>proposal</code>被选择。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们允许多个<code>proposal</code>被选择。但是需要保证所有被选择的<code>proposal</code>具有相同的值。通过对<code>proposal</code>编号的归纳，足以保证：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P2</strong>：<strong>如果一个值为<em>v</em>的<code>proposal</code>被选择，那么被选择的比该<code>proposal</code>编号大的<code>proposal</code>具有相同的值<em>v</em>。</strong></p>\n<p>由于数字是完全有序的，因此条件P2保证了仅选择一个值的关键安全性。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个<code>proposal</code>要被选择，建议必须至少由一个<code>acceptor</code>接受。 因此，我们可以通过满足以下条件来满足P2：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P2<sup>a</sup></strong>：<strong>如果一个值为<em>v</em>的<code>proposal</code>被选择。那么由任意的<code>acceptor</code>接受的被选择的比该<code>proposal</code>编号大的<code>proposal</code>具有相同的值<em>v</em>。</strong></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们始终保证<strong>P1</strong>来确保一些<code>proposal</code>被选择。因为通信是异步的。一个<code>proposal</code>可以被一些从没有接受到任何<code>proposal</code>的<code>acceptor</code><em>c</em>选择。假设一个新的<code>proposer</code>刚刚启动就接受到一个高编号的且值不同的<code>proposal</code>。<strong>P1</strong>则要求<em>c</em>接受该<code>proposal</code>，因此不满足要求<strong>P2<sup>a</sup></strong>.维持<strong>P1</strong>和<strong>P2<sup>a</sup></strong>需要加强P2<sup>a</sup>为：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P2<sup>b</sup></strong>:<strong>如果一个值为<em>v</em>的<code>proposal</code>被选择，那么每一个由任意的<code>proposer</code>提出的编号高的<code>proposal</code>都具有相同的<em>v</em>。</strong></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于一个<code>proposal</code>都需要在被任意<code>acceptor</code>接受之前都由<code>proposer</code>提出，因此满足了要求<strong>P2<sub>b</sub></strong>，就满足了要求<strong>P2<sub>a</sub></strong>,所以也就满足了<strong>P2</strong>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了发现如何满足要求<strong>P2<sup>b</sup></strong>,让我们考虑一下如何证明它成立。我们假设被选择的<code>proposal</code>具有编号<em>m</em>与值<em>v</em>并且表明证明任何发布的编号为<em>n</em>&gt;<em>m</em>的<code>proposal</code>也具有值<em>v</em>。我们可以通过对<em>n</em>进行归纳来简化证明，因此可以证明<code>proposal</code>编号<em>n</em>在值<em>v</em>的附加假设下每个提案编号都在<em>m</em>..(<em>n</em>−1)区间内并且值为<em>v</em>，其中<em>i..j<em>表示从</em>i<em>到</em>j<em>的一组数字。为了选择编号为</em>m<em>的<code>proposal</code>，必须有一些由大多数<code>acceptor</code>组成的集合</em>C<em>，以便</em>C<em>中的每个<code>acceptor</code>都接受它。将其与归纳假设相结合，选择</em>m<em>的假设就意味着：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</em>C<em>中的每个<code>acceptor</code>都接受了一个编号为</em>m</em>..(<em>n-1</em>)的<code>proposal</code>，并且任何<code>acceptor</code>接受的每个编号为<em>m</em>..(<em>n-1</em>)的<code>proposal</code>都具有值<em>v</em>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由大多数成员组成的集合<em>s</em>，至少包括一个<em>C</em>中的成员。我们可以通过确保以下不变式得出结论：编号为<em>n</em>的<code>proposal</code>具有值<em>v</em>.<br><strong>P2<sup>c</sup></strong>:<strong>对于任何值<em>v</em>和<em>n</em>，如果一个<code>proposal</code>具有编号<em>n</em>和值<em>v</em>，那么由主要<code>acceptor</code>组成的集合满足以下其中一个条件：</strong></p>\n<ul>\n<li><strong><em>S</em>中的<code>acceptor</code>不会接受任何编号小于<em>n</em>的<code>proposal</code>。</strong></li>\n<li><strong><em>S</em>中的<code>acceptor</code>接受的最大编号的<code>proposal</code>的值为<em>v</em>。</strong></li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此满足了<strong>P2<sup>c</sup></strong>的不变式即满足了<strong>P2<sup>b</sup>.</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了维持<strong>P2<sup>c</sup></strong>的不变式，<code>proposer</code>想要提出一个编号为<em>n</em>的<code>proposal</code>必须<code>learn</code>(如果有的话)已被或将要被大多数<code>acceptor</code>中的每个<code>acceptor</code>接受的编号小于<em>n</em>的最高编号的<code>proposal</code>。了解已经接受的<code>proposal</code>很容易；预测未来是否会被接受很难。<code>proposer</code>没有试图预测未来，而是通过提取不会有任何此类接受的承诺来控制未来。换句话说，<code>proposer</code>要求<code>acceptor</code>不接受任何其他编号小于<em>n</em>的<code>proposal</code>。 推导出以下用于发布提案的算法：</p>\n<ul>\n<li><strong>一个<code>proposal</code>选择编号为<em>n</em>的<code>proposal</code>并发送请求到包括半数以上个<code>acceptor</code>的集合，并要求得到以下其中一个回应：</strong><ol>\n<li><strong>一个不会接受编号值小于<em>n</em>的<code>proposal</code>的承诺。</strong></li>\n<li><strong>如果<code>acceptor</code>已经接受过<code>proposal</code>，则响应已接受的小于编号<em>n</em>的最大编号的<code>proposal</code>。</strong></li>\n</ol>\n</li>\n</ul>\n<p>将该请求称为编号为<em>n</em>的<code>prepare</code>请求。</p>\n<ul>\n<li><strong>如果<code>proposer</code>接受到大部分<code>acceptor</code>的请求响应，那么可以提出一个编号为<em>n</em>且值为<em>v</em>的<code>proposal</code>。这里的<em>v</em>是请求响应中编号最高的<code>proposal</code>中的值。或者如果响应中没有任何<code>proposal</code>，那么该值将由<code>proposer</code>自由选择。</strong></li>\n</ul>\n<p><code>proposer</code>通过发送<code>proposal</code>到包括半数以上个<code>acceptor</code>集合(需要与起初的请求集合不是同一个)，并期望接受该请求。将该请求称为<code>accept</code>请求。</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里描述的是关于<code>proposer</code>的算法。关于<code>acceptor</code>呢？<code>acceptor</code>可以从<code>proposer</code>接收两种类型的请求：<code>prepare</code>和<code>accept</code>请求。<code>acceptor</code>可以忽略任何请求而不会影响安全性。 因此，我们仅需说何时才允许响应请求。它总是可以响应<code>prepare</code>请求。 如果它没有答应不接受，它可以响应<code>accept</code>请求，接受<code>proposal</code>。 换一种说法：</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P1<sup>a</sup></strong>:<strong>如果<code>acceptor</code>没有响应编号大于<em>n</em>的<code>prepare</code>请求那么可以接收一个编号为<em>n</em>的<code>proposal</code>。</strong></p>\n<p>观察到<strong>P1<sup>a</sup></strong>包含<strong>P1</strong>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们现在拥有了一个完整的算法选择一个满足安全性要求的值-通过假设一个唯一的<code>proposal</code>编号。最终算法通过一个小的优化获得。<br>假设<code>acceptor</code>收到编号为<em>n</em>的<code>prepare</code>请求.但是已经响应过一个编号值大于<em>n</em>的<code>prepare</code>请求。因此承诺不会接受任何编号为<em>n</em>的新的<code>proposal</code>请求。这样，<code>acceptor</code>就没有理由响应新的<code>prepare</code>请求，因为它不会接受<code>proposer</code>想要发出的编号为<em>n</em>的<code>proposal</code>。 因此，我们让<code>acceptor</code>忽略这样的<code>prepare</code>请求。 我们也可以忽略它已经接受的<code>proposal</code>的<code>prepare</code>请求。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过这种优化，<code>acceptor</code>只需要记住它曾经接受的编号最高的<code>proposal</code>以及它已响应的编号最高的<code>prepare</code>请求的数量。无论如何失败，<strong>P2<sup>c</sup></strong>也必须保持不变，所以即使失败，接受者也必须记住该信息，然后重新启动。 请注意，只要<code>proposer</code>从不尝试发布具有相同编号的另一个<code>proposal</code>，它总是可以放弃该<code>proposal</code>并将其遗忘。<br>将<code>proposer</code>和<code>acceptor</code>的动作放在一起，我们看到该算法在以下两个阶段中运行：</p>\n<p><strong>阶段一</strong>：</p>\n<ol>\n<li><strong><code>proposer</code>选择一个编号为<em>n</em>的<code>proposal</code>并发送编号为<em>n</em>的<code>prepare</code>请求到大多数的<code>acceptor</code>。</strong></li>\n<li><strong>如果<code>acceptor</code>接受了编号为<em>n</em>的<code>prepare</code>请求，并且编号<em>n</em>比接受到的任何<code>prepare</code>请求的编号都要大，那么将会响应它接受到的编号最大的<code>proposal</code>(如果有的话)到<code>proposer</code>并且承诺不再接受任何编号小于<em>n</em>的<code>proposal</code>。</strong></li>\n</ol>\n<p><strong>阶段二</strong>：</p>\n<ol>\n<li><strong>如果<code>proposer</code>从大多数的<code>acceptor</code>接受到编号为<em>n</em>的请求响应。那么将会发送编号为<em>n</em>且值为<em>v</em>的<code>accept</code>请求到接受<code>prepare</code>请求的每一个<code>acceptor</code>。这里的<em>v</em>是所有<code>prepare</code>响应中编号最大的响应中的值。或者当<code>prepare</code>响应中没有<code>proposal</code>时，该值由自己选择。</strong></li>\n<li><strong>如果<code>acceptor</code>接收到编号为<em>n</em>的<code>accept</code>请求，除非它已经响应了一个编号大于<em>n</em>的<code>prepare</code>请求，否则它将接受该<code>proposal</code>。</strong></li>\n</ol>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个<code>proposer</code>可以提出多个<code>proposal</code>，只要它遵循每个<code>proposal</code>的算法即可。 它可以随时在协议中间放弃<code>proposal</code>。(即使<code>proposal</code>的请求和/或响应可能在<code>proposal</code>被放弃很长时间后到达目的地，也可以保持正确性。)如果某个<code>proposer</code>已经开始尝试发布编号更大的<code>proposal</code>，那么放弃<code>proposal</code>可能是一个好主意。因此，如果某个<code>acceptor</code>忽略了<code>prepare</code>或者是<code>accept</code>请求，那是因为已经接受了一个编号更大的<code>prepare</code>请求。所以<code>acceptor</code>应该通知<code>proposer</code>放弃它的<code>proposal</code>。这是对性能的优化并且不会影响正确性。</p>\n<h3 id=\"2-3-learn一个被选择的值\"><a href=\"#2-3-learn一个被选择的值\" class=\"headerlink\" title=\"2.3 learn一个被选择的值\"></a>2.3 <code>learn</code>一个被选择的值</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>learner</code>必须发现一个被大多数<code>acceptor</code>接受的<code>proposal</code>，才能<code>learn</code>被选择的值。最明显的算法是让每个<code>acceptor</code>在接受<code>proposal</code>时对所有<code>learner</code>做出回应，向他们发送<code>proposal</code>。这使<code>learner</code>能够尽快发现所选的值，但是它要求每个<code>acceptor</code>对每个<code>learner</code>做出回应-回应的数量等于<code>acceptor</code>数量与<code>learner</code>数量的乘积。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非拜占庭式失败的假设使一个<code>learner</code>很容易从另一个<code>learner</code>那里发现一个值已经被接受。我们可以让<code>acceptor</code>以他们的接受来回应一个特别的<code>learner</code>，当选择了一个值时，后者又会通知其他<code>learner</code>。这种方法需要所有<code>learner</code>进行额外的一轮努力才能发现所选的值。它也不太可靠，因为特别的<code>learner</code>可能会失败。但是，它需要的响应数量仅等于<code>acceptor</code>数量和<code>learner</code>数量之和。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更一般而言，<code>acceptor</code>可以用他们对某些特别的<code>learner</code>的接受做出响应，然后当选择了某个值时，每个<code>learner</code>都可以通知所有<code>learner</code>。使用更多的特别的<code>learner</code>以更高的通信复杂性为代价提供更高的可靠性。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于消息丢失，一个值的被选择可能会没有<code>learner</code>会发现。<code>learner</code>可以向<code>acceptor</code>询问他们接受了哪些<code>proposal</code>，但是<code>acceptor</code>的失败可能使得无法知道大多数人是否接受了特定<code>proposal</code>。在这种情况下，<code>learner</code>只有在选择新<code>proposal</code>时才能发现什么值被选择。 如果<code>learner</code>需要知道是否一个值被选择，则可以使用上述算法让<code>proposer</code>发布<code>proposal</code>。</p>\n<h3 id=\"2-4-流程\"><a href=\"#2-4-流程\" class=\"headerlink\" title=\"2.4 流程\"></a>2.4 流程</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很容易构造这样一个场景，在该场景中，两个<code>proposer</code>各自不断发布数量越来越多的<code>proposal</code>序列，而从未选择过一个。 <code>proposer</code><em>p</em>完成阶段1的编号为<em>n1</em>的<code>proposal</code>。然后，另一个<code>proposer</code><em>q</em>完成阶段1的编号<em>n2&gt;n1</em>的<code>proposal</code>。 <code>proposer</code><em>p</em>的第2阶段编号为<em>n1</em>的<code>proposal</code>请求被忽略，因为<code>acceptor</code>都承诺不接受任何编号少于<em>n2</em>的新<code>proposal</code>。 因此，<code>proposer</code><em>p</em>又开始以编号为<em>n3&gt;n2</em>的<code>proposal</code>进行阶段1，导致阶段2<code>proposer</code><em>q</em>的请求被忽略,并一直持续下去。为了保证进度，必须选择特别的<code>proposer</code>作为尝试发布<code>proposal</code>的唯一<code>proposer</code>。 如果特别的<code>proposer</code>可以与大多数<code>acceptor</code>成功通信，并且使用的<code>proposal</code>编号大于已使用的<code>proposal</code>的编号，那么它将成功发布被接受的<code>proposal</code>。并在得知某个<code>proposal</code>具有更高<code>proposal</code>编号的请求后通过放弃<code>proposal</code>再试一次，特别的<code>proposer</code>最终将选择足够高的<code>proposal</code>编号。如果系统（<code>proposer</code>，<code>acceptor</code>和通信网络）能够正常工作，那么可以通过选举一个特别的<code>proposer</code>来实现活跃性。Fischer，Lynch和Pattererson的著名成果表明，一种可靠的选择<code>proposer</code>的算法必须使用随机性或实时性，例如使用超时。但是，无论选举成功与否，都会确保安全。</p>\n<h3 id=\"2-5-实现\"><a href=\"#2-5-实现\" class=\"headerlink\" title=\"2.5 实现\"></a>2.5 实现</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Paxos算法假设网络的进程。在其共识算法中，每个进程都扮演着<code>proposer</code>，<code>acceptor</code>和<code>learner</code>的角色。该算法选择一个<code>leader</code>，该<code>leader</code>同时扮演特别的<code>proposer</code>与特别的<code>learner</code>。Paxos共识算法正是上述算法，其中请求和响应作为普通消息发送。（响应消息带有相应的<code>proposal</code>编号，以防止混淆。）在故障期间保留的稳定存储用于维护<code>acceptor</code>必须记住的信息。<code>acceptor</code>在实际发送响应之前将其预期的响应记录在稳定的存储中。剩下的就是描述如何保证没有两个<code>proposal</code>编号相同的机制。不同的<code>proposer</code>从不相交的数字集中选择他们的数字，因此两个不同的<code>proposer</code>从不发布具有相同编号的<code>proposal</code>。 每个<code>proposer</code>（在稳定的存储中）都会记住尝试发布的编号最高的<code>proposal</code>，并从第1阶段开始使用比其已经使用过的<code>proposer</code>编号更高的<code>proposer</code>编号。</p>\n<h2 id=\"3-状态机的实现\"><a href=\"#3-状态机的实现\" class=\"headerlink\" title=\"3 状态机的实现\"></a>3 状态机的实现</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实现分布式系统的一种简单方法是作为向中央服务器发出命令的客户端的集合。 服务器可以描述为以某种顺序执行客户端命令的确定性状态机。状态机具有当前状态。它通过将命令作为输入并产生输出和新状态来执行步骤。例如，分布式银行系统的客户可能是柜员，并且状态机状态可能由所有用户的帐户余额组成。 提现将通过执行状态机命令来执行，该命令会在且仅当余额大于提款金额时才减少帐户的余额，并产生旧余额和新余额作为输出。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果单个中央服务器发生故障，则该服务器的实施将失败。因此，我们改为使用服务器的集合，每个服务器独立实现状态机。 因为状态机是确定性的，所以如果所有服务器都执行相同的命令序列，则所有服务器将产生相同的状态序列和输出。 然后，发出命令的客户端可以使用任何服务器为其生成的输出。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了确保所有服务器执行相同的状态机命令序列，我们实现了Paxos共识算法的不同实例序列，第i个实例选择的值是序列中的第i个状态机命令。每个服务器在算法的每个实例中扮演所有角色(<code>proposer</code>，<code>acceptor</code>和<code>learner</code>)。现在，假设服务器组是固定的，因此共识算法的所有实例使用相同的角色。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在正常操作中，将选择一台服务器作为<code>leader</code>，在共识算法的所有实例中均充当特别的<code>proposer</code>(尝试发布<code>proposal</code>的唯一的<code>proposer</code>)。 客户将命令发送给<code>leader</code>，<code>leader</code>决定每个命令应出现的顺序。如果<code>leader</code>确定某个客户命令应为第135个命令，则它将尝试选择该命令作为共识算法的第135个实例的值。通常会成功。 它也可能由于故障而失败，或者因为另一台服务器也认为自己是<code>leader</code>并且认为另一条客户端命令应该为第135条命令。但是共识算法确保最多可以选择一个命令作为第135个命令。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方法的效率的关键在于，在Paxos共识算法中，直到第2阶段才选择要提出的值。回想一下，在完成<code>proposer</code>算法的第1阶段之后，要么确定了要提出的值，要么提议者可以自由提出任何值。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，将描述Paxos状态机实现在正常操作期间如何工作。稍后，将讨论可能出问题的地方。 考虑当前<code>leader</code>刚刚失败并选择了新<code>leader</code>时会发生什么。(系统启动是一种特殊情况，其中尚未提出任何命令。)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新的<code>leader</code>，在共识算法的所有情况下都是<code>learner</code>，应该知道已经选择的大多数命令。 假设它知道命令1–134、138和139，即共识算法实例1–134、138和139中选择的值。(将在后面看到如何在命令序列中出现这样的间隙.)然后，它执行实例135-137和大于139的所有实例的阶段1。(在下面描述如何做到这一点.)假设这些执行确定了在实例135和140中要提出的值，但在所有其他情况下都不受约束。领导者然后对实例135和140执行阶段2，从而选择命令135和140。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>leader</code>以及任何其他服务器一样<code>learn leader</code>知道的所有命令，现在可以执行命令1–135。但是，它无法执行它也知道的命令138-140，因为尚未选择命令136和137。<code>leader</code>可以将客户请求的下两个命令作为命令136和137。相反，我们通过建议一个特殊的<code>no-op</code>命令作为命令136和137，使状态保持不变，从而立即填补了空白。（这是通过执行共识算法实例136和137的阶段2来完成的。）一旦选择了这些<code>no-op</code>命令，就可以执行命令138-140。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在已选择命令1–140。<code>leader</code>还已经为大于共识算法140的所有实例完成了阶段1，并且可以自由地在那些实例的阶段2中提出任何值。它将命令号141分配给客户端请求的下一个命令，并建议将其作为共识算法实例141的阶段2中的值。 它提出了接收到的下一个客户端命令作为命令142，依此类推。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>leader</code>可以在获悉已选择其提出的命令141之前提出命令142。 它在提议命令141中发送的所有消息都可能丢失，并且有可能在任何其他服务器了解<code>leader</code>作为命令141提出的建议之前选择命令142。当<code>leader</code>未能收到在实例141中的消息对其阶段2的预期响应时，它将重传那些消息。如果一切顺利，将选择其建议的命令。但是，它可能失败，从而在选择的命令序列中留下空白。通常，假设<code>leader</code>可以提前获得α命令-也就是说，在选择命令<code>1</code>至<code>i</code>之后，它可以提出命令<code>i+1</code>至<code>i+α</code>。最多可能会出现<code>α-1</code>命令的间隔。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于实例135-137和大于139的所有实例,新选择的<code>leader</code>在上面的方案中使用共识算法执行第1阶段的次数不限。它对所有实例使用相同的编号，可以通过向其他服务器发送一条合理的短消息来实现此目的。在阶段1中，只有在<code>acceptor</code>已经从某个<code>proposer</code>那里收到阶段2消息的情况下，接受者才用简单的OK做出响应.(仅对于实例135和140是这种情况.)因此，服务器(充当<code>acceptor</code>)可以使用单个合理的短消息对所有实例进行响应。 因此，执行这些无限多个阶段1实例不会带来任何问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于<code>leader</code>失败选举新<code>leader</code>的情况很少见，因此执行状态机命令的有效成本（即，对命令/值达成共识）是仅执行共识算法第二阶段的成本。可以证明，存在故障时达成共识的任何算法的最小可能成本在Paxos共识算法的第2阶段。 因此，Paxos算法本质上是最佳的。<br>对系统正常运行的讨论假定只有一个<code>leader</code>，除了在当前<code>leader</code>失败和选举新<code>leader</code>之间的短暂时间之外。 在异常情况下，<code>leader</code>选举可能会失败。如果没有服务器充当<code>leader</code>，则不会提出新命令。如果多个服务器认为自己是<code>leader</code>，则它们都可以在同一共识算法实例中提出值，这可能会阻止选择任何值。但是，安全性得以保留-两个不同的服务器将永远不会在选择作为第i个状态机命令的值上发生分歧。仅选举一位<code>leader</code>才能确保取得进展。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果服务器组可以更改，则必须采用某种方法确定哪些服务器实现共识算法的哪些实例。最简单的方法是通过状态机本身。 当前服务器集可以成为状态的一部分，并且可以通过普通的状态机命令进行更改。我们可以允许<code>leader</code>提前获得<code>α</code>命令,通过让执行第<code>i</code>个状态机命令后的状态指定执行共识算法的实例<code>i+α</code>的服务器集。这允许任意复杂的重新配置算法的简单实现。</p>\n"},{"title":"搭建你的第一个区块链网络(四)","date":"2020-05-19T07:01:22.000Z","_content":"前一篇文章: [搭建你的第一个区块链网络(三)](https://www.cnblogs.com/cbkj-xd/p/12905706.html)\n\n## UTXO\n\n#### 组成部分\nUTXO是比特币中一个重要的概念，这一节我们来实现一个简单的UTXO。我们把UTXO的组成部分分为以下三点:\n\n* **UTXOId:** 标识该UTXO\n* **TxInput:** 交易输入，即coin的输入地址以及金额\n* **TxOutput:** 交易输出，即coin的输出地址以及金额\n\n其中``TxInput``与``TxOutput``分别具有以下几个属性:\n\n**TxInput:**  交易输入\n\n* **preTxId**: 指向的前一个UTXO的id\n* **value**： 输入的金额\n* **unLockScript:** 解锁脚本\n\n其中交易输入需要引用之前的UTXO的输出。这样很容易知道当前的交易输入的金额是由之前的哪一笔交易中的交易输出的金额传递的。\n保证每一笔未消费的金额都可以找到它的源地址。\n解锁脚本的作用是用于解锁当前交易输入所引用的交易输出的。因为每一笔金额都有所属，被锁定在某一个地址上面。只有该金额的所有者才具有权限消费进行消费。而解锁脚本一般都是一个数字签名。\n\n**TxOutput** 交易输出\n\n* **value** :输出的金额\n* **lockScript**: 锁定脚本\n\n每当一笔coin被转移，都会被锁定在一个地址上面，因此锁定脚本一般都是一个地址。\n\n对于每一笔UTXO，输入的金额一定是等于输出的金额的。另外UTXO有一个特点，就是不能够只花费其中一部分。而是需要全部消费，而多余的再返还给原地址。\n比如用户a具有10个coin被锁定在一个UTXO中，如果a需要转账给b5个coin，那么需要将10个coin全部花费掉，其中5个coin输出到b的地址，剩余的5个coin输出到a的地址。\n因此一笔UTXO可以有多个交易输出，同时也可以有多个输入。\n\n大致概念介绍差不多了，我们来实现它:\n\n```\n#TxInput.java\n//因为我们采用的序列化保存区块，而该数据需要写入区块，因此需要实现Serializable接口\npublic class TxInput implements Serializable{\n    private static final long serialVersionUID = 1L;\n    // 所引用的前一个交易ID\n    public String preTxId;\n    // 该输入中包含的coin\n    public int values;\n    // 解锁脚本 通常为数字签名\n    public String unLockScript;\n    public TxInput(String txId, TxOutput top, Wallet wallet) throws Exception {\n        //对引用的Txoutput中的地址进行签名，用于解锁引用的TxOutPut.\n        this.unLockScript = wallet.sign(top.getLockScript());\n        //记录引用的上一个交易ID\n        this.preTxId = txId;\n        //coin值等于引用的Txoutput的coin值\n        this.values = top.value;\n    }\n}\n```\n\n接下来是交易输出:\n\n```\n#TxOutput.java\n\n@Getter\npublic class TxOutput implements Serializable{\n    //同理需要实现Serializable接口\n    private static final long serialVersionUID = 1L;\n    // 交易输出的coin值。\n    public int value;\n    //锁定脚本 通常为地址\n    public String lockScript;\n    public TxOutput(int value,String address){\n        this.value = value;\n        this.lockScript = address;\n    }\n}\n```\n最后是UTXO的实现: 我们使用``Transaction``进行表示。\n```\n#Transaction.java\n\n@Getter\n@Setter\npublic class Transaction implements Serializable{\n    //为了后期调试方便，引入了log4j的包，导入方法和之前一样\n    private transient static final Logger LOGGER = Logger.getLogger(Transaction.class);\n    private static final long serialVersionUID = 1L;\n    //COINBASE之后再进行解释\n    private transient static final int COINBASE = 50;\n    //UTXOId\n    public String txId;\n    // 交易输入的集合\n    public ArrayList<TxInput> tips;\n    // 交易输出的集合 String:address\n    public HashMap<String, TxOutput> tops;\n\n    private Transaction() {\n        #这里只创建了保存交易输出的集合,因为涉及到Coinbase，暂时先不创建ArrayList\n        this.tops = new HashMap<>(4);\n    }\n    @Override\n    public String toString(){\n        return JSONObject.toJSONString(this);\n    }\n}\n```\n\nlog4j日志包:\n\n```\n        <dependency>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n            <version>1.2.17</version>\n        </dependency>\n```\n这里为了方便起见分别使用``ArrayList``和``HashMap``存储交易输入与输出.\n\n### 创建UTXO\n接下来是创建UTXO的核心方法，比较复杂，我们先来分析一下:\n首先传入的参数需要有: 发送coin的源地址，发送coin的目的地址，coin的值。\n返回值为一个``Transaction``实例。\n\n接下来分析如何创建UTXO：\n\n1. 首先需要遍历整个区块链，查找到所有**未消费的**被锁定在源地址的交易输出。\n2. 将查找到的所有包含符合条件的交易输出的UTXO记录在集合中。\n3. 遍历该集合，将每一笔UTXO中未消费的输出相加，直到满足转账金额或者是统计完全部UTXO。\n4. 将统计的每一笔UTXO中交易输出创建为新的交易输入用于消费。\n5. 判断是否coin值刚好等于需要转账的coin。如果相等则创建一个交易输出将coin转账到目的地址。\n6. 如果有多余的则再创建一个交易输出返回多余的coin到源地址。\n\nOK，分析完了可以开发了:\n\n```\n#Transaction.java\n    public static Transaction newUTXO(String fromAddress, String toAddress, int value)\n            throws NoSuchAlgorithmException, Exception {\n        //第一步遍历区块链统计UTXO\n        Transaction[] txs = Blockchain.getInstance().findAllUnspendableUTXO(fromAddress);\n        if (txs.length == 0) {\n            LOGGER.info(\"当前地址\"+fromAddress+\"没有未消费的UTXO！！！\");\n            throw new Exception(\"当前地址\"+fromAddress+\"没有未消费的UTXO,交易失败！！！\");\n        }\n        TxOutput top;\n        // 记录需要使用的TxOutput\n        HashMap<String, TxOutput> tops = new HashMap<String, TxOutput>();\n        int maxValue = 0;\n        // 遍历交易集合\n        for (int i = 0; i < txs.length; i++) {\n            // 查找包括地址为fromAddress的TxOutput\n            if (txs[i].tops.containsKey(fromAddress)) {\n                top = txs[i].tops.get(fromAddress);\n                // 添加进Map\n                tops.put(txs[i].txId, top);\n                // 记录该TxOutput中的value\n                maxValue += top.value;\n                // 如果大于需要使用的则退出\n                if (maxValue >= value) {\n                    break;\n                }\n            }\n        }\n        // 是否有足够的coin\n        if (maxValue >= value) {\n            // 创建tx\n            Transaction t = new Transaction();\n            t.tips = new ArrayList<TxInput>(tops.size());\n            // 遍历所有需要用到的Txoutput\n            tops.forEach((s, to) -> {\n                // 变为TxInput\n                try {\n                    t.tips.add(new TxInput(s, to, Wallet.getInstance()));\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            });\n            //如果值不均等\n            if(maxValue>value){\n                //创建TxOutput返还多余的coin\n                top = new TxOutput(maxValue-value, Wallet.getInstance().getAddress());\n                t.tops.put(top.getLockScript(), top);\n            }\n            //目的地址\n            top = new TxOutput(value, toAddress);\n            t.tops.put(top.getLockScript(), top);\n            LOGGER.info(\"创建UTXO: \"+t.toString());\n            return t;\n        }\n        LOGGER.info(\"当前地址余额不足!!,余额为\"+maxValue);\n        throw new Exception(\"当前地址余额不足!!,余额为\"+maxValue);\n    }\n```\n\n### 统计未消费的UTXO\n然后是另外一个核心方法，统计区块链中符合条件的全部未消费的UTXO：\n我们使用比较简单易理解的方式，先统计地址匹配的所有的交易输出。\n然后统计所有的满足条件的交易输入。交易输入需要满足两个条件：\n\n1. 地址是自己的地址\n2. 交易输入中引用的UTXOid可以追溯到。\n\n我们将符合条件的``TxInput``中引用的UTXOId在所有未消费的UTXO中匹配，\n如果匹配到说明该UTXO已经被花费掉了，我们移除掉花费掉的UTXO，剩下的就是满足条件的未消费的UTXO了。\n\n```\n#Blockchain.java\npublic Transaction[] findAllUnspendableUTXO(String address)\n            throws FileNotFoundException, ClassNotFoundException, IOException {\n        LOGGER.info(\"查找所有未消费的UTXO...............\");\n        HashMap<String, Transaction> txs = new HashMap<>();\n        Block block = this.block;\n        Transaction tx;\n        // 从当前区块向前遍历查找UTXO txOutput\n        do{\n            //从区块中获取交易信息\n            tx = block.getTransaction();\n            // 如果存在交易信息，且TxOutput地址包含address\n            if (tx != null && tx.getTops().containsKey(address)) {\n                txs.put(tx.getTxId(), tx);\n            }\n            //指向前一个区块\n            block = block.getPrevBlock();\n            //一直遍历到创世区块\n        }while(block!=null && block.hasPrevBlock()) ;\n        // 再遍历一次查找已消费的UTXO\n        block = this.block;\n        do {\n            tx = block.getTransaction();\n            if (tx != null) {\n                // 如果交易中的TxInput包含的交易ID存在于txs，移除\n                tx.getTips().forEach(tip -> {\n                    try {\n                        //需要满足两个条件，一是Txinput中引用的UTXOId存在，说明该UTXO已经被使用了\n                        //二是需要保证地址相同，确认该TxInput是coin所有者消费的\n                        if (Wallet.getInstance().verify(address,tip.unLockScript) \n                                && txs.containsKey(tip.preTxId))\n                            //满足两个条件则移除该UTXO\n                            txs.remove(tip.preTxId);\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                });\n            }\n            block = block.getPrevBlock();\n        }while (block!=null && block.hasPrevBlock());\n        //创建UTXO数组返回\n        Transaction[] t = new Transaction[txs.size()];\n        return txs.values().toArray(t);\n    }\n```\n看这里的代码：\n```\n if (Wallet.getInstance().verify(address,tip.unLockScript) && txs.containsKey(tip.preTxId))\n    ...\n```\n首先验证``TxInput``的解锁脚本是否对我们钱包的地址进行签名得到的，即验证这一笔输入是否是自己消费的。\n如果是自己消费的然后比对UTXO的Id，如果相同则说明这笔UTXO已经被消费掉了。那么需要移除它。\n在钱包中添加一个新的方法，用于验证解锁脚本是否可以解锁交易输出。我们简单采用哈希值匹配的方式模拟验证。\n```\n#Wallet.java\n    public boolean verify(String data,String sign) throws DecoderException, Exception {\n        LOGGER.info(\"验证签名: \"+data);\n        String[] str = data.split(\"%%%\");\n        // 原文     encry(hash(原文))\n        if(str.length!=2){\n            return false;\n        }\n        String hash2 = Hex.encodeHexString(this.decrypt(str[1]));\n        String hash3 = Util.getSHA256(data);\n        if(hash3.equals(hash2)){\n            LOGGER.info(\"签名验证成功！！\");\n            return true;\n        }\n        LOGGER.info(\"签名验证失败！！\");\n        return false;\n    }\n```\n\n### 更新区块信息\n加入了UTXO的概念，那我们需要更新区块以及区块链的属性信息了。\n\n```\n#Block.java\n@Getter\n@Setter\npublic class Block implements Serializable{\n    ...\n    //当前区块中的交易\n    public Transaction transaction;\n    ...\n    //添加一个新的构造方法\n    public Block(int blkNum,Transaction transaction,String prevBlockHash){\n        this.blkNum = blkNum;\n        this.transaction = transaction;\n        this.prevBlockHash = prevBlockHash;\n        this.timeStamp = Util.getTimeStamp();\n    }\n    ...\n}\n```\n然后是区块链，也要更新一个方法:\n```\n#Blockchain.java\npublic final class Blockchain {\n    ...\n    public Block addBlock(Transaction tx) throws IOException {\n        int num = this.block.getBlkNum();\n        Block block = new Block(num + 1, tx, this.block.curBlockHash);\n        // 每次将区块添加进区块链之前需要计算难度值\n        block.setNonce(Pow.calc(block));\n        // 计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum() + block.getData() + block.getPrevBlockHash()\n                + block.getPrevBlockHash() + block.getNonce());\n        block.setCurBlockHash(hash);\n        // 序列化\n        Storage.Serialize(block);\n        this.block = block;\n        LOGGER.info(\"当前区块信息为:\"+block.toString());\n        return this.block;\n    }\n    ...\n}\n```\n之前区块中将字符串保存为区块信息，我们更新为一笔交易。需要创建一笔交易才可以创建区块。\n\n### Coinbase\n\n关于UTXO，我们之前讲到每一笔输出都会对应着一个输入，那么第一笔被输出的coin是哪里来的呢，\n在比特币中，每产出一个区块将会奖励一定数量的BItcoin，称为Coinbase。同理，我们这里也实现它。\n因此第一笔被输出的coin来自于coinbase。我们将coinbase固定为50，正如之前设定的属性:\n\n```\n#Transaction.java\n    private transient static final int COINBASE = 50;\n```\n\n所以我们还需要一个生成coinbase的交易的构造方法:\n\n```\n#Blockchain.java\n    public static Transaction newCoinBase() throws NoSuchAlgorithmException, Exception {\n        Transaction t = new Transaction();\n        t.tips = new ArrayList<>();\n        t.tops.put(Wallet.getInstance().getAddress(), new TxOutput(COINBASE, Wallet.getInstance().getAddress()));\n        LOGGER.info(\"创建Coinbase.....\"+t.toString());\n        return t;\n    }\n```\n可以看到，交易输出的地址我们设置为钱包的地址。\n比较简单，接下来修改一下创世区块的生成方法，将coinbase的交易添加进去。\n\n```\n#Blockchain.java\n    private Block CrtGenesisBlock() throws NoSuchAlgorithmException, Exception {\n        // Block block = new Block(1,\"Genesis Block\",\"00000000000000000\");\n        Block block = new Block(1, Transaction.newCoinBase(), \"00000000000000000\");\n        ...\n    }\n```\n\n### 测试\n一切都完成了，测试一下:\n\n```\n#Test.java\npublic class Test {\n    public static void main(String[] args) throws NoSuchAlgorithmException, Exception {\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                Wallet.getInstance().getAddress(), \"address\", 30));\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                \"address\", \"address1\", 20));\n    }\n}\n```\n分析一下测试用例：\n\n```\nBlockchain.getInstance()\n#############################\n#Blockchain.java CrtGenesisBlock()\nBlock block = new Block(1, Transaction.newCoinBase(), \"00000000000000000\");\n#newCoinBase()\nt.tops.put(Wallet.getInstance().getAddress(), new TxOutput(COINBASE, Wallet.getInstance().getAddress()));\n```\n\n首先获取区块链实例，因此创建了创始区块，看上面的代码我们可以知道创建了一笔coinbase交易，50个coin被锁定在我们钱包的地址。\n\n```\nBlockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                Wallet.getInstance().getAddress(), \"address\", 30));\n```\n\n然后是第二个区块，我们创建了一个UTXO，从钱包的地址转移30个coin到地址``address``。\n\n```\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                \"address\", \"address1\", 20));\n```\n最后是第三个区块，从地址``address``转移20个coin到地址``address1``.\n测试一下：\n\n```\n...\n[INFO ] 2020-05-18 14:10:19,501 method:org.xd.chain.transaction.Transaction.newCoinBase(Transaction.java:42)\n创建Coinbase.....{\"tips\":[],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":50}}}\n...\n[INFO ] 2020-05-18 14:10:19,757 method:org.xd.chain.core.Blockchain.findAllUnspendableUTXO(Blockchain.java:117)\n查找所有未消费的UTXO...............\n[INFO ] 2020-05-18 14:10:19,804 method:org.xd.chain.wallet.Wallet.sign(Wallet.java:86)\n使用私钥对数据签名: R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\n[INFO ] 2020-05-18 14:10:20,382 method:org.xd.chain.transaction.Transaction.newUTXO(Transaction.java:100)\n创建UTXO: {\"tips\":[{\"unLockScript\":\"523133363335343866383934363532396464373339323262323635343762363131306639313539613330636339623034373434333563316330646233373834326364336334313030303832313832323237653335366338313138333064366235623536643836323838306238356632303335356366626333383736343136353361%%%4251d1ae7091f422bef3a95b29867ebeaeeedb0e20239a4edf96967d1a1f15a8f061873acc01aa86c726e1ad128aefeaaf5c447aed27e5729bdd24f0026d6c23\",\"values\":50}],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":20},\"address\":{\"lockScript\":\"address\",\"value\":30}}}\n...\n[INFO ] 2020-05-18 14:10:20,468 method:org.xd.chain.core.Blockchain.addBlock(Blockchain.java:86)\n当前区块信息为:{\"blkNum\":2,\"curBlockHash\":\"00005f0690489e8763bd3db0ce7112592cdb118507945c65d07bfe27e0ad3031\",\"nonce\":4350,\"prevBlockHash\":\"000011de81afdac44e08e81b9be434cbcb625808a9d0f8008275ab9a6ffb809f\",\"timeStamp\":\"2020-05-18 14:10:20\",\"transaction\":{\"tips\":[{\"unLockScript\":\"523133363335343866383934363532396464373339323262323635343762363131306639313539613330636339623034373434333563316330646233373834326364336334313030303832313832323237653335366338313138333064366235623536643836323838306238356632303335356366626333383736343136353361%%%4251d1ae7091f422bef3a95b29867ebeaeeedb0e20239a4edf96967d1a1f15a8f061873acc01aa86c726e1ad128aefeaaf5c447aed27e5729bdd24f0026d6c23\",\"values\":50}],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":20},\"address\":{\"lockScript\":\"address\",\"value\":30}}}}\n[INFO ] 2020-05-18 14:10:20,575 method:org.xd.chain.core.Blockchain.findAllUnspendableUTXO(Blockchain.java:117)\n查找所有未消费的UTXO...............\n...\n[INFO ] 2020-05-18 14:10:20,725 method:org.xd.chain.wallet.Wallet.verify(Wallet.java:124)\n验证签名: address\n...\n[INFO ] 2020-05-18 14:10:20,731 method:org.xd.chain.wallet.Wallet.sign(Wallet.java:86)\n使用私钥对数据签名: address\n[INFO ] 2020-05-18 14:10:20,734 method:org.xd.chain.transaction.Transaction.newUTXO(Transaction.java:100)\n创建UTXO: {\"tips\":[{\"unLockScript\":\"61646472657373%%%8ad16095a9f1947e323eb5ef3601a0cc2ad552ad3f7331406123577a1cc0c68dc614f3262505f079f7c3acfc1d681fdb432f7ba0f4ac3d69cb46dead5446b2cd\",\"values\":30}],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":10},\"address1\":{\"lockScript\":\"address1\",\"value\":20}}}\n...\n[INFO ] 2020-05-18 14:10:20,990 method:org.xd.chain.core.Blockchain.addBlock(Blockchain.java:86)\n当前区块信息为:{\"blkNum\":3,\"curBlockHash\":\"00006e8918bba9831374f578caa3d80fa936997598072145bc0654cbca2d084e\",\"nonce\":74607,\"prevBlockHash\":\"00005f0690489e8763bd3db0ce7112592cdb118507945c65d07bfe27e0ad3031\",\"timeStamp\":\"2020-05-18 14:10:20\",\"transaction\":{\"tips\":[{\"unLockScript\":\"61646472657373%%%8ad16095a9f1947e323eb5ef3601a0cc2ad552ad3f7331406123577a1cc0c68dc614f3262505f079f7c3acfc1d681fdb432f7ba0f4ac3d69cb46dead5446b2cd\",\"values\":30}],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":10},\"address1\":{\"lockScript\":\"address1\",\"value\":20}}}}\n```\n\n省略掉其他日志信息，看起来测试是没有问题的，共生成了3个区块。创世区块中有一笔Coinbase交易。区块2中成功转移30coin到地址``address``，返还20coin到原地址。区块3中成功从地址``address``转移20coin到地址``address1``，返还10coin到地址``address``。\n\n还有部分未完善部分，比如coinbase只在创世区块中生成了。每个区块中只含有一笔交易等等，后期慢慢完善。\n\n### Github仓库地址在这里，随时保持更新中.....\nGithub地址：[Jchain](https://github.com/newonexd/Jchain)","source":"_posts/blog/blockchain/Jchain4.md","raw":"---\ntitle: 搭建你的第一个区块链网络(四)\ndate: 2020-05-19 15:01:22\ntags: blockchain\n---\n前一篇文章: [搭建你的第一个区块链网络(三)](https://www.cnblogs.com/cbkj-xd/p/12905706.html)\n\n## UTXO\n\n#### 组成部分\nUTXO是比特币中一个重要的概念，这一节我们来实现一个简单的UTXO。我们把UTXO的组成部分分为以下三点:\n\n* **UTXOId:** 标识该UTXO\n* **TxInput:** 交易输入，即coin的输入地址以及金额\n* **TxOutput:** 交易输出，即coin的输出地址以及金额\n\n其中``TxInput``与``TxOutput``分别具有以下几个属性:\n\n**TxInput:**  交易输入\n\n* **preTxId**: 指向的前一个UTXO的id\n* **value**： 输入的金额\n* **unLockScript:** 解锁脚本\n\n其中交易输入需要引用之前的UTXO的输出。这样很容易知道当前的交易输入的金额是由之前的哪一笔交易中的交易输出的金额传递的。\n保证每一笔未消费的金额都可以找到它的源地址。\n解锁脚本的作用是用于解锁当前交易输入所引用的交易输出的。因为每一笔金额都有所属，被锁定在某一个地址上面。只有该金额的所有者才具有权限消费进行消费。而解锁脚本一般都是一个数字签名。\n\n**TxOutput** 交易输出\n\n* **value** :输出的金额\n* **lockScript**: 锁定脚本\n\n每当一笔coin被转移，都会被锁定在一个地址上面，因此锁定脚本一般都是一个地址。\n\n对于每一笔UTXO，输入的金额一定是等于输出的金额的。另外UTXO有一个特点，就是不能够只花费其中一部分。而是需要全部消费，而多余的再返还给原地址。\n比如用户a具有10个coin被锁定在一个UTXO中，如果a需要转账给b5个coin，那么需要将10个coin全部花费掉，其中5个coin输出到b的地址，剩余的5个coin输出到a的地址。\n因此一笔UTXO可以有多个交易输出，同时也可以有多个输入。\n\n大致概念介绍差不多了，我们来实现它:\n\n```\n#TxInput.java\n//因为我们采用的序列化保存区块，而该数据需要写入区块，因此需要实现Serializable接口\npublic class TxInput implements Serializable{\n    private static final long serialVersionUID = 1L;\n    // 所引用的前一个交易ID\n    public String preTxId;\n    // 该输入中包含的coin\n    public int values;\n    // 解锁脚本 通常为数字签名\n    public String unLockScript;\n    public TxInput(String txId, TxOutput top, Wallet wallet) throws Exception {\n        //对引用的Txoutput中的地址进行签名，用于解锁引用的TxOutPut.\n        this.unLockScript = wallet.sign(top.getLockScript());\n        //记录引用的上一个交易ID\n        this.preTxId = txId;\n        //coin值等于引用的Txoutput的coin值\n        this.values = top.value;\n    }\n}\n```\n\n接下来是交易输出:\n\n```\n#TxOutput.java\n\n@Getter\npublic class TxOutput implements Serializable{\n    //同理需要实现Serializable接口\n    private static final long serialVersionUID = 1L;\n    // 交易输出的coin值。\n    public int value;\n    //锁定脚本 通常为地址\n    public String lockScript;\n    public TxOutput(int value,String address){\n        this.value = value;\n        this.lockScript = address;\n    }\n}\n```\n最后是UTXO的实现: 我们使用``Transaction``进行表示。\n```\n#Transaction.java\n\n@Getter\n@Setter\npublic class Transaction implements Serializable{\n    //为了后期调试方便，引入了log4j的包，导入方法和之前一样\n    private transient static final Logger LOGGER = Logger.getLogger(Transaction.class);\n    private static final long serialVersionUID = 1L;\n    //COINBASE之后再进行解释\n    private transient static final int COINBASE = 50;\n    //UTXOId\n    public String txId;\n    // 交易输入的集合\n    public ArrayList<TxInput> tips;\n    // 交易输出的集合 String:address\n    public HashMap<String, TxOutput> tops;\n\n    private Transaction() {\n        #这里只创建了保存交易输出的集合,因为涉及到Coinbase，暂时先不创建ArrayList\n        this.tops = new HashMap<>(4);\n    }\n    @Override\n    public String toString(){\n        return JSONObject.toJSONString(this);\n    }\n}\n```\n\nlog4j日志包:\n\n```\n        <dependency>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n            <version>1.2.17</version>\n        </dependency>\n```\n这里为了方便起见分别使用``ArrayList``和``HashMap``存储交易输入与输出.\n\n### 创建UTXO\n接下来是创建UTXO的核心方法，比较复杂，我们先来分析一下:\n首先传入的参数需要有: 发送coin的源地址，发送coin的目的地址，coin的值。\n返回值为一个``Transaction``实例。\n\n接下来分析如何创建UTXO：\n\n1. 首先需要遍历整个区块链，查找到所有**未消费的**被锁定在源地址的交易输出。\n2. 将查找到的所有包含符合条件的交易输出的UTXO记录在集合中。\n3. 遍历该集合，将每一笔UTXO中未消费的输出相加，直到满足转账金额或者是统计完全部UTXO。\n4. 将统计的每一笔UTXO中交易输出创建为新的交易输入用于消费。\n5. 判断是否coin值刚好等于需要转账的coin。如果相等则创建一个交易输出将coin转账到目的地址。\n6. 如果有多余的则再创建一个交易输出返回多余的coin到源地址。\n\nOK，分析完了可以开发了:\n\n```\n#Transaction.java\n    public static Transaction newUTXO(String fromAddress, String toAddress, int value)\n            throws NoSuchAlgorithmException, Exception {\n        //第一步遍历区块链统计UTXO\n        Transaction[] txs = Blockchain.getInstance().findAllUnspendableUTXO(fromAddress);\n        if (txs.length == 0) {\n            LOGGER.info(\"当前地址\"+fromAddress+\"没有未消费的UTXO！！！\");\n            throw new Exception(\"当前地址\"+fromAddress+\"没有未消费的UTXO,交易失败！！！\");\n        }\n        TxOutput top;\n        // 记录需要使用的TxOutput\n        HashMap<String, TxOutput> tops = new HashMap<String, TxOutput>();\n        int maxValue = 0;\n        // 遍历交易集合\n        for (int i = 0; i < txs.length; i++) {\n            // 查找包括地址为fromAddress的TxOutput\n            if (txs[i].tops.containsKey(fromAddress)) {\n                top = txs[i].tops.get(fromAddress);\n                // 添加进Map\n                tops.put(txs[i].txId, top);\n                // 记录该TxOutput中的value\n                maxValue += top.value;\n                // 如果大于需要使用的则退出\n                if (maxValue >= value) {\n                    break;\n                }\n            }\n        }\n        // 是否有足够的coin\n        if (maxValue >= value) {\n            // 创建tx\n            Transaction t = new Transaction();\n            t.tips = new ArrayList<TxInput>(tops.size());\n            // 遍历所有需要用到的Txoutput\n            tops.forEach((s, to) -> {\n                // 变为TxInput\n                try {\n                    t.tips.add(new TxInput(s, to, Wallet.getInstance()));\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            });\n            //如果值不均等\n            if(maxValue>value){\n                //创建TxOutput返还多余的coin\n                top = new TxOutput(maxValue-value, Wallet.getInstance().getAddress());\n                t.tops.put(top.getLockScript(), top);\n            }\n            //目的地址\n            top = new TxOutput(value, toAddress);\n            t.tops.put(top.getLockScript(), top);\n            LOGGER.info(\"创建UTXO: \"+t.toString());\n            return t;\n        }\n        LOGGER.info(\"当前地址余额不足!!,余额为\"+maxValue);\n        throw new Exception(\"当前地址余额不足!!,余额为\"+maxValue);\n    }\n```\n\n### 统计未消费的UTXO\n然后是另外一个核心方法，统计区块链中符合条件的全部未消费的UTXO：\n我们使用比较简单易理解的方式，先统计地址匹配的所有的交易输出。\n然后统计所有的满足条件的交易输入。交易输入需要满足两个条件：\n\n1. 地址是自己的地址\n2. 交易输入中引用的UTXOid可以追溯到。\n\n我们将符合条件的``TxInput``中引用的UTXOId在所有未消费的UTXO中匹配，\n如果匹配到说明该UTXO已经被花费掉了，我们移除掉花费掉的UTXO，剩下的就是满足条件的未消费的UTXO了。\n\n```\n#Blockchain.java\npublic Transaction[] findAllUnspendableUTXO(String address)\n            throws FileNotFoundException, ClassNotFoundException, IOException {\n        LOGGER.info(\"查找所有未消费的UTXO...............\");\n        HashMap<String, Transaction> txs = new HashMap<>();\n        Block block = this.block;\n        Transaction tx;\n        // 从当前区块向前遍历查找UTXO txOutput\n        do{\n            //从区块中获取交易信息\n            tx = block.getTransaction();\n            // 如果存在交易信息，且TxOutput地址包含address\n            if (tx != null && tx.getTops().containsKey(address)) {\n                txs.put(tx.getTxId(), tx);\n            }\n            //指向前一个区块\n            block = block.getPrevBlock();\n            //一直遍历到创世区块\n        }while(block!=null && block.hasPrevBlock()) ;\n        // 再遍历一次查找已消费的UTXO\n        block = this.block;\n        do {\n            tx = block.getTransaction();\n            if (tx != null) {\n                // 如果交易中的TxInput包含的交易ID存在于txs，移除\n                tx.getTips().forEach(tip -> {\n                    try {\n                        //需要满足两个条件，一是Txinput中引用的UTXOId存在，说明该UTXO已经被使用了\n                        //二是需要保证地址相同，确认该TxInput是coin所有者消费的\n                        if (Wallet.getInstance().verify(address,tip.unLockScript) \n                                && txs.containsKey(tip.preTxId))\n                            //满足两个条件则移除该UTXO\n                            txs.remove(tip.preTxId);\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                });\n            }\n            block = block.getPrevBlock();\n        }while (block!=null && block.hasPrevBlock());\n        //创建UTXO数组返回\n        Transaction[] t = new Transaction[txs.size()];\n        return txs.values().toArray(t);\n    }\n```\n看这里的代码：\n```\n if (Wallet.getInstance().verify(address,tip.unLockScript) && txs.containsKey(tip.preTxId))\n    ...\n```\n首先验证``TxInput``的解锁脚本是否对我们钱包的地址进行签名得到的，即验证这一笔输入是否是自己消费的。\n如果是自己消费的然后比对UTXO的Id，如果相同则说明这笔UTXO已经被消费掉了。那么需要移除它。\n在钱包中添加一个新的方法，用于验证解锁脚本是否可以解锁交易输出。我们简单采用哈希值匹配的方式模拟验证。\n```\n#Wallet.java\n    public boolean verify(String data,String sign) throws DecoderException, Exception {\n        LOGGER.info(\"验证签名: \"+data);\n        String[] str = data.split(\"%%%\");\n        // 原文     encry(hash(原文))\n        if(str.length!=2){\n            return false;\n        }\n        String hash2 = Hex.encodeHexString(this.decrypt(str[1]));\n        String hash3 = Util.getSHA256(data);\n        if(hash3.equals(hash2)){\n            LOGGER.info(\"签名验证成功！！\");\n            return true;\n        }\n        LOGGER.info(\"签名验证失败！！\");\n        return false;\n    }\n```\n\n### 更新区块信息\n加入了UTXO的概念，那我们需要更新区块以及区块链的属性信息了。\n\n```\n#Block.java\n@Getter\n@Setter\npublic class Block implements Serializable{\n    ...\n    //当前区块中的交易\n    public Transaction transaction;\n    ...\n    //添加一个新的构造方法\n    public Block(int blkNum,Transaction transaction,String prevBlockHash){\n        this.blkNum = blkNum;\n        this.transaction = transaction;\n        this.prevBlockHash = prevBlockHash;\n        this.timeStamp = Util.getTimeStamp();\n    }\n    ...\n}\n```\n然后是区块链，也要更新一个方法:\n```\n#Blockchain.java\npublic final class Blockchain {\n    ...\n    public Block addBlock(Transaction tx) throws IOException {\n        int num = this.block.getBlkNum();\n        Block block = new Block(num + 1, tx, this.block.curBlockHash);\n        // 每次将区块添加进区块链之前需要计算难度值\n        block.setNonce(Pow.calc(block));\n        // 计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum() + block.getData() + block.getPrevBlockHash()\n                + block.getPrevBlockHash() + block.getNonce());\n        block.setCurBlockHash(hash);\n        // 序列化\n        Storage.Serialize(block);\n        this.block = block;\n        LOGGER.info(\"当前区块信息为:\"+block.toString());\n        return this.block;\n    }\n    ...\n}\n```\n之前区块中将字符串保存为区块信息，我们更新为一笔交易。需要创建一笔交易才可以创建区块。\n\n### Coinbase\n\n关于UTXO，我们之前讲到每一笔输出都会对应着一个输入，那么第一笔被输出的coin是哪里来的呢，\n在比特币中，每产出一个区块将会奖励一定数量的BItcoin，称为Coinbase。同理，我们这里也实现它。\n因此第一笔被输出的coin来自于coinbase。我们将coinbase固定为50，正如之前设定的属性:\n\n```\n#Transaction.java\n    private transient static final int COINBASE = 50;\n```\n\n所以我们还需要一个生成coinbase的交易的构造方法:\n\n```\n#Blockchain.java\n    public static Transaction newCoinBase() throws NoSuchAlgorithmException, Exception {\n        Transaction t = new Transaction();\n        t.tips = new ArrayList<>();\n        t.tops.put(Wallet.getInstance().getAddress(), new TxOutput(COINBASE, Wallet.getInstance().getAddress()));\n        LOGGER.info(\"创建Coinbase.....\"+t.toString());\n        return t;\n    }\n```\n可以看到，交易输出的地址我们设置为钱包的地址。\n比较简单，接下来修改一下创世区块的生成方法，将coinbase的交易添加进去。\n\n```\n#Blockchain.java\n    private Block CrtGenesisBlock() throws NoSuchAlgorithmException, Exception {\n        // Block block = new Block(1,\"Genesis Block\",\"00000000000000000\");\n        Block block = new Block(1, Transaction.newCoinBase(), \"00000000000000000\");\n        ...\n    }\n```\n\n### 测试\n一切都完成了，测试一下:\n\n```\n#Test.java\npublic class Test {\n    public static void main(String[] args) throws NoSuchAlgorithmException, Exception {\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                Wallet.getInstance().getAddress(), \"address\", 30));\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                \"address\", \"address1\", 20));\n    }\n}\n```\n分析一下测试用例：\n\n```\nBlockchain.getInstance()\n#############################\n#Blockchain.java CrtGenesisBlock()\nBlock block = new Block(1, Transaction.newCoinBase(), \"00000000000000000\");\n#newCoinBase()\nt.tops.put(Wallet.getInstance().getAddress(), new TxOutput(COINBASE, Wallet.getInstance().getAddress()));\n```\n\n首先获取区块链实例，因此创建了创始区块，看上面的代码我们可以知道创建了一笔coinbase交易，50个coin被锁定在我们钱包的地址。\n\n```\nBlockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                Wallet.getInstance().getAddress(), \"address\", 30));\n```\n\n然后是第二个区块，我们创建了一个UTXO，从钱包的地址转移30个coin到地址``address``。\n\n```\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                \"address\", \"address1\", 20));\n```\n最后是第三个区块，从地址``address``转移20个coin到地址``address1``.\n测试一下：\n\n```\n...\n[INFO ] 2020-05-18 14:10:19,501 method:org.xd.chain.transaction.Transaction.newCoinBase(Transaction.java:42)\n创建Coinbase.....{\"tips\":[],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":50}}}\n...\n[INFO ] 2020-05-18 14:10:19,757 method:org.xd.chain.core.Blockchain.findAllUnspendableUTXO(Blockchain.java:117)\n查找所有未消费的UTXO...............\n[INFO ] 2020-05-18 14:10:19,804 method:org.xd.chain.wallet.Wallet.sign(Wallet.java:86)\n使用私钥对数据签名: R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\n[INFO ] 2020-05-18 14:10:20,382 method:org.xd.chain.transaction.Transaction.newUTXO(Transaction.java:100)\n创建UTXO: {\"tips\":[{\"unLockScript\":\"523133363335343866383934363532396464373339323262323635343762363131306639313539613330636339623034373434333563316330646233373834326364336334313030303832313832323237653335366338313138333064366235623536643836323838306238356632303335356366626333383736343136353361%%%4251d1ae7091f422bef3a95b29867ebeaeeedb0e20239a4edf96967d1a1f15a8f061873acc01aa86c726e1ad128aefeaaf5c447aed27e5729bdd24f0026d6c23\",\"values\":50}],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":20},\"address\":{\"lockScript\":\"address\",\"value\":30}}}\n...\n[INFO ] 2020-05-18 14:10:20,468 method:org.xd.chain.core.Blockchain.addBlock(Blockchain.java:86)\n当前区块信息为:{\"blkNum\":2,\"curBlockHash\":\"00005f0690489e8763bd3db0ce7112592cdb118507945c65d07bfe27e0ad3031\",\"nonce\":4350,\"prevBlockHash\":\"000011de81afdac44e08e81b9be434cbcb625808a9d0f8008275ab9a6ffb809f\",\"timeStamp\":\"2020-05-18 14:10:20\",\"transaction\":{\"tips\":[{\"unLockScript\":\"523133363335343866383934363532396464373339323262323635343762363131306639313539613330636339623034373434333563316330646233373834326364336334313030303832313832323237653335366338313138333064366235623536643836323838306238356632303335356366626333383736343136353361%%%4251d1ae7091f422bef3a95b29867ebeaeeedb0e20239a4edf96967d1a1f15a8f061873acc01aa86c726e1ad128aefeaaf5c447aed27e5729bdd24f0026d6c23\",\"values\":50}],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":20},\"address\":{\"lockScript\":\"address\",\"value\":30}}}}\n[INFO ] 2020-05-18 14:10:20,575 method:org.xd.chain.core.Blockchain.findAllUnspendableUTXO(Blockchain.java:117)\n查找所有未消费的UTXO...............\n...\n[INFO ] 2020-05-18 14:10:20,725 method:org.xd.chain.wallet.Wallet.verify(Wallet.java:124)\n验证签名: address\n...\n[INFO ] 2020-05-18 14:10:20,731 method:org.xd.chain.wallet.Wallet.sign(Wallet.java:86)\n使用私钥对数据签名: address\n[INFO ] 2020-05-18 14:10:20,734 method:org.xd.chain.transaction.Transaction.newUTXO(Transaction.java:100)\n创建UTXO: {\"tips\":[{\"unLockScript\":\"61646472657373%%%8ad16095a9f1947e323eb5ef3601a0cc2ad552ad3f7331406123577a1cc0c68dc614f3262505f079f7c3acfc1d681fdb432f7ba0f4ac3d69cb46dead5446b2cd\",\"values\":30}],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":10},\"address1\":{\"lockScript\":\"address1\",\"value\":20}}}\n...\n[INFO ] 2020-05-18 14:10:20,990 method:org.xd.chain.core.Blockchain.addBlock(Blockchain.java:86)\n当前区块信息为:{\"blkNum\":3,\"curBlockHash\":\"00006e8918bba9831374f578caa3d80fa936997598072145bc0654cbca2d084e\",\"nonce\":74607,\"prevBlockHash\":\"00005f0690489e8763bd3db0ce7112592cdb118507945c65d07bfe27e0ad3031\",\"timeStamp\":\"2020-05-18 14:10:20\",\"transaction\":{\"tips\":[{\"unLockScript\":\"61646472657373%%%8ad16095a9f1947e323eb5ef3601a0cc2ad552ad3f7331406123577a1cc0c68dc614f3262505f079f7c3acfc1d681fdb432f7ba0f4ac3d69cb46dead5446b2cd\",\"values\":30}],\"tops\":{\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\":{\"lockScript\":\"R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\",\"value\":10},\"address1\":{\"lockScript\":\"address1\",\"value\":20}}}}\n```\n\n省略掉其他日志信息，看起来测试是没有问题的，共生成了3个区块。创世区块中有一笔Coinbase交易。区块2中成功转移30coin到地址``address``，返还20coin到原地址。区块3中成功从地址``address``转移20coin到地址``address1``，返还10coin到地址``address``。\n\n还有部分未完善部分，比如coinbase只在创世区块中生成了。每个区块中只含有一笔交易等等，后期慢慢完善。\n\n### Github仓库地址在这里，随时保持更新中.....\nGithub地址：[Jchain](https://github.com/newonexd/Jchain)","slug":"blog/blockchain/Jchain4","published":1,"updated":"2020-05-19T07:03:31.619Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyf60005k0vq08xyc017","content":"<p>前一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12905706.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(三)</a></p>\n<h2 id=\"UTXO\"><a href=\"#UTXO\" class=\"headerlink\" title=\"UTXO\"></a>UTXO</h2><h4 id=\"组成部分\"><a href=\"#组成部分\" class=\"headerlink\" title=\"组成部分\"></a>组成部分</h4><p>UTXO是比特币中一个重要的概念，这一节我们来实现一个简单的UTXO。我们把UTXO的组成部分分为以下三点:</p>\n<ul>\n<li><strong>UTXOId:</strong> 标识该UTXO</li>\n<li><strong>TxInput:</strong> 交易输入，即coin的输入地址以及金额</li>\n<li><strong>TxOutput:</strong> 交易输出，即coin的输出地址以及金额</li>\n</ul>\n<p>其中<code>TxInput</code>与<code>TxOutput</code>分别具有以下几个属性:</p>\n<p><strong>TxInput:</strong>  交易输入</p>\n<ul>\n<li><strong>preTxId</strong>: 指向的前一个UTXO的id</li>\n<li><strong>value</strong>： 输入的金额</li>\n<li><strong>unLockScript:</strong> 解锁脚本</li>\n</ul>\n<p>其中交易输入需要引用之前的UTXO的输出。这样很容易知道当前的交易输入的金额是由之前的哪一笔交易中的交易输出的金额传递的。<br>保证每一笔未消费的金额都可以找到它的源地址。<br>解锁脚本的作用是用于解锁当前交易输入所引用的交易输出的。因为每一笔金额都有所属，被锁定在某一个地址上面。只有该金额的所有者才具有权限消费进行消费。而解锁脚本一般都是一个数字签名。</p>\n<p><strong>TxOutput</strong> 交易输出</p>\n<ul>\n<li><strong>value</strong> :输出的金额</li>\n<li><strong>lockScript</strong>: 锁定脚本</li>\n</ul>\n<p>每当一笔coin被转移，都会被锁定在一个地址上面，因此锁定脚本一般都是一个地址。</p>\n<p>对于每一笔UTXO，输入的金额一定是等于输出的金额的。另外UTXO有一个特点，就是不能够只花费其中一部分。而是需要全部消费，而多余的再返还给原地址。<br>比如用户a具有10个coin被锁定在一个UTXO中，如果a需要转账给b5个coin，那么需要将10个coin全部花费掉，其中5个coin输出到b的地址，剩余的5个coin输出到a的地址。<br>因此一笔UTXO可以有多个交易输出，同时也可以有多个输入。</p>\n<p>大致概念介绍差不多了，我们来实现它:</p>\n<pre><code>#TxInput.java\n//因为我们采用的序列化保存区块，而该数据需要写入区块，因此需要实现Serializable接口\npublic class TxInput implements Serializable{\n    private static final long serialVersionUID = 1L;\n    // 所引用的前一个交易ID\n    public String preTxId;\n    // 该输入中包含的coin\n    public int values;\n    // 解锁脚本 通常为数字签名\n    public String unLockScript;\n    public TxInput(String txId, TxOutput top, Wallet wallet) throws Exception {\n        //对引用的Txoutput中的地址进行签名，用于解锁引用的TxOutPut.\n        this.unLockScript = wallet.sign(top.getLockScript());\n        //记录引用的上一个交易ID\n        this.preTxId = txId;\n        //coin值等于引用的Txoutput的coin值\n        this.values = top.value;\n    }\n}</code></pre><p>接下来是交易输出:</p>\n<pre><code>#TxOutput.java\n\n@Getter\npublic class TxOutput implements Serializable{\n    //同理需要实现Serializable接口\n    private static final long serialVersionUID = 1L;\n    // 交易输出的coin值。\n    public int value;\n    //锁定脚本 通常为地址\n    public String lockScript;\n    public TxOutput(int value,String address){\n        this.value = value;\n        this.lockScript = address;\n    }\n}</code></pre><p>最后是UTXO的实现: 我们使用<code>Transaction</code>进行表示。</p>\n<pre><code>#Transaction.java\n\n@Getter\n@Setter\npublic class Transaction implements Serializable{\n    //为了后期调试方便，引入了log4j的包，导入方法和之前一样\n    private transient static final Logger LOGGER = Logger.getLogger(Transaction.class);\n    private static final long serialVersionUID = 1L;\n    //COINBASE之后再进行解释\n    private transient static final int COINBASE = 50;\n    //UTXOId\n    public String txId;\n    // 交易输入的集合\n    public ArrayList&lt;TxInput&gt; tips;\n    // 交易输出的集合 String:address\n    public HashMap&lt;String, TxOutput&gt; tops;\n\n    private Transaction() {\n        #这里只创建了保存交易输出的集合,因为涉及到Coinbase，暂时先不创建ArrayList\n        this.tops = new HashMap&lt;&gt;(4);\n    }\n    @Override\n    public String toString(){\n        return JSONObject.toJSONString(this);\n    }\n}</code></pre><p>log4j日志包:</p>\n<pre><code>        &lt;dependency&gt;\n            &lt;groupId&gt;log4j&lt;/groupId&gt;\n            &lt;artifactId&gt;log4j&lt;/artifactId&gt;\n            &lt;version&gt;1.2.17&lt;/version&gt;\n        &lt;/dependency&gt;</code></pre><p>这里为了方便起见分别使用<code>ArrayList</code>和<code>HashMap</code>存储交易输入与输出.</p>\n<h3 id=\"创建UTXO\"><a href=\"#创建UTXO\" class=\"headerlink\" title=\"创建UTXO\"></a>创建UTXO</h3><p>接下来是创建UTXO的核心方法，比较复杂，我们先来分析一下:<br>首先传入的参数需要有: 发送coin的源地址，发送coin的目的地址，coin的值。<br>返回值为一个<code>Transaction</code>实例。</p>\n<p>接下来分析如何创建UTXO：</p>\n<ol>\n<li>首先需要遍历整个区块链，查找到所有<strong>未消费的</strong>被锁定在源地址的交易输出。</li>\n<li>将查找到的所有包含符合条件的交易输出的UTXO记录在集合中。</li>\n<li>遍历该集合，将每一笔UTXO中未消费的输出相加，直到满足转账金额或者是统计完全部UTXO。</li>\n<li>将统计的每一笔UTXO中交易输出创建为新的交易输入用于消费。</li>\n<li>判断是否coin值刚好等于需要转账的coin。如果相等则创建一个交易输出将coin转账到目的地址。</li>\n<li>如果有多余的则再创建一个交易输出返回多余的coin到源地址。</li>\n</ol>\n<p>OK，分析完了可以开发了:</p>\n<pre><code>#Transaction.java\n    public static Transaction newUTXO(String fromAddress, String toAddress, int value)\n            throws NoSuchAlgorithmException, Exception {\n        //第一步遍历区块链统计UTXO\n        Transaction[] txs = Blockchain.getInstance().findAllUnspendableUTXO(fromAddress);\n        if (txs.length == 0) {\n            LOGGER.info(&quot;当前地址&quot;+fromAddress+&quot;没有未消费的UTXO！！！&quot;);\n            throw new Exception(&quot;当前地址&quot;+fromAddress+&quot;没有未消费的UTXO,交易失败！！！&quot;);\n        }\n        TxOutput top;\n        // 记录需要使用的TxOutput\n        HashMap&lt;String, TxOutput&gt; tops = new HashMap&lt;String, TxOutput&gt;();\n        int maxValue = 0;\n        // 遍历交易集合\n        for (int i = 0; i &lt; txs.length; i++) {\n            // 查找包括地址为fromAddress的TxOutput\n            if (txs[i].tops.containsKey(fromAddress)) {\n                top = txs[i].tops.get(fromAddress);\n                // 添加进Map\n                tops.put(txs[i].txId, top);\n                // 记录该TxOutput中的value\n                maxValue += top.value;\n                // 如果大于需要使用的则退出\n                if (maxValue &gt;= value) {\n                    break;\n                }\n            }\n        }\n        // 是否有足够的coin\n        if (maxValue &gt;= value) {\n            // 创建tx\n            Transaction t = new Transaction();\n            t.tips = new ArrayList&lt;TxInput&gt;(tops.size());\n            // 遍历所有需要用到的Txoutput\n            tops.forEach((s, to) -&gt; {\n                // 变为TxInput\n                try {\n                    t.tips.add(new TxInput(s, to, Wallet.getInstance()));\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            });\n            //如果值不均等\n            if(maxValue&gt;value){\n                //创建TxOutput返还多余的coin\n                top = new TxOutput(maxValue-value, Wallet.getInstance().getAddress());\n                t.tops.put(top.getLockScript(), top);\n            }\n            //目的地址\n            top = new TxOutput(value, toAddress);\n            t.tops.put(top.getLockScript(), top);\n            LOGGER.info(&quot;创建UTXO: &quot;+t.toString());\n            return t;\n        }\n        LOGGER.info(&quot;当前地址余额不足!!,余额为&quot;+maxValue);\n        throw new Exception(&quot;当前地址余额不足!!,余额为&quot;+maxValue);\n    }</code></pre><h3 id=\"统计未消费的UTXO\"><a href=\"#统计未消费的UTXO\" class=\"headerlink\" title=\"统计未消费的UTXO\"></a>统计未消费的UTXO</h3><p>然后是另外一个核心方法，统计区块链中符合条件的全部未消费的UTXO：<br>我们使用比较简单易理解的方式，先统计地址匹配的所有的交易输出。<br>然后统计所有的满足条件的交易输入。交易输入需要满足两个条件：</p>\n<ol>\n<li>地址是自己的地址</li>\n<li>交易输入中引用的UTXOid可以追溯到。</li>\n</ol>\n<p>我们将符合条件的<code>TxInput</code>中引用的UTXOId在所有未消费的UTXO中匹配，<br>如果匹配到说明该UTXO已经被花费掉了，我们移除掉花费掉的UTXO，剩下的就是满足条件的未消费的UTXO了。</p>\n<pre><code>#Blockchain.java\npublic Transaction[] findAllUnspendableUTXO(String address)\n            throws FileNotFoundException, ClassNotFoundException, IOException {\n        LOGGER.info(&quot;查找所有未消费的UTXO...............&quot;);\n        HashMap&lt;String, Transaction&gt; txs = new HashMap&lt;&gt;();\n        Block block = this.block;\n        Transaction tx;\n        // 从当前区块向前遍历查找UTXO txOutput\n        do{\n            //从区块中获取交易信息\n            tx = block.getTransaction();\n            // 如果存在交易信息，且TxOutput地址包含address\n            if (tx != null &amp;&amp; tx.getTops().containsKey(address)) {\n                txs.put(tx.getTxId(), tx);\n            }\n            //指向前一个区块\n            block = block.getPrevBlock();\n            //一直遍历到创世区块\n        }while(block!=null &amp;&amp; block.hasPrevBlock()) ;\n        // 再遍历一次查找已消费的UTXO\n        block = this.block;\n        do {\n            tx = block.getTransaction();\n            if (tx != null) {\n                // 如果交易中的TxInput包含的交易ID存在于txs，移除\n                tx.getTips().forEach(tip -&gt; {\n                    try {\n                        //需要满足两个条件，一是Txinput中引用的UTXOId存在，说明该UTXO已经被使用了\n                        //二是需要保证地址相同，确认该TxInput是coin所有者消费的\n                        if (Wallet.getInstance().verify(address,tip.unLockScript) \n                                &amp;&amp; txs.containsKey(tip.preTxId))\n                            //满足两个条件则移除该UTXO\n                            txs.remove(tip.preTxId);\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                });\n            }\n            block = block.getPrevBlock();\n        }while (block!=null &amp;&amp; block.hasPrevBlock());\n        //创建UTXO数组返回\n        Transaction[] t = new Transaction[txs.size()];\n        return txs.values().toArray(t);\n    }</code></pre><p>看这里的代码：</p>\n<pre><code> if (Wallet.getInstance().verify(address,tip.unLockScript) &amp;&amp; txs.containsKey(tip.preTxId))\n    ...</code></pre><p>首先验证<code>TxInput</code>的解锁脚本是否对我们钱包的地址进行签名得到的，即验证这一笔输入是否是自己消费的。<br>如果是自己消费的然后比对UTXO的Id，如果相同则说明这笔UTXO已经被消费掉了。那么需要移除它。<br>在钱包中添加一个新的方法，用于验证解锁脚本是否可以解锁交易输出。我们简单采用哈希值匹配的方式模拟验证。</p>\n<pre><code>#Wallet.java\n    public boolean verify(String data,String sign) throws DecoderException, Exception {\n        LOGGER.info(&quot;验证签名: &quot;+data);\n        String[] str = data.split(&quot;%%%&quot;);\n        // 原文     encry(hash(原文))\n        if(str.length!=2){\n            return false;\n        }\n        String hash2 = Hex.encodeHexString(this.decrypt(str[1]));\n        String hash3 = Util.getSHA256(data);\n        if(hash3.equals(hash2)){\n            LOGGER.info(&quot;签名验证成功！！&quot;);\n            return true;\n        }\n        LOGGER.info(&quot;签名验证失败！！&quot;);\n        return false;\n    }</code></pre><h3 id=\"更新区块信息\"><a href=\"#更新区块信息\" class=\"headerlink\" title=\"更新区块信息\"></a>更新区块信息</h3><p>加入了UTXO的概念，那我们需要更新区块以及区块链的属性信息了。</p>\n<pre><code>#Block.java\n@Getter\n@Setter\npublic class Block implements Serializable{\n    ...\n    //当前区块中的交易\n    public Transaction transaction;\n    ...\n    //添加一个新的构造方法\n    public Block(int blkNum,Transaction transaction,String prevBlockHash){\n        this.blkNum = blkNum;\n        this.transaction = transaction;\n        this.prevBlockHash = prevBlockHash;\n        this.timeStamp = Util.getTimeStamp();\n    }\n    ...\n}</code></pre><p>然后是区块链，也要更新一个方法:</p>\n<pre><code>#Blockchain.java\npublic final class Blockchain {\n    ...\n    public Block addBlock(Transaction tx) throws IOException {\n        int num = this.block.getBlkNum();\n        Block block = new Block(num + 1, tx, this.block.curBlockHash);\n        // 每次将区块添加进区块链之前需要计算难度值\n        block.setNonce(Pow.calc(block));\n        // 计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum() + block.getData() + block.getPrevBlockHash()\n                + block.getPrevBlockHash() + block.getNonce());\n        block.setCurBlockHash(hash);\n        // 序列化\n        Storage.Serialize(block);\n        this.block = block;\n        LOGGER.info(&quot;当前区块信息为:&quot;+block.toString());\n        return this.block;\n    }\n    ...\n}</code></pre><p>之前区块中将字符串保存为区块信息，我们更新为一笔交易。需要创建一笔交易才可以创建区块。</p>\n<h3 id=\"Coinbase\"><a href=\"#Coinbase\" class=\"headerlink\" title=\"Coinbase\"></a>Coinbase</h3><p>关于UTXO，我们之前讲到每一笔输出都会对应着一个输入，那么第一笔被输出的coin是哪里来的呢，<br>在比特币中，每产出一个区块将会奖励一定数量的BItcoin，称为Coinbase。同理，我们这里也实现它。<br>因此第一笔被输出的coin来自于coinbase。我们将coinbase固定为50，正如之前设定的属性:</p>\n<pre><code>#Transaction.java\n    private transient static final int COINBASE = 50;</code></pre><p>所以我们还需要一个生成coinbase的交易的构造方法:</p>\n<pre><code>#Blockchain.java\n    public static Transaction newCoinBase() throws NoSuchAlgorithmException, Exception {\n        Transaction t = new Transaction();\n        t.tips = new ArrayList&lt;&gt;();\n        t.tops.put(Wallet.getInstance().getAddress(), new TxOutput(COINBASE, Wallet.getInstance().getAddress()));\n        LOGGER.info(&quot;创建Coinbase.....&quot;+t.toString());\n        return t;\n    }</code></pre><p>可以看到，交易输出的地址我们设置为钱包的地址。<br>比较简单，接下来修改一下创世区块的生成方法，将coinbase的交易添加进去。</p>\n<pre><code>#Blockchain.java\n    private Block CrtGenesisBlock() throws NoSuchAlgorithmException, Exception {\n        // Block block = new Block(1,&quot;Genesis Block&quot;,&quot;00000000000000000&quot;);\n        Block block = new Block(1, Transaction.newCoinBase(), &quot;00000000000000000&quot;);\n        ...\n    }</code></pre><h3 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h3><p>一切都完成了，测试一下:</p>\n<pre><code>#Test.java\npublic class Test {\n    public static void main(String[] args) throws NoSuchAlgorithmException, Exception {\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                Wallet.getInstance().getAddress(), &quot;address&quot;, 30));\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                &quot;address&quot;, &quot;address1&quot;, 20));\n    }\n}</code></pre><p>分析一下测试用例：</p>\n<pre><code>Blockchain.getInstance()\n#############################\n#Blockchain.java CrtGenesisBlock()\nBlock block = new Block(1, Transaction.newCoinBase(), &quot;00000000000000000&quot;);\n#newCoinBase()\nt.tops.put(Wallet.getInstance().getAddress(), new TxOutput(COINBASE, Wallet.getInstance().getAddress()));</code></pre><p>首先获取区块链实例，因此创建了创始区块，看上面的代码我们可以知道创建了一笔coinbase交易，50个coin被锁定在我们钱包的地址。</p>\n<pre><code>Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                Wallet.getInstance().getAddress(), &quot;address&quot;, 30));</code></pre><p>然后是第二个区块，我们创建了一个UTXO，从钱包的地址转移30个coin到地址<code>address</code>。</p>\n<pre><code>        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                &quot;address&quot;, &quot;address1&quot;, 20));</code></pre><p>最后是第三个区块，从地址<code>address</code>转移20个coin到地址<code>address1</code>.<br>测试一下：</p>\n<pre><code>...\n[INFO ] 2020-05-18 14:10:19,501 method:org.xd.chain.transaction.Transaction.newCoinBase(Transaction.java:42)\n创建Coinbase.....{&quot;tips&quot;:[],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:50}}}\n...\n[INFO ] 2020-05-18 14:10:19,757 method:org.xd.chain.core.Blockchain.findAllUnspendableUTXO(Blockchain.java:117)\n查找所有未消费的UTXO...............\n[INFO ] 2020-05-18 14:10:19,804 method:org.xd.chain.wallet.Wallet.sign(Wallet.java:86)\n使用私钥对数据签名: R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\n[INFO ] 2020-05-18 14:10:20,382 method:org.xd.chain.transaction.Transaction.newUTXO(Transaction.java:100)\n创建UTXO: {&quot;tips&quot;:[{&quot;unLockScript&quot;:&quot;523133363335343866383934363532396464373339323262323635343762363131306639313539613330636339623034373434333563316330646233373834326364336334313030303832313832323237653335366338313138333064366235623536643836323838306238356632303335356366626333383736343136353361%%%4251d1ae7091f422bef3a95b29867ebeaeeedb0e20239a4edf96967d1a1f15a8f061873acc01aa86c726e1ad128aefeaaf5c447aed27e5729bdd24f0026d6c23&quot;,&quot;values&quot;:50}],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:20},&quot;address&quot;:{&quot;lockScript&quot;:&quot;address&quot;,&quot;value&quot;:30}}}\n...\n[INFO ] 2020-05-18 14:10:20,468 method:org.xd.chain.core.Blockchain.addBlock(Blockchain.java:86)\n当前区块信息为:{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;00005f0690489e8763bd3db0ce7112592cdb118507945c65d07bfe27e0ad3031&quot;,&quot;nonce&quot;:4350,&quot;prevBlockHash&quot;:&quot;000011de81afdac44e08e81b9be434cbcb625808a9d0f8008275ab9a6ffb809f&quot;,&quot;timeStamp&quot;:&quot;2020-05-18 14:10:20&quot;,&quot;transaction&quot;:{&quot;tips&quot;:[{&quot;unLockScript&quot;:&quot;523133363335343866383934363532396464373339323262323635343762363131306639313539613330636339623034373434333563316330646233373834326364336334313030303832313832323237653335366338313138333064366235623536643836323838306238356632303335356366626333383736343136353361%%%4251d1ae7091f422bef3a95b29867ebeaeeedb0e20239a4edf96967d1a1f15a8f061873acc01aa86c726e1ad128aefeaaf5c447aed27e5729bdd24f0026d6c23&quot;,&quot;values&quot;:50}],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:20},&quot;address&quot;:{&quot;lockScript&quot;:&quot;address&quot;,&quot;value&quot;:30}}}}\n[INFO ] 2020-05-18 14:10:20,575 method:org.xd.chain.core.Blockchain.findAllUnspendableUTXO(Blockchain.java:117)\n查找所有未消费的UTXO...............\n...\n[INFO ] 2020-05-18 14:10:20,725 method:org.xd.chain.wallet.Wallet.verify(Wallet.java:124)\n验证签名: address\n...\n[INFO ] 2020-05-18 14:10:20,731 method:org.xd.chain.wallet.Wallet.sign(Wallet.java:86)\n使用私钥对数据签名: address\n[INFO ] 2020-05-18 14:10:20,734 method:org.xd.chain.transaction.Transaction.newUTXO(Transaction.java:100)\n创建UTXO: {&quot;tips&quot;:[{&quot;unLockScript&quot;:&quot;61646472657373%%%8ad16095a9f1947e323eb5ef3601a0cc2ad552ad3f7331406123577a1cc0c68dc614f3262505f079f7c3acfc1d681fdb432f7ba0f4ac3d69cb46dead5446b2cd&quot;,&quot;values&quot;:30}],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:10},&quot;address1&quot;:{&quot;lockScript&quot;:&quot;address1&quot;,&quot;value&quot;:20}}}\n...\n[INFO ] 2020-05-18 14:10:20,990 method:org.xd.chain.core.Blockchain.addBlock(Blockchain.java:86)\n当前区块信息为:{&quot;blkNum&quot;:3,&quot;curBlockHash&quot;:&quot;00006e8918bba9831374f578caa3d80fa936997598072145bc0654cbca2d084e&quot;,&quot;nonce&quot;:74607,&quot;prevBlockHash&quot;:&quot;00005f0690489e8763bd3db0ce7112592cdb118507945c65d07bfe27e0ad3031&quot;,&quot;timeStamp&quot;:&quot;2020-05-18 14:10:20&quot;,&quot;transaction&quot;:{&quot;tips&quot;:[{&quot;unLockScript&quot;:&quot;61646472657373%%%8ad16095a9f1947e323eb5ef3601a0cc2ad552ad3f7331406123577a1cc0c68dc614f3262505f079f7c3acfc1d681fdb432f7ba0f4ac3d69cb46dead5446b2cd&quot;,&quot;values&quot;:30}],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:10},&quot;address1&quot;:{&quot;lockScript&quot;:&quot;address1&quot;,&quot;value&quot;:20}}}}</code></pre><p>省略掉其他日志信息，看起来测试是没有问题的，共生成了3个区块。创世区块中有一笔Coinbase交易。区块2中成功转移30coin到地址<code>address</code>，返还20coin到原地址。区块3中成功从地址<code>address</code>转移20coin到地址<code>address1</code>，返还10coin到地址<code>address</code>。</p>\n<p>还有部分未完善部分，比如coinbase只在创世区块中生成了。每个区块中只含有一笔交易等等，后期慢慢完善。</p>\n<h3 id=\"Github仓库地址在这里，随时保持更新中…\"><a href=\"#Github仓库地址在这里，随时保持更新中…\" class=\"headerlink\" title=\"Github仓库地址在这里，随时保持更新中…..\"></a>Github仓库地址在这里，随时保持更新中…..</h3><p>Github地址：<a href=\"https://github.com/newonexd/Jchain\" target=\"_blank\" rel=\"noopener\">Jchain</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>前一篇文章: <a href=\"https://www.cnblogs.com/cbkj-xd/p/12905706.html\" target=\"_blank\" rel=\"noopener\">搭建你的第一个区块链网络(三)</a></p>\n<h2 id=\"UTXO\"><a href=\"#UTXO\" class=\"headerlink\" title=\"UTXO\"></a>UTXO</h2><h4 id=\"组成部分\"><a href=\"#组成部分\" class=\"headerlink\" title=\"组成部分\"></a>组成部分</h4><p>UTXO是比特币中一个重要的概念，这一节我们来实现一个简单的UTXO。我们把UTXO的组成部分分为以下三点:</p>\n<ul>\n<li><strong>UTXOId:</strong> 标识该UTXO</li>\n<li><strong>TxInput:</strong> 交易输入，即coin的输入地址以及金额</li>\n<li><strong>TxOutput:</strong> 交易输出，即coin的输出地址以及金额</li>\n</ul>\n<p>其中<code>TxInput</code>与<code>TxOutput</code>分别具有以下几个属性:</p>\n<p><strong>TxInput:</strong>  交易输入</p>\n<ul>\n<li><strong>preTxId</strong>: 指向的前一个UTXO的id</li>\n<li><strong>value</strong>： 输入的金额</li>\n<li><strong>unLockScript:</strong> 解锁脚本</li>\n</ul>\n<p>其中交易输入需要引用之前的UTXO的输出。这样很容易知道当前的交易输入的金额是由之前的哪一笔交易中的交易输出的金额传递的。<br>保证每一笔未消费的金额都可以找到它的源地址。<br>解锁脚本的作用是用于解锁当前交易输入所引用的交易输出的。因为每一笔金额都有所属，被锁定在某一个地址上面。只有该金额的所有者才具有权限消费进行消费。而解锁脚本一般都是一个数字签名。</p>\n<p><strong>TxOutput</strong> 交易输出</p>\n<ul>\n<li><strong>value</strong> :输出的金额</li>\n<li><strong>lockScript</strong>: 锁定脚本</li>\n</ul>\n<p>每当一笔coin被转移，都会被锁定在一个地址上面，因此锁定脚本一般都是一个地址。</p>\n<p>对于每一笔UTXO，输入的金额一定是等于输出的金额的。另外UTXO有一个特点，就是不能够只花费其中一部分。而是需要全部消费，而多余的再返还给原地址。<br>比如用户a具有10个coin被锁定在一个UTXO中，如果a需要转账给b5个coin，那么需要将10个coin全部花费掉，其中5个coin输出到b的地址，剩余的5个coin输出到a的地址。<br>因此一笔UTXO可以有多个交易输出，同时也可以有多个输入。</p>\n<p>大致概念介绍差不多了，我们来实现它:</p>\n<pre><code>#TxInput.java\n//因为我们采用的序列化保存区块，而该数据需要写入区块，因此需要实现Serializable接口\npublic class TxInput implements Serializable{\n    private static final long serialVersionUID = 1L;\n    // 所引用的前一个交易ID\n    public String preTxId;\n    // 该输入中包含的coin\n    public int values;\n    // 解锁脚本 通常为数字签名\n    public String unLockScript;\n    public TxInput(String txId, TxOutput top, Wallet wallet) throws Exception {\n        //对引用的Txoutput中的地址进行签名，用于解锁引用的TxOutPut.\n        this.unLockScript = wallet.sign(top.getLockScript());\n        //记录引用的上一个交易ID\n        this.preTxId = txId;\n        //coin值等于引用的Txoutput的coin值\n        this.values = top.value;\n    }\n}</code></pre><p>接下来是交易输出:</p>\n<pre><code>#TxOutput.java\n\n@Getter\npublic class TxOutput implements Serializable{\n    //同理需要实现Serializable接口\n    private static final long serialVersionUID = 1L;\n    // 交易输出的coin值。\n    public int value;\n    //锁定脚本 通常为地址\n    public String lockScript;\n    public TxOutput(int value,String address){\n        this.value = value;\n        this.lockScript = address;\n    }\n}</code></pre><p>最后是UTXO的实现: 我们使用<code>Transaction</code>进行表示。</p>\n<pre><code>#Transaction.java\n\n@Getter\n@Setter\npublic class Transaction implements Serializable{\n    //为了后期调试方便，引入了log4j的包，导入方法和之前一样\n    private transient static final Logger LOGGER = Logger.getLogger(Transaction.class);\n    private static final long serialVersionUID = 1L;\n    //COINBASE之后再进行解释\n    private transient static final int COINBASE = 50;\n    //UTXOId\n    public String txId;\n    // 交易输入的集合\n    public ArrayList&lt;TxInput&gt; tips;\n    // 交易输出的集合 String:address\n    public HashMap&lt;String, TxOutput&gt; tops;\n\n    private Transaction() {\n        #这里只创建了保存交易输出的集合,因为涉及到Coinbase，暂时先不创建ArrayList\n        this.tops = new HashMap&lt;&gt;(4);\n    }\n    @Override\n    public String toString(){\n        return JSONObject.toJSONString(this);\n    }\n}</code></pre><p>log4j日志包:</p>\n<pre><code>        &lt;dependency&gt;\n            &lt;groupId&gt;log4j&lt;/groupId&gt;\n            &lt;artifactId&gt;log4j&lt;/artifactId&gt;\n            &lt;version&gt;1.2.17&lt;/version&gt;\n        &lt;/dependency&gt;</code></pre><p>这里为了方便起见分别使用<code>ArrayList</code>和<code>HashMap</code>存储交易输入与输出.</p>\n<h3 id=\"创建UTXO\"><a href=\"#创建UTXO\" class=\"headerlink\" title=\"创建UTXO\"></a>创建UTXO</h3><p>接下来是创建UTXO的核心方法，比较复杂，我们先来分析一下:<br>首先传入的参数需要有: 发送coin的源地址，发送coin的目的地址，coin的值。<br>返回值为一个<code>Transaction</code>实例。</p>\n<p>接下来分析如何创建UTXO：</p>\n<ol>\n<li>首先需要遍历整个区块链，查找到所有<strong>未消费的</strong>被锁定在源地址的交易输出。</li>\n<li>将查找到的所有包含符合条件的交易输出的UTXO记录在集合中。</li>\n<li>遍历该集合，将每一笔UTXO中未消费的输出相加，直到满足转账金额或者是统计完全部UTXO。</li>\n<li>将统计的每一笔UTXO中交易输出创建为新的交易输入用于消费。</li>\n<li>判断是否coin值刚好等于需要转账的coin。如果相等则创建一个交易输出将coin转账到目的地址。</li>\n<li>如果有多余的则再创建一个交易输出返回多余的coin到源地址。</li>\n</ol>\n<p>OK，分析完了可以开发了:</p>\n<pre><code>#Transaction.java\n    public static Transaction newUTXO(String fromAddress, String toAddress, int value)\n            throws NoSuchAlgorithmException, Exception {\n        //第一步遍历区块链统计UTXO\n        Transaction[] txs = Blockchain.getInstance().findAllUnspendableUTXO(fromAddress);\n        if (txs.length == 0) {\n            LOGGER.info(&quot;当前地址&quot;+fromAddress+&quot;没有未消费的UTXO！！！&quot;);\n            throw new Exception(&quot;当前地址&quot;+fromAddress+&quot;没有未消费的UTXO,交易失败！！！&quot;);\n        }\n        TxOutput top;\n        // 记录需要使用的TxOutput\n        HashMap&lt;String, TxOutput&gt; tops = new HashMap&lt;String, TxOutput&gt;();\n        int maxValue = 0;\n        // 遍历交易集合\n        for (int i = 0; i &lt; txs.length; i++) {\n            // 查找包括地址为fromAddress的TxOutput\n            if (txs[i].tops.containsKey(fromAddress)) {\n                top = txs[i].tops.get(fromAddress);\n                // 添加进Map\n                tops.put(txs[i].txId, top);\n                // 记录该TxOutput中的value\n                maxValue += top.value;\n                // 如果大于需要使用的则退出\n                if (maxValue &gt;= value) {\n                    break;\n                }\n            }\n        }\n        // 是否有足够的coin\n        if (maxValue &gt;= value) {\n            // 创建tx\n            Transaction t = new Transaction();\n            t.tips = new ArrayList&lt;TxInput&gt;(tops.size());\n            // 遍历所有需要用到的Txoutput\n            tops.forEach((s, to) -&gt; {\n                // 变为TxInput\n                try {\n                    t.tips.add(new TxInput(s, to, Wallet.getInstance()));\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            });\n            //如果值不均等\n            if(maxValue&gt;value){\n                //创建TxOutput返还多余的coin\n                top = new TxOutput(maxValue-value, Wallet.getInstance().getAddress());\n                t.tops.put(top.getLockScript(), top);\n            }\n            //目的地址\n            top = new TxOutput(value, toAddress);\n            t.tops.put(top.getLockScript(), top);\n            LOGGER.info(&quot;创建UTXO: &quot;+t.toString());\n            return t;\n        }\n        LOGGER.info(&quot;当前地址余额不足!!,余额为&quot;+maxValue);\n        throw new Exception(&quot;当前地址余额不足!!,余额为&quot;+maxValue);\n    }</code></pre><h3 id=\"统计未消费的UTXO\"><a href=\"#统计未消费的UTXO\" class=\"headerlink\" title=\"统计未消费的UTXO\"></a>统计未消费的UTXO</h3><p>然后是另外一个核心方法，统计区块链中符合条件的全部未消费的UTXO：<br>我们使用比较简单易理解的方式，先统计地址匹配的所有的交易输出。<br>然后统计所有的满足条件的交易输入。交易输入需要满足两个条件：</p>\n<ol>\n<li>地址是自己的地址</li>\n<li>交易输入中引用的UTXOid可以追溯到。</li>\n</ol>\n<p>我们将符合条件的<code>TxInput</code>中引用的UTXOId在所有未消费的UTXO中匹配，<br>如果匹配到说明该UTXO已经被花费掉了，我们移除掉花费掉的UTXO，剩下的就是满足条件的未消费的UTXO了。</p>\n<pre><code>#Blockchain.java\npublic Transaction[] findAllUnspendableUTXO(String address)\n            throws FileNotFoundException, ClassNotFoundException, IOException {\n        LOGGER.info(&quot;查找所有未消费的UTXO...............&quot;);\n        HashMap&lt;String, Transaction&gt; txs = new HashMap&lt;&gt;();\n        Block block = this.block;\n        Transaction tx;\n        // 从当前区块向前遍历查找UTXO txOutput\n        do{\n            //从区块中获取交易信息\n            tx = block.getTransaction();\n            // 如果存在交易信息，且TxOutput地址包含address\n            if (tx != null &amp;&amp; tx.getTops().containsKey(address)) {\n                txs.put(tx.getTxId(), tx);\n            }\n            //指向前一个区块\n            block = block.getPrevBlock();\n            //一直遍历到创世区块\n        }while(block!=null &amp;&amp; block.hasPrevBlock()) ;\n        // 再遍历一次查找已消费的UTXO\n        block = this.block;\n        do {\n            tx = block.getTransaction();\n            if (tx != null) {\n                // 如果交易中的TxInput包含的交易ID存在于txs，移除\n                tx.getTips().forEach(tip -&gt; {\n                    try {\n                        //需要满足两个条件，一是Txinput中引用的UTXOId存在，说明该UTXO已经被使用了\n                        //二是需要保证地址相同，确认该TxInput是coin所有者消费的\n                        if (Wallet.getInstance().verify(address,tip.unLockScript) \n                                &amp;&amp; txs.containsKey(tip.preTxId))\n                            //满足两个条件则移除该UTXO\n                            txs.remove(tip.preTxId);\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                });\n            }\n            block = block.getPrevBlock();\n        }while (block!=null &amp;&amp; block.hasPrevBlock());\n        //创建UTXO数组返回\n        Transaction[] t = new Transaction[txs.size()];\n        return txs.values().toArray(t);\n    }</code></pre><p>看这里的代码：</p>\n<pre><code> if (Wallet.getInstance().verify(address,tip.unLockScript) &amp;&amp; txs.containsKey(tip.preTxId))\n    ...</code></pre><p>首先验证<code>TxInput</code>的解锁脚本是否对我们钱包的地址进行签名得到的，即验证这一笔输入是否是自己消费的。<br>如果是自己消费的然后比对UTXO的Id，如果相同则说明这笔UTXO已经被消费掉了。那么需要移除它。<br>在钱包中添加一个新的方法，用于验证解锁脚本是否可以解锁交易输出。我们简单采用哈希值匹配的方式模拟验证。</p>\n<pre><code>#Wallet.java\n    public boolean verify(String data,String sign) throws DecoderException, Exception {\n        LOGGER.info(&quot;验证签名: &quot;+data);\n        String[] str = data.split(&quot;%%%&quot;);\n        // 原文     encry(hash(原文))\n        if(str.length!=2){\n            return false;\n        }\n        String hash2 = Hex.encodeHexString(this.decrypt(str[1]));\n        String hash3 = Util.getSHA256(data);\n        if(hash3.equals(hash2)){\n            LOGGER.info(&quot;签名验证成功！！&quot;);\n            return true;\n        }\n        LOGGER.info(&quot;签名验证失败！！&quot;);\n        return false;\n    }</code></pre><h3 id=\"更新区块信息\"><a href=\"#更新区块信息\" class=\"headerlink\" title=\"更新区块信息\"></a>更新区块信息</h3><p>加入了UTXO的概念，那我们需要更新区块以及区块链的属性信息了。</p>\n<pre><code>#Block.java\n@Getter\n@Setter\npublic class Block implements Serializable{\n    ...\n    //当前区块中的交易\n    public Transaction transaction;\n    ...\n    //添加一个新的构造方法\n    public Block(int blkNum,Transaction transaction,String prevBlockHash){\n        this.blkNum = blkNum;\n        this.transaction = transaction;\n        this.prevBlockHash = prevBlockHash;\n        this.timeStamp = Util.getTimeStamp();\n    }\n    ...\n}</code></pre><p>然后是区块链，也要更新一个方法:</p>\n<pre><code>#Blockchain.java\npublic final class Blockchain {\n    ...\n    public Block addBlock(Transaction tx) throws IOException {\n        int num = this.block.getBlkNum();\n        Block block = new Block(num + 1, tx, this.block.curBlockHash);\n        // 每次将区块添加进区块链之前需要计算难度值\n        block.setNonce(Pow.calc(block));\n        // 计算区块哈希值\n        String hash = Util.getSHA256(block.getBlkNum() + block.getData() + block.getPrevBlockHash()\n                + block.getPrevBlockHash() + block.getNonce());\n        block.setCurBlockHash(hash);\n        // 序列化\n        Storage.Serialize(block);\n        this.block = block;\n        LOGGER.info(&quot;当前区块信息为:&quot;+block.toString());\n        return this.block;\n    }\n    ...\n}</code></pre><p>之前区块中将字符串保存为区块信息，我们更新为一笔交易。需要创建一笔交易才可以创建区块。</p>\n<h3 id=\"Coinbase\"><a href=\"#Coinbase\" class=\"headerlink\" title=\"Coinbase\"></a>Coinbase</h3><p>关于UTXO，我们之前讲到每一笔输出都会对应着一个输入，那么第一笔被输出的coin是哪里来的呢，<br>在比特币中，每产出一个区块将会奖励一定数量的BItcoin，称为Coinbase。同理，我们这里也实现它。<br>因此第一笔被输出的coin来自于coinbase。我们将coinbase固定为50，正如之前设定的属性:</p>\n<pre><code>#Transaction.java\n    private transient static final int COINBASE = 50;</code></pre><p>所以我们还需要一个生成coinbase的交易的构造方法:</p>\n<pre><code>#Blockchain.java\n    public static Transaction newCoinBase() throws NoSuchAlgorithmException, Exception {\n        Transaction t = new Transaction();\n        t.tips = new ArrayList&lt;&gt;();\n        t.tops.put(Wallet.getInstance().getAddress(), new TxOutput(COINBASE, Wallet.getInstance().getAddress()));\n        LOGGER.info(&quot;创建Coinbase.....&quot;+t.toString());\n        return t;\n    }</code></pre><p>可以看到，交易输出的地址我们设置为钱包的地址。<br>比较简单，接下来修改一下创世区块的生成方法，将coinbase的交易添加进去。</p>\n<pre><code>#Blockchain.java\n    private Block CrtGenesisBlock() throws NoSuchAlgorithmException, Exception {\n        // Block block = new Block(1,&quot;Genesis Block&quot;,&quot;00000000000000000&quot;);\n        Block block = new Block(1, Transaction.newCoinBase(), &quot;00000000000000000&quot;);\n        ...\n    }</code></pre><h3 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h3><p>一切都完成了，测试一下:</p>\n<pre><code>#Test.java\npublic class Test {\n    public static void main(String[] args) throws NoSuchAlgorithmException, Exception {\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                Wallet.getInstance().getAddress(), &quot;address&quot;, 30));\n        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                &quot;address&quot;, &quot;address1&quot;, 20));\n    }\n}</code></pre><p>分析一下测试用例：</p>\n<pre><code>Blockchain.getInstance()\n#############################\n#Blockchain.java CrtGenesisBlock()\nBlock block = new Block(1, Transaction.newCoinBase(), &quot;00000000000000000&quot;);\n#newCoinBase()\nt.tops.put(Wallet.getInstance().getAddress(), new TxOutput(COINBASE, Wallet.getInstance().getAddress()));</code></pre><p>首先获取区块链实例，因此创建了创始区块，看上面的代码我们可以知道创建了一笔coinbase交易，50个coin被锁定在我们钱包的地址。</p>\n<pre><code>Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                Wallet.getInstance().getAddress(), &quot;address&quot;, 30));</code></pre><p>然后是第二个区块，我们创建了一个UTXO，从钱包的地址转移30个coin到地址<code>address</code>。</p>\n<pre><code>        Blockchain.getInstance().addBlock(\n            Transaction.newUTXO(\n                &quot;address&quot;, &quot;address1&quot;, 20));</code></pre><p>最后是第三个区块，从地址<code>address</code>转移20个coin到地址<code>address1</code>.<br>测试一下：</p>\n<pre><code>...\n[INFO ] 2020-05-18 14:10:19,501 method:org.xd.chain.transaction.Transaction.newCoinBase(Transaction.java:42)\n创建Coinbase.....{&quot;tips&quot;:[],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:50}}}\n...\n[INFO ] 2020-05-18 14:10:19,757 method:org.xd.chain.core.Blockchain.findAllUnspendableUTXO(Blockchain.java:117)\n查找所有未消费的UTXO...............\n[INFO ] 2020-05-18 14:10:19,804 method:org.xd.chain.wallet.Wallet.sign(Wallet.java:86)\n使用私钥对数据签名: R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a\n[INFO ] 2020-05-18 14:10:20,382 method:org.xd.chain.transaction.Transaction.newUTXO(Transaction.java:100)\n创建UTXO: {&quot;tips&quot;:[{&quot;unLockScript&quot;:&quot;523133363335343866383934363532396464373339323262323635343762363131306639313539613330636339623034373434333563316330646233373834326364336334313030303832313832323237653335366338313138333064366235623536643836323838306238356632303335356366626333383736343136353361%%%4251d1ae7091f422bef3a95b29867ebeaeeedb0e20239a4edf96967d1a1f15a8f061873acc01aa86c726e1ad128aefeaaf5c447aed27e5729bdd24f0026d6c23&quot;,&quot;values&quot;:50}],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:20},&quot;address&quot;:{&quot;lockScript&quot;:&quot;address&quot;,&quot;value&quot;:30}}}\n...\n[INFO ] 2020-05-18 14:10:20,468 method:org.xd.chain.core.Blockchain.addBlock(Blockchain.java:86)\n当前区块信息为:{&quot;blkNum&quot;:2,&quot;curBlockHash&quot;:&quot;00005f0690489e8763bd3db0ce7112592cdb118507945c65d07bfe27e0ad3031&quot;,&quot;nonce&quot;:4350,&quot;prevBlockHash&quot;:&quot;000011de81afdac44e08e81b9be434cbcb625808a9d0f8008275ab9a6ffb809f&quot;,&quot;timeStamp&quot;:&quot;2020-05-18 14:10:20&quot;,&quot;transaction&quot;:{&quot;tips&quot;:[{&quot;unLockScript&quot;:&quot;523133363335343866383934363532396464373339323262323635343762363131306639313539613330636339623034373434333563316330646233373834326364336334313030303832313832323237653335366338313138333064366235623536643836323838306238356632303335356366626333383736343136353361%%%4251d1ae7091f422bef3a95b29867ebeaeeedb0e20239a4edf96967d1a1f15a8f061873acc01aa86c726e1ad128aefeaaf5c447aed27e5729bdd24f0026d6c23&quot;,&quot;values&quot;:50}],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:20},&quot;address&quot;:{&quot;lockScript&quot;:&quot;address&quot;,&quot;value&quot;:30}}}}\n[INFO ] 2020-05-18 14:10:20,575 method:org.xd.chain.core.Blockchain.findAllUnspendableUTXO(Blockchain.java:117)\n查找所有未消费的UTXO...............\n...\n[INFO ] 2020-05-18 14:10:20,725 method:org.xd.chain.wallet.Wallet.verify(Wallet.java:124)\n验证签名: address\n...\n[INFO ] 2020-05-18 14:10:20,731 method:org.xd.chain.wallet.Wallet.sign(Wallet.java:86)\n使用私钥对数据签名: address\n[INFO ] 2020-05-18 14:10:20,734 method:org.xd.chain.transaction.Transaction.newUTXO(Transaction.java:100)\n创建UTXO: {&quot;tips&quot;:[{&quot;unLockScript&quot;:&quot;61646472657373%%%8ad16095a9f1947e323eb5ef3601a0cc2ad552ad3f7331406123577a1cc0c68dc614f3262505f079f7c3acfc1d681fdb432f7ba0f4ac3d69cb46dead5446b2cd&quot;,&quot;values&quot;:30}],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:10},&quot;address1&quot;:{&quot;lockScript&quot;:&quot;address1&quot;,&quot;value&quot;:20}}}\n...\n[INFO ] 2020-05-18 14:10:20,990 method:org.xd.chain.core.Blockchain.addBlock(Blockchain.java:86)\n当前区块信息为:{&quot;blkNum&quot;:3,&quot;curBlockHash&quot;:&quot;00006e8918bba9831374f578caa3d80fa936997598072145bc0654cbca2d084e&quot;,&quot;nonce&quot;:74607,&quot;prevBlockHash&quot;:&quot;00005f0690489e8763bd3db0ce7112592cdb118507945c65d07bfe27e0ad3031&quot;,&quot;timeStamp&quot;:&quot;2020-05-18 14:10:20&quot;,&quot;transaction&quot;:{&quot;tips&quot;:[{&quot;unLockScript&quot;:&quot;61646472657373%%%8ad16095a9f1947e323eb5ef3601a0cc2ad552ad3f7331406123577a1cc0c68dc614f3262505f079f7c3acfc1d681fdb432f7ba0f4ac3d69cb46dead5446b2cd&quot;,&quot;values&quot;:30}],&quot;tops&quot;:{&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;:{&quot;lockScript&quot;:&quot;R1363548f8946529dd73922b26547b6110f9159a30cc9b0474435c1c0db37842cd3c4100082182227e356c811830d6b5b56d862880b85f20355cfbc387641653a&quot;,&quot;value&quot;:10},&quot;address1&quot;:{&quot;lockScript&quot;:&quot;address1&quot;,&quot;value&quot;:20}}}}</code></pre><p>省略掉其他日志信息，看起来测试是没有问题的，共生成了3个区块。创世区块中有一笔Coinbase交易。区块2中成功转移30coin到地址<code>address</code>，返还20coin到原地址。区块3中成功从地址<code>address</code>转移20coin到地址<code>address1</code>，返还10coin到地址<code>address</code>。</p>\n<p>还有部分未完善部分，比如coinbase只在创世区块中生成了。每个区块中只含有一笔交易等等，后期慢慢完善。</p>\n<h3 id=\"Github仓库地址在这里，随时保持更新中…\"><a href=\"#Github仓库地址在这里，随时保持更新中…\" class=\"headerlink\" title=\"Github仓库地址在这里，随时保持更新中…..\"></a>Github仓库地址在这里，随时保持更新中…..</h3><p>Github地址：<a href=\"https://github.com/newonexd/Jchain\" target=\"_blank\" rel=\"noopener\">Jchain</a></p>\n"},{"title":"PBFT之三阶段提交","date":"2020-01-09T12:24:32.000Z","_content":"# PBFT之三阶段提交\n\n## 1 前言\n&emsp;&emsp;Raft保证当复制状态机数量为*3f+1*时, 最多可以允许*f*个状态机虚假。\n&emsp;&emsp;一个*view*中只有一个*primary* 其他为副本。\n&emsp;&emsp;视图更改说明*primary*崩溃或失败。\n\n## 2 算法流程\n1. 客户端发送请求到*primary*调用服务操作\n2. *primary*广播请求到所有节点\n3. 节点执行请求并返回响应到客户端\n4. 客户端等待从不同的节点发送的结果相同的*f+1*个响应。响应内容为操作的结果。\n\n算法对节点的要求：\n1. 节点必须是确定性的(给予一系列参数执行操作必须产生相同的结果)。\n2. 节点必须以相同的状态启动\n\n### 2.1 客户端*c*\n&emsp;&emsp;客户端通过发送消息 `<REQUEST,o,t,c>` 到*primary*请求状态机执行操作*o*。\n&emsp;&emsp;*t*:时间戳用于确保该操作只执行一次,并且所有的请求都按照时间戳先后排序。\n&emsp;&emsp;由节点发送到客户端的消息包括(当前视图号*v*，允许客户端去跟踪视图发现当前的*primary*).\n\n&emsp;&emsp;节点直接发送响应到客户端，响应内容包括`<REPLY,v,t,c,i,r>`.\n*v*:当前视图号。\n*t*:响应请求的时间戳。\n*i*:节点ID\n*r*:执行操作得到的结果。\n\n* 客户端等待由不同的节点返回的具有相同的*t*和*r*的响应消息。\n* 如果客户端没有接收到足够的响应，将广播请求到所有节点。如果请求已经被处理，节点只简单地重新发送响应。\n* 节点保留发送到每一个客户端的最新的响应消息。\n* 否则，如果节点不是*primary*，将会重定向请求到*primary*,如果*primary*没有多播请求到集群，将会被怀疑是错误节点。如果有足够多的节点怀疑则会发生视图更新。\n\n\n## 3 正常情况下三阶段提交\n&emsp;&emsp;每一个节点的状态包括服务的状态。消息日志包括节点被接受的信息，以及节点当前的视图。\n&emsp;&emsp;当*primary*接受到客户端的请求*m*，将开始三个阶段的协议进行自动多播请求到节点。\n&emsp;&emsp;除非消息的数量超出协议中给定的最大消息数量否则*primary*立即开始该三阶段协议。如果消息超过最大消息数，将会将请求放置缓冲区。\n\n&emsp;&emsp;三阶段分为`pre-prepare,prepare,commit`。\n* `pre-prepare`和`prepare`阶段用于对在同一视图中发送的请求完全排序，即使提出请求排序的*primary*为虚假节点也是如此。\n* `prepare`和`commit`阶段用于确保在视图之间对提交的请求进行完全排序\n\n### 3.1 PRE-PREPARE阶段\n&emsp;&emsp;在`pre-prepare`阶段，*primary*定义了一个序列号*n*，到请求消息中。多播一个`pre-prepare`消息并联合消息*m*到所有节点。并将该消息添加到日志中。该消息内容为 `<<PRE-PREPARE,v,n,d>_s,m>` (`_s`代表签名)这里的*v*表明被发送的消息处于的视图。*m*是客户端的请求消息。*d*为*m*的摘要。\n&emsp;&emsp;为了保持消息较小。请求没有包括在`pre-prepare`消息中。这是很重要的因为`pre-prepare`消息用于作为该请求定义的序列号*n*在视图*v*中的证明。另外，它将协议与协议完全分离，以将请求传输到节点；允许我们为协议消息使用针对小消息优化的传输，对于大型请求针对大消息使用优化的传输。\n&emsp;&emsp;节点接收到提供的*pre-prepare*消息后:\n\n* 请求中的签名和`pre-prepare`消息是有效的，并且*d*是*m*的摘要。\n* 视图*v*是有效的。\n* 在视图*v*中没有接收到其他的具有序列号*n*的包含不同摘要的消息。\n* `pre-prepare`消息中的序列号在低的阈值*h*与高阈值*H*之间。\n\n&emsp;&emsp;最后一个条件用于阻止错误的*primary*为了耗尽序列号空间而选择一个非常大的值。\n\n&emsp;&emsp;如果节点*i*接受了 `<<PRE-PREPARE,v,n,d>_s,m>` 消息。节点将会进入`prepare`阶段，并多播 `<PREPARE,v,n,d,i>_s` 消息到所有其他的节点，并将该消息添加到它的日志中。否则将什么也不做。\n\n### 3.2 PREPARE阶段\n&emsp;&emsp;节点(包括*primary*)接收了`prepare`消息：\n\n* 签名是有效的\n* 视图号与节点当前视图相同\n* 序列号在*h*与*H*之间\n\n&emsp;&emsp;并添加他们到自己的日志中。\n\n&emsp;&emsp;只有当节点*i*已将以下消息添加到它的日志：\n\n* 请求*m*\n* 在视图*v*中具有序列号*n*且是请求*m*的`pre-prepare`消息(来自不同节点*2f*个)\n\n&emsp;&emsp;并且节点通过检查`prepare`消息与`pre-prepare`消息具有相同的视图，序列号和签名,才认为`prepared (m,v,n,i)` 消息为有效的。\n\n&emsp;&emsp;算法的`pre-prepare`和`prepare`阶段保证诚实节点同意视图中请求的总顺序。更准确的，确保以下的变量:\n\n* 对于任意的诚实节点*j*(包括*i=j*),如果`prepared (m,v,n,i)` 消息是有效的，那么`prepared (m’,v,n,j)` 消息是无效的。并且任何*D(m')* 不等于*D(m)*.\n* 因为`prepared (m,v,n,i)` 消息和 *R=3f+1*表明至少有*f+1*个诚实节点在视图*v*中发送了序列号为*n*的`pre-prepare`消息或者是`prepare`消息。\n* 因此，对于`prepared (m’,v,n,j)` 消息如果是有效的，那么需要至少一个诚实节点必须发送两个冲突的`prepare`消息(或者是视图为*v*的`primary`发送`pre-prepare`消息)，两个`prepare`消息具有相同的视图和序列号但是具有不同的摘要信息。但是这是不可能的因为节点不是虚假节点。\n* 最后，关于消息摘要强度的假设可确保*m*不等于 *m'* 并且 *D(m)* 等于 *D(m')* 是不可能的。\n\n### 3.3 COMMIT阶段\n&emsp;&emsp;当`prepared (m,v,n,i)` 消息为有效的那么节点*i*多播 `<COMMIT,v,n,D(m),i>_s` 消息到其他节点.这个过程为`commit`阶段。节点接收`commit`消息并添加该信息到日志中。\n\n* 签名是有效的\n* 消息中的视图号等于节点当前视图号\n* 序列号在*h*与*H*之间\n\n&emsp;&emsp;如果并且只有当对于所有在*f+1*诚实节点中的节点*i*，`prepared (m,v,n,i)` 消息都是有效的，那么`committed (m,v,n,i)` 消息则是有效的。\n&emsp;&emsp;如果并且只有当节点*i*从不同的节点接收到*2f+1*个`commit`消息(可能包括自己)，并且与请求*m*的`pre-prepare`消息匹配(具有相同的视图，序列号和摘要)。则`committed-local (m,v,n,i)` 消息是有效的。\n\n`commit`阶段确保以下变量:\n\n* 如果对于一些诚实节点*i*，`committed-local (m,v,n,i)` 消息是有效的。那么`committed(m,v,n)` 消息是有效的。\n* 诚实节点同意本地提交的请求的序列号，即使它们在每个节点上以不同的视图提交，进一步，在诚实节点上本地提交的任何请求最终都将在1个或多个诚实节点上提交。\n\n&emsp;&emsp;每一个节点*i*在当`committed-local(m,v,n,i)` 消息是有效的，并且*i*的状态反应了在所有请求中该请求的序列号是最小的情况下将会执行该操作。确保了所有诚实节点可以以相同的顺序执行请求，保证了安全性。在执行完请求操作后，节点将返回一个响应到客户端。\n&emsp;&emsp;当请求的时间戳小于最后一次回复的时间戳时节点抛弃该请求。保证只执行一次。\n\n&emsp;&emsp;不依赖消息顺序交付。因此可能节点乱序提交请求。这是无所谓的，因为节点保持了`pre-prepare`,`prepare`,和`commit`消息日志一直到该请求被执行。\n\n图展示了该算法的以一种正常的例子(没有*primary*虚假)的操作。节点0为*primary*，节点3为虚假节点。*C*为客户端.\n![图](/img/blog/pbft/1.png)","source":"_posts/blog/consensus/pbft-three_phase.md","raw":"---\ntitle: PBFT之三阶段提交\ndate: 2020-01-09 20:24:32\ntags: \n- Pbft\n- algorithm\ncategories:\n- algorithm\n---\n# PBFT之三阶段提交\n\n## 1 前言\n&emsp;&emsp;Raft保证当复制状态机数量为*3f+1*时, 最多可以允许*f*个状态机虚假。\n&emsp;&emsp;一个*view*中只有一个*primary* 其他为副本。\n&emsp;&emsp;视图更改说明*primary*崩溃或失败。\n\n## 2 算法流程\n1. 客户端发送请求到*primary*调用服务操作\n2. *primary*广播请求到所有节点\n3. 节点执行请求并返回响应到客户端\n4. 客户端等待从不同的节点发送的结果相同的*f+1*个响应。响应内容为操作的结果。\n\n算法对节点的要求：\n1. 节点必须是确定性的(给予一系列参数执行操作必须产生相同的结果)。\n2. 节点必须以相同的状态启动\n\n### 2.1 客户端*c*\n&emsp;&emsp;客户端通过发送消息 `<REQUEST,o,t,c>` 到*primary*请求状态机执行操作*o*。\n&emsp;&emsp;*t*:时间戳用于确保该操作只执行一次,并且所有的请求都按照时间戳先后排序。\n&emsp;&emsp;由节点发送到客户端的消息包括(当前视图号*v*，允许客户端去跟踪视图发现当前的*primary*).\n\n&emsp;&emsp;节点直接发送响应到客户端，响应内容包括`<REPLY,v,t,c,i,r>`.\n*v*:当前视图号。\n*t*:响应请求的时间戳。\n*i*:节点ID\n*r*:执行操作得到的结果。\n\n* 客户端等待由不同的节点返回的具有相同的*t*和*r*的响应消息。\n* 如果客户端没有接收到足够的响应，将广播请求到所有节点。如果请求已经被处理，节点只简单地重新发送响应。\n* 节点保留发送到每一个客户端的最新的响应消息。\n* 否则，如果节点不是*primary*，将会重定向请求到*primary*,如果*primary*没有多播请求到集群，将会被怀疑是错误节点。如果有足够多的节点怀疑则会发生视图更新。\n\n\n## 3 正常情况下三阶段提交\n&emsp;&emsp;每一个节点的状态包括服务的状态。消息日志包括节点被接受的信息，以及节点当前的视图。\n&emsp;&emsp;当*primary*接受到客户端的请求*m*，将开始三个阶段的协议进行自动多播请求到节点。\n&emsp;&emsp;除非消息的数量超出协议中给定的最大消息数量否则*primary*立即开始该三阶段协议。如果消息超过最大消息数，将会将请求放置缓冲区。\n\n&emsp;&emsp;三阶段分为`pre-prepare,prepare,commit`。\n* `pre-prepare`和`prepare`阶段用于对在同一视图中发送的请求完全排序，即使提出请求排序的*primary*为虚假节点也是如此。\n* `prepare`和`commit`阶段用于确保在视图之间对提交的请求进行完全排序\n\n### 3.1 PRE-PREPARE阶段\n&emsp;&emsp;在`pre-prepare`阶段，*primary*定义了一个序列号*n*，到请求消息中。多播一个`pre-prepare`消息并联合消息*m*到所有节点。并将该消息添加到日志中。该消息内容为 `<<PRE-PREPARE,v,n,d>_s,m>` (`_s`代表签名)这里的*v*表明被发送的消息处于的视图。*m*是客户端的请求消息。*d*为*m*的摘要。\n&emsp;&emsp;为了保持消息较小。请求没有包括在`pre-prepare`消息中。这是很重要的因为`pre-prepare`消息用于作为该请求定义的序列号*n*在视图*v*中的证明。另外，它将协议与协议完全分离，以将请求传输到节点；允许我们为协议消息使用针对小消息优化的传输，对于大型请求针对大消息使用优化的传输。\n&emsp;&emsp;节点接收到提供的*pre-prepare*消息后:\n\n* 请求中的签名和`pre-prepare`消息是有效的，并且*d*是*m*的摘要。\n* 视图*v*是有效的。\n* 在视图*v*中没有接收到其他的具有序列号*n*的包含不同摘要的消息。\n* `pre-prepare`消息中的序列号在低的阈值*h*与高阈值*H*之间。\n\n&emsp;&emsp;最后一个条件用于阻止错误的*primary*为了耗尽序列号空间而选择一个非常大的值。\n\n&emsp;&emsp;如果节点*i*接受了 `<<PRE-PREPARE,v,n,d>_s,m>` 消息。节点将会进入`prepare`阶段，并多播 `<PREPARE,v,n,d,i>_s` 消息到所有其他的节点，并将该消息添加到它的日志中。否则将什么也不做。\n\n### 3.2 PREPARE阶段\n&emsp;&emsp;节点(包括*primary*)接收了`prepare`消息：\n\n* 签名是有效的\n* 视图号与节点当前视图相同\n* 序列号在*h*与*H*之间\n\n&emsp;&emsp;并添加他们到自己的日志中。\n\n&emsp;&emsp;只有当节点*i*已将以下消息添加到它的日志：\n\n* 请求*m*\n* 在视图*v*中具有序列号*n*且是请求*m*的`pre-prepare`消息(来自不同节点*2f*个)\n\n&emsp;&emsp;并且节点通过检查`prepare`消息与`pre-prepare`消息具有相同的视图，序列号和签名,才认为`prepared (m,v,n,i)` 消息为有效的。\n\n&emsp;&emsp;算法的`pre-prepare`和`prepare`阶段保证诚实节点同意视图中请求的总顺序。更准确的，确保以下的变量:\n\n* 对于任意的诚实节点*j*(包括*i=j*),如果`prepared (m,v,n,i)` 消息是有效的，那么`prepared (m’,v,n,j)` 消息是无效的。并且任何*D(m')* 不等于*D(m)*.\n* 因为`prepared (m,v,n,i)` 消息和 *R=3f+1*表明至少有*f+1*个诚实节点在视图*v*中发送了序列号为*n*的`pre-prepare`消息或者是`prepare`消息。\n* 因此，对于`prepared (m’,v,n,j)` 消息如果是有效的，那么需要至少一个诚实节点必须发送两个冲突的`prepare`消息(或者是视图为*v*的`primary`发送`pre-prepare`消息)，两个`prepare`消息具有相同的视图和序列号但是具有不同的摘要信息。但是这是不可能的因为节点不是虚假节点。\n* 最后，关于消息摘要强度的假设可确保*m*不等于 *m'* 并且 *D(m)* 等于 *D(m')* 是不可能的。\n\n### 3.3 COMMIT阶段\n&emsp;&emsp;当`prepared (m,v,n,i)` 消息为有效的那么节点*i*多播 `<COMMIT,v,n,D(m),i>_s` 消息到其他节点.这个过程为`commit`阶段。节点接收`commit`消息并添加该信息到日志中。\n\n* 签名是有效的\n* 消息中的视图号等于节点当前视图号\n* 序列号在*h*与*H*之间\n\n&emsp;&emsp;如果并且只有当对于所有在*f+1*诚实节点中的节点*i*，`prepared (m,v,n,i)` 消息都是有效的，那么`committed (m,v,n,i)` 消息则是有效的。\n&emsp;&emsp;如果并且只有当节点*i*从不同的节点接收到*2f+1*个`commit`消息(可能包括自己)，并且与请求*m*的`pre-prepare`消息匹配(具有相同的视图，序列号和摘要)。则`committed-local (m,v,n,i)` 消息是有效的。\n\n`commit`阶段确保以下变量:\n\n* 如果对于一些诚实节点*i*，`committed-local (m,v,n,i)` 消息是有效的。那么`committed(m,v,n)` 消息是有效的。\n* 诚实节点同意本地提交的请求的序列号，即使它们在每个节点上以不同的视图提交，进一步，在诚实节点上本地提交的任何请求最终都将在1个或多个诚实节点上提交。\n\n&emsp;&emsp;每一个节点*i*在当`committed-local(m,v,n,i)` 消息是有效的，并且*i*的状态反应了在所有请求中该请求的序列号是最小的情况下将会执行该操作。确保了所有诚实节点可以以相同的顺序执行请求，保证了安全性。在执行完请求操作后，节点将返回一个响应到客户端。\n&emsp;&emsp;当请求的时间戳小于最后一次回复的时间戳时节点抛弃该请求。保证只执行一次。\n\n&emsp;&emsp;不依赖消息顺序交付。因此可能节点乱序提交请求。这是无所谓的，因为节点保持了`pre-prepare`,`prepare`,和`commit`消息日志一直到该请求被执行。\n\n图展示了该算法的以一种正常的例子(没有*primary*虚假)的操作。节点0为*primary*，节点3为虚假节点。*C*为客户端.\n![图](/img/blog/pbft/1.png)","slug":"blog/consensus/pbft-three_phase","published":1,"updated":"2020-05-11T03:44:37.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyfc0009k0vqb1vmdp2n","content":"<h1 id=\"PBFT之三阶段提交\"><a href=\"#PBFT之三阶段提交\" class=\"headerlink\" title=\"PBFT之三阶段提交\"></a>PBFT之三阶段提交</h1><h2 id=\"1-前言\"><a href=\"#1-前言\" class=\"headerlink\" title=\"1 前言\"></a>1 前言</h2><p>&emsp;&emsp;Raft保证当复制状态机数量为<em>3f+1<em>时, 最多可以允许</em>f<em>个状态机虚假。<br>&emsp;&emsp;一个</em>view<em>中只有一个</em>primary</em> 其他为副本。<br>&emsp;&emsp;视图更改说明<em>primary</em>崩溃或失败。</p>\n<h2 id=\"2-算法流程\"><a href=\"#2-算法流程\" class=\"headerlink\" title=\"2 算法流程\"></a>2 算法流程</h2><ol>\n<li>客户端发送请求到<em>primary</em>调用服务操作</li>\n<li><em>primary</em>广播请求到所有节点</li>\n<li>节点执行请求并返回响应到客户端</li>\n<li>客户端等待从不同的节点发送的结果相同的<em>f+1</em>个响应。响应内容为操作的结果。</li>\n</ol>\n<p>算法对节点的要求：</p>\n<ol>\n<li>节点必须是确定性的(给予一系列参数执行操作必须产生相同的结果)。</li>\n<li>节点必须以相同的状态启动</li>\n</ol>\n<h3 id=\"2-1-客户端c\"><a href=\"#2-1-客户端c\" class=\"headerlink\" title=\"2.1 客户端c\"></a>2.1 客户端<em>c</em></h3><p>&emsp;&emsp;客户端通过发送消息 <code>&lt;REQUEST,o,t,c&gt;</code> 到<em>primary<em>请求状态机执行操作</em>o<em>。<br>&emsp;&emsp;</em>t</em>:时间戳用于确保该操作只执行一次,并且所有的请求都按照时间戳先后排序。<br>&emsp;&emsp;由节点发送到客户端的消息包括(当前视图号<em>v</em>，允许客户端去跟踪视图发现当前的<em>primary</em>).</p>\n<p>&emsp;&emsp;节点直接发送响应到客户端，响应内容包括<code>&lt;REPLY,v,t,c,i,r&gt;</code>.<br><em>v</em>:当前视图号。<br><em>t</em>:响应请求的时间戳。<br><em>i</em>:节点ID<br><em>r</em>:执行操作得到的结果。</p>\n<ul>\n<li>客户端等待由不同的节点返回的具有相同的<em>t</em>和<em>r</em>的响应消息。</li>\n<li>如果客户端没有接收到足够的响应，将广播请求到所有节点。如果请求已经被处理，节点只简单地重新发送响应。</li>\n<li>节点保留发送到每一个客户端的最新的响应消息。</li>\n<li>否则，如果节点不是<em>primary<em>，将会重定向请求到</em>primary</em>,如果<em>primary</em>没有多播请求到集群，将会被怀疑是错误节点。如果有足够多的节点怀疑则会发生视图更新。</li>\n</ul>\n<h2 id=\"3-正常情况下三阶段提交\"><a href=\"#3-正常情况下三阶段提交\" class=\"headerlink\" title=\"3 正常情况下三阶段提交\"></a>3 正常情况下三阶段提交</h2><p>&emsp;&emsp;每一个节点的状态包括服务的状态。消息日志包括节点被接受的信息，以及节点当前的视图。<br>&emsp;&emsp;当<em>primary</em>接受到客户端的请求<em>m</em>，将开始三个阶段的协议进行自动多播请求到节点。<br>&emsp;&emsp;除非消息的数量超出协议中给定的最大消息数量否则<em>primary</em>立即开始该三阶段协议。如果消息超过最大消息数，将会将请求放置缓冲区。</p>\n<p>&emsp;&emsp;三阶段分为<code>pre-prepare,prepare,commit</code>。</p>\n<ul>\n<li><code>pre-prepare</code>和<code>prepare</code>阶段用于对在同一视图中发送的请求完全排序，即使提出请求排序的<em>primary</em>为虚假节点也是如此。</li>\n<li><code>prepare</code>和<code>commit</code>阶段用于确保在视图之间对提交的请求进行完全排序</li>\n</ul>\n<h3 id=\"3-1-PRE-PREPARE阶段\"><a href=\"#3-1-PRE-PREPARE阶段\" class=\"headerlink\" title=\"3.1 PRE-PREPARE阶段\"></a>3.1 PRE-PREPARE阶段</h3><p>&emsp;&emsp;在<code>pre-prepare</code>阶段，<em>primary</em>定义了一个序列号<em>n</em>，到请求消息中。多播一个<code>pre-prepare</code>消息并联合消息<em>m</em>到所有节点。并将该消息添加到日志中。该消息内容为 <code>&lt;&lt;PRE-PREPARE,v,n,d&gt;_s,m&gt;</code> (<code>_s</code>代表签名)这里的<em>v</em>表明被发送的消息处于的视图。<em>m</em>是客户端的请求消息。<em>d</em>为<em>m</em>的摘要。<br>&emsp;&emsp;为了保持消息较小。请求没有包括在<code>pre-prepare</code>消息中。这是很重要的因为<code>pre-prepare</code>消息用于作为该请求定义的序列号<em>n</em>在视图<em>v</em>中的证明。另外，它将协议与协议完全分离，以将请求传输到节点；允许我们为协议消息使用针对小消息优化的传输，对于大型请求针对大消息使用优化的传输。<br>&emsp;&emsp;节点接收到提供的<em>pre-prepare</em>消息后:</p>\n<ul>\n<li>请求中的签名和<code>pre-prepare</code>消息是有效的，并且<em>d</em>是<em>m</em>的摘要。</li>\n<li>视图<em>v</em>是有效的。</li>\n<li>在视图<em>v</em>中没有接收到其他的具有序列号<em>n</em>的包含不同摘要的消息。</li>\n<li><code>pre-prepare</code>消息中的序列号在低的阈值<em>h</em>与高阈值<em>H</em>之间。</li>\n</ul>\n<p>&emsp;&emsp;最后一个条件用于阻止错误的<em>primary</em>为了耗尽序列号空间而选择一个非常大的值。</p>\n<p>&emsp;&emsp;如果节点<em>i</em>接受了 <code>&lt;&lt;PRE-PREPARE,v,n,d&gt;_s,m&gt;</code> 消息。节点将会进入<code>prepare</code>阶段，并多播 <code>&lt;PREPARE,v,n,d,i&gt;_s</code> 消息到所有其他的节点，并将该消息添加到它的日志中。否则将什么也不做。</p>\n<h3 id=\"3-2-PREPARE阶段\"><a href=\"#3-2-PREPARE阶段\" class=\"headerlink\" title=\"3.2 PREPARE阶段\"></a>3.2 PREPARE阶段</h3><p>&emsp;&emsp;节点(包括<em>primary</em>)接收了<code>prepare</code>消息：</p>\n<ul>\n<li>签名是有效的</li>\n<li>视图号与节点当前视图相同</li>\n<li>序列号在<em>h</em>与<em>H</em>之间</li>\n</ul>\n<p>&emsp;&emsp;并添加他们到自己的日志中。</p>\n<p>&emsp;&emsp;只有当节点<em>i</em>已将以下消息添加到它的日志：</p>\n<ul>\n<li>请求<em>m</em></li>\n<li>在视图<em>v</em>中具有序列号<em>n</em>且是请求<em>m</em>的<code>pre-prepare</code>消息(来自不同节点<em>2f</em>个)</li>\n</ul>\n<p>&emsp;&emsp;并且节点通过检查<code>prepare</code>消息与<code>pre-prepare</code>消息具有相同的视图，序列号和签名,才认为<code>prepared (m,v,n,i)</code> 消息为有效的。</p>\n<p>&emsp;&emsp;算法的<code>pre-prepare</code>和<code>prepare</code>阶段保证诚实节点同意视图中请求的总顺序。更准确的，确保以下的变量:</p>\n<ul>\n<li>对于任意的诚实节点<em>j</em>(包括<em>i=j</em>),如果<code>prepared (m,v,n,i)</code> 消息是有效的，那么<code>prepared (m’,v,n,j)</code> 消息是无效的。并且任何<em>D(m’)</em> 不等于<em>D(m)</em>.</li>\n<li>因为<code>prepared (m,v,n,i)</code> 消息和 <em>R=3f+1</em>表明至少有<em>f+1</em>个诚实节点在视图<em>v</em>中发送了序列号为<em>n</em>的<code>pre-prepare</code>消息或者是<code>prepare</code>消息。</li>\n<li>因此，对于<code>prepared (m’,v,n,j)</code> 消息如果是有效的，那么需要至少一个诚实节点必须发送两个冲突的<code>prepare</code>消息(或者是视图为<em>v</em>的<code>primary</code>发送<code>pre-prepare</code>消息)，两个<code>prepare</code>消息具有相同的视图和序列号但是具有不同的摘要信息。但是这是不可能的因为节点不是虚假节点。</li>\n<li>最后，关于消息摘要强度的假设可确保<em>m</em>不等于 <em>m’</em> 并且 <em>D(m)</em> 等于 <em>D(m’)</em> 是不可能的。</li>\n</ul>\n<h3 id=\"3-3-COMMIT阶段\"><a href=\"#3-3-COMMIT阶段\" class=\"headerlink\" title=\"3.3 COMMIT阶段\"></a>3.3 COMMIT阶段</h3><p>&emsp;&emsp;当<code>prepared (m,v,n,i)</code> 消息为有效的那么节点<em>i</em>多播 <code>&lt;COMMIT,v,n,D(m),i&gt;_s</code> 消息到其他节点.这个过程为<code>commit</code>阶段。节点接收<code>commit</code>消息并添加该信息到日志中。</p>\n<ul>\n<li>签名是有效的</li>\n<li>消息中的视图号等于节点当前视图号</li>\n<li>序列号在<em>h</em>与<em>H</em>之间</li>\n</ul>\n<p>&emsp;&emsp;如果并且只有当对于所有在<em>f+1</em>诚实节点中的节点<em>i</em>，<code>prepared (m,v,n,i)</code> 消息都是有效的，那么<code>committed (m,v,n,i)</code> 消息则是有效的。<br>&emsp;&emsp;如果并且只有当节点<em>i</em>从不同的节点接收到<em>2f+1</em>个<code>commit</code>消息(可能包括自己)，并且与请求<em>m</em>的<code>pre-prepare</code>消息匹配(具有相同的视图，序列号和摘要)。则<code>committed-local (m,v,n,i)</code> 消息是有效的。</p>\n<p><code>commit</code>阶段确保以下变量:</p>\n<ul>\n<li>如果对于一些诚实节点<em>i</em>，<code>committed-local (m,v,n,i)</code> 消息是有效的。那么<code>committed(m,v,n)</code> 消息是有效的。</li>\n<li>诚实节点同意本地提交的请求的序列号，即使它们在每个节点上以不同的视图提交，进一步，在诚实节点上本地提交的任何请求最终都将在1个或多个诚实节点上提交。</li>\n</ul>\n<p>&emsp;&emsp;每一个节点<em>i</em>在当<code>committed-local(m,v,n,i)</code> 消息是有效的，并且<em>i</em>的状态反应了在所有请求中该请求的序列号是最小的情况下将会执行该操作。确保了所有诚实节点可以以相同的顺序执行请求，保证了安全性。在执行完请求操作后，节点将返回一个响应到客户端。<br>&emsp;&emsp;当请求的时间戳小于最后一次回复的时间戳时节点抛弃该请求。保证只执行一次。</p>\n<p>&emsp;&emsp;不依赖消息顺序交付。因此可能节点乱序提交请求。这是无所谓的，因为节点保持了<code>pre-prepare</code>,<code>prepare</code>,和<code>commit</code>消息日志一直到该请求被执行。</p>\n<p>图展示了该算法的以一种正常的例子(没有<em>primary</em>虚假)的操作。节点0为<em>primary</em>，节点3为虚假节点。<em>C</em>为客户端.<br><img src=\"/img/blog/pbft/1.png\" srcset=\"undefined\" alt=\"图\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"PBFT之三阶段提交\"><a href=\"#PBFT之三阶段提交\" class=\"headerlink\" title=\"PBFT之三阶段提交\"></a>PBFT之三阶段提交</h1><h2 id=\"1-前言\"><a href=\"#1-前言\" class=\"headerlink\" title=\"1 前言\"></a>1 前言</h2><p>&emsp;&emsp;Raft保证当复制状态机数量为<em>3f+1<em>时, 最多可以允许</em>f<em>个状态机虚假。<br>&emsp;&emsp;一个</em>view<em>中只有一个</em>primary</em> 其他为副本。<br>&emsp;&emsp;视图更改说明<em>primary</em>崩溃或失败。</p>\n<h2 id=\"2-算法流程\"><a href=\"#2-算法流程\" class=\"headerlink\" title=\"2 算法流程\"></a>2 算法流程</h2><ol>\n<li>客户端发送请求到<em>primary</em>调用服务操作</li>\n<li><em>primary</em>广播请求到所有节点</li>\n<li>节点执行请求并返回响应到客户端</li>\n<li>客户端等待从不同的节点发送的结果相同的<em>f+1</em>个响应。响应内容为操作的结果。</li>\n</ol>\n<p>算法对节点的要求：</p>\n<ol>\n<li>节点必须是确定性的(给予一系列参数执行操作必须产生相同的结果)。</li>\n<li>节点必须以相同的状态启动</li>\n</ol>\n<h3 id=\"2-1-客户端c\"><a href=\"#2-1-客户端c\" class=\"headerlink\" title=\"2.1 客户端c\"></a>2.1 客户端<em>c</em></h3><p>&emsp;&emsp;客户端通过发送消息 <code>&lt;REQUEST,o,t,c&gt;</code> 到<em>primary<em>请求状态机执行操作</em>o<em>。<br>&emsp;&emsp;</em>t</em>:时间戳用于确保该操作只执行一次,并且所有的请求都按照时间戳先后排序。<br>&emsp;&emsp;由节点发送到客户端的消息包括(当前视图号<em>v</em>，允许客户端去跟踪视图发现当前的<em>primary</em>).</p>\n<p>&emsp;&emsp;节点直接发送响应到客户端，响应内容包括<code>&lt;REPLY,v,t,c,i,r&gt;</code>.<br><em>v</em>:当前视图号。<br><em>t</em>:响应请求的时间戳。<br><em>i</em>:节点ID<br><em>r</em>:执行操作得到的结果。</p>\n<ul>\n<li>客户端等待由不同的节点返回的具有相同的<em>t</em>和<em>r</em>的响应消息。</li>\n<li>如果客户端没有接收到足够的响应，将广播请求到所有节点。如果请求已经被处理，节点只简单地重新发送响应。</li>\n<li>节点保留发送到每一个客户端的最新的响应消息。</li>\n<li>否则，如果节点不是<em>primary<em>，将会重定向请求到</em>primary</em>,如果<em>primary</em>没有多播请求到集群，将会被怀疑是错误节点。如果有足够多的节点怀疑则会发生视图更新。</li>\n</ul>\n<h2 id=\"3-正常情况下三阶段提交\"><a href=\"#3-正常情况下三阶段提交\" class=\"headerlink\" title=\"3 正常情况下三阶段提交\"></a>3 正常情况下三阶段提交</h2><p>&emsp;&emsp;每一个节点的状态包括服务的状态。消息日志包括节点被接受的信息，以及节点当前的视图。<br>&emsp;&emsp;当<em>primary</em>接受到客户端的请求<em>m</em>，将开始三个阶段的协议进行自动多播请求到节点。<br>&emsp;&emsp;除非消息的数量超出协议中给定的最大消息数量否则<em>primary</em>立即开始该三阶段协议。如果消息超过最大消息数，将会将请求放置缓冲区。</p>\n<p>&emsp;&emsp;三阶段分为<code>pre-prepare,prepare,commit</code>。</p>\n<ul>\n<li><code>pre-prepare</code>和<code>prepare</code>阶段用于对在同一视图中发送的请求完全排序，即使提出请求排序的<em>primary</em>为虚假节点也是如此。</li>\n<li><code>prepare</code>和<code>commit</code>阶段用于确保在视图之间对提交的请求进行完全排序</li>\n</ul>\n<h3 id=\"3-1-PRE-PREPARE阶段\"><a href=\"#3-1-PRE-PREPARE阶段\" class=\"headerlink\" title=\"3.1 PRE-PREPARE阶段\"></a>3.1 PRE-PREPARE阶段</h3><p>&emsp;&emsp;在<code>pre-prepare</code>阶段，<em>primary</em>定义了一个序列号<em>n</em>，到请求消息中。多播一个<code>pre-prepare</code>消息并联合消息<em>m</em>到所有节点。并将该消息添加到日志中。该消息内容为 <code>&lt;&lt;PRE-PREPARE,v,n,d&gt;_s,m&gt;</code> (<code>_s</code>代表签名)这里的<em>v</em>表明被发送的消息处于的视图。<em>m</em>是客户端的请求消息。<em>d</em>为<em>m</em>的摘要。<br>&emsp;&emsp;为了保持消息较小。请求没有包括在<code>pre-prepare</code>消息中。这是很重要的因为<code>pre-prepare</code>消息用于作为该请求定义的序列号<em>n</em>在视图<em>v</em>中的证明。另外，它将协议与协议完全分离，以将请求传输到节点；允许我们为协议消息使用针对小消息优化的传输，对于大型请求针对大消息使用优化的传输。<br>&emsp;&emsp;节点接收到提供的<em>pre-prepare</em>消息后:</p>\n<ul>\n<li>请求中的签名和<code>pre-prepare</code>消息是有效的，并且<em>d</em>是<em>m</em>的摘要。</li>\n<li>视图<em>v</em>是有效的。</li>\n<li>在视图<em>v</em>中没有接收到其他的具有序列号<em>n</em>的包含不同摘要的消息。</li>\n<li><code>pre-prepare</code>消息中的序列号在低的阈值<em>h</em>与高阈值<em>H</em>之间。</li>\n</ul>\n<p>&emsp;&emsp;最后一个条件用于阻止错误的<em>primary</em>为了耗尽序列号空间而选择一个非常大的值。</p>\n<p>&emsp;&emsp;如果节点<em>i</em>接受了 <code>&lt;&lt;PRE-PREPARE,v,n,d&gt;_s,m&gt;</code> 消息。节点将会进入<code>prepare</code>阶段，并多播 <code>&lt;PREPARE,v,n,d,i&gt;_s</code> 消息到所有其他的节点，并将该消息添加到它的日志中。否则将什么也不做。</p>\n<h3 id=\"3-2-PREPARE阶段\"><a href=\"#3-2-PREPARE阶段\" class=\"headerlink\" title=\"3.2 PREPARE阶段\"></a>3.2 PREPARE阶段</h3><p>&emsp;&emsp;节点(包括<em>primary</em>)接收了<code>prepare</code>消息：</p>\n<ul>\n<li>签名是有效的</li>\n<li>视图号与节点当前视图相同</li>\n<li>序列号在<em>h</em>与<em>H</em>之间</li>\n</ul>\n<p>&emsp;&emsp;并添加他们到自己的日志中。</p>\n<p>&emsp;&emsp;只有当节点<em>i</em>已将以下消息添加到它的日志：</p>\n<ul>\n<li>请求<em>m</em></li>\n<li>在视图<em>v</em>中具有序列号<em>n</em>且是请求<em>m</em>的<code>pre-prepare</code>消息(来自不同节点<em>2f</em>个)</li>\n</ul>\n<p>&emsp;&emsp;并且节点通过检查<code>prepare</code>消息与<code>pre-prepare</code>消息具有相同的视图，序列号和签名,才认为<code>prepared (m,v,n,i)</code> 消息为有效的。</p>\n<p>&emsp;&emsp;算法的<code>pre-prepare</code>和<code>prepare</code>阶段保证诚实节点同意视图中请求的总顺序。更准确的，确保以下的变量:</p>\n<ul>\n<li>对于任意的诚实节点<em>j</em>(包括<em>i=j</em>),如果<code>prepared (m,v,n,i)</code> 消息是有效的，那么<code>prepared (m’,v,n,j)</code> 消息是无效的。并且任何<em>D(m’)</em> 不等于<em>D(m)</em>.</li>\n<li>因为<code>prepared (m,v,n,i)</code> 消息和 <em>R=3f+1</em>表明至少有<em>f+1</em>个诚实节点在视图<em>v</em>中发送了序列号为<em>n</em>的<code>pre-prepare</code>消息或者是<code>prepare</code>消息。</li>\n<li>因此，对于<code>prepared (m’,v,n,j)</code> 消息如果是有效的，那么需要至少一个诚实节点必须发送两个冲突的<code>prepare</code>消息(或者是视图为<em>v</em>的<code>primary</code>发送<code>pre-prepare</code>消息)，两个<code>prepare</code>消息具有相同的视图和序列号但是具有不同的摘要信息。但是这是不可能的因为节点不是虚假节点。</li>\n<li>最后，关于消息摘要强度的假设可确保<em>m</em>不等于 <em>m’</em> 并且 <em>D(m)</em> 等于 <em>D(m’)</em> 是不可能的。</li>\n</ul>\n<h3 id=\"3-3-COMMIT阶段\"><a href=\"#3-3-COMMIT阶段\" class=\"headerlink\" title=\"3.3 COMMIT阶段\"></a>3.3 COMMIT阶段</h3><p>&emsp;&emsp;当<code>prepared (m,v,n,i)</code> 消息为有效的那么节点<em>i</em>多播 <code>&lt;COMMIT,v,n,D(m),i&gt;_s</code> 消息到其他节点.这个过程为<code>commit</code>阶段。节点接收<code>commit</code>消息并添加该信息到日志中。</p>\n<ul>\n<li>签名是有效的</li>\n<li>消息中的视图号等于节点当前视图号</li>\n<li>序列号在<em>h</em>与<em>H</em>之间</li>\n</ul>\n<p>&emsp;&emsp;如果并且只有当对于所有在<em>f+1</em>诚实节点中的节点<em>i</em>，<code>prepared (m,v,n,i)</code> 消息都是有效的，那么<code>committed (m,v,n,i)</code> 消息则是有效的。<br>&emsp;&emsp;如果并且只有当节点<em>i</em>从不同的节点接收到<em>2f+1</em>个<code>commit</code>消息(可能包括自己)，并且与请求<em>m</em>的<code>pre-prepare</code>消息匹配(具有相同的视图，序列号和摘要)。则<code>committed-local (m,v,n,i)</code> 消息是有效的。</p>\n<p><code>commit</code>阶段确保以下变量:</p>\n<ul>\n<li>如果对于一些诚实节点<em>i</em>，<code>committed-local (m,v,n,i)</code> 消息是有效的。那么<code>committed(m,v,n)</code> 消息是有效的。</li>\n<li>诚实节点同意本地提交的请求的序列号，即使它们在每个节点上以不同的视图提交，进一步，在诚实节点上本地提交的任何请求最终都将在1个或多个诚实节点上提交。</li>\n</ul>\n<p>&emsp;&emsp;每一个节点<em>i</em>在当<code>committed-local(m,v,n,i)</code> 消息是有效的，并且<em>i</em>的状态反应了在所有请求中该请求的序列号是最小的情况下将会执行该操作。确保了所有诚实节点可以以相同的顺序执行请求，保证了安全性。在执行完请求操作后，节点将返回一个响应到客户端。<br>&emsp;&emsp;当请求的时间戳小于最后一次回复的时间戳时节点抛弃该请求。保证只执行一次。</p>\n<p>&emsp;&emsp;不依赖消息顺序交付。因此可能节点乱序提交请求。这是无所谓的，因为节点保持了<code>pre-prepare</code>,<code>prepare</code>,和<code>commit</code>消息日志一直到该请求被执行。</p>\n<p>图展示了该算法的以一种正常的例子(没有<em>primary</em>虚假)的操作。节点0为<em>primary</em>，节点3为虚假节点。<em>C</em>为客户端.<br><img src=\"/img/blog/pbft/1.png\" srcset=\"undefined\" alt=\"图\"></p>\n"},{"title":"Raft算法之日志复制","date":"2020-01-05T05:45:59.000Z","_content":"上一篇文章:[Raft算法之Leader选举](https://ifican.top/2020/01/04/blog/consensus/raft-election/)\n&emsp;&emsp;之前说完了Raft算法中的Leader选举过程，本文将在上一篇文章的基础上说明日志复制。\n\n# Raft算法之日志复制\n&emsp;&emsp;先看以下日志所包含的基本内容:\n\n1. 可以被复制状态机执行的命令\n2. 任期号 :创建该日志时Leader所处的当前任期号\n3. 索引号 :整数，用于标识日志所在的位置\n\n日志的状态分为两种:未被提交，已被提交(日志为安全的，不会被删除或覆盖)。\n\n### 1 正常情况\n\n* 当`Leader`接收到由客户端发送的请求(请求中包含可以被复制状态机执行的命令)时，Leader将会把该请求作为新的内容添加到日志中(任期号为当前`Leader`所处的任期号，索引号为当前`Leader`本地存储的日志集合中的日志的最高索引号加1)。\n    * `Leader`**在当前任期内最多只能创建一个给定索引号的日志(即不可能在一个任期内创建两个以上的具有相同索引的日志条目)**\n* 然后将该日志通过`AppendEntries RPC`消息发送到网络中其他的服务器(以下简称`Follower`)，从而复制该日志。\n* 在网络中`Follower`接收到该日志消息后则会返回复制成功的回复。\n* 在`Leader`接收到网络中大部分的`Follower`的成功复制的回复之后，`Leader`便认为该日志**可以被提交**。此时`Leader`将会同时做三件事：\n\n1. 将该日志应用到`Leader`本地的复制状态机\n2. 向所有`Follower`发送消息通知所有接收到该日志的`Follower`将该日志进行提交，然后应用到各自本地的复制状态机\n3. 将执行结果通知客户端\n\n&emsp;&emsp;当该日志消息成功在网络中大部分`Follower`本地的复制状态机执行过后，则可认为该日志**已被提交**。在当前日志被提交的过程中，如果`Leader`先前的某些日志还没有被提交，则将会一同提交。\n&emsp;&emsp;而网络中有些`Follower`可能由于网络状态原因反应缓慢或者崩溃，那么`Leader`将会无限次地尝试重复发送`AppendEntries RPC`消息到该`Follower`。直到成功为止。\n\n#### 1.1 日志的一致性检查\n&emsp;&emsp;在上面，我们说了`Follower`在接收到`AppendEntries RPC`消息后则会返回复制成功的回复。实际上在接收到消息后会首先进行日志的一致性检查(正常情况下`Leader`与`Follower`的日志会保持一致，所以一致性检查不会失败),**一致性检查内容如下：**\n\n* 在`Leader`创建`AppendEntries RPC`消息时，消息中将会包含当前日志之前日志条目的任期号与索引号。\n* `Follower`在接受到`AppendEntries RPC`消息后，将会检查之前日志的任期号与索引号是否可以匹配到\n    * 如果匹配到则说明和`Leader`之前的日志是保持一致的。\n    * 如果没有匹配则会拒绝`AppendEntries RPC`消息.\n\n&emsp;&emsp;一致性检查是一个归纳的过程。**正常情况下**，网络中第一条日志一定满足日志的一致性检查，然后第二条日志中包含第一条日志的任期号与索引号，所以只要`Leader`与`Follower`的第一条日志保持一致，那么第二条日志也会满足一致性检查，从而之后的每一条日志都会满足一致性检查。\n\n&emsp;&emsp;从而得出了日志匹配属性:\n\n* 如果两个不同的日志实体具有相同的索引和任期号，那么他们存储有相同的命令。\n* 如果两个不同的日志实体具有相同的索引和任期号，则所有先前条目中的日志都相同。(由一致性检查结果得出)\n\n\n### 2 特殊情况\n&emsp;&emsp;而网络不可能一直处于正常情况。因为`Leader`或者某个`Follower`有可能会崩溃，从而导致日志不能一直保持一致。因此存在以下三种情况:\n\n1. `Follower`缺失当前`Leader`上存在的日志条目。\n2. `Follower`存在当前`Leader`不存在的日志条目。(比如旧的`Leader`仅仅将`AppendEntries RPC`消息发送到一部分`Follower`就崩溃掉，然后新的当选`Leader`的服务器恰好是没有收到该`AppendEntries RPC`消息的服务器)\n3. 或者`Follower`即缺失当前`Leader`上存在的日志条目，也存在当前`Leader`不存在的日志条目\n\n ![图1](/img/blog/raft/7.png)\n\n&emsp;&emsp;图中最上方是日志的索引号(1-12),每个方块代表一条日志信息，方块内数字代表该日志所处的任期号。图中当前`Leader`(图中最上方一行日志代表当前`Leader`日志)处于任期号为8的时刻。以此图说明以上三种情况存在的原因：\n\n* `Follower` *a,b*(`Follower`崩溃没有接收到`Leader`发送的`AppendEntries RPC`消息)满足以上说明的第一种情况。\n* (`Follower`*c*在任期为6的时刻，`Follower`*d*在任期为7的时刻)为`Leader`,但没有完全完成日志的发送便崩溃了.满足以上说明的第三种情况。\n* `Follower`*e*在任期为4的时刻,`Follower`*f*在任期为2,3的时刻为`Leader`,,但没有完全完成日志的发送便崩溃了,同时在其他服务器当选`Leader`时刻也没有接收到新的`Leader`发送的`AppendEntries RPC`消息,满足第三种情况。\n\n#### 2.1 日志不一致的解决方案\n&emsp;&emsp;`Leader`通过强迫`Follower`的日志重复自己的日志来处理不一致之处。这意味着`Follower`日志中的冲突日志将被`Leader`日志中的条目覆盖。因此`Leader`必须找到与`Follower`最开始日志发生冲突的位置,然后删除掉`Follower`上所有与`Leader`发生冲突的日志。然后将自己的日志发送给`Follower`以解决冲突。\n**`Leader`不会删除或覆盖自己本地的日志条目**\n\n&emsp;&emsp;这些步骤从之前说到的日志的一致性检查开始。\n\n* 当发生日志冲突时，`Follower`将会拒绝由`Leader`发送的`AppendEntries RPC`消息，并返回一个响应消息告知`Leader`日志发生了冲突。\n* `Leader`为每一个`Follower`维护一个`nextIndex`值。该值用于确定需要发送给该`Follower`的下一条日志的位置索引。(该值在当前服务器成功当选`Leader`后会重置为本地日志的最后一条索引号+1)\n* 当`Leader`了解到日志发生冲突之后，便递减`nextIndex`值。并重新发送`AppendEntries RPC`到该`Follower`。并不断重复这个过程，一直到`Follower`接受该消息。\n* 一旦`Follower`接受了`AppendEntries RPC`消息，`Leader`则根据`nextIndex`值可以确定发生冲突的位置，从而强迫`Follower`的日志重复自己的日志以解决冲突问题。\n\n ![图2](/img/blog/raft/8.png)\n\n* 情况*a*: 如图，服务器*S1*在任期为2的时刻仅将日志`<index:2,term:2>`发送到了服务器*S2*便崩溃掉。\n* 情况*b*: 服务器*S5*在任期为3的时刻当选`Leader`(*S5*的计时器率先超时，递增任期号为3因此高于服务器*S3,S4*，可以当选`Leader`)，但没来得及发送日志便崩溃掉。\n* 情况*c*: 服务器*S1*在任期为4的时刻再次当选`Leader`(*S1*重启时，任期仍然为2，收到新的`Leader`*S5*发送的心跳信息后更新任期为3，而在`Leader`*S5*崩溃后，服务器*S1*为第一个计时器超时的，因此发起投票，任期更新为4，大于网络中其他服务器任期，成功当选`Leader`),同时将日志`<index:2,term:2>`发送到了服务器*S2,S3*,但还没有通知服务器对日志进行提交便崩溃掉。\n* 情况*d*: 情况(*a->d*)如果在任期为2时服务器*S1*作为`Leader`崩溃掉，*S5*在任期为3的时刻当选`Leader`，由于日志`<index:2,term:2>`还没有被复制到大部分服务器上，并没有被提交，所以*S5*可以通过自己的日志`<index:2,term:3>`覆盖掉日志`<index:2,term:2>`。\n* 情况*e*: 情况(*a->e*)而如果在任期为2时服务器*S1*作为`Leader`，并将`<index:2,term:2>`发送到*S2,S3*,成功复制到大多数成员服务器上。并且成功提交了该日志，那么即便*S1*崩溃掉，*S5*也无法成功当选`Leader`，因为*S5*不具备网络中最新的已被提交的日志条目(**这里说明了上一篇文章[Raft算法之Leader选举](https://ifican.top/2020/01/04/blog/consensus/raft-election/#%E9%80%89%E4%B8%BE%E9%98%B6%E6%AE%B5-gt-%E6%AD%A3%E5%B8%B8%E8%BF%90%E8%A1%8C%E9%98%B6%E6%AE%B5)中选举`Leader`的要求中没有介绍的那一点要求**).\n\n#### 2.2 选举`Leader`的对日志的要求\n* Raft使用投票程序来防止`Candidate`赢得选举，除非其日志中包含所有已提交的日志条目。\n* `Candidate`必须联系集群的大多数才能被选举，这意味着每个提交的条目都必须存在于其中至少一台服务器中。如果`Candidate`的日志至少与该多数服务器日志中的日志一样最新(以下精确定义了**最新**),则它将保存所有已提交的条目。\n* Raft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新的。如果日志中的最后一个条目具有不同的任期，则带有较新任期的日志是最新的。如果日志以相同的任期结尾，则以索引更大的日志为准。\n\n&emsp;&emsp;**解决方案的优化**\n&emsp;&emsp;在`Follower`拒绝`AppendEntries RPC`消息时，可以选择将发生冲突的日志的任期与该任期内的第一条日志索引包含在拒绝消息中返回给`Leader`，从而使得`Leader`可以快速定位到发生冲突的位置。有了这些信息，`Leader`可以递减`nextIndex`来绕过该任期中所有冲突的条目。每个具有**冲突日志条目所处的任期**都需要一个`AppendEntries RPC`消息，而不是每个日志条目都需要一个`AppendEntries RPC`消息。\n\n\n### 3 日志复制安全性\n**Raft保证任何时刻这里的每一条属性都成立**\n\n* `Leader`只追加特性:`Leader`从不覆盖或删除它的日志条目，只追加新的。\n* 日志匹配: 如果两个日志包含的实体具有相同的索引和任期，那么直到给定索引为止，所有条目中的日志都是相同的。\n* `Leader`完整性:如果一个日志提示在给定的任期内被提交，那么该条目将出现在所有任期更高的领导者的日志中.\n* 状态机安全:如果服务器应用一条给定索引的日志实体到它的状态机，那么没有其他服务器可以应用一条不同的日志到相同的索引位置。\n\n#### 3.1 `Leader`完整性证明\n&emsp;&emsp;假设`Leader`完整性不成立，然后证明是矛盾的。\n&emsp;&emsp;假设任期为*T*的`Leader`提交了当前任期的日志条目，但是该日志没有被任期高于*T*的任期为*U*的未来的新的`Leader`所存储。\n\n1. 被提交任期为*T*的日志必须不存在于将要选举的任期为*U*的`Leader`的复制状态机中(因为`Leader`从不覆盖或删除它的日志条目)。\n2. 任期为*T*的`Leader`将日志复制到集群中的大部分成员本地。并且任期为*U*的`Leader`在选举阶段接收到集群中大部分成员的投票，因此至少集群中有一个成员(以下称为投票者)即接收到来自任期为*T*的`Leader`发送的日志，也为任期为*U*的`Leader`投了票。所以该投票者是证明矛盾的关键所在。\n3. 投票者必须在为任期为*U*的`Leader`投票之前将任期为*T*的`Leader`的发送的日志提交。不然投票者将会拒绝任期为*T*的`Leader`的`AppendEntries PRC`请求(因为一旦接收到任期为*U*的`Leader`投票请求，投票者的任期将会高于*T*)。\n4. 投票者当为任期为*U*的`Leader`投票时，将会一直存储该日志条目。假设在任期为*T*和*U*之间的每一个`Leader`都包含该日志条目(`Leader`从不删除日志条目，而`Follower`仅在与`Leader`冲突时才删除条目)。\n5. 投票者为任期为*U*的`Leader`投票，所以任期为*U*的`Leader`日志必须至少和投票者的日志一样新。这将导致产生两个矛盾之中的一个矛盾。\n6. 首先，如果投票者和任期为*U*的`Leader`具有相同的最新的日志任期。那么任期为*U*的`Leader`的日志至少和投票者的日志一样长。所以任期为*U*的`Leader`的日志将包含投票者所有的日志。这是一个矛盾，因为之前假设的投票者包含被提交的任期为*T*的日志，而任期为*U*的`Leader`不包含。\n7. 否则，任期为*U*的`Leader`的最后一条日志的任期号必须大于投票者的最后一条日志的任期号。而且，它比*T*大，因为投票者的上一个日志任期号至少为*T*(它包含任期*T*中的所有已提交的条目)。创建任期为*U*的`Leader`的最后一个日志条目的较早的`Leader`必须在其日志中(通过假设)包含已提交的条目。然后，通过日志匹配属性，任期为*U*的`Leader`的日志还必须包含已提交的条目，这是矛盾的。\n8. 这样就证明了矛盾，因此所有任期大于*T*的`Leader`都必须包含所有任期为*T*的被提交的日志。\n9. 日志匹配属性保证未来的`Leader`还将包含间接提交的日志条目。\n\n下一篇文章:[Raft算法之成员关系变化](https://ifican.top/2020/01/06/blog/consensus/raft-relationship/)","source":"_posts/blog/consensus/raft-log.md","raw":"---\ntitle: Raft算法之日志复制\ndate: 2020-01-05 13:45:59\ntags: \n- Raft\n- algorithm\ncategories:\n- algorithm\n---\n上一篇文章:[Raft算法之Leader选举](https://ifican.top/2020/01/04/blog/consensus/raft-election/)\n&emsp;&emsp;之前说完了Raft算法中的Leader选举过程，本文将在上一篇文章的基础上说明日志复制。\n\n# Raft算法之日志复制\n&emsp;&emsp;先看以下日志所包含的基本内容:\n\n1. 可以被复制状态机执行的命令\n2. 任期号 :创建该日志时Leader所处的当前任期号\n3. 索引号 :整数，用于标识日志所在的位置\n\n日志的状态分为两种:未被提交，已被提交(日志为安全的，不会被删除或覆盖)。\n\n### 1 正常情况\n\n* 当`Leader`接收到由客户端发送的请求(请求中包含可以被复制状态机执行的命令)时，Leader将会把该请求作为新的内容添加到日志中(任期号为当前`Leader`所处的任期号，索引号为当前`Leader`本地存储的日志集合中的日志的最高索引号加1)。\n    * `Leader`**在当前任期内最多只能创建一个给定索引号的日志(即不可能在一个任期内创建两个以上的具有相同索引的日志条目)**\n* 然后将该日志通过`AppendEntries RPC`消息发送到网络中其他的服务器(以下简称`Follower`)，从而复制该日志。\n* 在网络中`Follower`接收到该日志消息后则会返回复制成功的回复。\n* 在`Leader`接收到网络中大部分的`Follower`的成功复制的回复之后，`Leader`便认为该日志**可以被提交**。此时`Leader`将会同时做三件事：\n\n1. 将该日志应用到`Leader`本地的复制状态机\n2. 向所有`Follower`发送消息通知所有接收到该日志的`Follower`将该日志进行提交，然后应用到各自本地的复制状态机\n3. 将执行结果通知客户端\n\n&emsp;&emsp;当该日志消息成功在网络中大部分`Follower`本地的复制状态机执行过后，则可认为该日志**已被提交**。在当前日志被提交的过程中，如果`Leader`先前的某些日志还没有被提交，则将会一同提交。\n&emsp;&emsp;而网络中有些`Follower`可能由于网络状态原因反应缓慢或者崩溃，那么`Leader`将会无限次地尝试重复发送`AppendEntries RPC`消息到该`Follower`。直到成功为止。\n\n#### 1.1 日志的一致性检查\n&emsp;&emsp;在上面，我们说了`Follower`在接收到`AppendEntries RPC`消息后则会返回复制成功的回复。实际上在接收到消息后会首先进行日志的一致性检查(正常情况下`Leader`与`Follower`的日志会保持一致，所以一致性检查不会失败),**一致性检查内容如下：**\n\n* 在`Leader`创建`AppendEntries RPC`消息时，消息中将会包含当前日志之前日志条目的任期号与索引号。\n* `Follower`在接受到`AppendEntries RPC`消息后，将会检查之前日志的任期号与索引号是否可以匹配到\n    * 如果匹配到则说明和`Leader`之前的日志是保持一致的。\n    * 如果没有匹配则会拒绝`AppendEntries RPC`消息.\n\n&emsp;&emsp;一致性检查是一个归纳的过程。**正常情况下**，网络中第一条日志一定满足日志的一致性检查，然后第二条日志中包含第一条日志的任期号与索引号，所以只要`Leader`与`Follower`的第一条日志保持一致，那么第二条日志也会满足一致性检查，从而之后的每一条日志都会满足一致性检查。\n\n&emsp;&emsp;从而得出了日志匹配属性:\n\n* 如果两个不同的日志实体具有相同的索引和任期号，那么他们存储有相同的命令。\n* 如果两个不同的日志实体具有相同的索引和任期号，则所有先前条目中的日志都相同。(由一致性检查结果得出)\n\n\n### 2 特殊情况\n&emsp;&emsp;而网络不可能一直处于正常情况。因为`Leader`或者某个`Follower`有可能会崩溃，从而导致日志不能一直保持一致。因此存在以下三种情况:\n\n1. `Follower`缺失当前`Leader`上存在的日志条目。\n2. `Follower`存在当前`Leader`不存在的日志条目。(比如旧的`Leader`仅仅将`AppendEntries RPC`消息发送到一部分`Follower`就崩溃掉，然后新的当选`Leader`的服务器恰好是没有收到该`AppendEntries RPC`消息的服务器)\n3. 或者`Follower`即缺失当前`Leader`上存在的日志条目，也存在当前`Leader`不存在的日志条目\n\n ![图1](/img/blog/raft/7.png)\n\n&emsp;&emsp;图中最上方是日志的索引号(1-12),每个方块代表一条日志信息，方块内数字代表该日志所处的任期号。图中当前`Leader`(图中最上方一行日志代表当前`Leader`日志)处于任期号为8的时刻。以此图说明以上三种情况存在的原因：\n\n* `Follower` *a,b*(`Follower`崩溃没有接收到`Leader`发送的`AppendEntries RPC`消息)满足以上说明的第一种情况。\n* (`Follower`*c*在任期为6的时刻，`Follower`*d*在任期为7的时刻)为`Leader`,但没有完全完成日志的发送便崩溃了.满足以上说明的第三种情况。\n* `Follower`*e*在任期为4的时刻,`Follower`*f*在任期为2,3的时刻为`Leader`,,但没有完全完成日志的发送便崩溃了,同时在其他服务器当选`Leader`时刻也没有接收到新的`Leader`发送的`AppendEntries RPC`消息,满足第三种情况。\n\n#### 2.1 日志不一致的解决方案\n&emsp;&emsp;`Leader`通过强迫`Follower`的日志重复自己的日志来处理不一致之处。这意味着`Follower`日志中的冲突日志将被`Leader`日志中的条目覆盖。因此`Leader`必须找到与`Follower`最开始日志发生冲突的位置,然后删除掉`Follower`上所有与`Leader`发生冲突的日志。然后将自己的日志发送给`Follower`以解决冲突。\n**`Leader`不会删除或覆盖自己本地的日志条目**\n\n&emsp;&emsp;这些步骤从之前说到的日志的一致性检查开始。\n\n* 当发生日志冲突时，`Follower`将会拒绝由`Leader`发送的`AppendEntries RPC`消息，并返回一个响应消息告知`Leader`日志发生了冲突。\n* `Leader`为每一个`Follower`维护一个`nextIndex`值。该值用于确定需要发送给该`Follower`的下一条日志的位置索引。(该值在当前服务器成功当选`Leader`后会重置为本地日志的最后一条索引号+1)\n* 当`Leader`了解到日志发生冲突之后，便递减`nextIndex`值。并重新发送`AppendEntries RPC`到该`Follower`。并不断重复这个过程，一直到`Follower`接受该消息。\n* 一旦`Follower`接受了`AppendEntries RPC`消息，`Leader`则根据`nextIndex`值可以确定发生冲突的位置，从而强迫`Follower`的日志重复自己的日志以解决冲突问题。\n\n ![图2](/img/blog/raft/8.png)\n\n* 情况*a*: 如图，服务器*S1*在任期为2的时刻仅将日志`<index:2,term:2>`发送到了服务器*S2*便崩溃掉。\n* 情况*b*: 服务器*S5*在任期为3的时刻当选`Leader`(*S5*的计时器率先超时，递增任期号为3因此高于服务器*S3,S4*，可以当选`Leader`)，但没来得及发送日志便崩溃掉。\n* 情况*c*: 服务器*S1*在任期为4的时刻再次当选`Leader`(*S1*重启时，任期仍然为2，收到新的`Leader`*S5*发送的心跳信息后更新任期为3，而在`Leader`*S5*崩溃后，服务器*S1*为第一个计时器超时的，因此发起投票，任期更新为4，大于网络中其他服务器任期，成功当选`Leader`),同时将日志`<index:2,term:2>`发送到了服务器*S2,S3*,但还没有通知服务器对日志进行提交便崩溃掉。\n* 情况*d*: 情况(*a->d*)如果在任期为2时服务器*S1*作为`Leader`崩溃掉，*S5*在任期为3的时刻当选`Leader`，由于日志`<index:2,term:2>`还没有被复制到大部分服务器上，并没有被提交，所以*S5*可以通过自己的日志`<index:2,term:3>`覆盖掉日志`<index:2,term:2>`。\n* 情况*e*: 情况(*a->e*)而如果在任期为2时服务器*S1*作为`Leader`，并将`<index:2,term:2>`发送到*S2,S3*,成功复制到大多数成员服务器上。并且成功提交了该日志，那么即便*S1*崩溃掉，*S5*也无法成功当选`Leader`，因为*S5*不具备网络中最新的已被提交的日志条目(**这里说明了上一篇文章[Raft算法之Leader选举](https://ifican.top/2020/01/04/blog/consensus/raft-election/#%E9%80%89%E4%B8%BE%E9%98%B6%E6%AE%B5-gt-%E6%AD%A3%E5%B8%B8%E8%BF%90%E8%A1%8C%E9%98%B6%E6%AE%B5)中选举`Leader`的要求中没有介绍的那一点要求**).\n\n#### 2.2 选举`Leader`的对日志的要求\n* Raft使用投票程序来防止`Candidate`赢得选举，除非其日志中包含所有已提交的日志条目。\n* `Candidate`必须联系集群的大多数才能被选举，这意味着每个提交的条目都必须存在于其中至少一台服务器中。如果`Candidate`的日志至少与该多数服务器日志中的日志一样最新(以下精确定义了**最新**),则它将保存所有已提交的条目。\n* Raft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新的。如果日志中的最后一个条目具有不同的任期，则带有较新任期的日志是最新的。如果日志以相同的任期结尾，则以索引更大的日志为准。\n\n&emsp;&emsp;**解决方案的优化**\n&emsp;&emsp;在`Follower`拒绝`AppendEntries RPC`消息时，可以选择将发生冲突的日志的任期与该任期内的第一条日志索引包含在拒绝消息中返回给`Leader`，从而使得`Leader`可以快速定位到发生冲突的位置。有了这些信息，`Leader`可以递减`nextIndex`来绕过该任期中所有冲突的条目。每个具有**冲突日志条目所处的任期**都需要一个`AppendEntries RPC`消息，而不是每个日志条目都需要一个`AppendEntries RPC`消息。\n\n\n### 3 日志复制安全性\n**Raft保证任何时刻这里的每一条属性都成立**\n\n* `Leader`只追加特性:`Leader`从不覆盖或删除它的日志条目，只追加新的。\n* 日志匹配: 如果两个日志包含的实体具有相同的索引和任期，那么直到给定索引为止，所有条目中的日志都是相同的。\n* `Leader`完整性:如果一个日志提示在给定的任期内被提交，那么该条目将出现在所有任期更高的领导者的日志中.\n* 状态机安全:如果服务器应用一条给定索引的日志实体到它的状态机，那么没有其他服务器可以应用一条不同的日志到相同的索引位置。\n\n#### 3.1 `Leader`完整性证明\n&emsp;&emsp;假设`Leader`完整性不成立，然后证明是矛盾的。\n&emsp;&emsp;假设任期为*T*的`Leader`提交了当前任期的日志条目，但是该日志没有被任期高于*T*的任期为*U*的未来的新的`Leader`所存储。\n\n1. 被提交任期为*T*的日志必须不存在于将要选举的任期为*U*的`Leader`的复制状态机中(因为`Leader`从不覆盖或删除它的日志条目)。\n2. 任期为*T*的`Leader`将日志复制到集群中的大部分成员本地。并且任期为*U*的`Leader`在选举阶段接收到集群中大部分成员的投票，因此至少集群中有一个成员(以下称为投票者)即接收到来自任期为*T*的`Leader`发送的日志，也为任期为*U*的`Leader`投了票。所以该投票者是证明矛盾的关键所在。\n3. 投票者必须在为任期为*U*的`Leader`投票之前将任期为*T*的`Leader`的发送的日志提交。不然投票者将会拒绝任期为*T*的`Leader`的`AppendEntries PRC`请求(因为一旦接收到任期为*U*的`Leader`投票请求，投票者的任期将会高于*T*)。\n4. 投票者当为任期为*U*的`Leader`投票时，将会一直存储该日志条目。假设在任期为*T*和*U*之间的每一个`Leader`都包含该日志条目(`Leader`从不删除日志条目，而`Follower`仅在与`Leader`冲突时才删除条目)。\n5. 投票者为任期为*U*的`Leader`投票，所以任期为*U*的`Leader`日志必须至少和投票者的日志一样新。这将导致产生两个矛盾之中的一个矛盾。\n6. 首先，如果投票者和任期为*U*的`Leader`具有相同的最新的日志任期。那么任期为*U*的`Leader`的日志至少和投票者的日志一样长。所以任期为*U*的`Leader`的日志将包含投票者所有的日志。这是一个矛盾，因为之前假设的投票者包含被提交的任期为*T*的日志，而任期为*U*的`Leader`不包含。\n7. 否则，任期为*U*的`Leader`的最后一条日志的任期号必须大于投票者的最后一条日志的任期号。而且，它比*T*大，因为投票者的上一个日志任期号至少为*T*(它包含任期*T*中的所有已提交的条目)。创建任期为*U*的`Leader`的最后一个日志条目的较早的`Leader`必须在其日志中(通过假设)包含已提交的条目。然后，通过日志匹配属性，任期为*U*的`Leader`的日志还必须包含已提交的条目，这是矛盾的。\n8. 这样就证明了矛盾，因此所有任期大于*T*的`Leader`都必须包含所有任期为*T*的被提交的日志。\n9. 日志匹配属性保证未来的`Leader`还将包含间接提交的日志条目。\n\n下一篇文章:[Raft算法之成员关系变化](https://ifican.top/2020/01/06/blog/consensus/raft-relationship/)","slug":"blog/consensus/raft-log","published":1,"updated":"2020-05-11T03:44:37.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyfg000bk0vq5drm0kai","content":"<p>上一篇文章:<a href=\"https://ifican.top/2020/01/04/blog/consensus/raft-election/\" target=\"_blank\" rel=\"noopener\">Raft算法之Leader选举</a><br>&emsp;&emsp;之前说完了Raft算法中的Leader选举过程，本文将在上一篇文章的基础上说明日志复制。</p>\n<h1 id=\"Raft算法之日志复制\"><a href=\"#Raft算法之日志复制\" class=\"headerlink\" title=\"Raft算法之日志复制\"></a>Raft算法之日志复制</h1><p>&emsp;&emsp;先看以下日志所包含的基本内容:</p>\n<ol>\n<li>可以被复制状态机执行的命令</li>\n<li>任期号 :创建该日志时Leader所处的当前任期号</li>\n<li>索引号 :整数，用于标识日志所在的位置</li>\n</ol>\n<p>日志的状态分为两种:未被提交，已被提交(日志为安全的，不会被删除或覆盖)。</p>\n<h3 id=\"1-正常情况\"><a href=\"#1-正常情况\" class=\"headerlink\" title=\"1 正常情况\"></a>1 正常情况</h3><ul>\n<li>当<code>Leader</code>接收到由客户端发送的请求(请求中包含可以被复制状态机执行的命令)时，Leader将会把该请求作为新的内容添加到日志中(任期号为当前<code>Leader</code>所处的任期号，索引号为当前<code>Leader</code>本地存储的日志集合中的日志的最高索引号加1)。<ul>\n<li><code>Leader</code><strong>在当前任期内最多只能创建一个给定索引号的日志(即不可能在一个任期内创建两个以上的具有相同索引的日志条目)</strong></li>\n</ul>\n</li>\n<li>然后将该日志通过<code>AppendEntries RPC</code>消息发送到网络中其他的服务器(以下简称<code>Follower</code>)，从而复制该日志。</li>\n<li>在网络中<code>Follower</code>接收到该日志消息后则会返回复制成功的回复。</li>\n<li>在<code>Leader</code>接收到网络中大部分的<code>Follower</code>的成功复制的回复之后，<code>Leader</code>便认为该日志<strong>可以被提交</strong>。此时<code>Leader</code>将会同时做三件事：</li>\n</ul>\n<ol>\n<li>将该日志应用到<code>Leader</code>本地的复制状态机</li>\n<li>向所有<code>Follower</code>发送消息通知所有接收到该日志的<code>Follower</code>将该日志进行提交，然后应用到各自本地的复制状态机</li>\n<li>将执行结果通知客户端</li>\n</ol>\n<p>&emsp;&emsp;当该日志消息成功在网络中大部分<code>Follower</code>本地的复制状态机执行过后，则可认为该日志<strong>已被提交</strong>。在当前日志被提交的过程中，如果<code>Leader</code>先前的某些日志还没有被提交，则将会一同提交。<br>&emsp;&emsp;而网络中有些<code>Follower</code>可能由于网络状态原因反应缓慢或者崩溃，那么<code>Leader</code>将会无限次地尝试重复发送<code>AppendEntries RPC</code>消息到该<code>Follower</code>。直到成功为止。</p>\n<h4 id=\"1-1-日志的一致性检查\"><a href=\"#1-1-日志的一致性检查\" class=\"headerlink\" title=\"1.1 日志的一致性检查\"></a>1.1 日志的一致性检查</h4><p>&emsp;&emsp;在上面，我们说了<code>Follower</code>在接收到<code>AppendEntries RPC</code>消息后则会返回复制成功的回复。实际上在接收到消息后会首先进行日志的一致性检查(正常情况下<code>Leader</code>与<code>Follower</code>的日志会保持一致，所以一致性检查不会失败),<strong>一致性检查内容如下：</strong></p>\n<ul>\n<li>在<code>Leader</code>创建<code>AppendEntries RPC</code>消息时，消息中将会包含当前日志之前日志条目的任期号与索引号。</li>\n<li><code>Follower</code>在接受到<code>AppendEntries RPC</code>消息后，将会检查之前日志的任期号与索引号是否可以匹配到<ul>\n<li>如果匹配到则说明和<code>Leader</code>之前的日志是保持一致的。</li>\n<li>如果没有匹配则会拒绝<code>AppendEntries RPC</code>消息.</li>\n</ul>\n</li>\n</ul>\n<p>&emsp;&emsp;一致性检查是一个归纳的过程。<strong>正常情况下</strong>，网络中第一条日志一定满足日志的一致性检查，然后第二条日志中包含第一条日志的任期号与索引号，所以只要<code>Leader</code>与<code>Follower</code>的第一条日志保持一致，那么第二条日志也会满足一致性检查，从而之后的每一条日志都会满足一致性检查。</p>\n<p>&emsp;&emsp;从而得出了日志匹配属性:</p>\n<ul>\n<li>如果两个不同的日志实体具有相同的索引和任期号，那么他们存储有相同的命令。</li>\n<li>如果两个不同的日志实体具有相同的索引和任期号，则所有先前条目中的日志都相同。(由一致性检查结果得出)</li>\n</ul>\n<h3 id=\"2-特殊情况\"><a href=\"#2-特殊情况\" class=\"headerlink\" title=\"2 特殊情况\"></a>2 特殊情况</h3><p>&emsp;&emsp;而网络不可能一直处于正常情况。因为<code>Leader</code>或者某个<code>Follower</code>有可能会崩溃，从而导致日志不能一直保持一致。因此存在以下三种情况:</p>\n<ol>\n<li><p><code>Follower</code>缺失当前<code>Leader</code>上存在的日志条目。</p>\n</li>\n<li><p><code>Follower</code>存在当前<code>Leader</code>不存在的日志条目。(比如旧的<code>Leader</code>仅仅将<code>AppendEntries RPC</code>消息发送到一部分<code>Follower</code>就崩溃掉，然后新的当选<code>Leader</code>的服务器恰好是没有收到该<code>AppendEntries RPC</code>消息的服务器)</p>\n</li>\n<li><p>或者<code>Follower</code>即缺失当前<code>Leader</code>上存在的日志条目，也存在当前<code>Leader</code>不存在的日志条目</p>\n<p><img src=\"/img/blog/raft/7.png\" srcset=\"undefined\" alt=\"图1\"></p>\n</li>\n</ol>\n<p>&emsp;&emsp;图中最上方是日志的索引号(1-12),每个方块代表一条日志信息，方块内数字代表该日志所处的任期号。图中当前<code>Leader</code>(图中最上方一行日志代表当前<code>Leader</code>日志)处于任期号为8的时刻。以此图说明以上三种情况存在的原因：</p>\n<ul>\n<li><code>Follower</code> <em>a,b</em>(<code>Follower</code>崩溃没有接收到<code>Leader</code>发送的<code>AppendEntries RPC</code>消息)满足以上说明的第一种情况。</li>\n<li>(<code>Follower</code><em>c</em>在任期为6的时刻，<code>Follower</code><em>d</em>在任期为7的时刻)为<code>Leader</code>,但没有完全完成日志的发送便崩溃了.满足以上说明的第三种情况。</li>\n<li><code>Follower</code><em>e</em>在任期为4的时刻,<code>Follower</code><em>f</em>在任期为2,3的时刻为<code>Leader</code>,,但没有完全完成日志的发送便崩溃了,同时在其他服务器当选<code>Leader</code>时刻也没有接收到新的<code>Leader</code>发送的<code>AppendEntries RPC</code>消息,满足第三种情况。</li>\n</ul>\n<h4 id=\"2-1-日志不一致的解决方案\"><a href=\"#2-1-日志不一致的解决方案\" class=\"headerlink\" title=\"2.1 日志不一致的解决方案\"></a>2.1 日志不一致的解决方案</h4><p>&emsp;&emsp;<code>Leader</code>通过强迫<code>Follower</code>的日志重复自己的日志来处理不一致之处。这意味着<code>Follower</code>日志中的冲突日志将被<code>Leader</code>日志中的条目覆盖。因此<code>Leader</code>必须找到与<code>Follower</code>最开始日志发生冲突的位置,然后删除掉<code>Follower</code>上所有与<code>Leader</code>发生冲突的日志。然后将自己的日志发送给<code>Follower</code>以解决冲突。<br><strong><code>Leader</code>不会删除或覆盖自己本地的日志条目</strong></p>\n<p>&emsp;&emsp;这些步骤从之前说到的日志的一致性检查开始。</p>\n<ul>\n<li><p>当发生日志冲突时，<code>Follower</code>将会拒绝由<code>Leader</code>发送的<code>AppendEntries RPC</code>消息，并返回一个响应消息告知<code>Leader</code>日志发生了冲突。</p>\n</li>\n<li><p><code>Leader</code>为每一个<code>Follower</code>维护一个<code>nextIndex</code>值。该值用于确定需要发送给该<code>Follower</code>的下一条日志的位置索引。(该值在当前服务器成功当选<code>Leader</code>后会重置为本地日志的最后一条索引号+1)</p>\n</li>\n<li><p>当<code>Leader</code>了解到日志发生冲突之后，便递减<code>nextIndex</code>值。并重新发送<code>AppendEntries RPC</code>到该<code>Follower</code>。并不断重复这个过程，一直到<code>Follower</code>接受该消息。</p>\n</li>\n<li><p>一旦<code>Follower</code>接受了<code>AppendEntries RPC</code>消息，<code>Leader</code>则根据<code>nextIndex</code>值可以确定发生冲突的位置，从而强迫<code>Follower</code>的日志重复自己的日志以解决冲突问题。</p>\n<p><img src=\"/img/blog/raft/8.png\" srcset=\"undefined\" alt=\"图2\"></p>\n</li>\n<li><p>情况<em>a</em>: 如图，服务器<em>S1</em>在任期为2的时刻仅将日志<code>&lt;index:2,term:2&gt;</code>发送到了服务器<em>S2</em>便崩溃掉。</p>\n</li>\n<li><p>情况<em>b</em>: 服务器<em>S5</em>在任期为3的时刻当选<code>Leader</code>(<em>S5</em>的计时器率先超时，递增任期号为3因此高于服务器<em>S3,S4</em>，可以当选<code>Leader</code>)，但没来得及发送日志便崩溃掉。</p>\n</li>\n<li><p>情况<em>c</em>: 服务器<em>S1<em>在任期为4的时刻再次当选<code>Leader</code>(</em>S1<em>重启时，任期仍然为2，收到新的<code>Leader</code></em>S5<em>发送的心跳信息后更新任期为3，而在<code>Leader</code></em>S5<em>崩溃后，服务器</em>S1<em>为第一个计时器超时的，因此发起投票，任期更新为4，大于网络中其他服务器任期，成功当选<code>Leader</code>),同时将日志<code>&lt;index:2,term:2&gt;</code>发送到了服务器</em>S2,S3</em>,但还没有通知服务器对日志进行提交便崩溃掉。</p>\n</li>\n<li><p>情况<em>d</em>: 情况(<em>a-&gt;d</em>)如果在任期为2时服务器<em>S1</em>作为<code>Leader</code>崩溃掉，<em>S5</em>在任期为3的时刻当选<code>Leader</code>，由于日志<code>&lt;index:2,term:2&gt;</code>还没有被复制到大部分服务器上，并没有被提交，所以<em>S5</em>可以通过自己的日志<code>&lt;index:2,term:3&gt;</code>覆盖掉日志<code>&lt;index:2,term:2&gt;</code>。</p>\n</li>\n<li><p>情况<em>e</em>: 情况(<em>a-&gt;e</em>)而如果在任期为2时服务器<em>S1<em>作为<code>Leader</code>，并将<code>&lt;index:2,term:2&gt;</code>发送到</em>S2,S3</em>,成功复制到大多数成员服务器上。并且成功提交了该日志，那么即便<em>S1</em>崩溃掉，<em>S5</em>也无法成功当选<code>Leader</code>，因为<em>S5</em>不具备网络中最新的已被提交的日志条目(<strong>这里说明了上一篇文章<a href=\"https://ifican.top/2020/01/04/blog/consensus/raft-election/#%E9%80%89%E4%B8%BE%E9%98%B6%E6%AE%B5-gt-%E6%AD%A3%E5%B8%B8%E8%BF%90%E8%A1%8C%E9%98%B6%E6%AE%B5\" target=\"_blank\" rel=\"noopener\">Raft算法之Leader选举</a>中选举<code>Leader</code>的要求中没有介绍的那一点要求</strong>).</p>\n</li>\n</ul>\n<h4 id=\"2-2-选举Leader的对日志的要求\"><a href=\"#2-2-选举Leader的对日志的要求\" class=\"headerlink\" title=\"2.2 选举Leader的对日志的要求\"></a>2.2 选举<code>Leader</code>的对日志的要求</h4><ul>\n<li>Raft使用投票程序来防止<code>Candidate</code>赢得选举，除非其日志中包含所有已提交的日志条目。</li>\n<li><code>Candidate</code>必须联系集群的大多数才能被选举，这意味着每个提交的条目都必须存在于其中至少一台服务器中。如果<code>Candidate</code>的日志至少与该多数服务器日志中的日志一样最新(以下精确定义了<strong>最新</strong>),则它将保存所有已提交的条目。</li>\n<li>Raft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新的。如果日志中的最后一个条目具有不同的任期，则带有较新任期的日志是最新的。如果日志以相同的任期结尾，则以索引更大的日志为准。</li>\n</ul>\n<p>&emsp;&emsp;<strong>解决方案的优化</strong><br>&emsp;&emsp;在<code>Follower</code>拒绝<code>AppendEntries RPC</code>消息时，可以选择将发生冲突的日志的任期与该任期内的第一条日志索引包含在拒绝消息中返回给<code>Leader</code>，从而使得<code>Leader</code>可以快速定位到发生冲突的位置。有了这些信息，<code>Leader</code>可以递减<code>nextIndex</code>来绕过该任期中所有冲突的条目。每个具有<strong>冲突日志条目所处的任期</strong>都需要一个<code>AppendEntries RPC</code>消息，而不是每个日志条目都需要一个<code>AppendEntries RPC</code>消息。</p>\n<h3 id=\"3-日志复制安全性\"><a href=\"#3-日志复制安全性\" class=\"headerlink\" title=\"3 日志复制安全性\"></a>3 日志复制安全性</h3><p><strong>Raft保证任何时刻这里的每一条属性都成立</strong></p>\n<ul>\n<li><code>Leader</code>只追加特性:<code>Leader</code>从不覆盖或删除它的日志条目，只追加新的。</li>\n<li>日志匹配: 如果两个日志包含的实体具有相同的索引和任期，那么直到给定索引为止，所有条目中的日志都是相同的。</li>\n<li><code>Leader</code>完整性:如果一个日志提示在给定的任期内被提交，那么该条目将出现在所有任期更高的领导者的日志中.</li>\n<li>状态机安全:如果服务器应用一条给定索引的日志实体到它的状态机，那么没有其他服务器可以应用一条不同的日志到相同的索引位置。</li>\n</ul>\n<h4 id=\"3-1-Leader完整性证明\"><a href=\"#3-1-Leader完整性证明\" class=\"headerlink\" title=\"3.1 Leader完整性证明\"></a>3.1 <code>Leader</code>完整性证明</h4><p>&emsp;&emsp;假设<code>Leader</code>完整性不成立，然后证明是矛盾的。<br>&emsp;&emsp;假设任期为<em>T</em>的<code>Leader</code>提交了当前任期的日志条目，但是该日志没有被任期高于<em>T</em>的任期为<em>U</em>的未来的新的<code>Leader</code>所存储。</p>\n<ol>\n<li>被提交任期为<em>T</em>的日志必须不存在于将要选举的任期为<em>U</em>的<code>Leader</code>的复制状态机中(因为<code>Leader</code>从不覆盖或删除它的日志条目)。</li>\n<li>任期为<em>T</em>的<code>Leader</code>将日志复制到集群中的大部分成员本地。并且任期为<em>U</em>的<code>Leader</code>在选举阶段接收到集群中大部分成员的投票，因此至少集群中有一个成员(以下称为投票者)即接收到来自任期为<em>T</em>的<code>Leader</code>发送的日志，也为任期为<em>U</em>的<code>Leader</code>投了票。所以该投票者是证明矛盾的关键所在。</li>\n<li>投票者必须在为任期为<em>U</em>的<code>Leader</code>投票之前将任期为<em>T</em>的<code>Leader</code>的发送的日志提交。不然投票者将会拒绝任期为<em>T</em>的<code>Leader</code>的<code>AppendEntries PRC</code>请求(因为一旦接收到任期为<em>U</em>的<code>Leader</code>投票请求，投票者的任期将会高于<em>T</em>)。</li>\n<li>投票者当为任期为<em>U</em>的<code>Leader</code>投票时，将会一直存储该日志条目。假设在任期为<em>T</em>和<em>U</em>之间的每一个<code>Leader</code>都包含该日志条目(<code>Leader</code>从不删除日志条目，而<code>Follower</code>仅在与<code>Leader</code>冲突时才删除条目)。</li>\n<li>投票者为任期为<em>U</em>的<code>Leader</code>投票，所以任期为<em>U</em>的<code>Leader</code>日志必须至少和投票者的日志一样新。这将导致产生两个矛盾之中的一个矛盾。</li>\n<li>首先，如果投票者和任期为<em>U</em>的<code>Leader</code>具有相同的最新的日志任期。那么任期为<em>U</em>的<code>Leader</code>的日志至少和投票者的日志一样长。所以任期为<em>U</em>的<code>Leader</code>的日志将包含投票者所有的日志。这是一个矛盾，因为之前假设的投票者包含被提交的任期为<em>T</em>的日志，而任期为<em>U</em>的<code>Leader</code>不包含。</li>\n<li>否则，任期为<em>U</em>的<code>Leader</code>的最后一条日志的任期号必须大于投票者的最后一条日志的任期号。而且，它比<em>T</em>大，因为投票者的上一个日志任期号至少为<em>T</em>(它包含任期<em>T</em>中的所有已提交的条目)。创建任期为<em>U</em>的<code>Leader</code>的最后一个日志条目的较早的<code>Leader</code>必须在其日志中(通过假设)包含已提交的条目。然后，通过日志匹配属性，任期为<em>U</em>的<code>Leader</code>的日志还必须包含已提交的条目，这是矛盾的。</li>\n<li>这样就证明了矛盾，因此所有任期大于<em>T</em>的<code>Leader</code>都必须包含所有任期为<em>T</em>的被提交的日志。</li>\n<li>日志匹配属性保证未来的<code>Leader</code>还将包含间接提交的日志条目。</li>\n</ol>\n<p>下一篇文章:<a href=\"https://ifican.top/2020/01/06/blog/consensus/raft-relationship/\" target=\"_blank\" rel=\"noopener\">Raft算法之成员关系变化</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>上一篇文章:<a href=\"https://ifican.top/2020/01/04/blog/consensus/raft-election/\" target=\"_blank\" rel=\"noopener\">Raft算法之Leader选举</a><br>&emsp;&emsp;之前说完了Raft算法中的Leader选举过程，本文将在上一篇文章的基础上说明日志复制。</p>\n<h1 id=\"Raft算法之日志复制\"><a href=\"#Raft算法之日志复制\" class=\"headerlink\" title=\"Raft算法之日志复制\"></a>Raft算法之日志复制</h1><p>&emsp;&emsp;先看以下日志所包含的基本内容:</p>\n<ol>\n<li>可以被复制状态机执行的命令</li>\n<li>任期号 :创建该日志时Leader所处的当前任期号</li>\n<li>索引号 :整数，用于标识日志所在的位置</li>\n</ol>\n<p>日志的状态分为两种:未被提交，已被提交(日志为安全的，不会被删除或覆盖)。</p>\n<h3 id=\"1-正常情况\"><a href=\"#1-正常情况\" class=\"headerlink\" title=\"1 正常情况\"></a>1 正常情况</h3><ul>\n<li>当<code>Leader</code>接收到由客户端发送的请求(请求中包含可以被复制状态机执行的命令)时，Leader将会把该请求作为新的内容添加到日志中(任期号为当前<code>Leader</code>所处的任期号，索引号为当前<code>Leader</code>本地存储的日志集合中的日志的最高索引号加1)。<ul>\n<li><code>Leader</code><strong>在当前任期内最多只能创建一个给定索引号的日志(即不可能在一个任期内创建两个以上的具有相同索引的日志条目)</strong></li>\n</ul>\n</li>\n<li>然后将该日志通过<code>AppendEntries RPC</code>消息发送到网络中其他的服务器(以下简称<code>Follower</code>)，从而复制该日志。</li>\n<li>在网络中<code>Follower</code>接收到该日志消息后则会返回复制成功的回复。</li>\n<li>在<code>Leader</code>接收到网络中大部分的<code>Follower</code>的成功复制的回复之后，<code>Leader</code>便认为该日志<strong>可以被提交</strong>。此时<code>Leader</code>将会同时做三件事：</li>\n</ul>\n<ol>\n<li>将该日志应用到<code>Leader</code>本地的复制状态机</li>\n<li>向所有<code>Follower</code>发送消息通知所有接收到该日志的<code>Follower</code>将该日志进行提交，然后应用到各自本地的复制状态机</li>\n<li>将执行结果通知客户端</li>\n</ol>\n<p>&emsp;&emsp;当该日志消息成功在网络中大部分<code>Follower</code>本地的复制状态机执行过后，则可认为该日志<strong>已被提交</strong>。在当前日志被提交的过程中，如果<code>Leader</code>先前的某些日志还没有被提交，则将会一同提交。<br>&emsp;&emsp;而网络中有些<code>Follower</code>可能由于网络状态原因反应缓慢或者崩溃，那么<code>Leader</code>将会无限次地尝试重复发送<code>AppendEntries RPC</code>消息到该<code>Follower</code>。直到成功为止。</p>\n<h4 id=\"1-1-日志的一致性检查\"><a href=\"#1-1-日志的一致性检查\" class=\"headerlink\" title=\"1.1 日志的一致性检查\"></a>1.1 日志的一致性检查</h4><p>&emsp;&emsp;在上面，我们说了<code>Follower</code>在接收到<code>AppendEntries RPC</code>消息后则会返回复制成功的回复。实际上在接收到消息后会首先进行日志的一致性检查(正常情况下<code>Leader</code>与<code>Follower</code>的日志会保持一致，所以一致性检查不会失败),<strong>一致性检查内容如下：</strong></p>\n<ul>\n<li>在<code>Leader</code>创建<code>AppendEntries RPC</code>消息时，消息中将会包含当前日志之前日志条目的任期号与索引号。</li>\n<li><code>Follower</code>在接受到<code>AppendEntries RPC</code>消息后，将会检查之前日志的任期号与索引号是否可以匹配到<ul>\n<li>如果匹配到则说明和<code>Leader</code>之前的日志是保持一致的。</li>\n<li>如果没有匹配则会拒绝<code>AppendEntries RPC</code>消息.</li>\n</ul>\n</li>\n</ul>\n<p>&emsp;&emsp;一致性检查是一个归纳的过程。<strong>正常情况下</strong>，网络中第一条日志一定满足日志的一致性检查，然后第二条日志中包含第一条日志的任期号与索引号，所以只要<code>Leader</code>与<code>Follower</code>的第一条日志保持一致，那么第二条日志也会满足一致性检查，从而之后的每一条日志都会满足一致性检查。</p>\n<p>&emsp;&emsp;从而得出了日志匹配属性:</p>\n<ul>\n<li>如果两个不同的日志实体具有相同的索引和任期号，那么他们存储有相同的命令。</li>\n<li>如果两个不同的日志实体具有相同的索引和任期号，则所有先前条目中的日志都相同。(由一致性检查结果得出)</li>\n</ul>\n<h3 id=\"2-特殊情况\"><a href=\"#2-特殊情况\" class=\"headerlink\" title=\"2 特殊情况\"></a>2 特殊情况</h3><p>&emsp;&emsp;而网络不可能一直处于正常情况。因为<code>Leader</code>或者某个<code>Follower</code>有可能会崩溃，从而导致日志不能一直保持一致。因此存在以下三种情况:</p>\n<ol>\n<li><p><code>Follower</code>缺失当前<code>Leader</code>上存在的日志条目。</p>\n</li>\n<li><p><code>Follower</code>存在当前<code>Leader</code>不存在的日志条目。(比如旧的<code>Leader</code>仅仅将<code>AppendEntries RPC</code>消息发送到一部分<code>Follower</code>就崩溃掉，然后新的当选<code>Leader</code>的服务器恰好是没有收到该<code>AppendEntries RPC</code>消息的服务器)</p>\n</li>\n<li><p>或者<code>Follower</code>即缺失当前<code>Leader</code>上存在的日志条目，也存在当前<code>Leader</code>不存在的日志条目</p>\n<p><img src=\"/img/blog/raft/7.png\" srcset=\"undefined\" alt=\"图1\"></p>\n</li>\n</ol>\n<p>&emsp;&emsp;图中最上方是日志的索引号(1-12),每个方块代表一条日志信息，方块内数字代表该日志所处的任期号。图中当前<code>Leader</code>(图中最上方一行日志代表当前<code>Leader</code>日志)处于任期号为8的时刻。以此图说明以上三种情况存在的原因：</p>\n<ul>\n<li><code>Follower</code> <em>a,b</em>(<code>Follower</code>崩溃没有接收到<code>Leader</code>发送的<code>AppendEntries RPC</code>消息)满足以上说明的第一种情况。</li>\n<li>(<code>Follower</code><em>c</em>在任期为6的时刻，<code>Follower</code><em>d</em>在任期为7的时刻)为<code>Leader</code>,但没有完全完成日志的发送便崩溃了.满足以上说明的第三种情况。</li>\n<li><code>Follower</code><em>e</em>在任期为4的时刻,<code>Follower</code><em>f</em>在任期为2,3的时刻为<code>Leader</code>,,但没有完全完成日志的发送便崩溃了,同时在其他服务器当选<code>Leader</code>时刻也没有接收到新的<code>Leader</code>发送的<code>AppendEntries RPC</code>消息,满足第三种情况。</li>\n</ul>\n<h4 id=\"2-1-日志不一致的解决方案\"><a href=\"#2-1-日志不一致的解决方案\" class=\"headerlink\" title=\"2.1 日志不一致的解决方案\"></a>2.1 日志不一致的解决方案</h4><p>&emsp;&emsp;<code>Leader</code>通过强迫<code>Follower</code>的日志重复自己的日志来处理不一致之处。这意味着<code>Follower</code>日志中的冲突日志将被<code>Leader</code>日志中的条目覆盖。因此<code>Leader</code>必须找到与<code>Follower</code>最开始日志发生冲突的位置,然后删除掉<code>Follower</code>上所有与<code>Leader</code>发生冲突的日志。然后将自己的日志发送给<code>Follower</code>以解决冲突。<br><strong><code>Leader</code>不会删除或覆盖自己本地的日志条目</strong></p>\n<p>&emsp;&emsp;这些步骤从之前说到的日志的一致性检查开始。</p>\n<ul>\n<li><p>当发生日志冲突时，<code>Follower</code>将会拒绝由<code>Leader</code>发送的<code>AppendEntries RPC</code>消息，并返回一个响应消息告知<code>Leader</code>日志发生了冲突。</p>\n</li>\n<li><p><code>Leader</code>为每一个<code>Follower</code>维护一个<code>nextIndex</code>值。该值用于确定需要发送给该<code>Follower</code>的下一条日志的位置索引。(该值在当前服务器成功当选<code>Leader</code>后会重置为本地日志的最后一条索引号+1)</p>\n</li>\n<li><p>当<code>Leader</code>了解到日志发生冲突之后，便递减<code>nextIndex</code>值。并重新发送<code>AppendEntries RPC</code>到该<code>Follower</code>。并不断重复这个过程，一直到<code>Follower</code>接受该消息。</p>\n</li>\n<li><p>一旦<code>Follower</code>接受了<code>AppendEntries RPC</code>消息，<code>Leader</code>则根据<code>nextIndex</code>值可以确定发生冲突的位置，从而强迫<code>Follower</code>的日志重复自己的日志以解决冲突问题。</p>\n<p><img src=\"/img/blog/raft/8.png\" srcset=\"undefined\" alt=\"图2\"></p>\n</li>\n<li><p>情况<em>a</em>: 如图，服务器<em>S1</em>在任期为2的时刻仅将日志<code>&lt;index:2,term:2&gt;</code>发送到了服务器<em>S2</em>便崩溃掉。</p>\n</li>\n<li><p>情况<em>b</em>: 服务器<em>S5</em>在任期为3的时刻当选<code>Leader</code>(<em>S5</em>的计时器率先超时，递增任期号为3因此高于服务器<em>S3,S4</em>，可以当选<code>Leader</code>)，但没来得及发送日志便崩溃掉。</p>\n</li>\n<li><p>情况<em>c</em>: 服务器<em>S1<em>在任期为4的时刻再次当选<code>Leader</code>(</em>S1<em>重启时，任期仍然为2，收到新的<code>Leader</code></em>S5<em>发送的心跳信息后更新任期为3，而在<code>Leader</code></em>S5<em>崩溃后，服务器</em>S1<em>为第一个计时器超时的，因此发起投票，任期更新为4，大于网络中其他服务器任期，成功当选<code>Leader</code>),同时将日志<code>&lt;index:2,term:2&gt;</code>发送到了服务器</em>S2,S3</em>,但还没有通知服务器对日志进行提交便崩溃掉。</p>\n</li>\n<li><p>情况<em>d</em>: 情况(<em>a-&gt;d</em>)如果在任期为2时服务器<em>S1</em>作为<code>Leader</code>崩溃掉，<em>S5</em>在任期为3的时刻当选<code>Leader</code>，由于日志<code>&lt;index:2,term:2&gt;</code>还没有被复制到大部分服务器上，并没有被提交，所以<em>S5</em>可以通过自己的日志<code>&lt;index:2,term:3&gt;</code>覆盖掉日志<code>&lt;index:2,term:2&gt;</code>。</p>\n</li>\n<li><p>情况<em>e</em>: 情况(<em>a-&gt;e</em>)而如果在任期为2时服务器<em>S1<em>作为<code>Leader</code>，并将<code>&lt;index:2,term:2&gt;</code>发送到</em>S2,S3</em>,成功复制到大多数成员服务器上。并且成功提交了该日志，那么即便<em>S1</em>崩溃掉，<em>S5</em>也无法成功当选<code>Leader</code>，因为<em>S5</em>不具备网络中最新的已被提交的日志条目(<strong>这里说明了上一篇文章<a href=\"https://ifican.top/2020/01/04/blog/consensus/raft-election/#%E9%80%89%E4%B8%BE%E9%98%B6%E6%AE%B5-gt-%E6%AD%A3%E5%B8%B8%E8%BF%90%E8%A1%8C%E9%98%B6%E6%AE%B5\" target=\"_blank\" rel=\"noopener\">Raft算法之Leader选举</a>中选举<code>Leader</code>的要求中没有介绍的那一点要求</strong>).</p>\n</li>\n</ul>\n<h4 id=\"2-2-选举Leader的对日志的要求\"><a href=\"#2-2-选举Leader的对日志的要求\" class=\"headerlink\" title=\"2.2 选举Leader的对日志的要求\"></a>2.2 选举<code>Leader</code>的对日志的要求</h4><ul>\n<li>Raft使用投票程序来防止<code>Candidate</code>赢得选举，除非其日志中包含所有已提交的日志条目。</li>\n<li><code>Candidate</code>必须联系集群的大多数才能被选举，这意味着每个提交的条目都必须存在于其中至少一台服务器中。如果<code>Candidate</code>的日志至少与该多数服务器日志中的日志一样最新(以下精确定义了<strong>最新</strong>),则它将保存所有已提交的条目。</li>\n<li>Raft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新的。如果日志中的最后一个条目具有不同的任期，则带有较新任期的日志是最新的。如果日志以相同的任期结尾，则以索引更大的日志为准。</li>\n</ul>\n<p>&emsp;&emsp;<strong>解决方案的优化</strong><br>&emsp;&emsp;在<code>Follower</code>拒绝<code>AppendEntries RPC</code>消息时，可以选择将发生冲突的日志的任期与该任期内的第一条日志索引包含在拒绝消息中返回给<code>Leader</code>，从而使得<code>Leader</code>可以快速定位到发生冲突的位置。有了这些信息，<code>Leader</code>可以递减<code>nextIndex</code>来绕过该任期中所有冲突的条目。每个具有<strong>冲突日志条目所处的任期</strong>都需要一个<code>AppendEntries RPC</code>消息，而不是每个日志条目都需要一个<code>AppendEntries RPC</code>消息。</p>\n<h3 id=\"3-日志复制安全性\"><a href=\"#3-日志复制安全性\" class=\"headerlink\" title=\"3 日志复制安全性\"></a>3 日志复制安全性</h3><p><strong>Raft保证任何时刻这里的每一条属性都成立</strong></p>\n<ul>\n<li><code>Leader</code>只追加特性:<code>Leader</code>从不覆盖或删除它的日志条目，只追加新的。</li>\n<li>日志匹配: 如果两个日志包含的实体具有相同的索引和任期，那么直到给定索引为止，所有条目中的日志都是相同的。</li>\n<li><code>Leader</code>完整性:如果一个日志提示在给定的任期内被提交，那么该条目将出现在所有任期更高的领导者的日志中.</li>\n<li>状态机安全:如果服务器应用一条给定索引的日志实体到它的状态机，那么没有其他服务器可以应用一条不同的日志到相同的索引位置。</li>\n</ul>\n<h4 id=\"3-1-Leader完整性证明\"><a href=\"#3-1-Leader完整性证明\" class=\"headerlink\" title=\"3.1 Leader完整性证明\"></a>3.1 <code>Leader</code>完整性证明</h4><p>&emsp;&emsp;假设<code>Leader</code>完整性不成立，然后证明是矛盾的。<br>&emsp;&emsp;假设任期为<em>T</em>的<code>Leader</code>提交了当前任期的日志条目，但是该日志没有被任期高于<em>T</em>的任期为<em>U</em>的未来的新的<code>Leader</code>所存储。</p>\n<ol>\n<li>被提交任期为<em>T</em>的日志必须不存在于将要选举的任期为<em>U</em>的<code>Leader</code>的复制状态机中(因为<code>Leader</code>从不覆盖或删除它的日志条目)。</li>\n<li>任期为<em>T</em>的<code>Leader</code>将日志复制到集群中的大部分成员本地。并且任期为<em>U</em>的<code>Leader</code>在选举阶段接收到集群中大部分成员的投票，因此至少集群中有一个成员(以下称为投票者)即接收到来自任期为<em>T</em>的<code>Leader</code>发送的日志，也为任期为<em>U</em>的<code>Leader</code>投了票。所以该投票者是证明矛盾的关键所在。</li>\n<li>投票者必须在为任期为<em>U</em>的<code>Leader</code>投票之前将任期为<em>T</em>的<code>Leader</code>的发送的日志提交。不然投票者将会拒绝任期为<em>T</em>的<code>Leader</code>的<code>AppendEntries PRC</code>请求(因为一旦接收到任期为<em>U</em>的<code>Leader</code>投票请求，投票者的任期将会高于<em>T</em>)。</li>\n<li>投票者当为任期为<em>U</em>的<code>Leader</code>投票时，将会一直存储该日志条目。假设在任期为<em>T</em>和<em>U</em>之间的每一个<code>Leader</code>都包含该日志条目(<code>Leader</code>从不删除日志条目，而<code>Follower</code>仅在与<code>Leader</code>冲突时才删除条目)。</li>\n<li>投票者为任期为<em>U</em>的<code>Leader</code>投票，所以任期为<em>U</em>的<code>Leader</code>日志必须至少和投票者的日志一样新。这将导致产生两个矛盾之中的一个矛盾。</li>\n<li>首先，如果投票者和任期为<em>U</em>的<code>Leader</code>具有相同的最新的日志任期。那么任期为<em>U</em>的<code>Leader</code>的日志至少和投票者的日志一样长。所以任期为<em>U</em>的<code>Leader</code>的日志将包含投票者所有的日志。这是一个矛盾，因为之前假设的投票者包含被提交的任期为<em>T</em>的日志，而任期为<em>U</em>的<code>Leader</code>不包含。</li>\n<li>否则，任期为<em>U</em>的<code>Leader</code>的最后一条日志的任期号必须大于投票者的最后一条日志的任期号。而且，它比<em>T</em>大，因为投票者的上一个日志任期号至少为<em>T</em>(它包含任期<em>T</em>中的所有已提交的条目)。创建任期为<em>U</em>的<code>Leader</code>的最后一个日志条目的较早的<code>Leader</code>必须在其日志中(通过假设)包含已提交的条目。然后，通过日志匹配属性，任期为<em>U</em>的<code>Leader</code>的日志还必须包含已提交的条目，这是矛盾的。</li>\n<li>这样就证明了矛盾，因此所有任期大于<em>T</em>的<code>Leader</code>都必须包含所有任期为<em>T</em>的被提交的日志。</li>\n<li>日志匹配属性保证未来的<code>Leader</code>还将包含间接提交的日志条目。</li>\n</ol>\n<p>下一篇文章:<a href=\"https://ifican.top/2020/01/06/blog/consensus/raft-relationship/\" target=\"_blank\" rel=\"noopener\">Raft算法之成员关系变化</a></p>\n"},{"title":"Raft算法之Leader选举","date":"2020-01-04T12:29:26.000Z","_content":"&emsp;&emsp;记录一下对Raft算法的理解，算法的内容比较多，所以准备将算法的全部过程分成四个部分来写。分别是\n\n1. [Raft算法之Leader选举](https://ifican.top/2020/01/04/blog/consensus/raft-election/)\n2. [Raft算法之日志复制](https://ifican.top/2020/01/05/blog/consensus/raft-log/)\n3. [Raft算法之成员关系变化](https://ifican.top/2020/01/06/blog/consensus/raft-relationship/)\n4. [Raft算法之日志压缩](https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/)\n\n该文章为第一部分。\n\n# Raft算法之Leader选举\n## 简单介绍\n&emsp;&emsp;首先需要了解Raft中的一个关键词:`Term`,本文中以下部分简单称为任期。任期通过连续的整数编号表示并且是单调递增的，代表任意长度的一段时间。在网络中所有服务器都有自己的任期编号，在网络中大部分正常运行阶段，所有服务器的任期号都是相同的。\nRaft算法中服务器主要分为三种角色:`Leader`,`Follower`,`Candidate`，并且三种角色相互独立，也就是服务器在同一时间内只可能扮演其中一种角色。\n\n* `Leader`:用于对所有用户的请求进行处理。以及之后要说明的日志的复制等等。\n* `Follower`:不会主动发送消息，只响应来自`Leader`与`Candidate`的请求。\n* `Candidate`:用于选举新的`Leader`。\n\n&emsp;&emsp;在本文介绍的范围内，网络状态分为两种情况:**选举阶段**，**正常运行阶段**。(网络状态还可能会有成员变化阶段，但不在本文范围内，所以暂时先不考虑).\n并且每一个任期都是以选举阶段开始。但不一定以正常运行阶段结束。在某些情况下一个完整的任期可能全部为选举阶段。如下图:\n![任期更新](/img/blog/raft/5.png)\n\n## 选举阶段->正常运行阶段\n&emsp;&emsp;在网络初始化时，网络中所有的服务器都以`Follower`的角色启动。由于`Follower`只被动接收消息。所以全网中所有服务器都处于等待状态。同时每一个服务器都在本地维护一个计时器。\n\n* 计时器的作用很简单，就是判断当前阶段(选举阶段或正常运行阶段)是否超时。而当计时器超时后，任期将会\n\n\n&emsp;&emsp;所以在网络启动后所有服务器等待指定长度的一段时间过去以后。计时器将会超时。这时候计时器超时的服务器将转换自己的角色为`Candidate`。进入选举阶段。而进入选举阶段的`Candidate`将会做以下几件事:\n\n1. 将自己的任期号加1.\n2. 为自己投一票用以选举出新的`Leader`。\n3. 将本地的计时器重置\n4. 发送投票请求到网络中的其他所有的服务器。\n5. 等待下一次的计时器超时\n\n同时选举`Leader`具有以下几点要求:\n\n1. 每个服务器在一个任期内只能投一票，并且使先到者先得(即投票给自己收到的第一个请求投票的，**满足要求**的服务器的请求)\n2. 请求投票的消息中需要带有请求者所处的当前任期号。\n3. 投票者只会投票给任期号大于等于自己当前任期号的服务器。\n4. 关于日志的要求(下一篇文章再介绍)\n\n在选举状态会出现三种结果:\n\n1. 自己成功当选`Leader`\n2. 网络中其他服务器当选`Leader`\n3. 网络中没有服务器当选`Leader`\n\n&emsp;&emsp;当网络中某一个`Candidate`接收到网络中大多数成员的投票后，即可将自己的身份转换为`Leader`。在当选`Leader`后，该服务器将周期性地发送心跳信息(心跳信息包含成功当选`Leader`的服务器的当前任期号)到网络中其他服务器。在网络中其他的服务器收到心跳信息后检查心跳消息中的任期号是否大于等于自己的任期号。如果满足该条件的话`Candidate`将会转换为`Follower`状态，并重置计时器。而如果任期号小于自己的任期号，服务器将拒绝该心跳消息并继续处于`Candidate`状态。\n\n&emsp;&emsp;第三种情况为网络中没有服务器成功当选`Leader`。这种情况在当很多`Follower`同时成为`Candidate`时会发生。因为当角色转换为`Candidate`后将会将选票投给自己。从而导致选票被分散开来，没有`Candidate`可以得到网络中大部分节点的选票。从而没有节点可以成为`Leader`.这种情况下计时器将再次超时，网络状态将从选举阶段进入下一个选举阶段。同时`Candidate`将会再次执行上面说明的几件事。\n\n&emsp;&emsp;Raft算法采用了随机选举超时机制来避免出现这种情况。即当计时器超时后，服务器将随机延迟指定的时间后才进入选举阶段。\n由于随机延迟的原因，将降低服务器在同一时间选举超时的情况，可以有效避免选票分散的情况。\n\n## 正常运行阶段->选举阶段\n\n&emsp;&emsp;当`Leader`成功选举之后，将周期性发送心跳消息到网络中其他服务器。同时其他服务器将转换自己的角色为`Follower`。并且每次收到心跳消息后都会重置自己的计时器，防止超时再次进入选举阶段。\n\n&emsp;&emsp;而如果`Leader`因为特殊情况崩溃时，网络中的其他服务器将不再接收到心跳消息，在等待指定时间后计时器将会超时，从而再次进入选举阶段。\n\n* 而如果`Leader`崩溃时间较短，可以在其他服务器计时器超时之间恢复，并发送心跳消息，网络仍然可以恢复为`Leader`崩溃之前的状态。\n* 如果`Leader`崩溃时间较长，在网络中已有新的`Leader`选举产生后恢复，由于旧的`Leader`任期号将小于新的`Leader`，在旧的`Leader`接收到新的`Leader`发送的心跳消息后则会变为`Follower`状态。\n\n## 总结\n\n三种角色的转换情况:\n\n![角色变化](/img/blog/raft/4.png)\n### `Candidate`\n&emsp;&emsp;服务器角色变为`Candidate`后:\n1. 将自己的任期号加1.\n2. 为自己投一票用以选举出新的`Leader`。\n3. 将本地的计时器重置\n4. 发送投票请求到网络中的其他所有的服务器。\n5. 等待下一次的计时器超时\n\n* 当接收到心跳消息(心跳消息中的任期号大于等于自己的任期号)后，变为`Follower`状态。\n* 计时器超时，再次执行上面的5件事。\n* 当自己接收到大多数选票后，变为`Leader`状态。\n\n### `Follower`\n&emsp;&emsp;服务器角色变为`Follower`后:\n\n* 等待`Leader`或者`Candidate`发送消息给自己。\n    * 如果是心跳消息(心跳消息中的任期号大于等于自己的任期号)，则重置计时器。\n    * 如果是选举消息(选举消息中的任期号大于自己的任期号)，则将自己变为`Candidate`，任期号更新为选举消息中的较大的任期号。重置计时器并返回投票响应信息。\n* 或者网络处于正常运行状态时，如果收到客户端请求，将会将该请求重定向到`Leader`。\n* 如果在指定时间间隔内没有收到心跳消息或者是选举消息，则角色变为`Candidate`。\n\n\n### `Leader`\n\n&emsp;&emsp;服务器角色变为`Leader`后:\n\n* 重置计时器，并周期性发送心跳消息(带有自己的任期号)到网络中其他服务器。\n* 等待客户端请求消息。\n\n下一篇文章:[Raft算法之日志复制](https://ifican.top/2020/01/05/blog/consensus/raft-log/)","source":"_posts/blog/consensus/raft-election.md","raw":"---\ntitle: Raft算法之Leader选举\ndate: 2020-01-04 20:29:26\ntags: \n- Raft\n- algorithm\ncategories:\n- algorithm\n---\n&emsp;&emsp;记录一下对Raft算法的理解，算法的内容比较多，所以准备将算法的全部过程分成四个部分来写。分别是\n\n1. [Raft算法之Leader选举](https://ifican.top/2020/01/04/blog/consensus/raft-election/)\n2. [Raft算法之日志复制](https://ifican.top/2020/01/05/blog/consensus/raft-log/)\n3. [Raft算法之成员关系变化](https://ifican.top/2020/01/06/blog/consensus/raft-relationship/)\n4. [Raft算法之日志压缩](https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/)\n\n该文章为第一部分。\n\n# Raft算法之Leader选举\n## 简单介绍\n&emsp;&emsp;首先需要了解Raft中的一个关键词:`Term`,本文中以下部分简单称为任期。任期通过连续的整数编号表示并且是单调递增的，代表任意长度的一段时间。在网络中所有服务器都有自己的任期编号，在网络中大部分正常运行阶段，所有服务器的任期号都是相同的。\nRaft算法中服务器主要分为三种角色:`Leader`,`Follower`,`Candidate`，并且三种角色相互独立，也就是服务器在同一时间内只可能扮演其中一种角色。\n\n* `Leader`:用于对所有用户的请求进行处理。以及之后要说明的日志的复制等等。\n* `Follower`:不会主动发送消息，只响应来自`Leader`与`Candidate`的请求。\n* `Candidate`:用于选举新的`Leader`。\n\n&emsp;&emsp;在本文介绍的范围内，网络状态分为两种情况:**选举阶段**，**正常运行阶段**。(网络状态还可能会有成员变化阶段，但不在本文范围内，所以暂时先不考虑).\n并且每一个任期都是以选举阶段开始。但不一定以正常运行阶段结束。在某些情况下一个完整的任期可能全部为选举阶段。如下图:\n![任期更新](/img/blog/raft/5.png)\n\n## 选举阶段->正常运行阶段\n&emsp;&emsp;在网络初始化时，网络中所有的服务器都以`Follower`的角色启动。由于`Follower`只被动接收消息。所以全网中所有服务器都处于等待状态。同时每一个服务器都在本地维护一个计时器。\n\n* 计时器的作用很简单，就是判断当前阶段(选举阶段或正常运行阶段)是否超时。而当计时器超时后，任期将会\n\n\n&emsp;&emsp;所以在网络启动后所有服务器等待指定长度的一段时间过去以后。计时器将会超时。这时候计时器超时的服务器将转换自己的角色为`Candidate`。进入选举阶段。而进入选举阶段的`Candidate`将会做以下几件事:\n\n1. 将自己的任期号加1.\n2. 为自己投一票用以选举出新的`Leader`。\n3. 将本地的计时器重置\n4. 发送投票请求到网络中的其他所有的服务器。\n5. 等待下一次的计时器超时\n\n同时选举`Leader`具有以下几点要求:\n\n1. 每个服务器在一个任期内只能投一票，并且使先到者先得(即投票给自己收到的第一个请求投票的，**满足要求**的服务器的请求)\n2. 请求投票的消息中需要带有请求者所处的当前任期号。\n3. 投票者只会投票给任期号大于等于自己当前任期号的服务器。\n4. 关于日志的要求(下一篇文章再介绍)\n\n在选举状态会出现三种结果:\n\n1. 自己成功当选`Leader`\n2. 网络中其他服务器当选`Leader`\n3. 网络中没有服务器当选`Leader`\n\n&emsp;&emsp;当网络中某一个`Candidate`接收到网络中大多数成员的投票后，即可将自己的身份转换为`Leader`。在当选`Leader`后，该服务器将周期性地发送心跳信息(心跳信息包含成功当选`Leader`的服务器的当前任期号)到网络中其他服务器。在网络中其他的服务器收到心跳信息后检查心跳消息中的任期号是否大于等于自己的任期号。如果满足该条件的话`Candidate`将会转换为`Follower`状态，并重置计时器。而如果任期号小于自己的任期号，服务器将拒绝该心跳消息并继续处于`Candidate`状态。\n\n&emsp;&emsp;第三种情况为网络中没有服务器成功当选`Leader`。这种情况在当很多`Follower`同时成为`Candidate`时会发生。因为当角色转换为`Candidate`后将会将选票投给自己。从而导致选票被分散开来，没有`Candidate`可以得到网络中大部分节点的选票。从而没有节点可以成为`Leader`.这种情况下计时器将再次超时，网络状态将从选举阶段进入下一个选举阶段。同时`Candidate`将会再次执行上面说明的几件事。\n\n&emsp;&emsp;Raft算法采用了随机选举超时机制来避免出现这种情况。即当计时器超时后，服务器将随机延迟指定的时间后才进入选举阶段。\n由于随机延迟的原因，将降低服务器在同一时间选举超时的情况，可以有效避免选票分散的情况。\n\n## 正常运行阶段->选举阶段\n\n&emsp;&emsp;当`Leader`成功选举之后，将周期性发送心跳消息到网络中其他服务器。同时其他服务器将转换自己的角色为`Follower`。并且每次收到心跳消息后都会重置自己的计时器，防止超时再次进入选举阶段。\n\n&emsp;&emsp;而如果`Leader`因为特殊情况崩溃时，网络中的其他服务器将不再接收到心跳消息，在等待指定时间后计时器将会超时，从而再次进入选举阶段。\n\n* 而如果`Leader`崩溃时间较短，可以在其他服务器计时器超时之间恢复，并发送心跳消息，网络仍然可以恢复为`Leader`崩溃之前的状态。\n* 如果`Leader`崩溃时间较长，在网络中已有新的`Leader`选举产生后恢复，由于旧的`Leader`任期号将小于新的`Leader`，在旧的`Leader`接收到新的`Leader`发送的心跳消息后则会变为`Follower`状态。\n\n## 总结\n\n三种角色的转换情况:\n\n![角色变化](/img/blog/raft/4.png)\n### `Candidate`\n&emsp;&emsp;服务器角色变为`Candidate`后:\n1. 将自己的任期号加1.\n2. 为自己投一票用以选举出新的`Leader`。\n3. 将本地的计时器重置\n4. 发送投票请求到网络中的其他所有的服务器。\n5. 等待下一次的计时器超时\n\n* 当接收到心跳消息(心跳消息中的任期号大于等于自己的任期号)后，变为`Follower`状态。\n* 计时器超时，再次执行上面的5件事。\n* 当自己接收到大多数选票后，变为`Leader`状态。\n\n### `Follower`\n&emsp;&emsp;服务器角色变为`Follower`后:\n\n* 等待`Leader`或者`Candidate`发送消息给自己。\n    * 如果是心跳消息(心跳消息中的任期号大于等于自己的任期号)，则重置计时器。\n    * 如果是选举消息(选举消息中的任期号大于自己的任期号)，则将自己变为`Candidate`，任期号更新为选举消息中的较大的任期号。重置计时器并返回投票响应信息。\n* 或者网络处于正常运行状态时，如果收到客户端请求，将会将该请求重定向到`Leader`。\n* 如果在指定时间间隔内没有收到心跳消息或者是选举消息，则角色变为`Candidate`。\n\n\n### `Leader`\n\n&emsp;&emsp;服务器角色变为`Leader`后:\n\n* 重置计时器，并周期性发送心跳消息(带有自己的任期号)到网络中其他服务器。\n* 等待客户端请求消息。\n\n下一篇文章:[Raft算法之日志复制](https://ifican.top/2020/01/05/blog/consensus/raft-log/)","slug":"blog/consensus/raft-election","published":1,"updated":"2020-05-11T03:44:37.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyfk000ek0vqagvq6cch","content":"<p>&emsp;&emsp;记录一下对Raft算法的理解，算法的内容比较多，所以准备将算法的全部过程分成四个部分来写。分别是</p>\n<ol>\n<li><a href=\"https://ifican.top/2020/01/04/blog/consensus/raft-election/\" target=\"_blank\" rel=\"noopener\">Raft算法之Leader选举</a></li>\n<li><a href=\"https://ifican.top/2020/01/05/blog/consensus/raft-log/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志复制</a></li>\n<li><a href=\"https://ifican.top/2020/01/06/blog/consensus/raft-relationship/\" target=\"_blank\" rel=\"noopener\">Raft算法之成员关系变化</a></li>\n<li><a href=\"https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志压缩</a></li>\n</ol>\n<p>该文章为第一部分。</p>\n<h1 id=\"Raft算法之Leader选举\"><a href=\"#Raft算法之Leader选举\" class=\"headerlink\" title=\"Raft算法之Leader选举\"></a>Raft算法之Leader选举</h1><h2 id=\"简单介绍\"><a href=\"#简单介绍\" class=\"headerlink\" title=\"简单介绍\"></a>简单介绍</h2><p>&emsp;&emsp;首先需要了解Raft中的一个关键词:<code>Term</code>,本文中以下部分简单称为任期。任期通过连续的整数编号表示并且是单调递增的，代表任意长度的一段时间。在网络中所有服务器都有自己的任期编号，在网络中大部分正常运行阶段，所有服务器的任期号都是相同的。<br>Raft算法中服务器主要分为三种角色:<code>Leader</code>,<code>Follower</code>,<code>Candidate</code>，并且三种角色相互独立，也就是服务器在同一时间内只可能扮演其中一种角色。</p>\n<ul>\n<li><code>Leader</code>:用于对所有用户的请求进行处理。以及之后要说明的日志的复制等等。</li>\n<li><code>Follower</code>:不会主动发送消息，只响应来自<code>Leader</code>与<code>Candidate</code>的请求。</li>\n<li><code>Candidate</code>:用于选举新的<code>Leader</code>。</li>\n</ul>\n<p>&emsp;&emsp;在本文介绍的范围内，网络状态分为两种情况:<strong>选举阶段</strong>，<strong>正常运行阶段</strong>。(网络状态还可能会有成员变化阶段，但不在本文范围内，所以暂时先不考虑).<br>并且每一个任期都是以选举阶段开始。但不一定以正常运行阶段结束。在某些情况下一个完整的任期可能全部为选举阶段。如下图:<br><img src=\"/img/blog/raft/5.png\" srcset=\"undefined\" alt=\"任期更新\"></p>\n<h2 id=\"选举阶段-gt-正常运行阶段\"><a href=\"#选举阶段-gt-正常运行阶段\" class=\"headerlink\" title=\"选举阶段-&gt;正常运行阶段\"></a>选举阶段-&gt;正常运行阶段</h2><p>&emsp;&emsp;在网络初始化时，网络中所有的服务器都以<code>Follower</code>的角色启动。由于<code>Follower</code>只被动接收消息。所以全网中所有服务器都处于等待状态。同时每一个服务器都在本地维护一个计时器。</p>\n<ul>\n<li>计时器的作用很简单，就是判断当前阶段(选举阶段或正常运行阶段)是否超时。而当计时器超时后，任期将会</li>\n</ul>\n<p>&emsp;&emsp;所以在网络启动后所有服务器等待指定长度的一段时间过去以后。计时器将会超时。这时候计时器超时的服务器将转换自己的角色为<code>Candidate</code>。进入选举阶段。而进入选举阶段的<code>Candidate</code>将会做以下几件事:</p>\n<ol>\n<li>将自己的任期号加1.</li>\n<li>为自己投一票用以选举出新的<code>Leader</code>。</li>\n<li>将本地的计时器重置</li>\n<li>发送投票请求到网络中的其他所有的服务器。</li>\n<li>等待下一次的计时器超时</li>\n</ol>\n<p>同时选举<code>Leader</code>具有以下几点要求:</p>\n<ol>\n<li>每个服务器在一个任期内只能投一票，并且使先到者先得(即投票给自己收到的第一个请求投票的，<strong>满足要求</strong>的服务器的请求)</li>\n<li>请求投票的消息中需要带有请求者所处的当前任期号。</li>\n<li>投票者只会投票给任期号大于等于自己当前任期号的服务器。</li>\n<li>关于日志的要求(下一篇文章再介绍)</li>\n</ol>\n<p>在选举状态会出现三种结果:</p>\n<ol>\n<li>自己成功当选<code>Leader</code></li>\n<li>网络中其他服务器当选<code>Leader</code></li>\n<li>网络中没有服务器当选<code>Leader</code></li>\n</ol>\n<p>&emsp;&emsp;当网络中某一个<code>Candidate</code>接收到网络中大多数成员的投票后，即可将自己的身份转换为<code>Leader</code>。在当选<code>Leader</code>后，该服务器将周期性地发送心跳信息(心跳信息包含成功当选<code>Leader</code>的服务器的当前任期号)到网络中其他服务器。在网络中其他的服务器收到心跳信息后检查心跳消息中的任期号是否大于等于自己的任期号。如果满足该条件的话<code>Candidate</code>将会转换为<code>Follower</code>状态，并重置计时器。而如果任期号小于自己的任期号，服务器将拒绝该心跳消息并继续处于<code>Candidate</code>状态。</p>\n<p>&emsp;&emsp;第三种情况为网络中没有服务器成功当选<code>Leader</code>。这种情况在当很多<code>Follower</code>同时成为<code>Candidate</code>时会发生。因为当角色转换为<code>Candidate</code>后将会将选票投给自己。从而导致选票被分散开来，没有<code>Candidate</code>可以得到网络中大部分节点的选票。从而没有节点可以成为<code>Leader</code>.这种情况下计时器将再次超时，网络状态将从选举阶段进入下一个选举阶段。同时<code>Candidate</code>将会再次执行上面说明的几件事。</p>\n<p>&emsp;&emsp;Raft算法采用了随机选举超时机制来避免出现这种情况。即当计时器超时后，服务器将随机延迟指定的时间后才进入选举阶段。<br>由于随机延迟的原因，将降低服务器在同一时间选举超时的情况，可以有效避免选票分散的情况。</p>\n<h2 id=\"正常运行阶段-gt-选举阶段\"><a href=\"#正常运行阶段-gt-选举阶段\" class=\"headerlink\" title=\"正常运行阶段-&gt;选举阶段\"></a>正常运行阶段-&gt;选举阶段</h2><p>&emsp;&emsp;当<code>Leader</code>成功选举之后，将周期性发送心跳消息到网络中其他服务器。同时其他服务器将转换自己的角色为<code>Follower</code>。并且每次收到心跳消息后都会重置自己的计时器，防止超时再次进入选举阶段。</p>\n<p>&emsp;&emsp;而如果<code>Leader</code>因为特殊情况崩溃时，网络中的其他服务器将不再接收到心跳消息，在等待指定时间后计时器将会超时，从而再次进入选举阶段。</p>\n<ul>\n<li>而如果<code>Leader</code>崩溃时间较短，可以在其他服务器计时器超时之间恢复，并发送心跳消息，网络仍然可以恢复为<code>Leader</code>崩溃之前的状态。</li>\n<li>如果<code>Leader</code>崩溃时间较长，在网络中已有新的<code>Leader</code>选举产生后恢复，由于旧的<code>Leader</code>任期号将小于新的<code>Leader</code>，在旧的<code>Leader</code>接收到新的<code>Leader</code>发送的心跳消息后则会变为<code>Follower</code>状态。</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>三种角色的转换情况:</p>\n<p><img src=\"/img/blog/raft/4.png\" srcset=\"undefined\" alt=\"角色变化\"></p>\n<h3 id=\"Candidate\"><a href=\"#Candidate\" class=\"headerlink\" title=\"Candidate\"></a><code>Candidate</code></h3><p>&emsp;&emsp;服务器角色变为<code>Candidate</code>后:</p>\n<ol>\n<li>将自己的任期号加1.</li>\n<li>为自己投一票用以选举出新的<code>Leader</code>。</li>\n<li>将本地的计时器重置</li>\n<li>发送投票请求到网络中的其他所有的服务器。</li>\n<li>等待下一次的计时器超时</li>\n</ol>\n<ul>\n<li>当接收到心跳消息(心跳消息中的任期号大于等于自己的任期号)后，变为<code>Follower</code>状态。</li>\n<li>计时器超时，再次执行上面的5件事。</li>\n<li>当自己接收到大多数选票后，变为<code>Leader</code>状态。</li>\n</ul>\n<h3 id=\"Follower\"><a href=\"#Follower\" class=\"headerlink\" title=\"Follower\"></a><code>Follower</code></h3><p>&emsp;&emsp;服务器角色变为<code>Follower</code>后:</p>\n<ul>\n<li>等待<code>Leader</code>或者<code>Candidate</code>发送消息给自己。<ul>\n<li>如果是心跳消息(心跳消息中的任期号大于等于自己的任期号)，则重置计时器。</li>\n<li>如果是选举消息(选举消息中的任期号大于自己的任期号)，则将自己变为<code>Candidate</code>，任期号更新为选举消息中的较大的任期号。重置计时器并返回投票响应信息。</li>\n</ul>\n</li>\n<li>或者网络处于正常运行状态时，如果收到客户端请求，将会将该请求重定向到<code>Leader</code>。</li>\n<li>如果在指定时间间隔内没有收到心跳消息或者是选举消息，则角色变为<code>Candidate</code>。</li>\n</ul>\n<h3 id=\"Leader\"><a href=\"#Leader\" class=\"headerlink\" title=\"Leader\"></a><code>Leader</code></h3><p>&emsp;&emsp;服务器角色变为<code>Leader</code>后:</p>\n<ul>\n<li>重置计时器，并周期性发送心跳消息(带有自己的任期号)到网络中其他服务器。</li>\n<li>等待客户端请求消息。</li>\n</ul>\n<p>下一篇文章:<a href=\"https://ifican.top/2020/01/05/blog/consensus/raft-log/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志复制</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>&emsp;&emsp;记录一下对Raft算法的理解，算法的内容比较多，所以准备将算法的全部过程分成四个部分来写。分别是</p>\n<ol>\n<li><a href=\"https://ifican.top/2020/01/04/blog/consensus/raft-election/\" target=\"_blank\" rel=\"noopener\">Raft算法之Leader选举</a></li>\n<li><a href=\"https://ifican.top/2020/01/05/blog/consensus/raft-log/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志复制</a></li>\n<li><a href=\"https://ifican.top/2020/01/06/blog/consensus/raft-relationship/\" target=\"_blank\" rel=\"noopener\">Raft算法之成员关系变化</a></li>\n<li><a href=\"https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志压缩</a></li>\n</ol>\n<p>该文章为第一部分。</p>\n<h1 id=\"Raft算法之Leader选举\"><a href=\"#Raft算法之Leader选举\" class=\"headerlink\" title=\"Raft算法之Leader选举\"></a>Raft算法之Leader选举</h1><h2 id=\"简单介绍\"><a href=\"#简单介绍\" class=\"headerlink\" title=\"简单介绍\"></a>简单介绍</h2><p>&emsp;&emsp;首先需要了解Raft中的一个关键词:<code>Term</code>,本文中以下部分简单称为任期。任期通过连续的整数编号表示并且是单调递增的，代表任意长度的一段时间。在网络中所有服务器都有自己的任期编号，在网络中大部分正常运行阶段，所有服务器的任期号都是相同的。<br>Raft算法中服务器主要分为三种角色:<code>Leader</code>,<code>Follower</code>,<code>Candidate</code>，并且三种角色相互独立，也就是服务器在同一时间内只可能扮演其中一种角色。</p>\n<ul>\n<li><code>Leader</code>:用于对所有用户的请求进行处理。以及之后要说明的日志的复制等等。</li>\n<li><code>Follower</code>:不会主动发送消息，只响应来自<code>Leader</code>与<code>Candidate</code>的请求。</li>\n<li><code>Candidate</code>:用于选举新的<code>Leader</code>。</li>\n</ul>\n<p>&emsp;&emsp;在本文介绍的范围内，网络状态分为两种情况:<strong>选举阶段</strong>，<strong>正常运行阶段</strong>。(网络状态还可能会有成员变化阶段，但不在本文范围内，所以暂时先不考虑).<br>并且每一个任期都是以选举阶段开始。但不一定以正常运行阶段结束。在某些情况下一个完整的任期可能全部为选举阶段。如下图:<br><img src=\"/img/blog/raft/5.png\" srcset=\"undefined\" alt=\"任期更新\"></p>\n<h2 id=\"选举阶段-gt-正常运行阶段\"><a href=\"#选举阶段-gt-正常运行阶段\" class=\"headerlink\" title=\"选举阶段-&gt;正常运行阶段\"></a>选举阶段-&gt;正常运行阶段</h2><p>&emsp;&emsp;在网络初始化时，网络中所有的服务器都以<code>Follower</code>的角色启动。由于<code>Follower</code>只被动接收消息。所以全网中所有服务器都处于等待状态。同时每一个服务器都在本地维护一个计时器。</p>\n<ul>\n<li>计时器的作用很简单，就是判断当前阶段(选举阶段或正常运行阶段)是否超时。而当计时器超时后，任期将会</li>\n</ul>\n<p>&emsp;&emsp;所以在网络启动后所有服务器等待指定长度的一段时间过去以后。计时器将会超时。这时候计时器超时的服务器将转换自己的角色为<code>Candidate</code>。进入选举阶段。而进入选举阶段的<code>Candidate</code>将会做以下几件事:</p>\n<ol>\n<li>将自己的任期号加1.</li>\n<li>为自己投一票用以选举出新的<code>Leader</code>。</li>\n<li>将本地的计时器重置</li>\n<li>发送投票请求到网络中的其他所有的服务器。</li>\n<li>等待下一次的计时器超时</li>\n</ol>\n<p>同时选举<code>Leader</code>具有以下几点要求:</p>\n<ol>\n<li>每个服务器在一个任期内只能投一票，并且使先到者先得(即投票给自己收到的第一个请求投票的，<strong>满足要求</strong>的服务器的请求)</li>\n<li>请求投票的消息中需要带有请求者所处的当前任期号。</li>\n<li>投票者只会投票给任期号大于等于自己当前任期号的服务器。</li>\n<li>关于日志的要求(下一篇文章再介绍)</li>\n</ol>\n<p>在选举状态会出现三种结果:</p>\n<ol>\n<li>自己成功当选<code>Leader</code></li>\n<li>网络中其他服务器当选<code>Leader</code></li>\n<li>网络中没有服务器当选<code>Leader</code></li>\n</ol>\n<p>&emsp;&emsp;当网络中某一个<code>Candidate</code>接收到网络中大多数成员的投票后，即可将自己的身份转换为<code>Leader</code>。在当选<code>Leader</code>后，该服务器将周期性地发送心跳信息(心跳信息包含成功当选<code>Leader</code>的服务器的当前任期号)到网络中其他服务器。在网络中其他的服务器收到心跳信息后检查心跳消息中的任期号是否大于等于自己的任期号。如果满足该条件的话<code>Candidate</code>将会转换为<code>Follower</code>状态，并重置计时器。而如果任期号小于自己的任期号，服务器将拒绝该心跳消息并继续处于<code>Candidate</code>状态。</p>\n<p>&emsp;&emsp;第三种情况为网络中没有服务器成功当选<code>Leader</code>。这种情况在当很多<code>Follower</code>同时成为<code>Candidate</code>时会发生。因为当角色转换为<code>Candidate</code>后将会将选票投给自己。从而导致选票被分散开来，没有<code>Candidate</code>可以得到网络中大部分节点的选票。从而没有节点可以成为<code>Leader</code>.这种情况下计时器将再次超时，网络状态将从选举阶段进入下一个选举阶段。同时<code>Candidate</code>将会再次执行上面说明的几件事。</p>\n<p>&emsp;&emsp;Raft算法采用了随机选举超时机制来避免出现这种情况。即当计时器超时后，服务器将随机延迟指定的时间后才进入选举阶段。<br>由于随机延迟的原因，将降低服务器在同一时间选举超时的情况，可以有效避免选票分散的情况。</p>\n<h2 id=\"正常运行阶段-gt-选举阶段\"><a href=\"#正常运行阶段-gt-选举阶段\" class=\"headerlink\" title=\"正常运行阶段-&gt;选举阶段\"></a>正常运行阶段-&gt;选举阶段</h2><p>&emsp;&emsp;当<code>Leader</code>成功选举之后，将周期性发送心跳消息到网络中其他服务器。同时其他服务器将转换自己的角色为<code>Follower</code>。并且每次收到心跳消息后都会重置自己的计时器，防止超时再次进入选举阶段。</p>\n<p>&emsp;&emsp;而如果<code>Leader</code>因为特殊情况崩溃时，网络中的其他服务器将不再接收到心跳消息，在等待指定时间后计时器将会超时，从而再次进入选举阶段。</p>\n<ul>\n<li>而如果<code>Leader</code>崩溃时间较短，可以在其他服务器计时器超时之间恢复，并发送心跳消息，网络仍然可以恢复为<code>Leader</code>崩溃之前的状态。</li>\n<li>如果<code>Leader</code>崩溃时间较长，在网络中已有新的<code>Leader</code>选举产生后恢复，由于旧的<code>Leader</code>任期号将小于新的<code>Leader</code>，在旧的<code>Leader</code>接收到新的<code>Leader</code>发送的心跳消息后则会变为<code>Follower</code>状态。</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>三种角色的转换情况:</p>\n<p><img src=\"/img/blog/raft/4.png\" srcset=\"undefined\" alt=\"角色变化\"></p>\n<h3 id=\"Candidate\"><a href=\"#Candidate\" class=\"headerlink\" title=\"Candidate\"></a><code>Candidate</code></h3><p>&emsp;&emsp;服务器角色变为<code>Candidate</code>后:</p>\n<ol>\n<li>将自己的任期号加1.</li>\n<li>为自己投一票用以选举出新的<code>Leader</code>。</li>\n<li>将本地的计时器重置</li>\n<li>发送投票请求到网络中的其他所有的服务器。</li>\n<li>等待下一次的计时器超时</li>\n</ol>\n<ul>\n<li>当接收到心跳消息(心跳消息中的任期号大于等于自己的任期号)后，变为<code>Follower</code>状态。</li>\n<li>计时器超时，再次执行上面的5件事。</li>\n<li>当自己接收到大多数选票后，变为<code>Leader</code>状态。</li>\n</ul>\n<h3 id=\"Follower\"><a href=\"#Follower\" class=\"headerlink\" title=\"Follower\"></a><code>Follower</code></h3><p>&emsp;&emsp;服务器角色变为<code>Follower</code>后:</p>\n<ul>\n<li>等待<code>Leader</code>或者<code>Candidate</code>发送消息给自己。<ul>\n<li>如果是心跳消息(心跳消息中的任期号大于等于自己的任期号)，则重置计时器。</li>\n<li>如果是选举消息(选举消息中的任期号大于自己的任期号)，则将自己变为<code>Candidate</code>，任期号更新为选举消息中的较大的任期号。重置计时器并返回投票响应信息。</li>\n</ul>\n</li>\n<li>或者网络处于正常运行状态时，如果收到客户端请求，将会将该请求重定向到<code>Leader</code>。</li>\n<li>如果在指定时间间隔内没有收到心跳消息或者是选举消息，则角色变为<code>Candidate</code>。</li>\n</ul>\n<h3 id=\"Leader\"><a href=\"#Leader\" class=\"headerlink\" title=\"Leader\"></a><code>Leader</code></h3><p>&emsp;&emsp;服务器角色变为<code>Leader</code>后:</p>\n<ul>\n<li>重置计时器，并周期性发送心跳消息(带有自己的任期号)到网络中其他服务器。</li>\n<li>等待客户端请求消息。</li>\n</ul>\n<p>下一篇文章:<a href=\"https://ifican.top/2020/01/05/blog/consensus/raft-log/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志复制</a></p>\n"},{"title":"PBFT之视图更新","date":"2020-01-10T06:48:21.000Z","_content":"# PBFT之视图更新\n## 1 检查点\n为了保证安全，除非消息中的请求至少已由*f+1*个无故障的副本节点执行，并且可以从视图更新中向其他节点证明，否则消息必须保留在副本节点日志中。另外，如果某些副本丢失了所有非故障副本节点丢弃的消息，则需要通过转移全部或部分服务状态来使消息更新。因此，副本也需要一些证明状态正确的证据。\n在执行完所有操作之后生成这些证据的代价是昂贵的。相反，当执行的请求序列号可被某个常数(例如100)整除时，这些证据会定期生成。由执行完这些请求后根据状态机的状态产生的称为检查点，稳定的检查点就是该证据。\n副本节点维护一些服务状态的逻辑拷贝:最新的稳定的检查点，零个或多个不稳定的检查点，还有当前状态。写时复制技术可以用于减少存储额外状态拷贝的空间负载。\n作为证据的正确的检查点由以下步骤产生:\n1. 当一个副本节点*i*生成了检查点，将会多播一个消息`<CHECKPOINT,n,d,i>_s`到其他副本节点。\n    * *n*为状态中最新的被执行的请求序列号\n    * *d*为状态的摘要\n2. 每一个副本节点都会收集检查点信息，直到收集到*2f+1*个来自不同副本节点签名的具有相同序列号以及相同摘要*d*的消息\n3. 将它们添加到日志中。这*2f+1*个消息即是证明该检查点正确的证明。\n具有证明的检查点即是稳定的检查点，副本节点将会抛弃所有的序列号小于或等于日志中所有`pre-prepare,prepare,commit`消息中序列号的消息。同时也会抛弃之前所有的检查点以及检查点消息。\n检查点协议用于推进低阈值标记和高阈值标记(限制可以接收多少消息)，低阈值*h*等于最新的稳定的检查点的序列号。高阈值*H=h+k*，其中*k*是足够大的数以至于副本节点不用一直等待检查点变为稳定的。例如，如果检查点包括接近100个请求，那么*k*可能为200.\n\n## 2 视图更新过程\n视图更新协议可以在系统中的*primary*虚假或崩溃时为系统提供活性。超时则会触发视图更新防止副本节点无限期地等待接收请求去执行。\n\n* 如果副本接收到一个有效的请求并且还没有被执行，副本将会处于等待状态。\n* 副本在接受到请求后并且计时器还没有处于运行时则会启动计时器。\n* 当不再等待执行请求时，它将停止计时器，但如果此时正在等待执行其他请求，则将计时器重新启动。\n\n1. 如果节点*i*在视图为*v*时计时器超时，该节点将会开始视图更新并且将视图变为*v+1*。停止接收信息(除了检查点，视图更新和新视图消息外)，并多播`<VIEW-CHANGE,v+1,n,C,P,i>_s`消息到所有副本节点。\n\n    * *n*为节点*i*知道的最新的稳定检查点*s*的序列号。\n    * *C*为一组*2f+1*个有效的检查点消息，它们证明检查点*s*的正确性\n    * *P*是在节点*i*处的每个序列号高于*n*的请求消息*m*的`prepared`集合*Pm*的集合。\n    * 每一个集合*Pm*包含*2f*个匹配的有效的`pre-prepare`消息(不包括与客户端相关的信息)，由不同的副本节点签名的具有相同视图，序列号，*m*的摘要的`prepare`消息。\n\n2. 当*primary*在视图*v+1*时接收到*2f*个来自其他副本的视图为*v+1*的有效的`VIEW-CHANGE`消息后，将会多播一个`<NEW-VIEW,v+1,V,O>_s`消息到所有其他副本节点。\n\n    * *V*为一个包含接收到的由*primary*发送的视图更改为*v+1*的有效的`VIEW-CHANGE`消息的集合。\n    * *O*为一个*pre-prepare*消息的集合(不包含请求消息*m*),*O*通过以下内容计算:\n        * *primary*决定关于*V*中最新的稳定的检查点的最小序列号*min-s*以及在*V*中的的*prepare*消息中最大的序列号*max-s*.\n        * *primary*对每一个在*min-s*与*max-s*之间序列号*n*创建新的视图为*v+1*的*pre-prepare*消息。这里有两种情况:\n            * 在*P*中至少有一个由一些在*V*中序列号为*n*的`VIEW-CHANGE`消息组成的集合。\n            * 没有这样的集合\n\n    在第一种情况下，*primary*创建新的消息`<PRE-PREPARE,v+1,n,d>_s`，这里的*d*为在*V*中序列号为*n*的高视图编号的`pre-prepare`消息的请求摘要。\n    在第二种情况下，*primary*将会创建新的`<PRE-PREPARE,v+1,n,d_null>_s`消息，这里的*d_null*为指定的空的请求的摘要。空的请求在协议中和其他请求一样，只不过不进行执行。\n\n3. 接下来*primary*添加*O*中的消息到日志中。如果*min-s*大于他的最新稳定的检查点的序列号，*primary*还在其日志中插入序列号为*min-s*的检查点的稳定性证明，并丢弃日志中的信息。这时候视图进入了*v+1*，在该点以后可以接收视图为*v+1*的消息。\n4. 副本接收到关于视图为*v+1*的`NEW-VIEW`消息后，如果签名为正确的，且包含的内容对于视图是*v+1*是有效的，如果集合*O*是正确的(*O*的正确性通过与*primary*创建*O*时相同的计算过程进行验证).那么将该信息(正如*primary*描述的)添加到日志中。，多播在*O*中的`prepare`消息到其他所有副本中，并添加这些`prepare`消息到日志中，视图进入*v+1*。\n5. 此后，正如三阶段提交协议过程，副本节点重新执行在*min-s*与*max-s*中的协议，但是避免重新执行客户端的请求(通过使用存储的关于最新的发送到每一个客户端的回复信息)。\n\n副本可能会缺失某些请求消息*m*或者是稳定的检查点(由于没有发送`NEW-VIEW`消息)。这些缺失的信息可以从其他副本节点获取。\n例如，副本*i*可以从一个副本节点(它的检查点信息已在*V*中确认正确性)获取缺失的检查点状态*s*.由于这些*f+1*个副本节点是正确的，副本节点*i*将总是获取*s*或者是最新的稳定检查点。通过对状态进行分区并为每个分区加上被修改的它的最后一个请求的序列号标记，可以避免发送整个检查点。要使副本为最新版本，只需将副本发送到过期的分区，而不是整个检查点。","source":"_posts/blog/consensus/pbft-view.md","raw":"---\ntitle: PBFT之视图更新\ndate: 2020-01-10 14:48:21\ntags: \n- Pbft\n- algorithm\ncategories:\n- algorithm\n---\n# PBFT之视图更新\n## 1 检查点\n为了保证安全，除非消息中的请求至少已由*f+1*个无故障的副本节点执行，并且可以从视图更新中向其他节点证明，否则消息必须保留在副本节点日志中。另外，如果某些副本丢失了所有非故障副本节点丢弃的消息，则需要通过转移全部或部分服务状态来使消息更新。因此，副本也需要一些证明状态正确的证据。\n在执行完所有操作之后生成这些证据的代价是昂贵的。相反，当执行的请求序列号可被某个常数(例如100)整除时，这些证据会定期生成。由执行完这些请求后根据状态机的状态产生的称为检查点，稳定的检查点就是该证据。\n副本节点维护一些服务状态的逻辑拷贝:最新的稳定的检查点，零个或多个不稳定的检查点，还有当前状态。写时复制技术可以用于减少存储额外状态拷贝的空间负载。\n作为证据的正确的检查点由以下步骤产生:\n1. 当一个副本节点*i*生成了检查点，将会多播一个消息`<CHECKPOINT,n,d,i>_s`到其他副本节点。\n    * *n*为状态中最新的被执行的请求序列号\n    * *d*为状态的摘要\n2. 每一个副本节点都会收集检查点信息，直到收集到*2f+1*个来自不同副本节点签名的具有相同序列号以及相同摘要*d*的消息\n3. 将它们添加到日志中。这*2f+1*个消息即是证明该检查点正确的证明。\n具有证明的检查点即是稳定的检查点，副本节点将会抛弃所有的序列号小于或等于日志中所有`pre-prepare,prepare,commit`消息中序列号的消息。同时也会抛弃之前所有的检查点以及检查点消息。\n检查点协议用于推进低阈值标记和高阈值标记(限制可以接收多少消息)，低阈值*h*等于最新的稳定的检查点的序列号。高阈值*H=h+k*，其中*k*是足够大的数以至于副本节点不用一直等待检查点变为稳定的。例如，如果检查点包括接近100个请求，那么*k*可能为200.\n\n## 2 视图更新过程\n视图更新协议可以在系统中的*primary*虚假或崩溃时为系统提供活性。超时则会触发视图更新防止副本节点无限期地等待接收请求去执行。\n\n* 如果副本接收到一个有效的请求并且还没有被执行，副本将会处于等待状态。\n* 副本在接受到请求后并且计时器还没有处于运行时则会启动计时器。\n* 当不再等待执行请求时，它将停止计时器，但如果此时正在等待执行其他请求，则将计时器重新启动。\n\n1. 如果节点*i*在视图为*v*时计时器超时，该节点将会开始视图更新并且将视图变为*v+1*。停止接收信息(除了检查点，视图更新和新视图消息外)，并多播`<VIEW-CHANGE,v+1,n,C,P,i>_s`消息到所有副本节点。\n\n    * *n*为节点*i*知道的最新的稳定检查点*s*的序列号。\n    * *C*为一组*2f+1*个有效的检查点消息，它们证明检查点*s*的正确性\n    * *P*是在节点*i*处的每个序列号高于*n*的请求消息*m*的`prepared`集合*Pm*的集合。\n    * 每一个集合*Pm*包含*2f*个匹配的有效的`pre-prepare`消息(不包括与客户端相关的信息)，由不同的副本节点签名的具有相同视图，序列号，*m*的摘要的`prepare`消息。\n\n2. 当*primary*在视图*v+1*时接收到*2f*个来自其他副本的视图为*v+1*的有效的`VIEW-CHANGE`消息后，将会多播一个`<NEW-VIEW,v+1,V,O>_s`消息到所有其他副本节点。\n\n    * *V*为一个包含接收到的由*primary*发送的视图更改为*v+1*的有效的`VIEW-CHANGE`消息的集合。\n    * *O*为一个*pre-prepare*消息的集合(不包含请求消息*m*),*O*通过以下内容计算:\n        * *primary*决定关于*V*中最新的稳定的检查点的最小序列号*min-s*以及在*V*中的的*prepare*消息中最大的序列号*max-s*.\n        * *primary*对每一个在*min-s*与*max-s*之间序列号*n*创建新的视图为*v+1*的*pre-prepare*消息。这里有两种情况:\n            * 在*P*中至少有一个由一些在*V*中序列号为*n*的`VIEW-CHANGE`消息组成的集合。\n            * 没有这样的集合\n\n    在第一种情况下，*primary*创建新的消息`<PRE-PREPARE,v+1,n,d>_s`，这里的*d*为在*V*中序列号为*n*的高视图编号的`pre-prepare`消息的请求摘要。\n    在第二种情况下，*primary*将会创建新的`<PRE-PREPARE,v+1,n,d_null>_s`消息，这里的*d_null*为指定的空的请求的摘要。空的请求在协议中和其他请求一样，只不过不进行执行。\n\n3. 接下来*primary*添加*O*中的消息到日志中。如果*min-s*大于他的最新稳定的检查点的序列号，*primary*还在其日志中插入序列号为*min-s*的检查点的稳定性证明，并丢弃日志中的信息。这时候视图进入了*v+1*，在该点以后可以接收视图为*v+1*的消息。\n4. 副本接收到关于视图为*v+1*的`NEW-VIEW`消息后，如果签名为正确的，且包含的内容对于视图是*v+1*是有效的，如果集合*O*是正确的(*O*的正确性通过与*primary*创建*O*时相同的计算过程进行验证).那么将该信息(正如*primary*描述的)添加到日志中。，多播在*O*中的`prepare`消息到其他所有副本中，并添加这些`prepare`消息到日志中，视图进入*v+1*。\n5. 此后，正如三阶段提交协议过程，副本节点重新执行在*min-s*与*max-s*中的协议，但是避免重新执行客户端的请求(通过使用存储的关于最新的发送到每一个客户端的回复信息)。\n\n副本可能会缺失某些请求消息*m*或者是稳定的检查点(由于没有发送`NEW-VIEW`消息)。这些缺失的信息可以从其他副本节点获取。\n例如，副本*i*可以从一个副本节点(它的检查点信息已在*V*中确认正确性)获取缺失的检查点状态*s*.由于这些*f+1*个副本节点是正确的，副本节点*i*将总是获取*s*或者是最新的稳定检查点。通过对状态进行分区并为每个分区加上被修改的它的最后一个请求的序列号标记，可以避免发送整个检查点。要使副本为最新版本，只需将副本发送到过期的分区，而不是整个检查点。","slug":"blog/consensus/pbft-view","published":1,"updated":"2020-05-11T03:44:37.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyfn000gk0vq4fb083tc","content":"<h1 id=\"PBFT之视图更新\"><a href=\"#PBFT之视图更新\" class=\"headerlink\" title=\"PBFT之视图更新\"></a>PBFT之视图更新</h1><h2 id=\"1-检查点\"><a href=\"#1-检查点\" class=\"headerlink\" title=\"1 检查点\"></a>1 检查点</h2><p>为了保证安全，除非消息中的请求至少已由<em>f+1</em>个无故障的副本节点执行，并且可以从视图更新中向其他节点证明，否则消息必须保留在副本节点日志中。另外，如果某些副本丢失了所有非故障副本节点丢弃的消息，则需要通过转移全部或部分服务状态来使消息更新。因此，副本也需要一些证明状态正确的证据。<br>在执行完所有操作之后生成这些证据的代价是昂贵的。相反，当执行的请求序列号可被某个常数(例如100)整除时，这些证据会定期生成。由执行完这些请求后根据状态机的状态产生的称为检查点，稳定的检查点就是该证据。<br>副本节点维护一些服务状态的逻辑拷贝:最新的稳定的检查点，零个或多个不稳定的检查点，还有当前状态。写时复制技术可以用于减少存储额外状态拷贝的空间负载。<br>作为证据的正确的检查点由以下步骤产生:</p>\n<ol>\n<li>当一个副本节点<em>i</em>生成了检查点，将会多播一个消息<code>&lt;CHECKPOINT,n,d,i&gt;_s</code>到其他副本节点。<ul>\n<li><em>n</em>为状态中最新的被执行的请求序列号</li>\n<li><em>d</em>为状态的摘要</li>\n</ul>\n</li>\n<li>每一个副本节点都会收集检查点信息，直到收集到<em>2f+1</em>个来自不同副本节点签名的具有相同序列号以及相同摘要<em>d</em>的消息</li>\n<li>将它们添加到日志中。这<em>2f+1</em>个消息即是证明该检查点正确的证明。<br>具有证明的检查点即是稳定的检查点，副本节点将会抛弃所有的序列号小于或等于日志中所有<code>pre-prepare,prepare,commit</code>消息中序列号的消息。同时也会抛弃之前所有的检查点以及检查点消息。<br>检查点协议用于推进低阈值标记和高阈值标记(限制可以接收多少消息)，低阈值<em>h</em>等于最新的稳定的检查点的序列号。高阈值<em>H=h+k</em>，其中<em>k</em>是足够大的数以至于副本节点不用一直等待检查点变为稳定的。例如，如果检查点包括接近100个请求，那么<em>k</em>可能为200.</li>\n</ol>\n<h2 id=\"2-视图更新过程\"><a href=\"#2-视图更新过程\" class=\"headerlink\" title=\"2 视图更新过程\"></a>2 视图更新过程</h2><p>视图更新协议可以在系统中的<em>primary</em>虚假或崩溃时为系统提供活性。超时则会触发视图更新防止副本节点无限期地等待接收请求去执行。</p>\n<ul>\n<li>如果副本接收到一个有效的请求并且还没有被执行，副本将会处于等待状态。</li>\n<li>副本在接受到请求后并且计时器还没有处于运行时则会启动计时器。</li>\n<li>当不再等待执行请求时，它将停止计时器，但如果此时正在等待执行其他请求，则将计时器重新启动。</li>\n</ul>\n<ol>\n<li><p>如果节点<em>i</em>在视图为<em>v</em>时计时器超时，该节点将会开始视图更新并且将视图变为<em>v+1</em>。停止接收信息(除了检查点，视图更新和新视图消息外)，并多播<code>&lt;VIEW-CHANGE,v+1,n,C,P,i&gt;_s</code>消息到所有副本节点。</p>\n<ul>\n<li><em>n</em>为节点<em>i</em>知道的最新的稳定检查点<em>s</em>的序列号。</li>\n<li><em>C</em>为一组<em>2f+1</em>个有效的检查点消息，它们证明检查点<em>s</em>的正确性</li>\n<li><em>P</em>是在节点<em>i</em>处的每个序列号高于<em>n</em>的请求消息<em>m</em>的<code>prepared</code>集合<em>Pm</em>的集合。</li>\n<li>每一个集合<em>Pm</em>包含<em>2f</em>个匹配的有效的<code>pre-prepare</code>消息(不包括与客户端相关的信息)，由不同的副本节点签名的具有相同视图，序列号，<em>m</em>的摘要的<code>prepare</code>消息。</li>\n</ul>\n</li>\n<li><p>当<em>primary</em>在视图<em>v+1</em>时接收到<em>2f</em>个来自其他副本的视图为<em>v+1</em>的有效的<code>VIEW-CHANGE</code>消息后，将会多播一个<code>&lt;NEW-VIEW,v+1,V,O&gt;_s</code>消息到所有其他副本节点。</p>\n<ul>\n<li><p><em>V</em>为一个包含接收到的由<em>primary</em>发送的视图更改为<em>v+1</em>的有效的<code>VIEW-CHANGE</code>消息的集合。</p>\n</li>\n<li><p><em>O</em>为一个<em>pre-prepare<em>消息的集合(不包含请求消息</em>m</em>),<em>O</em>通过以下内容计算:</p>\n<ul>\n<li><em>primary<em>决定关于</em>V<em>中最新的稳定的检查点的最小序列号</em>min-s<em>以及在</em>V<em>中的的</em>prepare<em>消息中最大的序列号</em>max-s</em>.</li>\n<li><em>primary</em>对每一个在<em>min-s</em>与<em>max-s</em>之间序列号<em>n</em>创建新的视图为<em>v+1</em>的<em>pre-prepare</em>消息。这里有两种情况:<ul>\n<li>在<em>P</em>中至少有一个由一些在<em>V</em>中序列号为<em>n</em>的<code>VIEW-CHANGE</code>消息组成的集合。</li>\n<li>没有这样的集合</li>\n</ul>\n</li>\n</ul>\n<p>在第一种情况下，<em>primary</em>创建新的消息<code>&lt;PRE-PREPARE,v+1,n,d&gt;_s</code>，这里的<em>d</em>为在<em>V</em>中序列号为<em>n</em>的高视图编号的<code>pre-prepare</code>消息的请求摘要。<br>在第二种情况下，<em>primary</em>将会创建新的<code>&lt;PRE-PREPARE,v+1,n,d_null&gt;_s</code>消息，这里的<em>d_null</em>为指定的空的请求的摘要。空的请求在协议中和其他请求一样，只不过不进行执行。</p>\n</li>\n</ul>\n</li>\n<li><p>接下来<em>primary</em>添加<em>O</em>中的消息到日志中。如果<em>min-s</em>大于他的最新稳定的检查点的序列号，<em>primary</em>还在其日志中插入序列号为<em>min-s</em>的检查点的稳定性证明，并丢弃日志中的信息。这时候视图进入了<em>v+1</em>，在该点以后可以接收视图为<em>v+1</em>的消息。</p>\n</li>\n<li><p>副本接收到关于视图为<em>v+1</em>的<code>NEW-VIEW</code>消息后，如果签名为正确的，且包含的内容对于视图是<em>v+1</em>是有效的，如果集合<em>O</em>是正确的(<em>O</em>的正确性通过与<em>primary</em>创建<em>O</em>时相同的计算过程进行验证).那么将该信息(正如<em>primary</em>描述的)添加到日志中。，多播在<em>O</em>中的<code>prepare</code>消息到其他所有副本中，并添加这些<code>prepare</code>消息到日志中，视图进入<em>v+1</em>。</p>\n</li>\n<li><p>此后，正如三阶段提交协议过程，副本节点重新执行在<em>min-s</em>与<em>max-s</em>中的协议，但是避免重新执行客户端的请求(通过使用存储的关于最新的发送到每一个客户端的回复信息)。</p>\n</li>\n</ol>\n<p>副本可能会缺失某些请求消息<em>m</em>或者是稳定的检查点(由于没有发送<code>NEW-VIEW</code>消息)。这些缺失的信息可以从其他副本节点获取。<br>例如，副本<em>i</em>可以从一个副本节点(它的检查点信息已在<em>V</em>中确认正确性)获取缺失的检查点状态<em>s</em>.由于这些<em>f+1</em>个副本节点是正确的，副本节点<em>i</em>将总是获取<em>s</em>或者是最新的稳定检查点。通过对状态进行分区并为每个分区加上被修改的它的最后一个请求的序列号标记，可以避免发送整个检查点。要使副本为最新版本，只需将副本发送到过期的分区，而不是整个检查点。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"PBFT之视图更新\"><a href=\"#PBFT之视图更新\" class=\"headerlink\" title=\"PBFT之视图更新\"></a>PBFT之视图更新</h1><h2 id=\"1-检查点\"><a href=\"#1-检查点\" class=\"headerlink\" title=\"1 检查点\"></a>1 检查点</h2><p>为了保证安全，除非消息中的请求至少已由<em>f+1</em>个无故障的副本节点执行，并且可以从视图更新中向其他节点证明，否则消息必须保留在副本节点日志中。另外，如果某些副本丢失了所有非故障副本节点丢弃的消息，则需要通过转移全部或部分服务状态来使消息更新。因此，副本也需要一些证明状态正确的证据。<br>在执行完所有操作之后生成这些证据的代价是昂贵的。相反，当执行的请求序列号可被某个常数(例如100)整除时，这些证据会定期生成。由执行完这些请求后根据状态机的状态产生的称为检查点，稳定的检查点就是该证据。<br>副本节点维护一些服务状态的逻辑拷贝:最新的稳定的检查点，零个或多个不稳定的检查点，还有当前状态。写时复制技术可以用于减少存储额外状态拷贝的空间负载。<br>作为证据的正确的检查点由以下步骤产生:</p>\n<ol>\n<li>当一个副本节点<em>i</em>生成了检查点，将会多播一个消息<code>&lt;CHECKPOINT,n,d,i&gt;_s</code>到其他副本节点。<ul>\n<li><em>n</em>为状态中最新的被执行的请求序列号</li>\n<li><em>d</em>为状态的摘要</li>\n</ul>\n</li>\n<li>每一个副本节点都会收集检查点信息，直到收集到<em>2f+1</em>个来自不同副本节点签名的具有相同序列号以及相同摘要<em>d</em>的消息</li>\n<li>将它们添加到日志中。这<em>2f+1</em>个消息即是证明该检查点正确的证明。<br>具有证明的检查点即是稳定的检查点，副本节点将会抛弃所有的序列号小于或等于日志中所有<code>pre-prepare,prepare,commit</code>消息中序列号的消息。同时也会抛弃之前所有的检查点以及检查点消息。<br>检查点协议用于推进低阈值标记和高阈值标记(限制可以接收多少消息)，低阈值<em>h</em>等于最新的稳定的检查点的序列号。高阈值<em>H=h+k</em>，其中<em>k</em>是足够大的数以至于副本节点不用一直等待检查点变为稳定的。例如，如果检查点包括接近100个请求，那么<em>k</em>可能为200.</li>\n</ol>\n<h2 id=\"2-视图更新过程\"><a href=\"#2-视图更新过程\" class=\"headerlink\" title=\"2 视图更新过程\"></a>2 视图更新过程</h2><p>视图更新协议可以在系统中的<em>primary</em>虚假或崩溃时为系统提供活性。超时则会触发视图更新防止副本节点无限期地等待接收请求去执行。</p>\n<ul>\n<li>如果副本接收到一个有效的请求并且还没有被执行，副本将会处于等待状态。</li>\n<li>副本在接受到请求后并且计时器还没有处于运行时则会启动计时器。</li>\n<li>当不再等待执行请求时，它将停止计时器，但如果此时正在等待执行其他请求，则将计时器重新启动。</li>\n</ul>\n<ol>\n<li><p>如果节点<em>i</em>在视图为<em>v</em>时计时器超时，该节点将会开始视图更新并且将视图变为<em>v+1</em>。停止接收信息(除了检查点，视图更新和新视图消息外)，并多播<code>&lt;VIEW-CHANGE,v+1,n,C,P,i&gt;_s</code>消息到所有副本节点。</p>\n<ul>\n<li><em>n</em>为节点<em>i</em>知道的最新的稳定检查点<em>s</em>的序列号。</li>\n<li><em>C</em>为一组<em>2f+1</em>个有效的检查点消息，它们证明检查点<em>s</em>的正确性</li>\n<li><em>P</em>是在节点<em>i</em>处的每个序列号高于<em>n</em>的请求消息<em>m</em>的<code>prepared</code>集合<em>Pm</em>的集合。</li>\n<li>每一个集合<em>Pm</em>包含<em>2f</em>个匹配的有效的<code>pre-prepare</code>消息(不包括与客户端相关的信息)，由不同的副本节点签名的具有相同视图，序列号，<em>m</em>的摘要的<code>prepare</code>消息。</li>\n</ul>\n</li>\n<li><p>当<em>primary</em>在视图<em>v+1</em>时接收到<em>2f</em>个来自其他副本的视图为<em>v+1</em>的有效的<code>VIEW-CHANGE</code>消息后，将会多播一个<code>&lt;NEW-VIEW,v+1,V,O&gt;_s</code>消息到所有其他副本节点。</p>\n<ul>\n<li><p><em>V</em>为一个包含接收到的由<em>primary</em>发送的视图更改为<em>v+1</em>的有效的<code>VIEW-CHANGE</code>消息的集合。</p>\n</li>\n<li><p><em>O</em>为一个<em>pre-prepare<em>消息的集合(不包含请求消息</em>m</em>),<em>O</em>通过以下内容计算:</p>\n<ul>\n<li><em>primary<em>决定关于</em>V<em>中最新的稳定的检查点的最小序列号</em>min-s<em>以及在</em>V<em>中的的</em>prepare<em>消息中最大的序列号</em>max-s</em>.</li>\n<li><em>primary</em>对每一个在<em>min-s</em>与<em>max-s</em>之间序列号<em>n</em>创建新的视图为<em>v+1</em>的<em>pre-prepare</em>消息。这里有两种情况:<ul>\n<li>在<em>P</em>中至少有一个由一些在<em>V</em>中序列号为<em>n</em>的<code>VIEW-CHANGE</code>消息组成的集合。</li>\n<li>没有这样的集合</li>\n</ul>\n</li>\n</ul>\n<p>在第一种情况下，<em>primary</em>创建新的消息<code>&lt;PRE-PREPARE,v+1,n,d&gt;_s</code>，这里的<em>d</em>为在<em>V</em>中序列号为<em>n</em>的高视图编号的<code>pre-prepare</code>消息的请求摘要。<br>在第二种情况下，<em>primary</em>将会创建新的<code>&lt;PRE-PREPARE,v+1,n,d_null&gt;_s</code>消息，这里的<em>d_null</em>为指定的空的请求的摘要。空的请求在协议中和其他请求一样，只不过不进行执行。</p>\n</li>\n</ul>\n</li>\n<li><p>接下来<em>primary</em>添加<em>O</em>中的消息到日志中。如果<em>min-s</em>大于他的最新稳定的检查点的序列号，<em>primary</em>还在其日志中插入序列号为<em>min-s</em>的检查点的稳定性证明，并丢弃日志中的信息。这时候视图进入了<em>v+1</em>，在该点以后可以接收视图为<em>v+1</em>的消息。</p>\n</li>\n<li><p>副本接收到关于视图为<em>v+1</em>的<code>NEW-VIEW</code>消息后，如果签名为正确的，且包含的内容对于视图是<em>v+1</em>是有效的，如果集合<em>O</em>是正确的(<em>O</em>的正确性通过与<em>primary</em>创建<em>O</em>时相同的计算过程进行验证).那么将该信息(正如<em>primary</em>描述的)添加到日志中。，多播在<em>O</em>中的<code>prepare</code>消息到其他所有副本中，并添加这些<code>prepare</code>消息到日志中，视图进入<em>v+1</em>。</p>\n</li>\n<li><p>此后，正如三阶段提交协议过程，副本节点重新执行在<em>min-s</em>与<em>max-s</em>中的协议，但是避免重新执行客户端的请求(通过使用存储的关于最新的发送到每一个客户端的回复信息)。</p>\n</li>\n</ol>\n<p>副本可能会缺失某些请求消息<em>m</em>或者是稳定的检查点(由于没有发送<code>NEW-VIEW</code>消息)。这些缺失的信息可以从其他副本节点获取。<br>例如，副本<em>i</em>可以从一个副本节点(它的检查点信息已在<em>V</em>中确认正确性)获取缺失的检查点状态<em>s</em>.由于这些<em>f+1</em>个副本节点是正确的，副本节点<em>i</em>将总是获取<em>s</em>或者是最新的稳定检查点。通过对状态进行分区并为每个分区加上被修改的它的最后一个请求的序列号标记，可以避免发送整个检查点。要使副本为最新版本，只需将副本发送到过期的分区，而不是整个检查点。</p>\n"},{"title":"Raft算法之日志压缩","date":"2020-01-07T07:43:46.000Z","_content":"\n# Raft算法之日志压缩\n上一篇文章:[Raft算法之成员关系变化](https://ifican.top/2020/01/06/blog/consensus/raft-relationship/)\n\n最后的一部分是关于服务器日志压缩的，因为随着运行时间的增增长，日志信息也会变得越来越多，占有更多的空间。因此Raft采取了日志压缩的方法解决该问题，即将当前整个系统状态写入稳定存储的快照，然后该时间点之前的日志就可以丢弃掉，从而释放存储空间。\n\n## 1 快照结构\n![图](/img/blog/raft/12.png)\n从图中可见，快照包括以下几个部分内容:\n\n* lastIncludedIndex: 表明快照中最后一条日志的索引值。也就是说日志一直压缩到该索引值的位置。该值以前连续若干个索引值的日志被压缩为快照，而该值以后的日志则不在快照中。\n* lastIncludedTerm:表明快照中最后一条日志所在的任期值。\n* state machine state:复制状态机的当前状态。\n\n集群中每一个服务器都可以独立地进行拍摄快照(只对已提交的日志进行快照的拍摄)，其中`lastIncludedIndex`与`lastIncludedTerm`值的存在时为了通过之前讲到的在日志复制中需要做的一致性检查。当服务器完成了该快照的写入之后，就可以将从快照中最后一条日志一直到先前所有的日志删除。\n\n## 2 快照的发送\n\n正常情况下，`Leader`的日志将会与`Follower`保持一致，但并不是所有情况都处于正常情况下，有时候可能因为`Follower`的反应缓慢或崩溃造成与`Leader`的日志不一致。所以有时候需要`Leader`将快照信息发送给`Follower`。快照信息是通过一个称为`InstallSnapshot`的RPC消息发送的，该消息的结构如下:\n\n\n|**InstallSnapshot RPC**  |**由Leader调用并按顺序发送打包的快照到Follower。**  |\n| --- | --- |\n|**参数：**  |  |\n|term  |Leader的任期  |\n|leaderId  |用以Follower可以重定向客户端的请求到Leader  |\n|lastIncludedIndex  |快照将替换直到该索引的所有日志条目  |\n|lastIncludedTerm  |快照中最后一条日志所在的任期值  |\n|offset  |快照文件的偏移量,简单来说就是该快照代表多少数量的连续个日志实体  |\n|data[]  |从offset开始，快照内的数据数组|\n|done  |如果这是最后一个快照则为true，说明该快照之后的日志暂时不需要拍摄快照  |\n\n快照的发送会出现以下几种情况:\n\n1. `Follower`的日志信息不包括快照中的日志信息，即缺少日志。\n2. `Follower`的日志信息与快照中的日志信息发生冲突。\n3. `Follower`的日志信息要多于快照中的日志信息。\n\n至于前两种情况，`Follower`采取直接使用快照内容替代掉自己的日志。\n\n* `Follower`只含有部分快照中的日志信息，那么直接删掉然后使用快照取代。\n* `Follower`具有更多的日志信息的情况下，即`Follower`含有大于接收到的快照中的最后一条日志信息的索引的日志信息。那么直接使用快照代替快照中所包含的日志信息，至于快照之后的日志信息仍然保留。\n\n\n### 2.1 疑问\n为什么不像日志一样仅由`Leader`拍摄快照然后发送给`Follower`，而是允许每一个服务器独立生成快照信息呢？\n很简单的答案，为了减少带宽使用，以及资源的浪费。因为正常情况下`Follower`具有生成快照的所有信息，在自己本地直接生成快照所需要消耗的资源要远远小于通过网络发送所需要的资源。另外也是降低`Leader`设计的复杂。因为如果仅由`Leader`生成快照的话，`Leader`则需要在向`Follower`发送日志的同时，还要兼顾快照的发送。\n\n### 2.2 存在的问题\n\n* 还有另外两个问题会影响每个快照。首先，服务器必须决定何时进行快照。如果服务器过于频繁地进行快照，则会浪费磁盘带宽和能量。如果快照太不频繁，则有耗尽其存储容量的风险，并且会增加重新启动期间加载日志所需的时间。一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果此大小设置为明显大于快照的预期大小，则用于快照的磁盘带宽开销将很小。\n* 第二个问题是写快照可能要花费大量时间，拍摄快照会延迟正常操作。解决方案是使用写时复制技术，以便可以接受新的更新而不会影响快照的写入。例如，使用功能性数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如，Linux上的fork）来创建整个状态机的内存快照。","source":"_posts/blog/consensus/raft-snapshot.md","raw":"---\ntitle: Raft算法之日志压缩\ndate: 2020-01-07 15:43:46\ntags: \n- Raft\n- algorithm\ncategories:\n- algorithm\n---\n\n# Raft算法之日志压缩\n上一篇文章:[Raft算法之成员关系变化](https://ifican.top/2020/01/06/blog/consensus/raft-relationship/)\n\n最后的一部分是关于服务器日志压缩的，因为随着运行时间的增增长，日志信息也会变得越来越多，占有更多的空间。因此Raft采取了日志压缩的方法解决该问题，即将当前整个系统状态写入稳定存储的快照，然后该时间点之前的日志就可以丢弃掉，从而释放存储空间。\n\n## 1 快照结构\n![图](/img/blog/raft/12.png)\n从图中可见，快照包括以下几个部分内容:\n\n* lastIncludedIndex: 表明快照中最后一条日志的索引值。也就是说日志一直压缩到该索引值的位置。该值以前连续若干个索引值的日志被压缩为快照，而该值以后的日志则不在快照中。\n* lastIncludedTerm:表明快照中最后一条日志所在的任期值。\n* state machine state:复制状态机的当前状态。\n\n集群中每一个服务器都可以独立地进行拍摄快照(只对已提交的日志进行快照的拍摄)，其中`lastIncludedIndex`与`lastIncludedTerm`值的存在时为了通过之前讲到的在日志复制中需要做的一致性检查。当服务器完成了该快照的写入之后，就可以将从快照中最后一条日志一直到先前所有的日志删除。\n\n## 2 快照的发送\n\n正常情况下，`Leader`的日志将会与`Follower`保持一致，但并不是所有情况都处于正常情况下，有时候可能因为`Follower`的反应缓慢或崩溃造成与`Leader`的日志不一致。所以有时候需要`Leader`将快照信息发送给`Follower`。快照信息是通过一个称为`InstallSnapshot`的RPC消息发送的，该消息的结构如下:\n\n\n|**InstallSnapshot RPC**  |**由Leader调用并按顺序发送打包的快照到Follower。**  |\n| --- | --- |\n|**参数：**  |  |\n|term  |Leader的任期  |\n|leaderId  |用以Follower可以重定向客户端的请求到Leader  |\n|lastIncludedIndex  |快照将替换直到该索引的所有日志条目  |\n|lastIncludedTerm  |快照中最后一条日志所在的任期值  |\n|offset  |快照文件的偏移量,简单来说就是该快照代表多少数量的连续个日志实体  |\n|data[]  |从offset开始，快照内的数据数组|\n|done  |如果这是最后一个快照则为true，说明该快照之后的日志暂时不需要拍摄快照  |\n\n快照的发送会出现以下几种情况:\n\n1. `Follower`的日志信息不包括快照中的日志信息，即缺少日志。\n2. `Follower`的日志信息与快照中的日志信息发生冲突。\n3. `Follower`的日志信息要多于快照中的日志信息。\n\n至于前两种情况，`Follower`采取直接使用快照内容替代掉自己的日志。\n\n* `Follower`只含有部分快照中的日志信息，那么直接删掉然后使用快照取代。\n* `Follower`具有更多的日志信息的情况下，即`Follower`含有大于接收到的快照中的最后一条日志信息的索引的日志信息。那么直接使用快照代替快照中所包含的日志信息，至于快照之后的日志信息仍然保留。\n\n\n### 2.1 疑问\n为什么不像日志一样仅由`Leader`拍摄快照然后发送给`Follower`，而是允许每一个服务器独立生成快照信息呢？\n很简单的答案，为了减少带宽使用，以及资源的浪费。因为正常情况下`Follower`具有生成快照的所有信息，在自己本地直接生成快照所需要消耗的资源要远远小于通过网络发送所需要的资源。另外也是降低`Leader`设计的复杂。因为如果仅由`Leader`生成快照的话，`Leader`则需要在向`Follower`发送日志的同时，还要兼顾快照的发送。\n\n### 2.2 存在的问题\n\n* 还有另外两个问题会影响每个快照。首先，服务器必须决定何时进行快照。如果服务器过于频繁地进行快照，则会浪费磁盘带宽和能量。如果快照太不频繁，则有耗尽其存储容量的风险，并且会增加重新启动期间加载日志所需的时间。一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果此大小设置为明显大于快照的预期大小，则用于快照的磁盘带宽开销将很小。\n* 第二个问题是写快照可能要花费大量时间，拍摄快照会延迟正常操作。解决方案是使用写时复制技术，以便可以接受新的更新而不会影响快照的写入。例如，使用功能性数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如，Linux上的fork）来创建整个状态机的内存快照。","slug":"blog/consensus/raft-snapshot","published":1,"updated":"2020-05-11T03:44:37.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyfw000jk0vq8gsyhvkh","content":"<h1 id=\"Raft算法之日志压缩\"><a href=\"#Raft算法之日志压缩\" class=\"headerlink\" title=\"Raft算法之日志压缩\"></a>Raft算法之日志压缩</h1><p>上一篇文章:<a href=\"https://ifican.top/2020/01/06/blog/consensus/raft-relationship/\" target=\"_blank\" rel=\"noopener\">Raft算法之成员关系变化</a></p>\n<p>最后的一部分是关于服务器日志压缩的，因为随着运行时间的增增长，日志信息也会变得越来越多，占有更多的空间。因此Raft采取了日志压缩的方法解决该问题，即将当前整个系统状态写入稳定存储的快照，然后该时间点之前的日志就可以丢弃掉，从而释放存储空间。</p>\n<h2 id=\"1-快照结构\"><a href=\"#1-快照结构\" class=\"headerlink\" title=\"1 快照结构\"></a>1 快照结构</h2><p><img src=\"/img/blog/raft/12.png\" srcset=\"undefined\" alt=\"图\"><br>从图中可见，快照包括以下几个部分内容:</p>\n<ul>\n<li>lastIncludedIndex: 表明快照中最后一条日志的索引值。也就是说日志一直压缩到该索引值的位置。该值以前连续若干个索引值的日志被压缩为快照，而该值以后的日志则不在快照中。</li>\n<li>lastIncludedTerm:表明快照中最后一条日志所在的任期值。</li>\n<li>state machine state:复制状态机的当前状态。</li>\n</ul>\n<p>集群中每一个服务器都可以独立地进行拍摄快照(只对已提交的日志进行快照的拍摄)，其中<code>lastIncludedIndex</code>与<code>lastIncludedTerm</code>值的存在时为了通过之前讲到的在日志复制中需要做的一致性检查。当服务器完成了该快照的写入之后，就可以将从快照中最后一条日志一直到先前所有的日志删除。</p>\n<h2 id=\"2-快照的发送\"><a href=\"#2-快照的发送\" class=\"headerlink\" title=\"2 快照的发送\"></a>2 快照的发送</h2><p>正常情况下，<code>Leader</code>的日志将会与<code>Follower</code>保持一致，但并不是所有情况都处于正常情况下，有时候可能因为<code>Follower</code>的反应缓慢或崩溃造成与<code>Leader</code>的日志不一致。所以有时候需要<code>Leader</code>将快照信息发送给<code>Follower</code>。快照信息是通过一个称为<code>InstallSnapshot</code>的RPC消息发送的，该消息的结构如下:</p>\n<table>\n<thead>\n<tr>\n<th><strong>InstallSnapshot RPC</strong></th>\n<th><strong>由Leader调用并按顺序发送打包的快照到Follower。</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>参数：</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>Leader的任期</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>用以Follower可以重定向客户端的请求到Leader</td>\n</tr>\n<tr>\n<td>lastIncludedIndex</td>\n<td>快照将替换直到该索引的所有日志条目</td>\n</tr>\n<tr>\n<td>lastIncludedTerm</td>\n<td>快照中最后一条日志所在的任期值</td>\n</tr>\n<tr>\n<td>offset</td>\n<td>快照文件的偏移量,简单来说就是该快照代表多少数量的连续个日志实体</td>\n</tr>\n<tr>\n<td>data[]</td>\n<td>从offset开始，快照内的数据数组</td>\n</tr>\n<tr>\n<td>done</td>\n<td>如果这是最后一个快照则为true，说明该快照之后的日志暂时不需要拍摄快照</td>\n</tr>\n</tbody></table>\n<p>快照的发送会出现以下几种情况:</p>\n<ol>\n<li><code>Follower</code>的日志信息不包括快照中的日志信息，即缺少日志。</li>\n<li><code>Follower</code>的日志信息与快照中的日志信息发生冲突。</li>\n<li><code>Follower</code>的日志信息要多于快照中的日志信息。</li>\n</ol>\n<p>至于前两种情况，<code>Follower</code>采取直接使用快照内容替代掉自己的日志。</p>\n<ul>\n<li><code>Follower</code>只含有部分快照中的日志信息，那么直接删掉然后使用快照取代。</li>\n<li><code>Follower</code>具有更多的日志信息的情况下，即<code>Follower</code>含有大于接收到的快照中的最后一条日志信息的索引的日志信息。那么直接使用快照代替快照中所包含的日志信息，至于快照之后的日志信息仍然保留。</li>\n</ul>\n<h3 id=\"2-1-疑问\"><a href=\"#2-1-疑问\" class=\"headerlink\" title=\"2.1 疑问\"></a>2.1 疑问</h3><p>为什么不像日志一样仅由<code>Leader</code>拍摄快照然后发送给<code>Follower</code>，而是允许每一个服务器独立生成快照信息呢？<br>很简单的答案，为了减少带宽使用，以及资源的浪费。因为正常情况下<code>Follower</code>具有生成快照的所有信息，在自己本地直接生成快照所需要消耗的资源要远远小于通过网络发送所需要的资源。另外也是降低<code>Leader</code>设计的复杂。因为如果仅由<code>Leader</code>生成快照的话，<code>Leader</code>则需要在向<code>Follower</code>发送日志的同时，还要兼顾快照的发送。</p>\n<h3 id=\"2-2-存在的问题\"><a href=\"#2-2-存在的问题\" class=\"headerlink\" title=\"2.2 存在的问题\"></a>2.2 存在的问题</h3><ul>\n<li>还有另外两个问题会影响每个快照。首先，服务器必须决定何时进行快照。如果服务器过于频繁地进行快照，则会浪费磁盘带宽和能量。如果快照太不频繁，则有耗尽其存储容量的风险，并且会增加重新启动期间加载日志所需的时间。一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果此大小设置为明显大于快照的预期大小，则用于快照的磁盘带宽开销将很小。</li>\n<li>第二个问题是写快照可能要花费大量时间，拍摄快照会延迟正常操作。解决方案是使用写时复制技术，以便可以接受新的更新而不会影响快照的写入。例如，使用功能性数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如，Linux上的fork）来创建整个状态机的内存快照。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Raft算法之日志压缩\"><a href=\"#Raft算法之日志压缩\" class=\"headerlink\" title=\"Raft算法之日志压缩\"></a>Raft算法之日志压缩</h1><p>上一篇文章:<a href=\"https://ifican.top/2020/01/06/blog/consensus/raft-relationship/\" target=\"_blank\" rel=\"noopener\">Raft算法之成员关系变化</a></p>\n<p>最后的一部分是关于服务器日志压缩的，因为随着运行时间的增增长，日志信息也会变得越来越多，占有更多的空间。因此Raft采取了日志压缩的方法解决该问题，即将当前整个系统状态写入稳定存储的快照，然后该时间点之前的日志就可以丢弃掉，从而释放存储空间。</p>\n<h2 id=\"1-快照结构\"><a href=\"#1-快照结构\" class=\"headerlink\" title=\"1 快照结构\"></a>1 快照结构</h2><p><img src=\"/img/blog/raft/12.png\" srcset=\"undefined\" alt=\"图\"><br>从图中可见，快照包括以下几个部分内容:</p>\n<ul>\n<li>lastIncludedIndex: 表明快照中最后一条日志的索引值。也就是说日志一直压缩到该索引值的位置。该值以前连续若干个索引值的日志被压缩为快照，而该值以后的日志则不在快照中。</li>\n<li>lastIncludedTerm:表明快照中最后一条日志所在的任期值。</li>\n<li>state machine state:复制状态机的当前状态。</li>\n</ul>\n<p>集群中每一个服务器都可以独立地进行拍摄快照(只对已提交的日志进行快照的拍摄)，其中<code>lastIncludedIndex</code>与<code>lastIncludedTerm</code>值的存在时为了通过之前讲到的在日志复制中需要做的一致性检查。当服务器完成了该快照的写入之后，就可以将从快照中最后一条日志一直到先前所有的日志删除。</p>\n<h2 id=\"2-快照的发送\"><a href=\"#2-快照的发送\" class=\"headerlink\" title=\"2 快照的发送\"></a>2 快照的发送</h2><p>正常情况下，<code>Leader</code>的日志将会与<code>Follower</code>保持一致，但并不是所有情况都处于正常情况下，有时候可能因为<code>Follower</code>的反应缓慢或崩溃造成与<code>Leader</code>的日志不一致。所以有时候需要<code>Leader</code>将快照信息发送给<code>Follower</code>。快照信息是通过一个称为<code>InstallSnapshot</code>的RPC消息发送的，该消息的结构如下:</p>\n<table>\n<thead>\n<tr>\n<th><strong>InstallSnapshot RPC</strong></th>\n<th><strong>由Leader调用并按顺序发送打包的快照到Follower。</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>参数：</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>Leader的任期</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>用以Follower可以重定向客户端的请求到Leader</td>\n</tr>\n<tr>\n<td>lastIncludedIndex</td>\n<td>快照将替换直到该索引的所有日志条目</td>\n</tr>\n<tr>\n<td>lastIncludedTerm</td>\n<td>快照中最后一条日志所在的任期值</td>\n</tr>\n<tr>\n<td>offset</td>\n<td>快照文件的偏移量,简单来说就是该快照代表多少数量的连续个日志实体</td>\n</tr>\n<tr>\n<td>data[]</td>\n<td>从offset开始，快照内的数据数组</td>\n</tr>\n<tr>\n<td>done</td>\n<td>如果这是最后一个快照则为true，说明该快照之后的日志暂时不需要拍摄快照</td>\n</tr>\n</tbody></table>\n<p>快照的发送会出现以下几种情况:</p>\n<ol>\n<li><code>Follower</code>的日志信息不包括快照中的日志信息，即缺少日志。</li>\n<li><code>Follower</code>的日志信息与快照中的日志信息发生冲突。</li>\n<li><code>Follower</code>的日志信息要多于快照中的日志信息。</li>\n</ol>\n<p>至于前两种情况，<code>Follower</code>采取直接使用快照内容替代掉自己的日志。</p>\n<ul>\n<li><code>Follower</code>只含有部分快照中的日志信息，那么直接删掉然后使用快照取代。</li>\n<li><code>Follower</code>具有更多的日志信息的情况下，即<code>Follower</code>含有大于接收到的快照中的最后一条日志信息的索引的日志信息。那么直接使用快照代替快照中所包含的日志信息，至于快照之后的日志信息仍然保留。</li>\n</ul>\n<h3 id=\"2-1-疑问\"><a href=\"#2-1-疑问\" class=\"headerlink\" title=\"2.1 疑问\"></a>2.1 疑问</h3><p>为什么不像日志一样仅由<code>Leader</code>拍摄快照然后发送给<code>Follower</code>，而是允许每一个服务器独立生成快照信息呢？<br>很简单的答案，为了减少带宽使用，以及资源的浪费。因为正常情况下<code>Follower</code>具有生成快照的所有信息，在自己本地直接生成快照所需要消耗的资源要远远小于通过网络发送所需要的资源。另外也是降低<code>Leader</code>设计的复杂。因为如果仅由<code>Leader</code>生成快照的话，<code>Leader</code>则需要在向<code>Follower</code>发送日志的同时，还要兼顾快照的发送。</p>\n<h3 id=\"2-2-存在的问题\"><a href=\"#2-2-存在的问题\" class=\"headerlink\" title=\"2.2 存在的问题\"></a>2.2 存在的问题</h3><ul>\n<li>还有另外两个问题会影响每个快照。首先，服务器必须决定何时进行快照。如果服务器过于频繁地进行快照，则会浪费磁盘带宽和能量。如果快照太不频繁，则有耗尽其存储容量的风险，并且会增加重新启动期间加载日志所需的时间。一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果此大小设置为明显大于快照的预期大小，则用于快照的磁盘带宽开销将很小。</li>\n<li>第二个问题是写快照可能要花费大量时间，拍摄快照会延迟正常操作。解决方案是使用写时复制技术，以便可以接受新的更新而不会影响快照的写入。例如，使用功能性数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如，Linux上的fork）来创建整个状态机的内存快照。</li>\n</ul>\n"},{"title":"Raft算法之成员关系变化","date":"2020-01-06T13:01:07.000Z","_content":"上一篇文章:[Raft算法之日志复制](https://ifican.top/2020/01/05/blog/consensus/raft-log/)\n\n# Raft算法之成员关系变化\n&emsp;&emsp;有时候可能会遇到需要对集群中的成员数量进行更新的操作，比较简单的做法将更新操作分为两个阶段进行，在第一个阶段将全部的使用旧的配置文件的集群*C_old*成员全部关闭，所以将不能对客户端的请求进行处理。然后在第二个阶段使用新的配置文件启动集群成员。一个很明显的劣势在于更新成员数量的时候有一段时间是无法对客户端请求进行处理的。\n&emsp;&emsp;Raft使用了一种新的方案对成员进行更新。在两阶段更新之间加入了一个配置转换阶段，称为联合共识。引入联合共识阶段，集群在进行成员关系变化的同时，不需要关闭集群成员，从而可以在更新成员数量的过程中也可以对客户端的请求进行处理。\n&emsp;&emsp;在联合共识阶段具有以下几点属性:\n\n* 日志条目均复制到使用两种配置的所有服务器。\n* 来自任一配置的任何服务器都可以充当领导者.\n* 选举和日志的提交需要分别来自新旧配置的大多数人接受。\n\n&emsp;&emsp;联合共识允许集群中的单个服务器在不同的时间从旧的配置转换为新的配置，从而不会影响安全性。并且在整个配置更新期间可以继续为客户端提供服务。\n\n## 1 配置更新过程\n### 1.1 理想情况\n&emsp;&emsp;以向集群中添加新的成员为例，正常情况下假设该过程不涉及客户端发送的其他的新的请求:\n&emsp;&emsp;假设旧的配置文件称为*C_o*,新的配置文件称为*C_n*,旧的集群称为*C_old*,新添加的成员称为*C_new*.\n\n* 当集群*C_old*在正常运行过程中(当前使用旧的配置文件*C_o*)，接收到来自客户端关于添加新成员的请求。\n    * `Leader`接收到则直接处理，`Follower`接收到则会重定向到`Leader`.\n* `Leader`创建一个用于更新配置的新的日志文件*C_o_n*(该日志配置文件表示*C_old*与*C_new*成员共存)，该配置文件按照正常流程复制到集群中大多数服务器(包括*C_old,C_new*)\n    * 包括新成员*C_new*，**服务器始终使用其日志中的最新配置，而不管该条目是否被提交**，`Leader`将使用*C_o_n*规则来确定何时提交*C_o_n*的日志条目。\n    * 也就是说本地只持有*C_o*配置日志文件的成员仍然使用旧的配置文件。当接收到*C_o_n*配置文件之后不论是否已经应用到复制状态机，都会使用*C_o_n*配置文件作为服务器的配置文件。\n* 当新的日志文件*C_o_n*成功在集群中提交之后，进入了联合共识阶段。\n* 进入联合共识阶段之后，`Leader`创建一个新的用于配置更新的新的配置文件*C_n*,并将该日志发送到大部分*C_new*服务器(文献中是这么说的，至此还没搞明白为什么不是所有的服务器)。\n\n![图](/img/blog/raft/110.png)\n\n* 当配置日志文件*C_n*成功提交之后，则表明成员更新过程结束，集群使用新的配置文件*C_n*按照正常的流程继续运行。\n\n&emsp;&emsp;如果不考虑客户端发送的新的请求以及服务器崩溃的情况下，可以把配置更新看做一个普通的日志文件，按照正常流程发送，提交，应用后便成功完成配置的更新。唯一不同的是普通的日志文件需要提交过后才会应用到复制状态机，而配置文件日志则是当服务器接收到之后，不论是否已经提交，接收到的配置信息都会生效。\n\n### 1.2 联合共识阶段\n&emsp;&emsp;联合共识阶段:指的是*C_o_n*配置日志文件成功提交到集群中的大多数服务器，且*C_n*配置日志文件还没有提交到集群中的大多数服务器之间的时间段。\n&emsp;&emsp;在该阶段，任何操作(选举或者是其他的日志请求)对于*C_old*和*C_new*的成员来说都不能单独做出决策。即需要*C_old*与*C_new*中的大部分服务器同时做出决策。(因为日志的提交条件是成功复制到大多数的服务器，所以当*C_o_n*日志文件被提交之后，有可能还存在部分的服务器没有接收到*C_o_n*日志文件,仍然处于*C_old*阶段,*C_new*的成员也是如此)\n\n## 2 `Leader`崩溃情况\n\n![图](/img/blog/raft/11.png)\n\n\n&emsp;&emsp;分别从以下几个时间点说一下`Leader`在各个阶段发生崩溃的措施:\n\n1. *C_o_n*配置日志文件未提交之前.\n2. *C_o_n*配置日志文件提交之后，且*C_n*配置日志文件未提交前之间(联合共识阶段)的时间段\n3. *C_n*配置日志文件提交之后.\n\n### 2.1 *C_o_n*配置日志文件未提交之前\n&emsp;&emsp;从集群初始正常运行状态一直到*C_o_n*配置日志文件被提交这段时间，如果`Leader`奔溃，那么当选`Leader`的成员可能是使用*C_o*的成员，也可能是接收到*C_o_n*配置日志文件的成员。因为*C_o_n*配置日志文件还未被提交，所以`C_old`的成员可以单独做出决策。而`C_new`的成员还不能单独做出决策。\n\n### 2.2 联合共识阶段\n&emsp;&emsp; *C_o_n*配置日志文件提交之后，且*C_n*配置日志文件未提交前之间的时间段，由于*C_o_n*配置日志文件只有当复制到*C_old*和*C_new*两者中大多数成员之后才被提交，所以当提交*C_o_n*配置日志文件之后，使用*C_o_n*配置日志文件的成员占全部服务器成员的大多数，因此，如果`Leader`崩溃，那么只能从使用*C_o_n*配置日志文件的成员中选取`Leader`。此时对于*C_old*和*C_new*的成员来说都不能单独做出决策，因此也不能在使用*C_o*以及*C_n*的成员中选取`Leader`.\n\n### 2.3 *C_n*配置日志文件提交之后\n&emsp;&emsp;当该日志提交之后，实际上已经完成了网络中成员关系的更新。所以`Leader`的选举即可和正常运行阶段相同。\n\n\n## 3 存在的问题\n&emsp;&emsp;在成员关系更新阶段，主要存在三个问题:\n\n1. 新添加的成员可能不会存储任何之前的日志条目，如果将它们加入集群，在日志条目与`Leader`完成同步之前，是无法提交新的日志条目的。\n2. `Leader`可能不属于新配置集群中的一部分。\n3. 假设更新成员关系是对集群中的成员进行删除，那么被删除的节点可能会扰乱集群。\n\n### 3.1 问题一\n&emsp;&emsp;针对该问题，Raft的做法是引入一个新的状态，即允许新的成员以一种不具备决策权(选举和参与日志提交)的身份加入集群，因此在选举`Leader`或者是统计日志是否已经分发到大部分成员时，将不会考虑该成员。一直到该成员的日志存储状态追赶上集群中的其他成员，再赋予该成员决策权。\n### 3.2 问题二\n**原因**：&emsp;&emsp;该问题产生的原因是可能新添加到集群中的新成员的数量要远远多于旧集群的数量(**个人理解，如果有错误欢迎指出**)。由于之前说到的*C_n*配置日志文件需要发送到*C_new*中的大多数成员，而`Leader`并不属于`C_new`中的一员。所以在发送*C_n*配置日志文件的时段，`Leader`将会对*C_new*的成员进行管理。\n**解决方案**:&emsp;&emsp;当*C_n*日志成功完成提交时，该`Leader`自动转换身份为`Follower`，然后从*C_new*的成员中选举出一个新的`Leader`.\n### 3.3 问题三\n**原因**:&emsp;&emsp;被删除的服务器如果没有关闭，那么他们将不会接收到心跳信息和日志信息，从而不断发生超时，最后导致任期不断增加(高于集群中所有成员的任期)，然后不断向集群中发送请求投票消息。集群中的`Leader`将变为`Follower`，集群中将不断开始新的选举。从而扰乱集群的正常运行。\n**解决方案**: &emsp;&emsp;Raft引入了一个最小选举超时时间，意思是如果集群中存在`Leader`时，并且接收到心跳信息之后在最小选举超时时间内接受到请求投票消息，那么将会忽略掉该投票消息。\n\n下一篇文章:[Raft算法之日志压缩](https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/)","source":"_posts/blog/consensus/raft-relationship.md","raw":"---\ntitle: Raft算法之成员关系变化\ndate: 2020-01-06 21:01:07\ntags: \n- Raft\n- algorithm\ncategories:\n- algorithm\n---\n上一篇文章:[Raft算法之日志复制](https://ifican.top/2020/01/05/blog/consensus/raft-log/)\n\n# Raft算法之成员关系变化\n&emsp;&emsp;有时候可能会遇到需要对集群中的成员数量进行更新的操作，比较简单的做法将更新操作分为两个阶段进行，在第一个阶段将全部的使用旧的配置文件的集群*C_old*成员全部关闭，所以将不能对客户端的请求进行处理。然后在第二个阶段使用新的配置文件启动集群成员。一个很明显的劣势在于更新成员数量的时候有一段时间是无法对客户端请求进行处理的。\n&emsp;&emsp;Raft使用了一种新的方案对成员进行更新。在两阶段更新之间加入了一个配置转换阶段，称为联合共识。引入联合共识阶段，集群在进行成员关系变化的同时，不需要关闭集群成员，从而可以在更新成员数量的过程中也可以对客户端的请求进行处理。\n&emsp;&emsp;在联合共识阶段具有以下几点属性:\n\n* 日志条目均复制到使用两种配置的所有服务器。\n* 来自任一配置的任何服务器都可以充当领导者.\n* 选举和日志的提交需要分别来自新旧配置的大多数人接受。\n\n&emsp;&emsp;联合共识允许集群中的单个服务器在不同的时间从旧的配置转换为新的配置，从而不会影响安全性。并且在整个配置更新期间可以继续为客户端提供服务。\n\n## 1 配置更新过程\n### 1.1 理想情况\n&emsp;&emsp;以向集群中添加新的成员为例，正常情况下假设该过程不涉及客户端发送的其他的新的请求:\n&emsp;&emsp;假设旧的配置文件称为*C_o*,新的配置文件称为*C_n*,旧的集群称为*C_old*,新添加的成员称为*C_new*.\n\n* 当集群*C_old*在正常运行过程中(当前使用旧的配置文件*C_o*)，接收到来自客户端关于添加新成员的请求。\n    * `Leader`接收到则直接处理，`Follower`接收到则会重定向到`Leader`.\n* `Leader`创建一个用于更新配置的新的日志文件*C_o_n*(该日志配置文件表示*C_old*与*C_new*成员共存)，该配置文件按照正常流程复制到集群中大多数服务器(包括*C_old,C_new*)\n    * 包括新成员*C_new*，**服务器始终使用其日志中的最新配置，而不管该条目是否被提交**，`Leader`将使用*C_o_n*规则来确定何时提交*C_o_n*的日志条目。\n    * 也就是说本地只持有*C_o*配置日志文件的成员仍然使用旧的配置文件。当接收到*C_o_n*配置文件之后不论是否已经应用到复制状态机，都会使用*C_o_n*配置文件作为服务器的配置文件。\n* 当新的日志文件*C_o_n*成功在集群中提交之后，进入了联合共识阶段。\n* 进入联合共识阶段之后，`Leader`创建一个新的用于配置更新的新的配置文件*C_n*,并将该日志发送到大部分*C_new*服务器(文献中是这么说的，至此还没搞明白为什么不是所有的服务器)。\n\n![图](/img/blog/raft/110.png)\n\n* 当配置日志文件*C_n*成功提交之后，则表明成员更新过程结束，集群使用新的配置文件*C_n*按照正常的流程继续运行。\n\n&emsp;&emsp;如果不考虑客户端发送的新的请求以及服务器崩溃的情况下，可以把配置更新看做一个普通的日志文件，按照正常流程发送，提交，应用后便成功完成配置的更新。唯一不同的是普通的日志文件需要提交过后才会应用到复制状态机，而配置文件日志则是当服务器接收到之后，不论是否已经提交，接收到的配置信息都会生效。\n\n### 1.2 联合共识阶段\n&emsp;&emsp;联合共识阶段:指的是*C_o_n*配置日志文件成功提交到集群中的大多数服务器，且*C_n*配置日志文件还没有提交到集群中的大多数服务器之间的时间段。\n&emsp;&emsp;在该阶段，任何操作(选举或者是其他的日志请求)对于*C_old*和*C_new*的成员来说都不能单独做出决策。即需要*C_old*与*C_new*中的大部分服务器同时做出决策。(因为日志的提交条件是成功复制到大多数的服务器，所以当*C_o_n*日志文件被提交之后，有可能还存在部分的服务器没有接收到*C_o_n*日志文件,仍然处于*C_old*阶段,*C_new*的成员也是如此)\n\n## 2 `Leader`崩溃情况\n\n![图](/img/blog/raft/11.png)\n\n\n&emsp;&emsp;分别从以下几个时间点说一下`Leader`在各个阶段发生崩溃的措施:\n\n1. *C_o_n*配置日志文件未提交之前.\n2. *C_o_n*配置日志文件提交之后，且*C_n*配置日志文件未提交前之间(联合共识阶段)的时间段\n3. *C_n*配置日志文件提交之后.\n\n### 2.1 *C_o_n*配置日志文件未提交之前\n&emsp;&emsp;从集群初始正常运行状态一直到*C_o_n*配置日志文件被提交这段时间，如果`Leader`奔溃，那么当选`Leader`的成员可能是使用*C_o*的成员，也可能是接收到*C_o_n*配置日志文件的成员。因为*C_o_n*配置日志文件还未被提交，所以`C_old`的成员可以单独做出决策。而`C_new`的成员还不能单独做出决策。\n\n### 2.2 联合共识阶段\n&emsp;&emsp; *C_o_n*配置日志文件提交之后，且*C_n*配置日志文件未提交前之间的时间段，由于*C_o_n*配置日志文件只有当复制到*C_old*和*C_new*两者中大多数成员之后才被提交，所以当提交*C_o_n*配置日志文件之后，使用*C_o_n*配置日志文件的成员占全部服务器成员的大多数，因此，如果`Leader`崩溃，那么只能从使用*C_o_n*配置日志文件的成员中选取`Leader`。此时对于*C_old*和*C_new*的成员来说都不能单独做出决策，因此也不能在使用*C_o*以及*C_n*的成员中选取`Leader`.\n\n### 2.3 *C_n*配置日志文件提交之后\n&emsp;&emsp;当该日志提交之后，实际上已经完成了网络中成员关系的更新。所以`Leader`的选举即可和正常运行阶段相同。\n\n\n## 3 存在的问题\n&emsp;&emsp;在成员关系更新阶段，主要存在三个问题:\n\n1. 新添加的成员可能不会存储任何之前的日志条目，如果将它们加入集群，在日志条目与`Leader`完成同步之前，是无法提交新的日志条目的。\n2. `Leader`可能不属于新配置集群中的一部分。\n3. 假设更新成员关系是对集群中的成员进行删除，那么被删除的节点可能会扰乱集群。\n\n### 3.1 问题一\n&emsp;&emsp;针对该问题，Raft的做法是引入一个新的状态，即允许新的成员以一种不具备决策权(选举和参与日志提交)的身份加入集群，因此在选举`Leader`或者是统计日志是否已经分发到大部分成员时，将不会考虑该成员。一直到该成员的日志存储状态追赶上集群中的其他成员，再赋予该成员决策权。\n### 3.2 问题二\n**原因**：&emsp;&emsp;该问题产生的原因是可能新添加到集群中的新成员的数量要远远多于旧集群的数量(**个人理解，如果有错误欢迎指出**)。由于之前说到的*C_n*配置日志文件需要发送到*C_new*中的大多数成员，而`Leader`并不属于`C_new`中的一员。所以在发送*C_n*配置日志文件的时段，`Leader`将会对*C_new*的成员进行管理。\n**解决方案**:&emsp;&emsp;当*C_n*日志成功完成提交时，该`Leader`自动转换身份为`Follower`，然后从*C_new*的成员中选举出一个新的`Leader`.\n### 3.3 问题三\n**原因**:&emsp;&emsp;被删除的服务器如果没有关闭，那么他们将不会接收到心跳信息和日志信息，从而不断发生超时，最后导致任期不断增加(高于集群中所有成员的任期)，然后不断向集群中发送请求投票消息。集群中的`Leader`将变为`Follower`，集群中将不断开始新的选举。从而扰乱集群的正常运行。\n**解决方案**: &emsp;&emsp;Raft引入了一个最小选举超时时间，意思是如果集群中存在`Leader`时，并且接收到心跳信息之后在最小选举超时时间内接受到请求投票消息，那么将会忽略掉该投票消息。\n\n下一篇文章:[Raft算法之日志压缩](https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/)","slug":"blog/consensus/raft-relationship","published":1,"updated":"2020-05-11T03:44:37.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyg3000lk0vq3t69a895","content":"<p>上一篇文章:<a href=\"https://ifican.top/2020/01/05/blog/consensus/raft-log/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志复制</a></p>\n<h1 id=\"Raft算法之成员关系变化\"><a href=\"#Raft算法之成员关系变化\" class=\"headerlink\" title=\"Raft算法之成员关系变化\"></a>Raft算法之成员关系变化</h1><p>&emsp;&emsp;有时候可能会遇到需要对集群中的成员数量进行更新的操作，比较简单的做法将更新操作分为两个阶段进行，在第一个阶段将全部的使用旧的配置文件的集群<em>C_old</em>成员全部关闭，所以将不能对客户端的请求进行处理。然后在第二个阶段使用新的配置文件启动集群成员。一个很明显的劣势在于更新成员数量的时候有一段时间是无法对客户端请求进行处理的。<br>&emsp;&emsp;Raft使用了一种新的方案对成员进行更新。在两阶段更新之间加入了一个配置转换阶段，称为联合共识。引入联合共识阶段，集群在进行成员关系变化的同时，不需要关闭集群成员，从而可以在更新成员数量的过程中也可以对客户端的请求进行处理。<br>&emsp;&emsp;在联合共识阶段具有以下几点属性:</p>\n<ul>\n<li>日志条目均复制到使用两种配置的所有服务器。</li>\n<li>来自任一配置的任何服务器都可以充当领导者.</li>\n<li>选举和日志的提交需要分别来自新旧配置的大多数人接受。</li>\n</ul>\n<p>&emsp;&emsp;联合共识允许集群中的单个服务器在不同的时间从旧的配置转换为新的配置，从而不会影响安全性。并且在整个配置更新期间可以继续为客户端提供服务。</p>\n<h2 id=\"1-配置更新过程\"><a href=\"#1-配置更新过程\" class=\"headerlink\" title=\"1 配置更新过程\"></a>1 配置更新过程</h2><h3 id=\"1-1-理想情况\"><a href=\"#1-1-理想情况\" class=\"headerlink\" title=\"1.1 理想情况\"></a>1.1 理想情况</h3><p>&emsp;&emsp;以向集群中添加新的成员为例，正常情况下假设该过程不涉及客户端发送的其他的新的请求:<br>&emsp;&emsp;假设旧的配置文件称为<em>C_o</em>,新的配置文件称为<em>C_n</em>,旧的集群称为<em>C_old</em>,新添加的成员称为<em>C_new</em>.</p>\n<ul>\n<li>当集群<em>C_old<em>在正常运行过程中(当前使用旧的配置文件</em>C_o</em>)，接收到来自客户端关于添加新成员的请求。<ul>\n<li><code>Leader</code>接收到则直接处理，<code>Follower</code>接收到则会重定向到<code>Leader</code>.</li>\n</ul>\n</li>\n<li><code>Leader</code>创建一个用于更新配置的新的日志文件<em>C_o_n</em>(该日志配置文件表示<em>C_old<em>与</em>C_new<em>成员共存)，该配置文件按照正常流程复制到集群中大多数服务器(包括</em>C_old,C_new</em>)<ul>\n<li>包括新成员<em>C_new</em>，<strong>服务器始终使用其日志中的最新配置，而不管该条目是否被提交</strong>，<code>Leader</code>将使用<em>C_o_n</em>规则来确定何时提交<em>C_o_n</em>的日志条目。</li>\n<li>也就是说本地只持有<em>C_o</em>配置日志文件的成员仍然使用旧的配置文件。当接收到<em>C_o_n</em>配置文件之后不论是否已经应用到复制状态机，都会使用<em>C_o_n</em>配置文件作为服务器的配置文件。</li>\n</ul>\n</li>\n<li>当新的日志文件<em>C_o_n</em>成功在集群中提交之后，进入了联合共识阶段。</li>\n<li>进入联合共识阶段之后，<code>Leader</code>创建一个新的用于配置更新的新的配置文件<em>C_n</em>,并将该日志发送到大部分<em>C_new</em>服务器(文献中是这么说的，至此还没搞明白为什么不是所有的服务器)。</li>\n</ul>\n<p><img src=\"/img/blog/raft/110.png\" srcset=\"undefined\" alt=\"图\"></p>\n<ul>\n<li>当配置日志文件<em>C_n</em>成功提交之后，则表明成员更新过程结束，集群使用新的配置文件<em>C_n</em>按照正常的流程继续运行。</li>\n</ul>\n<p>&emsp;&emsp;如果不考虑客户端发送的新的请求以及服务器崩溃的情况下，可以把配置更新看做一个普通的日志文件，按照正常流程发送，提交，应用后便成功完成配置的更新。唯一不同的是普通的日志文件需要提交过后才会应用到复制状态机，而配置文件日志则是当服务器接收到之后，不论是否已经提交，接收到的配置信息都会生效。</p>\n<h3 id=\"1-2-联合共识阶段\"><a href=\"#1-2-联合共识阶段\" class=\"headerlink\" title=\"1.2 联合共识阶段\"></a>1.2 联合共识阶段</h3><p>&emsp;&emsp;联合共识阶段:指的是<em>C_o_n</em>配置日志文件成功提交到集群中的大多数服务器，且<em>C_n</em>配置日志文件还没有提交到集群中的大多数服务器之间的时间段。<br>&emsp;&emsp;在该阶段，任何操作(选举或者是其他的日志请求)对于<em>C_old</em>和<em>C_new</em>的成员来说都不能单独做出决策。即需要<em>C_old</em>与<em>C_new</em>中的大部分服务器同时做出决策。(因为日志的提交条件是成功复制到大多数的服务器，所以当<em>C_o_n</em>日志文件被提交之后，有可能还存在部分的服务器没有接收到<em>C_o_n</em>日志文件,仍然处于<em>C_old</em>阶段,<em>C_new</em>的成员也是如此)</p>\n<h2 id=\"2-Leader崩溃情况\"><a href=\"#2-Leader崩溃情况\" class=\"headerlink\" title=\"2 Leader崩溃情况\"></a>2 <code>Leader</code>崩溃情况</h2><p><img src=\"/img/blog/raft/11.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>&emsp;&emsp;分别从以下几个时间点说一下<code>Leader</code>在各个阶段发生崩溃的措施:</p>\n<ol>\n<li><em>C_o_n</em>配置日志文件未提交之前.</li>\n<li><em>C_o_n</em>配置日志文件提交之后，且<em>C_n</em>配置日志文件未提交前之间(联合共识阶段)的时间段</li>\n<li><em>C_n</em>配置日志文件提交之后.</li>\n</ol>\n<h3 id=\"2-1-C-o-n配置日志文件未提交之前\"><a href=\"#2-1-C-o-n配置日志文件未提交之前\" class=\"headerlink\" title=\"2.1 C_o_n配置日志文件未提交之前\"></a>2.1 <em>C_o_n</em>配置日志文件未提交之前</h3><p>&emsp;&emsp;从集群初始正常运行状态一直到<em>C_o_n</em>配置日志文件被提交这段时间，如果<code>Leader</code>奔溃，那么当选<code>Leader</code>的成员可能是使用<em>C_o</em>的成员，也可能是接收到<em>C_o_n</em>配置日志文件的成员。因为<em>C_o_n</em>配置日志文件还未被提交，所以<code>C_old</code>的成员可以单独做出决策。而<code>C_new</code>的成员还不能单独做出决策。</p>\n<h3 id=\"2-2-联合共识阶段\"><a href=\"#2-2-联合共识阶段\" class=\"headerlink\" title=\"2.2 联合共识阶段\"></a>2.2 联合共识阶段</h3><p>&emsp;&emsp; <em>C_o_n</em>配置日志文件提交之后，且<em>C_n</em>配置日志文件未提交前之间的时间段，由于<em>C_o_n</em>配置日志文件只有当复制到<em>C_old</em>和<em>C_new</em>两者中大多数成员之后才被提交，所以当提交<em>C_o_n</em>配置日志文件之后，使用<em>C_o_n</em>配置日志文件的成员占全部服务器成员的大多数，因此，如果<code>Leader</code>崩溃，那么只能从使用<em>C_o_n</em>配置日志文件的成员中选取<code>Leader</code>。此时对于<em>C_old</em>和<em>C_new</em>的成员来说都不能单独做出决策，因此也不能在使用<em>C_o</em>以及<em>C_n</em>的成员中选取<code>Leader</code>.</p>\n<h3 id=\"2-3-C-n配置日志文件提交之后\"><a href=\"#2-3-C-n配置日志文件提交之后\" class=\"headerlink\" title=\"2.3 C_n配置日志文件提交之后\"></a>2.3 <em>C_n</em>配置日志文件提交之后</h3><p>&emsp;&emsp;当该日志提交之后，实际上已经完成了网络中成员关系的更新。所以<code>Leader</code>的选举即可和正常运行阶段相同。</p>\n<h2 id=\"3-存在的问题\"><a href=\"#3-存在的问题\" class=\"headerlink\" title=\"3 存在的问题\"></a>3 存在的问题</h2><p>&emsp;&emsp;在成员关系更新阶段，主要存在三个问题:</p>\n<ol>\n<li>新添加的成员可能不会存储任何之前的日志条目，如果将它们加入集群，在日志条目与<code>Leader</code>完成同步之前，是无法提交新的日志条目的。</li>\n<li><code>Leader</code>可能不属于新配置集群中的一部分。</li>\n<li>假设更新成员关系是对集群中的成员进行删除，那么被删除的节点可能会扰乱集群。</li>\n</ol>\n<h3 id=\"3-1-问题一\"><a href=\"#3-1-问题一\" class=\"headerlink\" title=\"3.1 问题一\"></a>3.1 问题一</h3><p>&emsp;&emsp;针对该问题，Raft的做法是引入一个新的状态，即允许新的成员以一种不具备决策权(选举和参与日志提交)的身份加入集群，因此在选举<code>Leader</code>或者是统计日志是否已经分发到大部分成员时，将不会考虑该成员。一直到该成员的日志存储状态追赶上集群中的其他成员，再赋予该成员决策权。</p>\n<h3 id=\"3-2-问题二\"><a href=\"#3-2-问题二\" class=\"headerlink\" title=\"3.2 问题二\"></a>3.2 问题二</h3><p><strong>原因</strong>：&emsp;&emsp;该问题产生的原因是可能新添加到集群中的新成员的数量要远远多于旧集群的数量(<strong>个人理解，如果有错误欢迎指出</strong>)。由于之前说到的<em>C_n</em>配置日志文件需要发送到<em>C_new</em>中的大多数成员，而<code>Leader</code>并不属于<code>C_new</code>中的一员。所以在发送<em>C_n</em>配置日志文件的时段，<code>Leader</code>将会对<em>C_new</em>的成员进行管理。<br><strong>解决方案</strong>:&emsp;&emsp;当<em>C_n</em>日志成功完成提交时，该<code>Leader</code>自动转换身份为<code>Follower</code>，然后从<em>C_new</em>的成员中选举出一个新的<code>Leader</code>.</p>\n<h3 id=\"3-3-问题三\"><a href=\"#3-3-问题三\" class=\"headerlink\" title=\"3.3 问题三\"></a>3.3 问题三</h3><p><strong>原因</strong>:&emsp;&emsp;被删除的服务器如果没有关闭，那么他们将不会接收到心跳信息和日志信息，从而不断发生超时，最后导致任期不断增加(高于集群中所有成员的任期)，然后不断向集群中发送请求投票消息。集群中的<code>Leader</code>将变为<code>Follower</code>，集群中将不断开始新的选举。从而扰乱集群的正常运行。<br><strong>解决方案</strong>: &emsp;&emsp;Raft引入了一个最小选举超时时间，意思是如果集群中存在<code>Leader</code>时，并且接收到心跳信息之后在最小选举超时时间内接受到请求投票消息，那么将会忽略掉该投票消息。</p>\n<p>下一篇文章:<a href=\"https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志压缩</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>上一篇文章:<a href=\"https://ifican.top/2020/01/05/blog/consensus/raft-log/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志复制</a></p>\n<h1 id=\"Raft算法之成员关系变化\"><a href=\"#Raft算法之成员关系变化\" class=\"headerlink\" title=\"Raft算法之成员关系变化\"></a>Raft算法之成员关系变化</h1><p>&emsp;&emsp;有时候可能会遇到需要对集群中的成员数量进行更新的操作，比较简单的做法将更新操作分为两个阶段进行，在第一个阶段将全部的使用旧的配置文件的集群<em>C_old</em>成员全部关闭，所以将不能对客户端的请求进行处理。然后在第二个阶段使用新的配置文件启动集群成员。一个很明显的劣势在于更新成员数量的时候有一段时间是无法对客户端请求进行处理的。<br>&emsp;&emsp;Raft使用了一种新的方案对成员进行更新。在两阶段更新之间加入了一个配置转换阶段，称为联合共识。引入联合共识阶段，集群在进行成员关系变化的同时，不需要关闭集群成员，从而可以在更新成员数量的过程中也可以对客户端的请求进行处理。<br>&emsp;&emsp;在联合共识阶段具有以下几点属性:</p>\n<ul>\n<li>日志条目均复制到使用两种配置的所有服务器。</li>\n<li>来自任一配置的任何服务器都可以充当领导者.</li>\n<li>选举和日志的提交需要分别来自新旧配置的大多数人接受。</li>\n</ul>\n<p>&emsp;&emsp;联合共识允许集群中的单个服务器在不同的时间从旧的配置转换为新的配置，从而不会影响安全性。并且在整个配置更新期间可以继续为客户端提供服务。</p>\n<h2 id=\"1-配置更新过程\"><a href=\"#1-配置更新过程\" class=\"headerlink\" title=\"1 配置更新过程\"></a>1 配置更新过程</h2><h3 id=\"1-1-理想情况\"><a href=\"#1-1-理想情况\" class=\"headerlink\" title=\"1.1 理想情况\"></a>1.1 理想情况</h3><p>&emsp;&emsp;以向集群中添加新的成员为例，正常情况下假设该过程不涉及客户端发送的其他的新的请求:<br>&emsp;&emsp;假设旧的配置文件称为<em>C_o</em>,新的配置文件称为<em>C_n</em>,旧的集群称为<em>C_old</em>,新添加的成员称为<em>C_new</em>.</p>\n<ul>\n<li>当集群<em>C_old<em>在正常运行过程中(当前使用旧的配置文件</em>C_o</em>)，接收到来自客户端关于添加新成员的请求。<ul>\n<li><code>Leader</code>接收到则直接处理，<code>Follower</code>接收到则会重定向到<code>Leader</code>.</li>\n</ul>\n</li>\n<li><code>Leader</code>创建一个用于更新配置的新的日志文件<em>C_o_n</em>(该日志配置文件表示<em>C_old<em>与</em>C_new<em>成员共存)，该配置文件按照正常流程复制到集群中大多数服务器(包括</em>C_old,C_new</em>)<ul>\n<li>包括新成员<em>C_new</em>，<strong>服务器始终使用其日志中的最新配置，而不管该条目是否被提交</strong>，<code>Leader</code>将使用<em>C_o_n</em>规则来确定何时提交<em>C_o_n</em>的日志条目。</li>\n<li>也就是说本地只持有<em>C_o</em>配置日志文件的成员仍然使用旧的配置文件。当接收到<em>C_o_n</em>配置文件之后不论是否已经应用到复制状态机，都会使用<em>C_o_n</em>配置文件作为服务器的配置文件。</li>\n</ul>\n</li>\n<li>当新的日志文件<em>C_o_n</em>成功在集群中提交之后，进入了联合共识阶段。</li>\n<li>进入联合共识阶段之后，<code>Leader</code>创建一个新的用于配置更新的新的配置文件<em>C_n</em>,并将该日志发送到大部分<em>C_new</em>服务器(文献中是这么说的，至此还没搞明白为什么不是所有的服务器)。</li>\n</ul>\n<p><img src=\"/img/blog/raft/110.png\" srcset=\"undefined\" alt=\"图\"></p>\n<ul>\n<li>当配置日志文件<em>C_n</em>成功提交之后，则表明成员更新过程结束，集群使用新的配置文件<em>C_n</em>按照正常的流程继续运行。</li>\n</ul>\n<p>&emsp;&emsp;如果不考虑客户端发送的新的请求以及服务器崩溃的情况下，可以把配置更新看做一个普通的日志文件，按照正常流程发送，提交，应用后便成功完成配置的更新。唯一不同的是普通的日志文件需要提交过后才会应用到复制状态机，而配置文件日志则是当服务器接收到之后，不论是否已经提交，接收到的配置信息都会生效。</p>\n<h3 id=\"1-2-联合共识阶段\"><a href=\"#1-2-联合共识阶段\" class=\"headerlink\" title=\"1.2 联合共识阶段\"></a>1.2 联合共识阶段</h3><p>&emsp;&emsp;联合共识阶段:指的是<em>C_o_n</em>配置日志文件成功提交到集群中的大多数服务器，且<em>C_n</em>配置日志文件还没有提交到集群中的大多数服务器之间的时间段。<br>&emsp;&emsp;在该阶段，任何操作(选举或者是其他的日志请求)对于<em>C_old</em>和<em>C_new</em>的成员来说都不能单独做出决策。即需要<em>C_old</em>与<em>C_new</em>中的大部分服务器同时做出决策。(因为日志的提交条件是成功复制到大多数的服务器，所以当<em>C_o_n</em>日志文件被提交之后，有可能还存在部分的服务器没有接收到<em>C_o_n</em>日志文件,仍然处于<em>C_old</em>阶段,<em>C_new</em>的成员也是如此)</p>\n<h2 id=\"2-Leader崩溃情况\"><a href=\"#2-Leader崩溃情况\" class=\"headerlink\" title=\"2 Leader崩溃情况\"></a>2 <code>Leader</code>崩溃情况</h2><p><img src=\"/img/blog/raft/11.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>&emsp;&emsp;分别从以下几个时间点说一下<code>Leader</code>在各个阶段发生崩溃的措施:</p>\n<ol>\n<li><em>C_o_n</em>配置日志文件未提交之前.</li>\n<li><em>C_o_n</em>配置日志文件提交之后，且<em>C_n</em>配置日志文件未提交前之间(联合共识阶段)的时间段</li>\n<li><em>C_n</em>配置日志文件提交之后.</li>\n</ol>\n<h3 id=\"2-1-C-o-n配置日志文件未提交之前\"><a href=\"#2-1-C-o-n配置日志文件未提交之前\" class=\"headerlink\" title=\"2.1 C_o_n配置日志文件未提交之前\"></a>2.1 <em>C_o_n</em>配置日志文件未提交之前</h3><p>&emsp;&emsp;从集群初始正常运行状态一直到<em>C_o_n</em>配置日志文件被提交这段时间，如果<code>Leader</code>奔溃，那么当选<code>Leader</code>的成员可能是使用<em>C_o</em>的成员，也可能是接收到<em>C_o_n</em>配置日志文件的成员。因为<em>C_o_n</em>配置日志文件还未被提交，所以<code>C_old</code>的成员可以单独做出决策。而<code>C_new</code>的成员还不能单独做出决策。</p>\n<h3 id=\"2-2-联合共识阶段\"><a href=\"#2-2-联合共识阶段\" class=\"headerlink\" title=\"2.2 联合共识阶段\"></a>2.2 联合共识阶段</h3><p>&emsp;&emsp; <em>C_o_n</em>配置日志文件提交之后，且<em>C_n</em>配置日志文件未提交前之间的时间段，由于<em>C_o_n</em>配置日志文件只有当复制到<em>C_old</em>和<em>C_new</em>两者中大多数成员之后才被提交，所以当提交<em>C_o_n</em>配置日志文件之后，使用<em>C_o_n</em>配置日志文件的成员占全部服务器成员的大多数，因此，如果<code>Leader</code>崩溃，那么只能从使用<em>C_o_n</em>配置日志文件的成员中选取<code>Leader</code>。此时对于<em>C_old</em>和<em>C_new</em>的成员来说都不能单独做出决策，因此也不能在使用<em>C_o</em>以及<em>C_n</em>的成员中选取<code>Leader</code>.</p>\n<h3 id=\"2-3-C-n配置日志文件提交之后\"><a href=\"#2-3-C-n配置日志文件提交之后\" class=\"headerlink\" title=\"2.3 C_n配置日志文件提交之后\"></a>2.3 <em>C_n</em>配置日志文件提交之后</h3><p>&emsp;&emsp;当该日志提交之后，实际上已经完成了网络中成员关系的更新。所以<code>Leader</code>的选举即可和正常运行阶段相同。</p>\n<h2 id=\"3-存在的问题\"><a href=\"#3-存在的问题\" class=\"headerlink\" title=\"3 存在的问题\"></a>3 存在的问题</h2><p>&emsp;&emsp;在成员关系更新阶段，主要存在三个问题:</p>\n<ol>\n<li>新添加的成员可能不会存储任何之前的日志条目，如果将它们加入集群，在日志条目与<code>Leader</code>完成同步之前，是无法提交新的日志条目的。</li>\n<li><code>Leader</code>可能不属于新配置集群中的一部分。</li>\n<li>假设更新成员关系是对集群中的成员进行删除，那么被删除的节点可能会扰乱集群。</li>\n</ol>\n<h3 id=\"3-1-问题一\"><a href=\"#3-1-问题一\" class=\"headerlink\" title=\"3.1 问题一\"></a>3.1 问题一</h3><p>&emsp;&emsp;针对该问题，Raft的做法是引入一个新的状态，即允许新的成员以一种不具备决策权(选举和参与日志提交)的身份加入集群，因此在选举<code>Leader</code>或者是统计日志是否已经分发到大部分成员时，将不会考虑该成员。一直到该成员的日志存储状态追赶上集群中的其他成员，再赋予该成员决策权。</p>\n<h3 id=\"3-2-问题二\"><a href=\"#3-2-问题二\" class=\"headerlink\" title=\"3.2 问题二\"></a>3.2 问题二</h3><p><strong>原因</strong>：&emsp;&emsp;该问题产生的原因是可能新添加到集群中的新成员的数量要远远多于旧集群的数量(<strong>个人理解，如果有错误欢迎指出</strong>)。由于之前说到的<em>C_n</em>配置日志文件需要发送到<em>C_new</em>中的大多数成员，而<code>Leader</code>并不属于<code>C_new</code>中的一员。所以在发送<em>C_n</em>配置日志文件的时段，<code>Leader</code>将会对<em>C_new</em>的成员进行管理。<br><strong>解决方案</strong>:&emsp;&emsp;当<em>C_n</em>日志成功完成提交时，该<code>Leader</code>自动转换身份为<code>Follower</code>，然后从<em>C_new</em>的成员中选举出一个新的<code>Leader</code>.</p>\n<h3 id=\"3-3-问题三\"><a href=\"#3-3-问题三\" class=\"headerlink\" title=\"3.3 问题三\"></a>3.3 问题三</h3><p><strong>原因</strong>:&emsp;&emsp;被删除的服务器如果没有关闭，那么他们将不会接收到心跳信息和日志信息，从而不断发生超时，最后导致任期不断增加(高于集群中所有成员的任期)，然后不断向集群中发送请求投票消息。集群中的<code>Leader</code>将变为<code>Follower</code>，集群中将不断开始新的选举。从而扰乱集群的正常运行。<br><strong>解决方案</strong>: &emsp;&emsp;Raft引入了一个最小选举超时时间，意思是如果集群中存在<code>Leader</code>时，并且接收到心跳信息之后在最小选举超时时间内接受到请求投票消息，那么将会忽略掉该投票消息。</p>\n<p>下一篇文章:<a href=\"https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志压缩</a></p>\n"},{"title":"CouchDB基本操作","date":"2019-12-26T12:37:17.000Z","_content":"## CouchDB操作\n\n判断数据库是否正常运行:\n```\ncurl http://localhost:5984/_up | jq .\n```\n获取CouchDB唯一标识符(UUID):\n```\ncurl http://localhost:5984/_uuids | jq .\n```\n获取CouchDB数据库信息:\n```\ncurl http://localhost:5984/ | jq .\n```\n## 节点操作\n\n### 查询节点\n### 查询所有节点\n查询当前节点连接的所有节点以及集群中的节点：\n```\ncurl -u admin:admin http://localhost:5984/_membership\n```\n### 查询单个节点状态\n```\ncurl -u admin:admin http://localhost:5984/_node/{node-name}/_stats\n# 查询本地节点状态\ncurl -u admin:admin http://localhost:5984/_node/local/_stats \n```\n## 数据库操作\n\n### 查询数据库\n\n查询所有数据库:\n```\ncurl http://localhost:5984/_all_dbs | jq .\n```\n查询某个数据库详细信息:\n```\ncurl http://localhost:5984/{db_name} | jq .\n```\n\n查询数据库更新事件:\n```\ncurl -u admin:admin http://localhost:5984/_db_updates | jq .\n```\n\n查询数据库设计文档:\n```\ncurl -u admin:admin http://localhost:5984/data/_design_docs | jq .\n```\n\n### 创建数据库\n创建名称为`data`的数据库，分片数为1，副本数为2(包括源数据库).\n\n* `-u`指定用户名与密码\n* `-X`指定请求方法为`PUT`(不加`-X`默认为`GET`)\n```\ncurl -u admin:admin -X PUT http://localhost:5984/data?q=1&n=2 | jq .\n```\n\n### 删除数据库\n删除刚刚创建的数据库`data`。\n```\ncurl -u admin:admin -X DELETE http://localhost:5984/data | jq .\n```\n\n### 更新数据库\n为指定的数据库创建复合键:\n```\ncurl -X POST  \\\n    -H \"Content-Type:application/json\" \\\n    -H \"Host:localhost:5984\" \\\n    -u admin:admin \\\n    http://localhost:5984/data/_all_docs \\\n    -d \"{ \\\"_id\\\": [ \\\"abc\\\",\\\"bcd\\\" ]}\" | jq .\n```\n\n为指定的本地数据库创建复合键:\n```\ncurl -X POST  \\\n    -H \"Content-Type:application/json\" \\\n    -H \"Host:localhost:5984\" \\\n    -u admin:admin \\\n    http://localhost:5984/data/_local_docs \\\n    -d \"{ \\\"_id\\\": [ \\\"abc\\\",\\\"bcd\\\" ]}\" | jq .\n```\n### 数据库索引\n查询指定数据库索引:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_index | jq .\n```\n为指定数据库创建索引:\n\n* 索引字段为`foo`\n* 索引名称为`foo-index`\n* 索引类型为`json`\n```\ncurl -X POST \\\n    -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    -H \"localhost:5984\" \\\n    http://localhost:5984/data/_index \\\n    -d \"{ \\\"index\\\": { \\\"fields\\\": [\\\"foo\\\" ]}, \\\"name\\\":\\\"foo-index\\\",\\\"type\\\":\\\"json\\\"}\" | jq .\n```\n删除索引:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    -X DELETE \\ http://localhost:5984/data/_index/{ddoc}/json/{index_name}\n```\n\n清空所有视图索引文件:\n```\ncurl -X POST -u admin:admin http://localhost:5984/data/_view_cleanup\n```\n### 数据库分片\n查询指定的数据库分片信息:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_shards | jq .\n```\n根据文档ID查询指定的分片上存储的文档信息:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_shards/{docid} | jq .\n```\n强制进行数据库分片信息同步:\n```\ncurl -u admin:admin \\\n    -X POST \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_sync_shards | jq .\n```\n### 数据库压缩\n压缩指定的数据库:\n```\ncurl -u admin:admin \\\n    -X POST \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_compact | jq .\n```\n\n### 数据库安全\n获取当前数据库安全对象:\n```\ncurl -u admin:admin http://localhost:5984/data/_security | jq .\n```\n## 文档操作\n\n### 查询文档\n查询数据库`data`中所有文档:\n```\ncurl -u admin:admin -H \"Content-Type:application/json\" http://localhost:5984/data/_all_docs | jq .\n```\n\n查询数据库`data`中的指定文档:\n`cec6606d0ddaca2b555ebb8404a772a0`为指定的文档ID\n```\ncurl -u admin:admin http://localhost:5984/data/cec6606d0ddaca2b555ebb8404a772a0 | jq .\n```\n\n查询数据库中文档的更新信息:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_changes | jq .\n```\n\n查询本地数据库中的文档:\n```\ncurl -u admin:admin http://localhost:5984/data/_local_docs | jq .\n```\n### 创建文档\n向数据库`data`中创建`id`为`id`,标题为`demo`的新文档:\n\n```\ncurl -X POST \\\n    -H \"Content-Type:application/json\" \\\n    -u admin:admin \\\n    http://localhost:5984/data/ \\\n    -d \"{ \\\"_id\\\":\\\"id\\\",\\\"title\\\":\\\"demo\\\"}\" | jq .\n```\n\n### 删除文档\n从数据库`data`中删除指定的文档:\n```\ncurl -X DELETE -u admin:admin http://localhost:5984/data/cec6606d0ddaca2b555ebb8404a772a0\n```\n","source":"_posts/blog/couchDB/CouchDB基本操作.md","raw":"---\ntitle: CouchDB基本操作\ndate: 2019-12-26 20:37:17\ntags: CouchDb\ncategories: CouchDb应用\n---\n## CouchDB操作\n\n判断数据库是否正常运行:\n```\ncurl http://localhost:5984/_up | jq .\n```\n获取CouchDB唯一标识符(UUID):\n```\ncurl http://localhost:5984/_uuids | jq .\n```\n获取CouchDB数据库信息:\n```\ncurl http://localhost:5984/ | jq .\n```\n## 节点操作\n\n### 查询节点\n### 查询所有节点\n查询当前节点连接的所有节点以及集群中的节点：\n```\ncurl -u admin:admin http://localhost:5984/_membership\n```\n### 查询单个节点状态\n```\ncurl -u admin:admin http://localhost:5984/_node/{node-name}/_stats\n# 查询本地节点状态\ncurl -u admin:admin http://localhost:5984/_node/local/_stats \n```\n## 数据库操作\n\n### 查询数据库\n\n查询所有数据库:\n```\ncurl http://localhost:5984/_all_dbs | jq .\n```\n查询某个数据库详细信息:\n```\ncurl http://localhost:5984/{db_name} | jq .\n```\n\n查询数据库更新事件:\n```\ncurl -u admin:admin http://localhost:5984/_db_updates | jq .\n```\n\n查询数据库设计文档:\n```\ncurl -u admin:admin http://localhost:5984/data/_design_docs | jq .\n```\n\n### 创建数据库\n创建名称为`data`的数据库，分片数为1，副本数为2(包括源数据库).\n\n* `-u`指定用户名与密码\n* `-X`指定请求方法为`PUT`(不加`-X`默认为`GET`)\n```\ncurl -u admin:admin -X PUT http://localhost:5984/data?q=1&n=2 | jq .\n```\n\n### 删除数据库\n删除刚刚创建的数据库`data`。\n```\ncurl -u admin:admin -X DELETE http://localhost:5984/data | jq .\n```\n\n### 更新数据库\n为指定的数据库创建复合键:\n```\ncurl -X POST  \\\n    -H \"Content-Type:application/json\" \\\n    -H \"Host:localhost:5984\" \\\n    -u admin:admin \\\n    http://localhost:5984/data/_all_docs \\\n    -d \"{ \\\"_id\\\": [ \\\"abc\\\",\\\"bcd\\\" ]}\" | jq .\n```\n\n为指定的本地数据库创建复合键:\n```\ncurl -X POST  \\\n    -H \"Content-Type:application/json\" \\\n    -H \"Host:localhost:5984\" \\\n    -u admin:admin \\\n    http://localhost:5984/data/_local_docs \\\n    -d \"{ \\\"_id\\\": [ \\\"abc\\\",\\\"bcd\\\" ]}\" | jq .\n```\n### 数据库索引\n查询指定数据库索引:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_index | jq .\n```\n为指定数据库创建索引:\n\n* 索引字段为`foo`\n* 索引名称为`foo-index`\n* 索引类型为`json`\n```\ncurl -X POST \\\n    -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    -H \"localhost:5984\" \\\n    http://localhost:5984/data/_index \\\n    -d \"{ \\\"index\\\": { \\\"fields\\\": [\\\"foo\\\" ]}, \\\"name\\\":\\\"foo-index\\\",\\\"type\\\":\\\"json\\\"}\" | jq .\n```\n删除索引:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    -X DELETE \\ http://localhost:5984/data/_index/{ddoc}/json/{index_name}\n```\n\n清空所有视图索引文件:\n```\ncurl -X POST -u admin:admin http://localhost:5984/data/_view_cleanup\n```\n### 数据库分片\n查询指定的数据库分片信息:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_shards | jq .\n```\n根据文档ID查询指定的分片上存储的文档信息:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_shards/{docid} | jq .\n```\n强制进行数据库分片信息同步:\n```\ncurl -u admin:admin \\\n    -X POST \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_sync_shards | jq .\n```\n### 数据库压缩\n压缩指定的数据库:\n```\ncurl -u admin:admin \\\n    -X POST \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_compact | jq .\n```\n\n### 数据库安全\n获取当前数据库安全对象:\n```\ncurl -u admin:admin http://localhost:5984/data/_security | jq .\n```\n## 文档操作\n\n### 查询文档\n查询数据库`data`中所有文档:\n```\ncurl -u admin:admin -H \"Content-Type:application/json\" http://localhost:5984/data/_all_docs | jq .\n```\n\n查询数据库`data`中的指定文档:\n`cec6606d0ddaca2b555ebb8404a772a0`为指定的文档ID\n```\ncurl -u admin:admin http://localhost:5984/data/cec6606d0ddaca2b555ebb8404a772a0 | jq .\n```\n\n查询数据库中文档的更新信息:\n```\ncurl -u admin:admin \\\n    -H \"Content-Type:application/json\" \\\n    http://localhost:5984/data/_changes | jq .\n```\n\n查询本地数据库中的文档:\n```\ncurl -u admin:admin http://localhost:5984/data/_local_docs | jq .\n```\n### 创建文档\n向数据库`data`中创建`id`为`id`,标题为`demo`的新文档:\n\n```\ncurl -X POST \\\n    -H \"Content-Type:application/json\" \\\n    -u admin:admin \\\n    http://localhost:5984/data/ \\\n    -d \"{ \\\"_id\\\":\\\"id\\\",\\\"title\\\":\\\"demo\\\"}\" | jq .\n```\n\n### 删除文档\n从数据库`data`中删除指定的文档:\n```\ncurl -X DELETE -u admin:admin http://localhost:5984/data/cec6606d0ddaca2b555ebb8404a772a0\n```\n","slug":"blog/couchDB/CouchDB基本操作","published":1,"updated":"2020-05-11T03:44:37.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyg7000ok0vq50de9frc","content":"<h2 id=\"CouchDB操作\"><a href=\"#CouchDB操作\" class=\"headerlink\" title=\"CouchDB操作\"></a>CouchDB操作</h2><p>判断数据库是否正常运行:</p>\n<pre><code>curl http://localhost:5984/_up | jq .</code></pre><p>获取CouchDB唯一标识符(UUID):</p>\n<pre><code>curl http://localhost:5984/_uuids | jq .</code></pre><p>获取CouchDB数据库信息:</p>\n<pre><code>curl http://localhost:5984/ | jq .</code></pre><h2 id=\"节点操作\"><a href=\"#节点操作\" class=\"headerlink\" title=\"节点操作\"></a>节点操作</h2><h3 id=\"查询节点\"><a href=\"#查询节点\" class=\"headerlink\" title=\"查询节点\"></a>查询节点</h3><h3 id=\"查询所有节点\"><a href=\"#查询所有节点\" class=\"headerlink\" title=\"查询所有节点\"></a>查询所有节点</h3><p>查询当前节点连接的所有节点以及集群中的节点：</p>\n<pre><code>curl -u admin:admin http://localhost:5984/_membership</code></pre><h3 id=\"查询单个节点状态\"><a href=\"#查询单个节点状态\" class=\"headerlink\" title=\"查询单个节点状态\"></a>查询单个节点状态</h3><pre><code>curl -u admin:admin http://localhost:5984/_node/{node-name}/_stats\n# 查询本地节点状态\ncurl -u admin:admin http://localhost:5984/_node/local/_stats </code></pre><h2 id=\"数据库操作\"><a href=\"#数据库操作\" class=\"headerlink\" title=\"数据库操作\"></a>数据库操作</h2><h3 id=\"查询数据库\"><a href=\"#查询数据库\" class=\"headerlink\" title=\"查询数据库\"></a>查询数据库</h3><p>查询所有数据库:</p>\n<pre><code>curl http://localhost:5984/_all_dbs | jq .</code></pre><p>查询某个数据库详细信息:</p>\n<pre><code>curl http://localhost:5984/{db_name} | jq .</code></pre><p>查询数据库更新事件:</p>\n<pre><code>curl -u admin:admin http://localhost:5984/_db_updates | jq .</code></pre><p>查询数据库设计文档:</p>\n<pre><code>curl -u admin:admin http://localhost:5984/data/_design_docs | jq .</code></pre><h3 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h3><p>创建名称为<code>data</code>的数据库，分片数为1，副本数为2(包括源数据库).</p>\n<ul>\n<li><code>-u</code>指定用户名与密码</li>\n<li><code>-X</code>指定请求方法为<code>PUT</code>(不加<code>-X</code>默认为<code>GET</code>)<pre><code>curl -u admin:admin -X PUT http://localhost:5984/data?q=1&amp;n=2 | jq .</code></pre></li>\n</ul>\n<h3 id=\"删除数据库\"><a href=\"#删除数据库\" class=\"headerlink\" title=\"删除数据库\"></a>删除数据库</h3><p>删除刚刚创建的数据库<code>data</code>。</p>\n<pre><code>curl -u admin:admin -X DELETE http://localhost:5984/data | jq .</code></pre><h3 id=\"更新数据库\"><a href=\"#更新数据库\" class=\"headerlink\" title=\"更新数据库\"></a>更新数据库</h3><p>为指定的数据库创建复合键:</p>\n<pre><code>curl -X POST  \\\n    -H &quot;Content-Type:application/json&quot; \\\n    -H &quot;Host:localhost:5984&quot; \\\n    -u admin:admin \\\n    http://localhost:5984/data/_all_docs \\\n    -d &quot;{ \\&quot;_id\\&quot;: [ \\&quot;abc\\&quot;,\\&quot;bcd\\&quot; ]}&quot; | jq .</code></pre><p>为指定的本地数据库创建复合键:</p>\n<pre><code>curl -X POST  \\\n    -H &quot;Content-Type:application/json&quot; \\\n    -H &quot;Host:localhost:5984&quot; \\\n    -u admin:admin \\\n    http://localhost:5984/data/_local_docs \\\n    -d &quot;{ \\&quot;_id\\&quot;: [ \\&quot;abc\\&quot;,\\&quot;bcd\\&quot; ]}&quot; | jq .</code></pre><h3 id=\"数据库索引\"><a href=\"#数据库索引\" class=\"headerlink\" title=\"数据库索引\"></a>数据库索引</h3><p>查询指定数据库索引:</p>\n<pre><code>curl -u admin:admin \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_index | jq .</code></pre><p>为指定数据库创建索引:</p>\n<ul>\n<li>索引字段为<code>foo</code></li>\n<li>索引名称为<code>foo-index</code></li>\n<li>索引类型为<code>json</code><pre><code>curl -X POST \\\n  -u admin:admin \\\n  -H &quot;Content-Type:application/json&quot; \\\n  -H &quot;localhost:5984&quot; \\\n  http://localhost:5984/data/_index \\\n  -d &quot;{ \\&quot;index\\&quot;: { \\&quot;fields\\&quot;: [\\&quot;foo\\&quot; ]}, \\&quot;name\\&quot;:\\&quot;foo-index\\&quot;,\\&quot;type\\&quot;:\\&quot;json\\&quot;}&quot; | jq .</code></pre>删除索引:<pre><code>curl -u admin:admin \\\n  -H &quot;Content-Type:application/json&quot; \\\n  -X DELETE \\ http://localhost:5984/data/_index/{ddoc}/json/{index_name}</code></pre></li>\n</ul>\n<p>清空所有视图索引文件:</p>\n<pre><code>curl -X POST -u admin:admin http://localhost:5984/data/_view_cleanup</code></pre><h3 id=\"数据库分片\"><a href=\"#数据库分片\" class=\"headerlink\" title=\"数据库分片\"></a>数据库分片</h3><p>查询指定的数据库分片信息:</p>\n<pre><code>curl -u admin:admin \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_shards | jq .</code></pre><p>根据文档ID查询指定的分片上存储的文档信息:</p>\n<pre><code>curl -u admin:admin \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_shards/{docid} | jq .</code></pre><p>强制进行数据库分片信息同步:</p>\n<pre><code>curl -u admin:admin \\\n    -X POST \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_sync_shards | jq .</code></pre><h3 id=\"数据库压缩\"><a href=\"#数据库压缩\" class=\"headerlink\" title=\"数据库压缩\"></a>数据库压缩</h3><p>压缩指定的数据库:</p>\n<pre><code>curl -u admin:admin \\\n    -X POST \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_compact | jq .</code></pre><h3 id=\"数据库安全\"><a href=\"#数据库安全\" class=\"headerlink\" title=\"数据库安全\"></a>数据库安全</h3><p>获取当前数据库安全对象:</p>\n<pre><code>curl -u admin:admin http://localhost:5984/data/_security | jq .</code></pre><h2 id=\"文档操作\"><a href=\"#文档操作\" class=\"headerlink\" title=\"文档操作\"></a>文档操作</h2><h3 id=\"查询文档\"><a href=\"#查询文档\" class=\"headerlink\" title=\"查询文档\"></a>查询文档</h3><p>查询数据库<code>data</code>中所有文档:</p>\n<pre><code>curl -u admin:admin -H &quot;Content-Type:application/json&quot; http://localhost:5984/data/_all_docs | jq .</code></pre><p>查询数据库<code>data</code>中的指定文档:<br><code>cec6606d0ddaca2b555ebb8404a772a0</code>为指定的文档ID</p>\n<pre><code>curl -u admin:admin http://localhost:5984/data/cec6606d0ddaca2b555ebb8404a772a0 | jq .</code></pre><p>查询数据库中文档的更新信息:</p>\n<pre><code>curl -u admin:admin \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_changes | jq .</code></pre><p>查询本地数据库中的文档:</p>\n<pre><code>curl -u admin:admin http://localhost:5984/data/_local_docs | jq .</code></pre><h3 id=\"创建文档\"><a href=\"#创建文档\" class=\"headerlink\" title=\"创建文档\"></a>创建文档</h3><p>向数据库<code>data</code>中创建<code>id</code>为<code>id</code>,标题为<code>demo</code>的新文档:</p>\n<pre><code>curl -X POST \\\n    -H &quot;Content-Type:application/json&quot; \\\n    -u admin:admin \\\n    http://localhost:5984/data/ \\\n    -d &quot;{ \\&quot;_id\\&quot;:\\&quot;id\\&quot;,\\&quot;title\\&quot;:\\&quot;demo\\&quot;}&quot; | jq .</code></pre><h3 id=\"删除文档\"><a href=\"#删除文档\" class=\"headerlink\" title=\"删除文档\"></a>删除文档</h3><p>从数据库<code>data</code>中删除指定的文档:</p>\n<pre><code>curl -X DELETE -u admin:admin http://localhost:5984/data/cec6606d0ddaca2b555ebb8404a772a0</code></pre>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"CouchDB操作\"><a href=\"#CouchDB操作\" class=\"headerlink\" title=\"CouchDB操作\"></a>CouchDB操作</h2><p>判断数据库是否正常运行:</p>\n<pre><code>curl http://localhost:5984/_up | jq .</code></pre><p>获取CouchDB唯一标识符(UUID):</p>\n<pre><code>curl http://localhost:5984/_uuids | jq .</code></pre><p>获取CouchDB数据库信息:</p>\n<pre><code>curl http://localhost:5984/ | jq .</code></pre><h2 id=\"节点操作\"><a href=\"#节点操作\" class=\"headerlink\" title=\"节点操作\"></a>节点操作</h2><h3 id=\"查询节点\"><a href=\"#查询节点\" class=\"headerlink\" title=\"查询节点\"></a>查询节点</h3><h3 id=\"查询所有节点\"><a href=\"#查询所有节点\" class=\"headerlink\" title=\"查询所有节点\"></a>查询所有节点</h3><p>查询当前节点连接的所有节点以及集群中的节点：</p>\n<pre><code>curl -u admin:admin http://localhost:5984/_membership</code></pre><h3 id=\"查询单个节点状态\"><a href=\"#查询单个节点状态\" class=\"headerlink\" title=\"查询单个节点状态\"></a>查询单个节点状态</h3><pre><code>curl -u admin:admin http://localhost:5984/_node/{node-name}/_stats\n# 查询本地节点状态\ncurl -u admin:admin http://localhost:5984/_node/local/_stats </code></pre><h2 id=\"数据库操作\"><a href=\"#数据库操作\" class=\"headerlink\" title=\"数据库操作\"></a>数据库操作</h2><h3 id=\"查询数据库\"><a href=\"#查询数据库\" class=\"headerlink\" title=\"查询数据库\"></a>查询数据库</h3><p>查询所有数据库:</p>\n<pre><code>curl http://localhost:5984/_all_dbs | jq .</code></pre><p>查询某个数据库详细信息:</p>\n<pre><code>curl http://localhost:5984/{db_name} | jq .</code></pre><p>查询数据库更新事件:</p>\n<pre><code>curl -u admin:admin http://localhost:5984/_db_updates | jq .</code></pre><p>查询数据库设计文档:</p>\n<pre><code>curl -u admin:admin http://localhost:5984/data/_design_docs | jq .</code></pre><h3 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h3><p>创建名称为<code>data</code>的数据库，分片数为1，副本数为2(包括源数据库).</p>\n<ul>\n<li><code>-u</code>指定用户名与密码</li>\n<li><code>-X</code>指定请求方法为<code>PUT</code>(不加<code>-X</code>默认为<code>GET</code>)<pre><code>curl -u admin:admin -X PUT http://localhost:5984/data?q=1&amp;n=2 | jq .</code></pre></li>\n</ul>\n<h3 id=\"删除数据库\"><a href=\"#删除数据库\" class=\"headerlink\" title=\"删除数据库\"></a>删除数据库</h3><p>删除刚刚创建的数据库<code>data</code>。</p>\n<pre><code>curl -u admin:admin -X DELETE http://localhost:5984/data | jq .</code></pre><h3 id=\"更新数据库\"><a href=\"#更新数据库\" class=\"headerlink\" title=\"更新数据库\"></a>更新数据库</h3><p>为指定的数据库创建复合键:</p>\n<pre><code>curl -X POST  \\\n    -H &quot;Content-Type:application/json&quot; \\\n    -H &quot;Host:localhost:5984&quot; \\\n    -u admin:admin \\\n    http://localhost:5984/data/_all_docs \\\n    -d &quot;{ \\&quot;_id\\&quot;: [ \\&quot;abc\\&quot;,\\&quot;bcd\\&quot; ]}&quot; | jq .</code></pre><p>为指定的本地数据库创建复合键:</p>\n<pre><code>curl -X POST  \\\n    -H &quot;Content-Type:application/json&quot; \\\n    -H &quot;Host:localhost:5984&quot; \\\n    -u admin:admin \\\n    http://localhost:5984/data/_local_docs \\\n    -d &quot;{ \\&quot;_id\\&quot;: [ \\&quot;abc\\&quot;,\\&quot;bcd\\&quot; ]}&quot; | jq .</code></pre><h3 id=\"数据库索引\"><a href=\"#数据库索引\" class=\"headerlink\" title=\"数据库索引\"></a>数据库索引</h3><p>查询指定数据库索引:</p>\n<pre><code>curl -u admin:admin \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_index | jq .</code></pre><p>为指定数据库创建索引:</p>\n<ul>\n<li>索引字段为<code>foo</code></li>\n<li>索引名称为<code>foo-index</code></li>\n<li>索引类型为<code>json</code><pre><code>curl -X POST \\\n  -u admin:admin \\\n  -H &quot;Content-Type:application/json&quot; \\\n  -H &quot;localhost:5984&quot; \\\n  http://localhost:5984/data/_index \\\n  -d &quot;{ \\&quot;index\\&quot;: { \\&quot;fields\\&quot;: [\\&quot;foo\\&quot; ]}, \\&quot;name\\&quot;:\\&quot;foo-index\\&quot;,\\&quot;type\\&quot;:\\&quot;json\\&quot;}&quot; | jq .</code></pre>删除索引:<pre><code>curl -u admin:admin \\\n  -H &quot;Content-Type:application/json&quot; \\\n  -X DELETE \\ http://localhost:5984/data/_index/{ddoc}/json/{index_name}</code></pre></li>\n</ul>\n<p>清空所有视图索引文件:</p>\n<pre><code>curl -X POST -u admin:admin http://localhost:5984/data/_view_cleanup</code></pre><h3 id=\"数据库分片\"><a href=\"#数据库分片\" class=\"headerlink\" title=\"数据库分片\"></a>数据库分片</h3><p>查询指定的数据库分片信息:</p>\n<pre><code>curl -u admin:admin \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_shards | jq .</code></pre><p>根据文档ID查询指定的分片上存储的文档信息:</p>\n<pre><code>curl -u admin:admin \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_shards/{docid} | jq .</code></pre><p>强制进行数据库分片信息同步:</p>\n<pre><code>curl -u admin:admin \\\n    -X POST \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_sync_shards | jq .</code></pre><h3 id=\"数据库压缩\"><a href=\"#数据库压缩\" class=\"headerlink\" title=\"数据库压缩\"></a>数据库压缩</h3><p>压缩指定的数据库:</p>\n<pre><code>curl -u admin:admin \\\n    -X POST \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_compact | jq .</code></pre><h3 id=\"数据库安全\"><a href=\"#数据库安全\" class=\"headerlink\" title=\"数据库安全\"></a>数据库安全</h3><p>获取当前数据库安全对象:</p>\n<pre><code>curl -u admin:admin http://localhost:5984/data/_security | jq .</code></pre><h2 id=\"文档操作\"><a href=\"#文档操作\" class=\"headerlink\" title=\"文档操作\"></a>文档操作</h2><h3 id=\"查询文档\"><a href=\"#查询文档\" class=\"headerlink\" title=\"查询文档\"></a>查询文档</h3><p>查询数据库<code>data</code>中所有文档:</p>\n<pre><code>curl -u admin:admin -H &quot;Content-Type:application/json&quot; http://localhost:5984/data/_all_docs | jq .</code></pre><p>查询数据库<code>data</code>中的指定文档:<br><code>cec6606d0ddaca2b555ebb8404a772a0</code>为指定的文档ID</p>\n<pre><code>curl -u admin:admin http://localhost:5984/data/cec6606d0ddaca2b555ebb8404a772a0 | jq .</code></pre><p>查询数据库中文档的更新信息:</p>\n<pre><code>curl -u admin:admin \\\n    -H &quot;Content-Type:application/json&quot; \\\n    http://localhost:5984/data/_changes | jq .</code></pre><p>查询本地数据库中的文档:</p>\n<pre><code>curl -u admin:admin http://localhost:5984/data/_local_docs | jq .</code></pre><h3 id=\"创建文档\"><a href=\"#创建文档\" class=\"headerlink\" title=\"创建文档\"></a>创建文档</h3><p>向数据库<code>data</code>中创建<code>id</code>为<code>id</code>,标题为<code>demo</code>的新文档:</p>\n<pre><code>curl -X POST \\\n    -H &quot;Content-Type:application/json&quot; \\\n    -u admin:admin \\\n    http://localhost:5984/data/ \\\n    -d &quot;{ \\&quot;_id\\&quot;:\\&quot;id\\&quot;,\\&quot;title\\&quot;:\\&quot;demo\\&quot;}&quot; | jq .</code></pre><h3 id=\"删除文档\"><a href=\"#删除文档\" class=\"headerlink\" title=\"删除文档\"></a>删除文档</h3><p>从数据库<code>data</code>中删除指定的文档:</p>\n<pre><code>curl -X DELETE -u admin:admin http://localhost:5984/data/cec6606d0ddaca2b555ebb8404a772a0</code></pre>"},{"title":"Raft算法论文(部分)","date":"2020-01-04T07:07:35.000Z","_content":"原文地址->[Raft算法](https://raft.github.io/raft.pdf)\n\n# 摘要\nRaft是用于管理被复制的日志的共识算法。它与multi-Paxos算法产生的效果相同，并且和Paxos算法一样高效。但是结构与Paxos不同。这使得Raft算法比Paxos算法更容易理解。也为构建实际系统提供了更好的基础。为了加强理解，Raft将几个关键元素分离，比如leader选举，日志复制，安全性。并增强了一致性，以减少必须考虑的状态数。一项用户研究的结果表明，与Paxos相比，Raft算法更易于学生学习。Raft也提供了用于更新集群成员关系的新的机制。它使用重叠的多数来保证安全。\n\n# 1 介绍\n共识算法允许一组计算机的集合作为一个一致的的小组工作，这些小组可以承受某些成员的故障。正因为如此，共识机制在构建可信的大规模软件系统中起着至关重要的作用。在过去的十年中，Paxos一直主导着共识算法的讨论。很多共识算法都是基于Paxos或者受它的影响。Paxos称为了教受学生关于共识算法的主要工具。\n不幸的是，尽管进行了许多尝试以使Paxos更加平易近人，Paxos仍然非常难以理解。此外，其体系结构需要复杂的更改以支持实际系统。结果，系统构建者和学生都与Paxos斗争。\n在我们与Paxos斗争之后，我们着手寻找一种新的共识算法，该算法可以为系统构建和教育提供更好的基础。我们的方法与众不同，因为我们的主要目标是易于理解：我们能否为实际系统定义共识算法，并以比Paxos容易学习的方式对其进行描述？此外，我们希望该算法有助于系统开发人员必不可少的直觉的发展。重要的不仅是算法能起作用，而且要很清除它为什么起作用。\n这项工作的结果是一个称为Raft的共识算法。在设计Raft时，我们应用了特定的技术来提高可理解性，包括分解（Raft分离了领导者选举，日志复制和安全性）以及状态空间减少（相对于Paxos，Raft减少了不确定性的程度以及服务器之间可能不一致的方式）。 一项对两所大学的43名学生进行的用户研究表明，Raft比Paxos容易理解得多：在学习了两种算法之后，其中33位学生比Rax更好地回答了有关Raft的问题。\nRaft在许多方面与现有的共识算法相似（最著名的是Oki和Liskov的Viewstamped复制），但是它具有几个新颖的功能：\n\n* 强壮的leader:与其他共识算法相比，Raft使用更强大的领导方式。例如，日志条目仅从leader者流向其他服务器。 这简化了复制日志的管理，并使Raft更易于理解。\n* Leader选举:Raft使用随机计时器选举leader。这可以为任何共识算法已经要求的心跳添加少量机制，同时可以快速而轻松地解决冲突。\n* 成员关系变化:Raft更改集群中服务器组的机制使用了一种新的联合共识方法，其中，两种不同配置的大多数在转换过程中会重叠。 这允许群集在配置更改期间继续正常运行。\n\n我们认为Raft在教育目的和实施基础上均优于Paxos和其他共识算法。它比其他算法更简单，更易懂。本文对其进行了足够详尽的描述以满足实际系统的需求。它具有多种开源实现，并被多家公司使用；其安全性能已得到正式规定和证明；并且其效率可与其他算法相比。\n本文的其余部分介绍了复制状态机问题(第2节)，讨论了Paxos的优缺点(第3节),描述了我们对可理解性的一般方法(第4节)，介绍了Raft共识算法(第5–8节),评估Raft(第9节),并讨论相关工作(第10节)。\n\n# 2 复制状态机\n共识算法通常出现在复制状态机的环境中。通过这种方法，服务器集合上的状态机可以计算相同状态的相同副本，即使某些服务器宕机也可以继续运行。 复制状态机用于解决分布式系统中的各种容错问题。例如，具有单个集群领导者的大型系统，例如GFS，HDFS和RAMCloud，通常使用单独的复制状态机来管理领导者选举并存储配置信息，这些信息必须在领导者崩溃中幸存。复制状态机的示例包括Chubby和ZooKeeper。\n复制状态机通常使用复制日志来实现，如图1所示。每个服务器都存储一个包含一系列命令的日志，其状态机按顺序执行这些命令。每个日志以相同的顺序包含相同的命令，因此每个状态机处理相同的命令序列。由于状态机是确定性的，每个计算相同的状态和输出的顺序相同。\n保持复制日志的一致性是共识算法的工作。服务器上的共识模块从客户端接收命令，并将其添加到其日志中。它与其他服务器上的共识模块进行通信，以确保即使某些服务器发生故障，每个日志最终仍会以相同顺序包含相同的请求。正确复制命令后，每台服务器的状态机都将以日志顺序对其进行处理，然后将输出返回给客户端。服务器似乎形成了单个高度可靠的状态机。\n\t\t\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145431490-1698442829.png\" width = \"300\" height = \"180\" alt=\"图1\" align=center />\n\n#### 图1：复制状态机架构。 共识算法管理包含来自客户端的状态机命令的复制日志。 状态机处理来自日志的相同命令序列，因此它们产生相同的输出。\n实际系统的共识算法通常具有以下属性：\n\n* 它们可确保在所有非拜占庭条件下的安全性（绝不会返回错误的结果），包括网络延迟，分区，数据包丢失，复制和重新排序。\n* 只要大多数服务器都可以运行并且可以相互通信并与客户端进行通信，它们就可以正常运行（可用）。因此，由五个服务器组成的典型集群可以容忍任何两个服务器的故障。假定服务器因停止而发生故障；它们稍后可能会从稳定存储上的状态中恢复并重新加入群集。\n* 它们不依赖于时序来确保日志的一致性：错误的时钟和极端的消息延迟可能在最坏的情况下导致可用性问题。\n* 在通常情况下，只要集群的大多数都响应了一次远程过程调用，命令就可以完成。少数速度较慢的服务器不必影响整体系统性能。\n\n# 3 Paxos算法怎么了\n...\n# 4 可理解性的设计\n...\n# 5 Raft共识算法\n\nRaft是一个如第二部分描述的对被复制的日志进行管理的算法。图2以简明形式总结了该算法以供参考，图3列出了该算法的关键属性。这些图的元素将在本节的其余部分中进行分段讨论。\nRaft通过首先选举一位杰出的Leader，然后赋予Leader完全的责任来管理复制日志来实现共识。Leader接受来自客户端的日志条目，将其复制到其他服务器上，并告诉服务器何时可以安全地将日志条目应用于其状态机。拥有一个Leader可以简化复制日志的管理。例如，Leader可以决定在何处放置新条目而无需咨询其他服务器，并且数据以简单的方式从Leader流向其他服务器。Leader可能会失败或与其他服务器断开连接，在这种情况下，将选出新的Leader。\n使用Leader方法，Raft将共识问题分解为三个相对独立的子问题，这些子问题将在以下小节中进行讨论：\n\n* Leader选举:当存在的Leader失败后必须选出一个新的Leader(5.2部分)。\n* 日志复制:Leader必须接受客户端发送的日志条目并通过集群复制他们，强制其他日志接受自己的(5.3部分)。\n* 安全性:Raft的关键安全属性是图3中的状态机安全属性：如果任何服务器已将特定的日志条目应用于其状态机，则没有其他服务器可以在同一日志索引下应用不同的命令。5.4节介绍了Raft如何确保此属性；解决方案包括对第5.2节所述的选举机制的附加限制。\n\n\n\n| 状态 |  |\n| --- | --- |\n|**所有服务器上的一致状态**  | **在响应RPCs前稳定更新存储** |\n|currentTerm | 最新的服务器任期(第一次引导启动时初始化为0)单调递增 |\n|votedFor | 当前任期投票的candidateId(如果没有则为null),为谁投票则对应的值为谁 |\n|log[]  | 日志条目集合，被Leader接收到的每一条日志包含状态机的命令和任期。(第一条索引为1) |\n|**所有服务器上的隔离状态**  |  |\n|commitIndex  |已知的被提交的被最高的日志索引(初始为0，单调递增)  |\n|lastApplied  |被应用到状态机的最高的日志条目索引(初始为0，单调递增)  |\n|**Leader上的隔离状态**  |**在选举后重新初始化**  |\n| nextIndex[]  | Leader对于每一台服务器，将要发送的下一条日志条目(被Leader初始化为最后一条日志索引+1) |\n|matchIndex[] | Leader对于每一台服务器，已知的被复制到服务器上的最高的日志条目索引(初始为0，单调递增) |\n\n\n| **追加日志条目RPC** | **由Leader调用完成日志复制(5.3节)，也可以用于心跳信息(5.2节)** |\n| --- | --- |\n|**参数:**  |  |\n|term  |Leader的任期|\n|leaderId  |follower可以重定向客户端  |\n|prevLogIndex  | 紧接新记录之前的日志条目索引(即上一条日志条目索引) |\n|prevLogTerm  |上一条日志条目索引的任期  |\n|entries[]  |用于存储的日志实体(心跳信息为空，以至于更高效的发送)  |\n|leaderCommit  |Leader的提交的索引  |\n|**结果:**  |  |\n|term  |当前任期，用于Leader更新自己的任期  |\n|success  |如果follower包含的日志实体匹配到prevLogIndex和PrevLogTerm  |\n|**接收者实现:**||\n|1.如果任期小于当前任期回复false|5.1节|\n|2.如果包含的日志实体没有匹配到prevLogIndex和PrevLogTerm回复false|5.3节|\n|3.如果存在日志实体与新的日志实体冲突(相同的索引但任期不同),删除存在的日志实体并选择新的|5.3节|\n|4.追加日志中尚未存在的任何新条目||\n|5.如果leaderCommit大于commintIndex,将commitIndex设置为(leaderCommit,最后一条新的日志实体索引)中最小的那个||\n\n\n\n| **请求投票RPC** |**由candidates调用用于收集投票数(5.2节)**  |\n| --- | --- |\n|**参数:**  |  |\n|term  |candidate的任期  |\n|candidateId  |请求投票的candidateID  |\n|lastLogIndex  |candidate的最后一条日志条目索引 (5.4节) |\n|lastLogTerm  |candidate的最后一条日志条目的任期(5.4节)  |\n|**结果:**  |  |\n|term  |当前任期，用以candidate更新自己的任期  |\n|voteGranted  |如果candidate接受了投票则为true  |\n|**接收者实现**  |  |\n|1.如果任期小于当前任期回复false  |5.1节  |\n|2.如果votedFor为空或者candidateId,且candidate的日志\n至少与接收者的日志一样新，同意投票 |5.2节 5.4节  |\n\n#### 服务器的规则:\n**所有服务器:**\n\n* 如果commitIndex大于lastApplied;增加lastApplied,应用log[lastApplied]到状态机(5.3节)\n* 如果RPC请求或响应中的任期T大于currentTerm；设置currentTerm为T，变为follower(5.1节)\n\n**所有Follower(5.2节)**\n\n* 响应来自candidates和Leader的RPC消息。\n* 如果直到选举超时也没有接受到由当前Leader发送的追加日志条目RPC消息或者对candidate的投票，变为candidate\n\n**所有Candidate(5.2节)**\n\n* 当转换为Candidate后，启动选举过程：\n    * 增加currentTerm\n    * 为自己投票\n    * 重置选举计时器\n    * 发送请求投票RPC消息到其他所有服务器\n* 如果接收到大多数成员的投票信息，变为Leader\n* 如果接收到来自新的Leader的追加日志条目RPC消息，转换为follower\n* 如果选举超时，启动新的选举过程\n\n**Leader**\n\n* 选举过后：将初始的空追加日志条目RP(心跳)消息发送到每个服务器；在空闲时间重复此操作以防止选举超时(5.2节)\n* 如果接收到来自客户端的命令，追加日志到本地，在日志条目应用到状态机后回复客户端。\n* 如果对于follower，lastLogIndex大于等于nextIndex，发送带有从nextIndex开始的日志条目的追加日志条目RPC消息。\n    * 如果响应成功，更新对于该follower的nextIndex和matchIndex\n    * 如果因为日志的不一致性导致追加日志实体消息失败，递减nextIndex并重试\n* 如果存在N并且N大于commitIndex,并且大多数matchIndex[i]大于等于N，且log[N]的任期与currentTerm相等，将commitIndex设置为N(5.3 5.4节)\n\n#### 图2\n\n#### 选举安全\n在给定的任期，最多只能选举一个Leader。5.2节\n**Leader只追加特性:** Leader从不覆盖或删除它的日志条目，只追加新的。5.3节\n**日志匹配:** 如果两个日志包含的实体具有相同的索引和任期，那么直到给定索引为止，所有条目中的日志都是相同的。\n**Leader完备性:** 如果一个日志提示在给定的任期内被提交，那么该条目将出现在领导者的日志中，显示所有编号较高的条目。\n**状态机安全:** 如果服务器应用一条给定索引的日志实体到它的状态机，那么没有其他服务器可以应用一条不同的日志到相同的索引位置。\n\n#### 图3：Raft保证任何时刻这里的每一条属性都是成立的。\n\n\n## 5.1 Raft基础\n一个Raft集群包含多个服务器；五是一个典型数字，它允许系统容忍两个服务器故障。在任何给定时间，每个服务器都处于以下三种状态之一：Leader，Follower或Candidate。在正常操作中，只有一个Leader，而其他所有服务器都是Follower。 Follower是被动的：他们自己不发出请求，而只是响应Leader和Candidate的请求。Leader处理所有客户请求(如果客户联系Follower，则Follower将其重定向到Leader)。第3种状态Candidate用于选举新的Leader。图4显示了状态及其转换。 过渡将在下面讨论。\nRaft将时间划分为任意长度的项，如图5所示。项用连续的整数编号。每个任期都以选举开始，在选举中，一个或多个Candidate试图按照5.2节中的描述成为Leader。 如果Candidate在选举中获胜，则它将在剩余任期中担任Leader。在某些情况下，选举将导致投票分裂。在这种情况下，任期将以无Leader结束；新任期(以新的选举)将很快开始。Raft确保给定任期内最多有一位Leader。\n不同的服务器可能会在不同时间观察任期之间的转换，并且在某些情况下，服务器可能不会观察到选举甚至整个任期。任期在Raft中充当逻辑时钟，它们使服务器能够检测过时的信息，例如陈旧的Leader。每个服务器存储一个当前的任期号，该任期号随时间单调增加。只要服务器进行通信，就会交换当前任期；如果一台服务器的当前任期小于另一台服务器，则它将其当前任期更新为较大的值。如果Candidate或Leader发现其任期已过时，它将立即恢复为Follower状态。如果服务器收到带有过期条款编号的请求，则服务器将拒绝该请求。\nRaft式服务器使用远程过程调用（RPC）进行通信，并且基本共识算法仅需要两种类型的RPC。RequestVote RPC由Candidate在选举期间启动（第5.2节），而AppendEntries RPC由Leader启动以复制日志条目并提供心跳形式(第5.3节)的AppendEntries RPC消息。第7节添加了第三个RPC，用于在服务器之间传输快照。如果服务器未及时收到响应，则服务器会重试RPC，并且它们并行发出RPC消息以获得最佳性能。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145525858-271973058.png\" width = \"300\" height = \"150\" alt=\"图4\" align=center />\n#### 图4：服务器状态。Follower仅响应来自其他服务器的请求。如果Follower未收到任何通讯，它将成为Candidate并发起选举。从整个集群的大多数中获得选票的Candidate将成为新的Leader。Leader通常会运作直到失败。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145554351-712530469.png\" width = \"300\" height = \"150\" alt=\"图5\" align=center />\n#### 图5:时间分为几个任期，每个任期都以选举开始。选举成功后，由一位Leader管理集群，直到任期结束。在这种情况下可能选举失败导致任期届满而未选出Leader。任期之间的转换可以在不同的服务器上的不同时间观察到。\n## 5.2 Leader选举\nRaft使用心跳机制触发Leader选举。当服务器启动的角色为Follower。服务器保持Follower状态一直到接收到来自Leader或者Candidate的有效的RPC消息。Leader为了维护它的权利将发送周期性的心跳消息(不带有日志实体的AppendEntries RPCs消息)到所有的Follower。如果一个Follower在一整个周期时间内没有接收到任何通信消息则称为选举超时。他们将假设没有可以访问的Leader并开始新的投票选举新的Leader。\n为了开始一次选举，Follower递增它的当前任期并将状态转换为Candidate。然后为自己投一票并并行发送RequestVote RPC消息到集群中其他的所有服务器。Candidate的状态将会一直保持一直到这三种情况中其中一个发生:\n\n1. 赢得选举\n2. 另外一个服务器称为了Leader\n3. 在当前投票周期内没有赢得选举的服务器。\n\n将会在下面分别讨论这三种情况。\n如果Candidate在同一任期内从整个集群中获得大多数服务器的票数，则将赢得选举。在给定的期限内，每台服务器将按先到先得的原则为最多一个Candidate投票(注：第5.4节增加了投票的其他限制)。多数规则确保最多只有一名Candidate可以赢得特定任期的选举(图3中的选举安全属性)。Candidate赢得选举后，便成为Leader。然后，它将心跳消息发送到所有其他服务器以建立其权限并阻止新的选举。\n在等待投票时，Candidate可能会从声称是Leader的另一台服务器收到AppendEntries RPC消息。如果Leader的任期(在其RPC消息中可以获得)至少与Candidate当前任期一样大，则Candidate将Leader视为合法，并返回到Follower状态。 如果RPC中的任期小于Candidate当前的任期，则Candidate将拒绝RPC并继续处于Candidate状态。\n第三种可能的结果是，Candidate既不会赢得选举也不会输掉选举：如果同时有许多Follower成为Candidate，那么票数可能会分散，从而任何Candidate都不会获得多数投票。当这种情况发生时，每个Candidate都将超时，并通过增加其任期并启动另一轮RequestVote RPC来开始新的选举。但是，如果不采取额外措施，分散投票可以无限期地重复。\nRaft使用随机的选举超时来确保分散票很少发生，并且可以快速解决。为了避免投票分散，首先从固定间隔(例如150-300毫秒)中随机选择选举超时。这会分散服务器超时的时间，因此在大多数情况下，只有一台服务器会超时。它会赢得选举并在其他任何服务器超时之前发送心跳信号。使用相同的机制来处理分散投票。每位候选人在选举开始时都会重新启动其随机选举超时时间，并等待该超时时间过去后才开始下一次选举。这减少了在新选举中再次进行分散投票的可能性。第9.3节显示，这种方法可以迅速选举出一位Leader。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145629826-958115128.png\" width = \"300\" height = \"180\" alt=\"图6\" align=center />\n#### 图6:日志由条目组成，这些条目按顺序编号。每个条目包含创建它的任期（每个框中的数字）和状态机的命令。如果可以安全地将条目应用于状态机，则认为该条目已提交。\n选举是如何理解指导我们在设计备选方案之间进行选择的一个示例。最初，我们计划使用排名系统：为每个Candidate分配一个唯一的排名，该排名用于在竞争Candidate之间进行选择。如果某个Candidate发现了另一名更高级别的Candidate，它将返回到Follower状态，以便更高级别的Candidate可以更轻松地赢得下一次选举。我们发现，这种方法在可用性方面产生了一些细微的问题(排名较低的服务器可能需要超时，如果排名较高的服务器出现故障，则可能再次成为Candidate，但是如果这样做过早，则可以重置选举Leader的进度)。我们对算法进行了数次调整，但每次调整后都会出现新的极端情况。 最终，我们得出结论，随机重试方法更加明显和易于理解。\n\n## 5.3 日志复制\n选举Leader后，便开始为客户的请求提供服务。每个客户端请求都包含可以由复制状态机执行的命令。Leader将命令作为新条目添加到其日志中，然后与其他每个服务器并行发出AppendEntries RPC消息，以复制该条目。在安全地复制了条目之后（如下所述），Leader将该条目应用于其状态机，并将执行结果返回给客户端。如果Follower崩溃或运行缓慢，或者丢失了网络数据包，则领导者会无限次（即使在响应客户端之后）重试附加该RPC消息，直到所有Follower最终存储所有日志条目为止。\n日志的组织结构如图6所示。当Leader收到条目时，每个日志条目都会存储一个状态机命令以及任期号。日志条目中的任期号用于检测日志之间的不一致并确保图3中的某些属性。每个日志条目还具有一个整数索引，用于标识其在日志中的位置。\nLeader决定什么时候可以安全地对状态机进行日志记录。这样的条目称为已提交。Raft保证提交的条目是持久的，并且最终将由所有可用状态机执行。一旦创建条目的Leader已在大多数服务器上复制了该日志条目（例如，图6中的条目7），则提交该日志条目。这还将提交Leader日志中的所有先前条目，包括先前Leader创建的条目。第5.4节讨论了Leader变更后应用此规则时的一些细微之处，并且还表明了对提交的定义是安全的。Leader保持记录被提交的日志的最高索引，并将该索引包括在将来的AppendEntries RPC（包括心跳）中，以便其他服务器发现。跟随者得知日志条目已提交后，便将该条目应用于其本地状态机（按日志顺序）。\n我们设计了Raft日志机制来维持不同服务器上的日志之间的高度一致性。这不仅简化了系统的行为并使其更具可预测性，而且是确保安全的重要组成部分。Raft维护以下属性，它们共同构成了图3中的Log Matching属性：\n\n* 如果两个不同的日志实体具有相同的索引和任期号，那么他们存储有相同的命令。\n* 如果两个不同的日志实体具有相同的索引和任期号，则所有先前条目中的日志都相同。\n\n第一个属性来自以下事实：Leader在给定期限内最多创建一个具有给定日志索引的条目，并且日志条目从不更改其在日志中的位置。第二个属性由AppendEntries执行的简单一致性检查保证。在发送AppendEntries RPC时，Leader将在其日志中紧接新条目之前包含条目的索引和任期号。如果Follower在其日志中找不到具有相同索引和任期的条目，则它拒绝新条目。一致性检查是一个归纳步骤：日志的初始空状态满足Log Matching属性，并且只要扩展日志，一致性检查都会保留Log Matching属性。 因此，只要AppendEntries成功返回，Leader就会知道Follower的日志与它自己通过新条目记录的日志相同。\n在正常操作期间，Leader和Follower的日志保持一致，因此AppendEntries一致性检查永远不会失败。但是，Leader崩溃可能会使日志不一致（旧的Leader可能没有完全复制其日志中的所有条目）。这些不一致会加剧一系列的Leader和Follower崩溃。图7说明了Follower的日志与新Follower的日志可能不同的方式。 Follower可能缺少Leader上存在的条目，它可能具有Leader上不存在的额外条目，或者两者都有。日志中的缺失条目和多余条目可能跨越多个任期。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145703880-1796421720.png\" width = \"300\" height = \"150\" alt=\"图7\" align=center />\n#### 图7:当最上面的为Leader时，Follower日志中可能会出现任何情况（a–f）。每个框代表一个日志条目；框中的数字是其用语。Follower可能缺少条目（a–b），可能有多余的未提交条目（c–d），或者两者都有（e–f）。 例如，如果该服务器是第2任期的Leader，则可能会发生场景（f）。它迅速重启，成为第三学期的Leader，并在其日志中添加了更多条目； 在提交任期2或任期3中的任何条目之前，服务器再次崩溃并保持关闭状态。\n\n在Raft中，Leader通过强迫Follower的日志重复自己的日志来处理不一致之处。这意味着Follower日志中的冲突条目将被Leader日志中的条目覆盖。第5.4节将说明，当再加上一个限制条件时则是安全的。\n为了使Follower的日志与自己的日志保持一致，Leader必须找到两个日志一致的最新日志条目的位置，在该位置之后删除Follower日志中的所有条目，并将该位置之后的Leader的所有条目发送给Follower。所有这些操作都是响应AppendEntries RPC执行的一致性检查而发生的。Leader为每个关注者维护一个nextIndex，这是Leader将发送给该Follower的下一个日志条目的索引。当成功选举为Leader时，它将所有nextIndex值初始化为刚好在其日志中的最后一个索引之后的索引（图7中的11）。如果Follower的日志与Leader的日志不一致，则下一个AppendEntries RPC中的AppendEntries一致性检查将失败。在拒绝之后，Leader递减nextIndex并重试AppendEntries RPC。最终nextIndex将到达Leader和Follower日志匹配的位置。发生这种情况时，AppendEntries将成功执行，这将删除Follower日志中的所有冲突条目，并添加Leader的日志中的条目（如果有）。一旦AppendEntries成功通过，Follower的日志便与Leader的日志保持一致，并且在本任期的其余时间中都将保持这种状态。\n如果需要，可以优化协议以减少拒绝的AppendEntries RPC的数量。例如，当发送拒绝AppendEntries请求的消息时，Follower可以将冲突条目的任期以及该任期存储的第一个索引包括在内。有了这些信息，Leader可以递减nextIndex来绕过该任期中所有冲突的条目。每个具有冲突条目的任期都需要一个AppendEntries RPC消息，而不是每个条目一个RPC。 在实践中，我们怀疑这种优化是否必要，因为故障很少发生，并且不太可能出现许多不一致的情况。\n通过这种机制，Leader在启动时无需采取任何特殊措施即可恢复日志的一致性。它只是开始正常运行，并且响应于AppendEntries一致性检查的失败，日志自动收敛。Leader永远不会覆盖或删除其自己的日志中的条目（图3中的Leader Append-Only属性）。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145725706-1972318156.png\" width = \"300\" height = \"150\" alt=\"图8\" align=center />\n#### 图8:一个时序显示Leader为何无法使用较早任期的日志条目来确定提交。 在（a）中，S1是Leader，并部分复制索引2处的日志条目。S5在S3，S4及其本身的投票下当选为任期3的Leader，并在日志索引2处接受不同的条目。S1重新启动，被选为Leader，然后继续复制。至此，任期2的日志条目已在大多数服务器上复制，但尚未提交。如果S1像（d）中那样崩溃，则S5可以被选为Leader（来自S2，S3和S4的投票），并用任期3中的条目覆盖该条目。但是，如果S1在崩溃之前在大多数服务器上其当前任期复制了一个条目，如（e）所示，该条目已提交（S5无法赢得选举）。此时，日志中的所有先前条目也将被提交。\n这种日志复制机制展现了第2节中描述的理想的共识属性：只要大多数服务器都在运行，Raft可以接受，复制和应用新的日志条目。通常情况下，可以通过一轮RPC将新条目复制到大多数集群中。一个慢速Follower不会影响性能。\n\n## 5.4 安全性\n前面的章节描述了Raft如何选择Leader并复制日志条目。但是，到目前为止描述的机制还不足以确保每个状态机以相同的顺序执行完全相同的命令。例如，当Leader提交多个日志条目时，Follower可能不可用。然后可以当选为Leader并用新的日志条目覆盖这些条目。结果，不同的状态机可能执行不同的命令序列。\n本节通过添加限制哪些服务器可以当选领导者来完善Raft算法。该限制可确保任何给定任期的Leader都包含先前任期中提交的所有条目（图3中的“领导者完整性”属性）。给定选举限制，我们便使承诺规则更加精确。最后，我们为Leader完整性属性提供了一个证明草图，并显示了它如何导致复制状态机的正确行为。\n\n### 5.4.1 选举限制\n在任何基于Leader的共识算法中，Leader最终必须存储所有提交的日志条目。在某些共识算法中，例如“加盖时间戳的复制” ，即使最初并不包含所有提交的条目，也可以选择一个Leader。这些算法包含其他机制，无论是在选举过程中还是选举后不久，可以识别丢失的条目并将其发送给新的Leader。不幸的是，这导致了相当大的额外机制和复杂性。Raft使用一种更简单的方法来保证自新任Leader选举之日都具有所有先前提交的条目，而无需将这些条目转移给Leader。这意味着日志条目仅在一个方向上（从Leader到Follower）流动，并且Leader永远不会覆盖其日志中的现有条目。\nRaft使用投票程序来防止Candidate赢得选举，除非其日志中包含所有已提交的条目。Candidate必须联系集群的大多数才能被选举，这意味着每个提交的条目都必须存在于其中至少一台服务器中。如果Candidate的日志至少与该多数服务器日志中的日志一样最新（以下精确定义了“最新”），则它将保存所有已提交的条目。RequestVote RPC实施了此限制：RPC包含有关Candidate日志的信息，如果投票者自己的日志比Candidate的日志最新，则投票者将拒绝投票。\nRaft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新的。 如果日志中的最后一个条目具有不同的任期，则带有较新任期的日志是最新的。如果日志以相同的任期结尾，则以索引更大的日志为准。\n\n### 5.4.2 提交之前任期的日志\n如第5.3节所述，Leader知道，一旦该条目存储在大多数服务器上，就会提交当前项的输入。如果Leader在提交条目之前崩溃，将来的Leader将尝试完成复制条目。但是，Leader不能立即得出以下结论：一旦上一个任期的条目存储在大多数服务器上，就将其提交。 图-8说明了一种情况，其中旧的日志条目存储在大多数服务器上，但将来的Leader仍可以覆盖。\n为了消除如图8所示的问题，Raft决不通过计算副本数来提交前项的日志条目。只有Leader当前任期的日志条目才通过计算副本数来提交；一旦以这种方式提交了当前任期的条目，则由于“日志匹配”属性而间接提交了所有先前的条目。在某些情况下，Leader可以安全地断定已提交了较旧的日志输入（例如，如果该条目存储在每个服务器上），但是Raft为简化起见采取了更为保守的方法。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145750026-1894945621.png\" width = \"300\" height = \"140\" alt=\"图9\" align=center />\n#### 图9:如果S1（任期T的Leader）从其任期中提交了新的日志条目，并且S5当选为下一任期U的Leader，那么必须至少有一个服务器（S3）接受了该日志条目并投票支持S5。\nRaft会在承诺规则中带来额外的复杂性，因为当Leader从先前条款中复制条目时，日志条目将保留其原始任期号。在其他共识算法中，如果新的Leader从先前的“任期号”中复制条目，则必须使用其新的“任期号”来进行复制。Raft的方法使推理日志条目变得更加容易，因为它们在整个周期和跨日志期间保持相同的任期编号。此外，与其他算法相比，Raft中的新Leader发送的先前条目的日志条目要少（其他算法必须发送冗余日志条目以对其重新编号，然后才能提交）。\n\n### 5.4.3 安全性讨论\n给定完整的Raft算法，我们现在可以更精确地论证“领导者完整性属性”成立（此论点基于安全性证明；请参见9.2节）。 我们假设Leader完整性属性不成立，那么我们证明了一个矛盾。 假设任期*T*的Leader提交了其任期的日志条目，但该日志条目未由某个将来任期的Leader存储。考虑最小项任期*U>T*，其领导者*leaderU* 不存储该条目。\n\n1. 提交时，必须在*LeaderU*的日志中没有已提交的条目（Leader绝不会删除或覆盖条目）。\n2. *LeaderT*在大多数集群中复制了该条目，*leaderU*从大多数集群中获得了投票。因此，如图9所示，至少有一个服务器（“投票者”）既接受了*LeaderT*的条目又投票给*LeaderU*，投票者是达成矛盾的关键。\n3. 投票者必须在接受*LeaderU*投票之前已经接受了*LeaderT*提交的条目；否则，它将拒绝来自*LeaderT*的AppendEntries请求（其当前期限将高于*T*）。\n4. 当每个投票者对*leaderU*投票时，投票者仍然存储该条目，因为每个居间的Leader都包含该条目（通过假设），Leader从不删除条目，而Follower仅在与Leader冲突时才删除条目。\n5. 投票者将投票结果授予了*LeaderU*，因此*LeaderU*的日志必须与投票者的日志一样最新。这导致两个矛盾之一。\n6. 首先，如果投票者和*LeaderU*共享相同的最后一个日志条款，那么*LeaderU*的日志必须至少与投票者的日志一样长，因此其日志包含投票者日志中的每个条目。这是一个矛盾，因为投票者中包含已承诺的条目，而*leaderU*被假定为不包含。\n7. 否则，*leaderU*的最后一条日志的任期号必须大于投票者的最后一条日志的任期号。而且，它比*T*大，因为投票者的上一个日志任期号至少为*T*（它包含任期*T*中的所有已提交的条目）。创建*leaderU*的最后一个日志条目的较早的Leader必须在其日志中（通过假设）包含已提交的条目。然后，通过日志匹配属性，*leaderU*的日志还必须包含已提交的条目，这是矛盾的。\n8. 这样就完成了矛盾。因此，所有任期大于*T*的Leader都必须包含在任期*T*中提交的所有任期为*T*的日志。\n9. 日志匹配属性保证未来的Leader还将包含间接提交的条目，例如图8（d）中的索引2。\n\n给定“Leader完整性”属性，我们可以从图3证明“状态机安全性”属性，该状态表明，如果服务器已将给定索引的日志条目应用于其状态机，则其他任何服务器都不会为该状态机在相同的索引应用不同的日志条目。服务器在将日志条目应用于其状态机时，其日志必须与通过该条目的Leader的日志相同，并且必须提交该条目。现在考虑任何服务器应用给定日志索引的最低任期；日志完整性属性可确保Leader将存储所有较高任期的相同的日志条目，因此，服务器将在最新的任期中应用的索引将应用相同的值。 因此，状态机安全属性成立。\n最后，Raft要求服务器以日志索引顺序应用条目。结合状态机安全属性，这意味着所有服务器将以相同的顺序将完全相同的日志条目集应用于其状态机。\n\n## 5.5 Follower与Candidate崩溃\n到目前为止，我们只关注Leader的失败。Follower与Candidate崩溃比Leader崩溃要容易得多，并且两者的处理方式相同。 如果Follower与Candidate崩溃，则将来发送给它的RequestVote和AppendEntries RPC将失败。 Raft通过无限期重试来处理这些故障；如果崩溃的服务器重新启动，则RPC将成功完成。如果服务器在完成RPC之后但在响应之前崩溃，则在重新启动后它将再次收到相同的RPC。Raft中的RPC是幂等的，因此不会造成伤害。例如，如果Follower收到一个AppendEntries请求，其中包括其日志中已经存在的日志条目，则它会忽略新请求中的那些相同的条目。\n\n## 5.6 时间和可用性\n我们对Raft的要求之一是安全不得取决于时间安排：系统不得仅由于某些事件比预期的快或慢发生而产生不正确的结果。 但是，可用性（系统及时响应客户端的能力）必定取决于时间。 例如，如果消息交换花费的时间比服务器崩溃之间的典型时间长，则Candidate将不会停留足够长的时间来赢得选举。 没有稳定的Leader，Raft无法取得进步。\n领导人选举是Raft最重要的方面，时机至关重要。只要系统满足以下时间要求，Raft将能够选举和维持稳定的Leader：\n**broadcastTime ≪ electionTimeout ≪ MTBF**\n在这种不平等的情况下，*broadcastTime*是服务器将RPC并行发送到集群中的每个服务器并接收其响应所花费的平均时间。 *electionTimeout*是第5.2节所述的选举超时；*MTBF*是单个服务器两次故障之间的平均时间。广播时间应比选举超时小一个数量级，以便Leader能够可靠地发送所需的心跳消息，以防止Follower开始选举。考虑到用于选举超时的随机方法，这种不平等也使得分散投票变得不太可能。选举超时应该比*MTBF*小几个数量级，以便系统稳步前进。当Leader崩溃时，该系统将在大约选举超时时间内不可用；我们希望这只代表总时间的一小部分。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145827202-614379881.png\" width = \"300\" height = \"150\" alt=\"图10\" align=center />\n#### 图10:直接从一种配置切换到另一种配置是不安全的，因为不同的服务器将在不同的时间进行切换。在此示例中，群集从三台服务器增长到五台。不幸的是，在某个时间点上，可以为同一任期选举两名不同的Leader，其中一位拥有多数旧配置（C_old），另一位拥有多数新配置（C_new）。\n\n广播时间和*MTBF*是基础系统的属性，而选举超时是我们必须选择的东西。Raft的RPC通常需要接收者将信息持久存储到稳定的存储中，因此根据存储技术的不同，转换时间可能从0.5毫秒到20毫秒不等。选举超时可能在10毫秒至500毫秒之间。 典型服务器的*MTBF*长达数月或更长时间，可以轻松满足计时要求。\n\n# 6 集群成员关系变化\n到现在为止，我们还假设集群配置（参与共识算法的服务器集合）是固定的。实际上，有时需要更改配置，例如在服务器出现故障时更换服务器或更改复制程度。尽管可以通过使整个群集脱机，更新配置文件，然后重新启动群集来完成此操作，但这将使群集在转换期间不可用。此外，如果有任何手动步骤，则可能会导致操作员出错。为了避免这些问题，我们决定自动进行配置更改，并将其合并到Raft共识算法中。\n为了确保配置更改机制的安全，在过渡期间必须没有任何可能在同一任期内选举两名领导者的意义。不幸的是，任何将服务器直接从旧配置切换到新配置的方法都是不安全的。不可能一次自动切换所有服务器，因此群集在过渡期间可能会分成两个独立的多数（见图10）。\n为了确保安全，更改配置必须使用两阶段方法。有两种方法可以实现两个阶段。例如，某些系统使用第一阶段来禁用旧配置，因此它无法处理客户端请求；然后第二阶段启用新配置。在Raft中，集群首先切换到过渡配置，我们称为联合共识；提交联合共识后，系统将过渡到新配置。联合共识将新旧配置结合在一起：\n\n* 两种配置中的日志条目均复制到所有服务器。\n* 来自任一配置的任何服务器都可以充当领导者。\n* 协议（用于选举和入职承诺）需要分别来自新旧配置的大多数人。\n\n联合共识允许单个服务器在不同时间在配置之间转换，而不会影响安全性。此外，联合共识允许群集在整个配置更改期间继续为客户请求提供服务。\n群集配置使用复制日志中的特殊条目进行存储和通信。图11说明了配置更改过程。当Leader收到将配置从*C_old*更改为*C_new*的请求时，它将存储用于联合共识的配置（*C_old*，图中的*new*）作为日志条目，并使用前述机制复制该条目。给定服务器将新配置条目添加到其日志后，它将使用该配置进行所有将来的决策（服务器始终使用其日志中的最新配置，而不管该条目是否被提交）。这意味着Leader将使用*C_old_new*规则来确定何时提交*C_old_new*的日志条目。如果Leader崩溃，则可以根据获胜的Candidate是否收到了*C_old_new*来在*C_old*或*C_old_new*下选择新的Leader。无论如何，*C_new*不能在此期间做出单方面决定。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145845730-1927441378.png\" width = \"300\" height = \"150\" alt=\"图11\" align=center />\n#### 图11:配置更改的时间表。虚线表示已创建但尚未提交的配置条目，实线表示最新的提交的配置条目。Leader首先在其日志中创建*C_old_new*配置条目，并将其提交给*C_old_new*（大多数*C_old*和*C_new*）。 然后，它创建*C_new*条目并将其提交给大多数*C_new*。*C_old*和*C_new*都无法独立做出决策。\n\n一旦提交了*C_old_new，C_old*和*C_new*都无法在未经另一方批准的情况下做出决策，并且Leader 完备性确保只有具有*C_old_new*日志条目的服务器才能被选为Leader。现在，Leader可以安全地创建描述*C_new*的日志条目并将其复制到集群。同样，该配置将在看到每台服务器后立即生效。在*C_new*的规则下提交新配置后，旧配置将不相关，并且可以关闭不在新配置中的服务器。如图11所示，*C_old*和*C_new*都没有时间可以单方面做出决定。这样可以保证安全。\n还需要解决三个问题以进行重新配置。第一个问题是新服务器最初可能不会存储任何日志条目。如果以这种状态将它们添加到群集，则它们可能要花很长时间才能赶上，在此期间可能无法提交新的日志条目.为了避免可用性差距，Raft在配置更改之前引入了一个附加阶段，在该阶段中，新服务器以无表决权的成员的身份加入群集（领导者将日志条目复制到它们，但多数情况下不考虑它们）。一旦新服务器赶上了群集的其余部分，重新配置就可以如上所述进行。\n第二个问题是集群Leader可能不属于新配置。在这种情况下，Leader一旦提交了*C_new*日志条目，便会下台（返回到Follower状态）。这意味着在一段时间（*C_new*提交）时，Leader将管理一个不包含自身的集群。它复制日志条目，但不占多数。提交*C_new*时会发生Leader转换，因为这是新配置可以独立运行的第一步（始终可以从*C_new*中选择一个领导者）。 在此之前，可能只有*C_old*中的服务器可以被选为Leader。\n第三个问题是，删除的服务器（那些不在*C_new*中的服务器）会破坏群集。这些服务器不会接收心跳，因此它们将超时并开始新的选择。然后，他们将使用新的任期编号发送RequestVote RPC，这将导致当前Leader恢复为Follower状态。最终将选举新的Leader，但是被删除的服务器将再次超时，并且该过程将重复进行，从而导致可用性降低。\n为防止此问题，服务器在认为当前的Leader存在时会忽略RequestVote RPC。具体来说，如果服务器在当前Leader的最小选举超时时间内收到RequestVote RPC，则该服务器不会更新其任期或授予其投票权。这不会影响正常的选举，在正常的选举中，每个服务器在开始选举之前至少等待最小选举超时。但是，它有助于避免因移动服务器而造成的中断：如果Leader能够对其集群发出心跳信号，那么它将不会被更高任期的成员所取代。\n\n# 7 日志压缩\n在正常运行期间，Raft的日志会增长，以合并更多的客户请求，但在实际系统中，它无法无限制地增长。随着日志的增长，它会占用更多空间，并需要更多时间来重放。 如果没有某种机制来丢弃日志中累积的过时信息，这最终将导致可用性问题。\n快照是最简单的压缩方法。 在快照中，将整个当前系统状态写入稳定存储上的快照，然后丢弃该点之前的整个日志。 快照在Chubby和ZooKeeper中使用，本节的其余部分介绍在Raft中进行快照。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145907980-1608425176.png\" width = \"300\" height = \"150\" alt=\"图12\" align=center />\n#### 图12:服务器用新快照替换其日志（索引1至5）中已提交的条目，该快照仅存储当前状态（在此示例中为变量x和y）。快照的最后一个包含索引和任期的作用是将快照放置在条目6之前的日志中。\n诸如日志清理和日志结构的合并树之类的递增压缩方法也是可能的。它们一次处理一部分数据，因此它们会随着时间的推移更均匀地分散压缩负载。他们首先选择一个累积了许多已删除和覆盖的对象的数据区域，然后再更紧凑地重写该区域中的活动对象并释放该区域。与快照相比，这需要大量的附加机制和复杂性，从而通过始终对整个数据集进行操作来简化问题。 尽管清除日志需要修改Raft，但是状态机可以使用与快照相同的接口来实现LSM树。\n图12显示了Raft中快照的基本概念。每个服务器独立地拍摄快照，仅覆盖其日志中的已提交条目。状态机的大部分工作都由状态机组成，将其当前状态写入快照。Raft在快照中还包含少量元数据：最后包含的索引是快照替换的日志中最后一个条目的索引（状态机已应用的最后一个条目），最后包含的任期是该日志的任期。保留这些内容是为了支持快照之后的第一个日志条目的AppendEntries一致性检查，因为该条目需要先前的日志索引和任期。为了启用集群成员资格更改（第6节），快照还包括自上次包含索引起的日志中的最新配置。服务器完成快照的写入后，它可能会删除最后一个包含的索引中的所有日志条目以及所有先前的快照。\n\n\n|**安装快照RPC**  |**由Leader调用并按顺序发送打包的快照到Follower。**  |\n| --- | --- |\n|**参数：**  |  |\n|term  |Leader的任期  |\n|leaderId  |用以Follower可以重定向客户端的请求到Leader  |\n|lastIncludedIndex  |快照将替换直到该索引的所有日志条目  |\n|lastIncludedTerm  |lastIncludedIndex的任期  |\n|offset  |快照文件的偏移量  |\n|data[]  |从offset开始，快照内的数据数组  |\n|done  |如果这是最后一个快照则为true  |\n|**结果:**  |  |\n|term  |当前任期，用以Leader更新自己  |\n|**接收者实现:**  |  |\n|1.如果任期小于当前任期则立即回复  |  |\n|2.如果第一次打包创建新的快照文件(offset为0)  |  |\n|3.从给予的offset写数据到快照中  |  |\n|4.如果失败的话回复并等待更多打包的数据  |  |\n|5.存储快照文件，抛弃所有存在的或并行的相同的索引快照文件。  |  |\n|6.如果在快照文件中包含的实体最后存在日志实体具有相同的索引与任期，保持日志实体并回复  |  |\n|7.抛弃整个日志  |  |\n|8.重置状态机并使用快照内容(加载快照集群配置)  |  |\n\n尽管服务器通常通常独立拍摄快照，但Leader有时必须将快照发送给落后的Follower。当Leader已经丢弃了需要发送给Follower的下一个日志条目时，就会发生这种情况。幸运的是，在正常操作中这种情况不太可能发生：与Leader保持同步的Follower将已经有此条目。但是，异常慢的Follower或加入集群的新服务器（第6节）则不会。 使此类Follower保持最新状态的方法是，Leader可以通过网络向其发送快照。\nLeader使用一个称为InstallSnapshot的新RPC将快照发送给落后的Follower。请参见图13。当Follower收到带有此RPC的快照时，它必须决定如何处理其现有的日志记录。通常，快照将包含收件人日志中尚未包含的新信息。在这种情况下，Follower将丢弃其整个日志；它全部被快照取代，并且可能具有与快照冲突的未提交条目。相反，如果Follower收到描述其日志前缀的快照（由于重新传输或错误操作），则快照所覆盖的日志条目将被删除，但快照之后的条目仍然有效并且必须保留。\n这种快照方法背离了Raft强大的Leader原则，因为Follower可以在不了解Leader的情况下进行快照。但是，我们认为这种偏离是合理的。尽管拥有Leader可以避免在达成共识时发生决策冲突，但快照时已经达成共识，因此没有决策冲突。数据仍然仅从Leader流向Follower，只是Follower现在可以重组其数据。\n我们考虑了另一种基于Leader的方法，其中只有Leader将创建快照，然后将快照发送给其每个Follower。但是，这有两个缺点。首先，将快照发送给每个Follower会浪费网络带宽并减慢快照过程。每个Follower已经具有生成自己的快照所需的信息，通常，服务器从其本地状态生成快照要比通过网络发送和接收快照便宜得多。其次，Leader的实施将更加复杂。例如，Leader将需要同时向跟随者发送快照，同时向其复制新的日志条目，以免阻塞新的客户要求。\n还有另外两个问题会影响每个快照形式。首先，服务器必须决定何时进行快照。如果服务器过于频繁地进行快照，则会浪费磁盘带宽和能量。如果快照太不频繁，则有耗尽其存储容量的风险，并且会增加重新启动期间重放日志所需的时间。一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果此大小设置为明显大于快照的预期大小，则用于快照的磁盘带宽开销将很小。\n第二个性能问题是写快照可能要花费大量时间，我们不希望这样做会延迟正常操作。解决方案是使用写时复制技术，以便可以接受新的更新而不会影响快照的写入。例如，使用功能性数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如，Linux上的fork）来创建整个状态机的内存快照（我们的实现使用这种方法）。\n\n# 8 客户端内部交互\n本节描述客户端如何与Raft交互，包括客户端如何找到集群领导者以及Raft如何支持线性化语义。这些问题适用于所有基于共识的系统，并且Raft的解决方案与其他系统类似。\nRaft的客户将所有请求发送给Leader。客户端首次启动时，它会连接到随机选择的服务器。如果客户的首选不是Leader，则该服务器将拒绝客户的请求，并提供有关其最近听到的Leader的信息（AppendEntries请求包括Leader的网络地址）。如果Leader崩溃，客户请求将超时；客户端，然后使用随机选择的服务器重试。\n我们对Raft的目标是实现线性化的语义（每个操作似乎在调用和响应之间的某个时刻立即执行一次，恰好一次）。但是，到目前为止，Raft可以多次执行命令：例如，如果Leader在提交日志条目之后但在响应客户端之前崩溃，则客户端将使用新的Leader重试该命令，从而导致该失败再次执行。解决方案是让客户端为每个命令分配唯一的序列号。然后，状态机将跟踪为每个客户端处理的最新序列号以及相关的响应。如果收到序列号已被执行的命令，它将立即响应而无需重新执行该请求。\n可以执行只读操作，而无需在日志中写入任何内容。但是，如果没有其他措施，这将存在返回陈旧数据的风险，因为响应请求的Leader可能已经受到了一个不知道的新Leader的支持。可读的读取一定不能返回陈旧的数据，并且Raft需要采取两项额外的预防措施来保证不使用日志就可以做到这一点。首先，Leader必须掌握提交条目的最新信息。Leader完整性属性可以保证Leader具有所有承担的职责，但是在任期开始之初，它可能不知道是谁。为了找出答案，它需要从其任期中提交一个条目。Raft通过让每个Leader在任期开始时在日志中输入一个空白的禁止操作条目来解决此问题。其次，Leader必须在处理只读请求之前检查其是否已处置（如果选择了新的Leader，其信息可能会过时）。Raft通过让Leader在响应只读请求之前与大多数集群交换心跳消息来解决此问题。或者，Leader可以依靠心跳机制来提供某种形式的租约，但这将依赖于时序以确保安全性（它假设时钟范围有界）。","source":"_posts/blog/consensus/raft.md","raw":"---\ntitle: Raft算法论文(部分)\ndate: 2020-01-04 15:07:35\ntags: \n- Raft\n- algorithm\ncategories:\n- algorithm\n---\n原文地址->[Raft算法](https://raft.github.io/raft.pdf)\n\n# 摘要\nRaft是用于管理被复制的日志的共识算法。它与multi-Paxos算法产生的效果相同，并且和Paxos算法一样高效。但是结构与Paxos不同。这使得Raft算法比Paxos算法更容易理解。也为构建实际系统提供了更好的基础。为了加强理解，Raft将几个关键元素分离，比如leader选举，日志复制，安全性。并增强了一致性，以减少必须考虑的状态数。一项用户研究的结果表明，与Paxos相比，Raft算法更易于学生学习。Raft也提供了用于更新集群成员关系的新的机制。它使用重叠的多数来保证安全。\n\n# 1 介绍\n共识算法允许一组计算机的集合作为一个一致的的小组工作，这些小组可以承受某些成员的故障。正因为如此，共识机制在构建可信的大规模软件系统中起着至关重要的作用。在过去的十年中，Paxos一直主导着共识算法的讨论。很多共识算法都是基于Paxos或者受它的影响。Paxos称为了教受学生关于共识算法的主要工具。\n不幸的是，尽管进行了许多尝试以使Paxos更加平易近人，Paxos仍然非常难以理解。此外，其体系结构需要复杂的更改以支持实际系统。结果，系统构建者和学生都与Paxos斗争。\n在我们与Paxos斗争之后，我们着手寻找一种新的共识算法，该算法可以为系统构建和教育提供更好的基础。我们的方法与众不同，因为我们的主要目标是易于理解：我们能否为实际系统定义共识算法，并以比Paxos容易学习的方式对其进行描述？此外，我们希望该算法有助于系统开发人员必不可少的直觉的发展。重要的不仅是算法能起作用，而且要很清除它为什么起作用。\n这项工作的结果是一个称为Raft的共识算法。在设计Raft时，我们应用了特定的技术来提高可理解性，包括分解（Raft分离了领导者选举，日志复制和安全性）以及状态空间减少（相对于Paxos，Raft减少了不确定性的程度以及服务器之间可能不一致的方式）。 一项对两所大学的43名学生进行的用户研究表明，Raft比Paxos容易理解得多：在学习了两种算法之后，其中33位学生比Rax更好地回答了有关Raft的问题。\nRaft在许多方面与现有的共识算法相似（最著名的是Oki和Liskov的Viewstamped复制），但是它具有几个新颖的功能：\n\n* 强壮的leader:与其他共识算法相比，Raft使用更强大的领导方式。例如，日志条目仅从leader者流向其他服务器。 这简化了复制日志的管理，并使Raft更易于理解。\n* Leader选举:Raft使用随机计时器选举leader。这可以为任何共识算法已经要求的心跳添加少量机制，同时可以快速而轻松地解决冲突。\n* 成员关系变化:Raft更改集群中服务器组的机制使用了一种新的联合共识方法，其中，两种不同配置的大多数在转换过程中会重叠。 这允许群集在配置更改期间继续正常运行。\n\n我们认为Raft在教育目的和实施基础上均优于Paxos和其他共识算法。它比其他算法更简单，更易懂。本文对其进行了足够详尽的描述以满足实际系统的需求。它具有多种开源实现，并被多家公司使用；其安全性能已得到正式规定和证明；并且其效率可与其他算法相比。\n本文的其余部分介绍了复制状态机问题(第2节)，讨论了Paxos的优缺点(第3节),描述了我们对可理解性的一般方法(第4节)，介绍了Raft共识算法(第5–8节),评估Raft(第9节),并讨论相关工作(第10节)。\n\n# 2 复制状态机\n共识算法通常出现在复制状态机的环境中。通过这种方法，服务器集合上的状态机可以计算相同状态的相同副本，即使某些服务器宕机也可以继续运行。 复制状态机用于解决分布式系统中的各种容错问题。例如，具有单个集群领导者的大型系统，例如GFS，HDFS和RAMCloud，通常使用单独的复制状态机来管理领导者选举并存储配置信息，这些信息必须在领导者崩溃中幸存。复制状态机的示例包括Chubby和ZooKeeper。\n复制状态机通常使用复制日志来实现，如图1所示。每个服务器都存储一个包含一系列命令的日志，其状态机按顺序执行这些命令。每个日志以相同的顺序包含相同的命令，因此每个状态机处理相同的命令序列。由于状态机是确定性的，每个计算相同的状态和输出的顺序相同。\n保持复制日志的一致性是共识算法的工作。服务器上的共识模块从客户端接收命令，并将其添加到其日志中。它与其他服务器上的共识模块进行通信，以确保即使某些服务器发生故障，每个日志最终仍会以相同顺序包含相同的请求。正确复制命令后，每台服务器的状态机都将以日志顺序对其进行处理，然后将输出返回给客户端。服务器似乎形成了单个高度可靠的状态机。\n\t\t\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145431490-1698442829.png\" width = \"300\" height = \"180\" alt=\"图1\" align=center />\n\n#### 图1：复制状态机架构。 共识算法管理包含来自客户端的状态机命令的复制日志。 状态机处理来自日志的相同命令序列，因此它们产生相同的输出。\n实际系统的共识算法通常具有以下属性：\n\n* 它们可确保在所有非拜占庭条件下的安全性（绝不会返回错误的结果），包括网络延迟，分区，数据包丢失，复制和重新排序。\n* 只要大多数服务器都可以运行并且可以相互通信并与客户端进行通信，它们就可以正常运行（可用）。因此，由五个服务器组成的典型集群可以容忍任何两个服务器的故障。假定服务器因停止而发生故障；它们稍后可能会从稳定存储上的状态中恢复并重新加入群集。\n* 它们不依赖于时序来确保日志的一致性：错误的时钟和极端的消息延迟可能在最坏的情况下导致可用性问题。\n* 在通常情况下，只要集群的大多数都响应了一次远程过程调用，命令就可以完成。少数速度较慢的服务器不必影响整体系统性能。\n\n# 3 Paxos算法怎么了\n...\n# 4 可理解性的设计\n...\n# 5 Raft共识算法\n\nRaft是一个如第二部分描述的对被复制的日志进行管理的算法。图2以简明形式总结了该算法以供参考，图3列出了该算法的关键属性。这些图的元素将在本节的其余部分中进行分段讨论。\nRaft通过首先选举一位杰出的Leader，然后赋予Leader完全的责任来管理复制日志来实现共识。Leader接受来自客户端的日志条目，将其复制到其他服务器上，并告诉服务器何时可以安全地将日志条目应用于其状态机。拥有一个Leader可以简化复制日志的管理。例如，Leader可以决定在何处放置新条目而无需咨询其他服务器，并且数据以简单的方式从Leader流向其他服务器。Leader可能会失败或与其他服务器断开连接，在这种情况下，将选出新的Leader。\n使用Leader方法，Raft将共识问题分解为三个相对独立的子问题，这些子问题将在以下小节中进行讨论：\n\n* Leader选举:当存在的Leader失败后必须选出一个新的Leader(5.2部分)。\n* 日志复制:Leader必须接受客户端发送的日志条目并通过集群复制他们，强制其他日志接受自己的(5.3部分)。\n* 安全性:Raft的关键安全属性是图3中的状态机安全属性：如果任何服务器已将特定的日志条目应用于其状态机，则没有其他服务器可以在同一日志索引下应用不同的命令。5.4节介绍了Raft如何确保此属性；解决方案包括对第5.2节所述的选举机制的附加限制。\n\n\n\n| 状态 |  |\n| --- | --- |\n|**所有服务器上的一致状态**  | **在响应RPCs前稳定更新存储** |\n|currentTerm | 最新的服务器任期(第一次引导启动时初始化为0)单调递增 |\n|votedFor | 当前任期投票的candidateId(如果没有则为null),为谁投票则对应的值为谁 |\n|log[]  | 日志条目集合，被Leader接收到的每一条日志包含状态机的命令和任期。(第一条索引为1) |\n|**所有服务器上的隔离状态**  |  |\n|commitIndex  |已知的被提交的被最高的日志索引(初始为0，单调递增)  |\n|lastApplied  |被应用到状态机的最高的日志条目索引(初始为0，单调递增)  |\n|**Leader上的隔离状态**  |**在选举后重新初始化**  |\n| nextIndex[]  | Leader对于每一台服务器，将要发送的下一条日志条目(被Leader初始化为最后一条日志索引+1) |\n|matchIndex[] | Leader对于每一台服务器，已知的被复制到服务器上的最高的日志条目索引(初始为0，单调递增) |\n\n\n| **追加日志条目RPC** | **由Leader调用完成日志复制(5.3节)，也可以用于心跳信息(5.2节)** |\n| --- | --- |\n|**参数:**  |  |\n|term  |Leader的任期|\n|leaderId  |follower可以重定向客户端  |\n|prevLogIndex  | 紧接新记录之前的日志条目索引(即上一条日志条目索引) |\n|prevLogTerm  |上一条日志条目索引的任期  |\n|entries[]  |用于存储的日志实体(心跳信息为空，以至于更高效的发送)  |\n|leaderCommit  |Leader的提交的索引  |\n|**结果:**  |  |\n|term  |当前任期，用于Leader更新自己的任期  |\n|success  |如果follower包含的日志实体匹配到prevLogIndex和PrevLogTerm  |\n|**接收者实现:**||\n|1.如果任期小于当前任期回复false|5.1节|\n|2.如果包含的日志实体没有匹配到prevLogIndex和PrevLogTerm回复false|5.3节|\n|3.如果存在日志实体与新的日志实体冲突(相同的索引但任期不同),删除存在的日志实体并选择新的|5.3节|\n|4.追加日志中尚未存在的任何新条目||\n|5.如果leaderCommit大于commintIndex,将commitIndex设置为(leaderCommit,最后一条新的日志实体索引)中最小的那个||\n\n\n\n| **请求投票RPC** |**由candidates调用用于收集投票数(5.2节)**  |\n| --- | --- |\n|**参数:**  |  |\n|term  |candidate的任期  |\n|candidateId  |请求投票的candidateID  |\n|lastLogIndex  |candidate的最后一条日志条目索引 (5.4节) |\n|lastLogTerm  |candidate的最后一条日志条目的任期(5.4节)  |\n|**结果:**  |  |\n|term  |当前任期，用以candidate更新自己的任期  |\n|voteGranted  |如果candidate接受了投票则为true  |\n|**接收者实现**  |  |\n|1.如果任期小于当前任期回复false  |5.1节  |\n|2.如果votedFor为空或者candidateId,且candidate的日志\n至少与接收者的日志一样新，同意投票 |5.2节 5.4节  |\n\n#### 服务器的规则:\n**所有服务器:**\n\n* 如果commitIndex大于lastApplied;增加lastApplied,应用log[lastApplied]到状态机(5.3节)\n* 如果RPC请求或响应中的任期T大于currentTerm；设置currentTerm为T，变为follower(5.1节)\n\n**所有Follower(5.2节)**\n\n* 响应来自candidates和Leader的RPC消息。\n* 如果直到选举超时也没有接受到由当前Leader发送的追加日志条目RPC消息或者对candidate的投票，变为candidate\n\n**所有Candidate(5.2节)**\n\n* 当转换为Candidate后，启动选举过程：\n    * 增加currentTerm\n    * 为自己投票\n    * 重置选举计时器\n    * 发送请求投票RPC消息到其他所有服务器\n* 如果接收到大多数成员的投票信息，变为Leader\n* 如果接收到来自新的Leader的追加日志条目RPC消息，转换为follower\n* 如果选举超时，启动新的选举过程\n\n**Leader**\n\n* 选举过后：将初始的空追加日志条目RP(心跳)消息发送到每个服务器；在空闲时间重复此操作以防止选举超时(5.2节)\n* 如果接收到来自客户端的命令，追加日志到本地，在日志条目应用到状态机后回复客户端。\n* 如果对于follower，lastLogIndex大于等于nextIndex，发送带有从nextIndex开始的日志条目的追加日志条目RPC消息。\n    * 如果响应成功，更新对于该follower的nextIndex和matchIndex\n    * 如果因为日志的不一致性导致追加日志实体消息失败，递减nextIndex并重试\n* 如果存在N并且N大于commitIndex,并且大多数matchIndex[i]大于等于N，且log[N]的任期与currentTerm相等，将commitIndex设置为N(5.3 5.4节)\n\n#### 图2\n\n#### 选举安全\n在给定的任期，最多只能选举一个Leader。5.2节\n**Leader只追加特性:** Leader从不覆盖或删除它的日志条目，只追加新的。5.3节\n**日志匹配:** 如果两个日志包含的实体具有相同的索引和任期，那么直到给定索引为止，所有条目中的日志都是相同的。\n**Leader完备性:** 如果一个日志提示在给定的任期内被提交，那么该条目将出现在领导者的日志中，显示所有编号较高的条目。\n**状态机安全:** 如果服务器应用一条给定索引的日志实体到它的状态机，那么没有其他服务器可以应用一条不同的日志到相同的索引位置。\n\n#### 图3：Raft保证任何时刻这里的每一条属性都是成立的。\n\n\n## 5.1 Raft基础\n一个Raft集群包含多个服务器；五是一个典型数字，它允许系统容忍两个服务器故障。在任何给定时间，每个服务器都处于以下三种状态之一：Leader，Follower或Candidate。在正常操作中，只有一个Leader，而其他所有服务器都是Follower。 Follower是被动的：他们自己不发出请求，而只是响应Leader和Candidate的请求。Leader处理所有客户请求(如果客户联系Follower，则Follower将其重定向到Leader)。第3种状态Candidate用于选举新的Leader。图4显示了状态及其转换。 过渡将在下面讨论。\nRaft将时间划分为任意长度的项，如图5所示。项用连续的整数编号。每个任期都以选举开始，在选举中，一个或多个Candidate试图按照5.2节中的描述成为Leader。 如果Candidate在选举中获胜，则它将在剩余任期中担任Leader。在某些情况下，选举将导致投票分裂。在这种情况下，任期将以无Leader结束；新任期(以新的选举)将很快开始。Raft确保给定任期内最多有一位Leader。\n不同的服务器可能会在不同时间观察任期之间的转换，并且在某些情况下，服务器可能不会观察到选举甚至整个任期。任期在Raft中充当逻辑时钟，它们使服务器能够检测过时的信息，例如陈旧的Leader。每个服务器存储一个当前的任期号，该任期号随时间单调增加。只要服务器进行通信，就会交换当前任期；如果一台服务器的当前任期小于另一台服务器，则它将其当前任期更新为较大的值。如果Candidate或Leader发现其任期已过时，它将立即恢复为Follower状态。如果服务器收到带有过期条款编号的请求，则服务器将拒绝该请求。\nRaft式服务器使用远程过程调用（RPC）进行通信，并且基本共识算法仅需要两种类型的RPC。RequestVote RPC由Candidate在选举期间启动（第5.2节），而AppendEntries RPC由Leader启动以复制日志条目并提供心跳形式(第5.3节)的AppendEntries RPC消息。第7节添加了第三个RPC，用于在服务器之间传输快照。如果服务器未及时收到响应，则服务器会重试RPC，并且它们并行发出RPC消息以获得最佳性能。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145525858-271973058.png\" width = \"300\" height = \"150\" alt=\"图4\" align=center />\n#### 图4：服务器状态。Follower仅响应来自其他服务器的请求。如果Follower未收到任何通讯，它将成为Candidate并发起选举。从整个集群的大多数中获得选票的Candidate将成为新的Leader。Leader通常会运作直到失败。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145554351-712530469.png\" width = \"300\" height = \"150\" alt=\"图5\" align=center />\n#### 图5:时间分为几个任期，每个任期都以选举开始。选举成功后，由一位Leader管理集群，直到任期结束。在这种情况下可能选举失败导致任期届满而未选出Leader。任期之间的转换可以在不同的服务器上的不同时间观察到。\n## 5.2 Leader选举\nRaft使用心跳机制触发Leader选举。当服务器启动的角色为Follower。服务器保持Follower状态一直到接收到来自Leader或者Candidate的有效的RPC消息。Leader为了维护它的权利将发送周期性的心跳消息(不带有日志实体的AppendEntries RPCs消息)到所有的Follower。如果一个Follower在一整个周期时间内没有接收到任何通信消息则称为选举超时。他们将假设没有可以访问的Leader并开始新的投票选举新的Leader。\n为了开始一次选举，Follower递增它的当前任期并将状态转换为Candidate。然后为自己投一票并并行发送RequestVote RPC消息到集群中其他的所有服务器。Candidate的状态将会一直保持一直到这三种情况中其中一个发生:\n\n1. 赢得选举\n2. 另外一个服务器称为了Leader\n3. 在当前投票周期内没有赢得选举的服务器。\n\n将会在下面分别讨论这三种情况。\n如果Candidate在同一任期内从整个集群中获得大多数服务器的票数，则将赢得选举。在给定的期限内，每台服务器将按先到先得的原则为最多一个Candidate投票(注：第5.4节增加了投票的其他限制)。多数规则确保最多只有一名Candidate可以赢得特定任期的选举(图3中的选举安全属性)。Candidate赢得选举后，便成为Leader。然后，它将心跳消息发送到所有其他服务器以建立其权限并阻止新的选举。\n在等待投票时，Candidate可能会从声称是Leader的另一台服务器收到AppendEntries RPC消息。如果Leader的任期(在其RPC消息中可以获得)至少与Candidate当前任期一样大，则Candidate将Leader视为合法，并返回到Follower状态。 如果RPC中的任期小于Candidate当前的任期，则Candidate将拒绝RPC并继续处于Candidate状态。\n第三种可能的结果是，Candidate既不会赢得选举也不会输掉选举：如果同时有许多Follower成为Candidate，那么票数可能会分散，从而任何Candidate都不会获得多数投票。当这种情况发生时，每个Candidate都将超时，并通过增加其任期并启动另一轮RequestVote RPC来开始新的选举。但是，如果不采取额外措施，分散投票可以无限期地重复。\nRaft使用随机的选举超时来确保分散票很少发生，并且可以快速解决。为了避免投票分散，首先从固定间隔(例如150-300毫秒)中随机选择选举超时。这会分散服务器超时的时间，因此在大多数情况下，只有一台服务器会超时。它会赢得选举并在其他任何服务器超时之前发送心跳信号。使用相同的机制来处理分散投票。每位候选人在选举开始时都会重新启动其随机选举超时时间，并等待该超时时间过去后才开始下一次选举。这减少了在新选举中再次进行分散投票的可能性。第9.3节显示，这种方法可以迅速选举出一位Leader。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145629826-958115128.png\" width = \"300\" height = \"180\" alt=\"图6\" align=center />\n#### 图6:日志由条目组成，这些条目按顺序编号。每个条目包含创建它的任期（每个框中的数字）和状态机的命令。如果可以安全地将条目应用于状态机，则认为该条目已提交。\n选举是如何理解指导我们在设计备选方案之间进行选择的一个示例。最初，我们计划使用排名系统：为每个Candidate分配一个唯一的排名，该排名用于在竞争Candidate之间进行选择。如果某个Candidate发现了另一名更高级别的Candidate，它将返回到Follower状态，以便更高级别的Candidate可以更轻松地赢得下一次选举。我们发现，这种方法在可用性方面产生了一些细微的问题(排名较低的服务器可能需要超时，如果排名较高的服务器出现故障，则可能再次成为Candidate，但是如果这样做过早，则可以重置选举Leader的进度)。我们对算法进行了数次调整，但每次调整后都会出现新的极端情况。 最终，我们得出结论，随机重试方法更加明显和易于理解。\n\n## 5.3 日志复制\n选举Leader后，便开始为客户的请求提供服务。每个客户端请求都包含可以由复制状态机执行的命令。Leader将命令作为新条目添加到其日志中，然后与其他每个服务器并行发出AppendEntries RPC消息，以复制该条目。在安全地复制了条目之后（如下所述），Leader将该条目应用于其状态机，并将执行结果返回给客户端。如果Follower崩溃或运行缓慢，或者丢失了网络数据包，则领导者会无限次（即使在响应客户端之后）重试附加该RPC消息，直到所有Follower最终存储所有日志条目为止。\n日志的组织结构如图6所示。当Leader收到条目时，每个日志条目都会存储一个状态机命令以及任期号。日志条目中的任期号用于检测日志之间的不一致并确保图3中的某些属性。每个日志条目还具有一个整数索引，用于标识其在日志中的位置。\nLeader决定什么时候可以安全地对状态机进行日志记录。这样的条目称为已提交。Raft保证提交的条目是持久的，并且最终将由所有可用状态机执行。一旦创建条目的Leader已在大多数服务器上复制了该日志条目（例如，图6中的条目7），则提交该日志条目。这还将提交Leader日志中的所有先前条目，包括先前Leader创建的条目。第5.4节讨论了Leader变更后应用此规则时的一些细微之处，并且还表明了对提交的定义是安全的。Leader保持记录被提交的日志的最高索引，并将该索引包括在将来的AppendEntries RPC（包括心跳）中，以便其他服务器发现。跟随者得知日志条目已提交后，便将该条目应用于其本地状态机（按日志顺序）。\n我们设计了Raft日志机制来维持不同服务器上的日志之间的高度一致性。这不仅简化了系统的行为并使其更具可预测性，而且是确保安全的重要组成部分。Raft维护以下属性，它们共同构成了图3中的Log Matching属性：\n\n* 如果两个不同的日志实体具有相同的索引和任期号，那么他们存储有相同的命令。\n* 如果两个不同的日志实体具有相同的索引和任期号，则所有先前条目中的日志都相同。\n\n第一个属性来自以下事实：Leader在给定期限内最多创建一个具有给定日志索引的条目，并且日志条目从不更改其在日志中的位置。第二个属性由AppendEntries执行的简单一致性检查保证。在发送AppendEntries RPC时，Leader将在其日志中紧接新条目之前包含条目的索引和任期号。如果Follower在其日志中找不到具有相同索引和任期的条目，则它拒绝新条目。一致性检查是一个归纳步骤：日志的初始空状态满足Log Matching属性，并且只要扩展日志，一致性检查都会保留Log Matching属性。 因此，只要AppendEntries成功返回，Leader就会知道Follower的日志与它自己通过新条目记录的日志相同。\n在正常操作期间，Leader和Follower的日志保持一致，因此AppendEntries一致性检查永远不会失败。但是，Leader崩溃可能会使日志不一致（旧的Leader可能没有完全复制其日志中的所有条目）。这些不一致会加剧一系列的Leader和Follower崩溃。图7说明了Follower的日志与新Follower的日志可能不同的方式。 Follower可能缺少Leader上存在的条目，它可能具有Leader上不存在的额外条目，或者两者都有。日志中的缺失条目和多余条目可能跨越多个任期。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145703880-1796421720.png\" width = \"300\" height = \"150\" alt=\"图7\" align=center />\n#### 图7:当最上面的为Leader时，Follower日志中可能会出现任何情况（a–f）。每个框代表一个日志条目；框中的数字是其用语。Follower可能缺少条目（a–b），可能有多余的未提交条目（c–d），或者两者都有（e–f）。 例如，如果该服务器是第2任期的Leader，则可能会发生场景（f）。它迅速重启，成为第三学期的Leader，并在其日志中添加了更多条目； 在提交任期2或任期3中的任何条目之前，服务器再次崩溃并保持关闭状态。\n\n在Raft中，Leader通过强迫Follower的日志重复自己的日志来处理不一致之处。这意味着Follower日志中的冲突条目将被Leader日志中的条目覆盖。第5.4节将说明，当再加上一个限制条件时则是安全的。\n为了使Follower的日志与自己的日志保持一致，Leader必须找到两个日志一致的最新日志条目的位置，在该位置之后删除Follower日志中的所有条目，并将该位置之后的Leader的所有条目发送给Follower。所有这些操作都是响应AppendEntries RPC执行的一致性检查而发生的。Leader为每个关注者维护一个nextIndex，这是Leader将发送给该Follower的下一个日志条目的索引。当成功选举为Leader时，它将所有nextIndex值初始化为刚好在其日志中的最后一个索引之后的索引（图7中的11）。如果Follower的日志与Leader的日志不一致，则下一个AppendEntries RPC中的AppendEntries一致性检查将失败。在拒绝之后，Leader递减nextIndex并重试AppendEntries RPC。最终nextIndex将到达Leader和Follower日志匹配的位置。发生这种情况时，AppendEntries将成功执行，这将删除Follower日志中的所有冲突条目，并添加Leader的日志中的条目（如果有）。一旦AppendEntries成功通过，Follower的日志便与Leader的日志保持一致，并且在本任期的其余时间中都将保持这种状态。\n如果需要，可以优化协议以减少拒绝的AppendEntries RPC的数量。例如，当发送拒绝AppendEntries请求的消息时，Follower可以将冲突条目的任期以及该任期存储的第一个索引包括在内。有了这些信息，Leader可以递减nextIndex来绕过该任期中所有冲突的条目。每个具有冲突条目的任期都需要一个AppendEntries RPC消息，而不是每个条目一个RPC。 在实践中，我们怀疑这种优化是否必要，因为故障很少发生，并且不太可能出现许多不一致的情况。\n通过这种机制，Leader在启动时无需采取任何特殊措施即可恢复日志的一致性。它只是开始正常运行，并且响应于AppendEntries一致性检查的失败，日志自动收敛。Leader永远不会覆盖或删除其自己的日志中的条目（图3中的Leader Append-Only属性）。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145725706-1972318156.png\" width = \"300\" height = \"150\" alt=\"图8\" align=center />\n#### 图8:一个时序显示Leader为何无法使用较早任期的日志条目来确定提交。 在（a）中，S1是Leader，并部分复制索引2处的日志条目。S5在S3，S4及其本身的投票下当选为任期3的Leader，并在日志索引2处接受不同的条目。S1重新启动，被选为Leader，然后继续复制。至此，任期2的日志条目已在大多数服务器上复制，但尚未提交。如果S1像（d）中那样崩溃，则S5可以被选为Leader（来自S2，S3和S4的投票），并用任期3中的条目覆盖该条目。但是，如果S1在崩溃之前在大多数服务器上其当前任期复制了一个条目，如（e）所示，该条目已提交（S5无法赢得选举）。此时，日志中的所有先前条目也将被提交。\n这种日志复制机制展现了第2节中描述的理想的共识属性：只要大多数服务器都在运行，Raft可以接受，复制和应用新的日志条目。通常情况下，可以通过一轮RPC将新条目复制到大多数集群中。一个慢速Follower不会影响性能。\n\n## 5.4 安全性\n前面的章节描述了Raft如何选择Leader并复制日志条目。但是，到目前为止描述的机制还不足以确保每个状态机以相同的顺序执行完全相同的命令。例如，当Leader提交多个日志条目时，Follower可能不可用。然后可以当选为Leader并用新的日志条目覆盖这些条目。结果，不同的状态机可能执行不同的命令序列。\n本节通过添加限制哪些服务器可以当选领导者来完善Raft算法。该限制可确保任何给定任期的Leader都包含先前任期中提交的所有条目（图3中的“领导者完整性”属性）。给定选举限制，我们便使承诺规则更加精确。最后，我们为Leader完整性属性提供了一个证明草图，并显示了它如何导致复制状态机的正确行为。\n\n### 5.4.1 选举限制\n在任何基于Leader的共识算法中，Leader最终必须存储所有提交的日志条目。在某些共识算法中，例如“加盖时间戳的复制” ，即使最初并不包含所有提交的条目，也可以选择一个Leader。这些算法包含其他机制，无论是在选举过程中还是选举后不久，可以识别丢失的条目并将其发送给新的Leader。不幸的是，这导致了相当大的额外机制和复杂性。Raft使用一种更简单的方法来保证自新任Leader选举之日都具有所有先前提交的条目，而无需将这些条目转移给Leader。这意味着日志条目仅在一个方向上（从Leader到Follower）流动，并且Leader永远不会覆盖其日志中的现有条目。\nRaft使用投票程序来防止Candidate赢得选举，除非其日志中包含所有已提交的条目。Candidate必须联系集群的大多数才能被选举，这意味着每个提交的条目都必须存在于其中至少一台服务器中。如果Candidate的日志至少与该多数服务器日志中的日志一样最新（以下精确定义了“最新”），则它将保存所有已提交的条目。RequestVote RPC实施了此限制：RPC包含有关Candidate日志的信息，如果投票者自己的日志比Candidate的日志最新，则投票者将拒绝投票。\nRaft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新的。 如果日志中的最后一个条目具有不同的任期，则带有较新任期的日志是最新的。如果日志以相同的任期结尾，则以索引更大的日志为准。\n\n### 5.4.2 提交之前任期的日志\n如第5.3节所述，Leader知道，一旦该条目存储在大多数服务器上，就会提交当前项的输入。如果Leader在提交条目之前崩溃，将来的Leader将尝试完成复制条目。但是，Leader不能立即得出以下结论：一旦上一个任期的条目存储在大多数服务器上，就将其提交。 图-8说明了一种情况，其中旧的日志条目存储在大多数服务器上，但将来的Leader仍可以覆盖。\n为了消除如图8所示的问题，Raft决不通过计算副本数来提交前项的日志条目。只有Leader当前任期的日志条目才通过计算副本数来提交；一旦以这种方式提交了当前任期的条目，则由于“日志匹配”属性而间接提交了所有先前的条目。在某些情况下，Leader可以安全地断定已提交了较旧的日志输入（例如，如果该条目存储在每个服务器上），但是Raft为简化起见采取了更为保守的方法。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145750026-1894945621.png\" width = \"300\" height = \"140\" alt=\"图9\" align=center />\n#### 图9:如果S1（任期T的Leader）从其任期中提交了新的日志条目，并且S5当选为下一任期U的Leader，那么必须至少有一个服务器（S3）接受了该日志条目并投票支持S5。\nRaft会在承诺规则中带来额外的复杂性，因为当Leader从先前条款中复制条目时，日志条目将保留其原始任期号。在其他共识算法中，如果新的Leader从先前的“任期号”中复制条目，则必须使用其新的“任期号”来进行复制。Raft的方法使推理日志条目变得更加容易，因为它们在整个周期和跨日志期间保持相同的任期编号。此外，与其他算法相比，Raft中的新Leader发送的先前条目的日志条目要少（其他算法必须发送冗余日志条目以对其重新编号，然后才能提交）。\n\n### 5.4.3 安全性讨论\n给定完整的Raft算法，我们现在可以更精确地论证“领导者完整性属性”成立（此论点基于安全性证明；请参见9.2节）。 我们假设Leader完整性属性不成立，那么我们证明了一个矛盾。 假设任期*T*的Leader提交了其任期的日志条目，但该日志条目未由某个将来任期的Leader存储。考虑最小项任期*U>T*，其领导者*leaderU* 不存储该条目。\n\n1. 提交时，必须在*LeaderU*的日志中没有已提交的条目（Leader绝不会删除或覆盖条目）。\n2. *LeaderT*在大多数集群中复制了该条目，*leaderU*从大多数集群中获得了投票。因此，如图9所示，至少有一个服务器（“投票者”）既接受了*LeaderT*的条目又投票给*LeaderU*，投票者是达成矛盾的关键。\n3. 投票者必须在接受*LeaderU*投票之前已经接受了*LeaderT*提交的条目；否则，它将拒绝来自*LeaderT*的AppendEntries请求（其当前期限将高于*T*）。\n4. 当每个投票者对*leaderU*投票时，投票者仍然存储该条目，因为每个居间的Leader都包含该条目（通过假设），Leader从不删除条目，而Follower仅在与Leader冲突时才删除条目。\n5. 投票者将投票结果授予了*LeaderU*，因此*LeaderU*的日志必须与投票者的日志一样最新。这导致两个矛盾之一。\n6. 首先，如果投票者和*LeaderU*共享相同的最后一个日志条款，那么*LeaderU*的日志必须至少与投票者的日志一样长，因此其日志包含投票者日志中的每个条目。这是一个矛盾，因为投票者中包含已承诺的条目，而*leaderU*被假定为不包含。\n7. 否则，*leaderU*的最后一条日志的任期号必须大于投票者的最后一条日志的任期号。而且，它比*T*大，因为投票者的上一个日志任期号至少为*T*（它包含任期*T*中的所有已提交的条目）。创建*leaderU*的最后一个日志条目的较早的Leader必须在其日志中（通过假设）包含已提交的条目。然后，通过日志匹配属性，*leaderU*的日志还必须包含已提交的条目，这是矛盾的。\n8. 这样就完成了矛盾。因此，所有任期大于*T*的Leader都必须包含在任期*T*中提交的所有任期为*T*的日志。\n9. 日志匹配属性保证未来的Leader还将包含间接提交的条目，例如图8（d）中的索引2。\n\n给定“Leader完整性”属性，我们可以从图3证明“状态机安全性”属性，该状态表明，如果服务器已将给定索引的日志条目应用于其状态机，则其他任何服务器都不会为该状态机在相同的索引应用不同的日志条目。服务器在将日志条目应用于其状态机时，其日志必须与通过该条目的Leader的日志相同，并且必须提交该条目。现在考虑任何服务器应用给定日志索引的最低任期；日志完整性属性可确保Leader将存储所有较高任期的相同的日志条目，因此，服务器将在最新的任期中应用的索引将应用相同的值。 因此，状态机安全属性成立。\n最后，Raft要求服务器以日志索引顺序应用条目。结合状态机安全属性，这意味着所有服务器将以相同的顺序将完全相同的日志条目集应用于其状态机。\n\n## 5.5 Follower与Candidate崩溃\n到目前为止，我们只关注Leader的失败。Follower与Candidate崩溃比Leader崩溃要容易得多，并且两者的处理方式相同。 如果Follower与Candidate崩溃，则将来发送给它的RequestVote和AppendEntries RPC将失败。 Raft通过无限期重试来处理这些故障；如果崩溃的服务器重新启动，则RPC将成功完成。如果服务器在完成RPC之后但在响应之前崩溃，则在重新启动后它将再次收到相同的RPC。Raft中的RPC是幂等的，因此不会造成伤害。例如，如果Follower收到一个AppendEntries请求，其中包括其日志中已经存在的日志条目，则它会忽略新请求中的那些相同的条目。\n\n## 5.6 时间和可用性\n我们对Raft的要求之一是安全不得取决于时间安排：系统不得仅由于某些事件比预期的快或慢发生而产生不正确的结果。 但是，可用性（系统及时响应客户端的能力）必定取决于时间。 例如，如果消息交换花费的时间比服务器崩溃之间的典型时间长，则Candidate将不会停留足够长的时间来赢得选举。 没有稳定的Leader，Raft无法取得进步。\n领导人选举是Raft最重要的方面，时机至关重要。只要系统满足以下时间要求，Raft将能够选举和维持稳定的Leader：\n**broadcastTime ≪ electionTimeout ≪ MTBF**\n在这种不平等的情况下，*broadcastTime*是服务器将RPC并行发送到集群中的每个服务器并接收其响应所花费的平均时间。 *electionTimeout*是第5.2节所述的选举超时；*MTBF*是单个服务器两次故障之间的平均时间。广播时间应比选举超时小一个数量级，以便Leader能够可靠地发送所需的心跳消息，以防止Follower开始选举。考虑到用于选举超时的随机方法，这种不平等也使得分散投票变得不太可能。选举超时应该比*MTBF*小几个数量级，以便系统稳步前进。当Leader崩溃时，该系统将在大约选举超时时间内不可用；我们希望这只代表总时间的一小部分。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145827202-614379881.png\" width = \"300\" height = \"150\" alt=\"图10\" align=center />\n#### 图10:直接从一种配置切换到另一种配置是不安全的，因为不同的服务器将在不同的时间进行切换。在此示例中，群集从三台服务器增长到五台。不幸的是，在某个时间点上，可以为同一任期选举两名不同的Leader，其中一位拥有多数旧配置（C_old），另一位拥有多数新配置（C_new）。\n\n广播时间和*MTBF*是基础系统的属性，而选举超时是我们必须选择的东西。Raft的RPC通常需要接收者将信息持久存储到稳定的存储中，因此根据存储技术的不同，转换时间可能从0.5毫秒到20毫秒不等。选举超时可能在10毫秒至500毫秒之间。 典型服务器的*MTBF*长达数月或更长时间，可以轻松满足计时要求。\n\n# 6 集群成员关系变化\n到现在为止，我们还假设集群配置（参与共识算法的服务器集合）是固定的。实际上，有时需要更改配置，例如在服务器出现故障时更换服务器或更改复制程度。尽管可以通过使整个群集脱机，更新配置文件，然后重新启动群集来完成此操作，但这将使群集在转换期间不可用。此外，如果有任何手动步骤，则可能会导致操作员出错。为了避免这些问题，我们决定自动进行配置更改，并将其合并到Raft共识算法中。\n为了确保配置更改机制的安全，在过渡期间必须没有任何可能在同一任期内选举两名领导者的意义。不幸的是，任何将服务器直接从旧配置切换到新配置的方法都是不安全的。不可能一次自动切换所有服务器，因此群集在过渡期间可能会分成两个独立的多数（见图10）。\n为了确保安全，更改配置必须使用两阶段方法。有两种方法可以实现两个阶段。例如，某些系统使用第一阶段来禁用旧配置，因此它无法处理客户端请求；然后第二阶段启用新配置。在Raft中，集群首先切换到过渡配置，我们称为联合共识；提交联合共识后，系统将过渡到新配置。联合共识将新旧配置结合在一起：\n\n* 两种配置中的日志条目均复制到所有服务器。\n* 来自任一配置的任何服务器都可以充当领导者。\n* 协议（用于选举和入职承诺）需要分别来自新旧配置的大多数人。\n\n联合共识允许单个服务器在不同时间在配置之间转换，而不会影响安全性。此外，联合共识允许群集在整个配置更改期间继续为客户请求提供服务。\n群集配置使用复制日志中的特殊条目进行存储和通信。图11说明了配置更改过程。当Leader收到将配置从*C_old*更改为*C_new*的请求时，它将存储用于联合共识的配置（*C_old*，图中的*new*）作为日志条目，并使用前述机制复制该条目。给定服务器将新配置条目添加到其日志后，它将使用该配置进行所有将来的决策（服务器始终使用其日志中的最新配置，而不管该条目是否被提交）。这意味着Leader将使用*C_old_new*规则来确定何时提交*C_old_new*的日志条目。如果Leader崩溃，则可以根据获胜的Candidate是否收到了*C_old_new*来在*C_old*或*C_old_new*下选择新的Leader。无论如何，*C_new*不能在此期间做出单方面决定。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145845730-1927441378.png\" width = \"300\" height = \"150\" alt=\"图11\" align=center />\n#### 图11:配置更改的时间表。虚线表示已创建但尚未提交的配置条目，实线表示最新的提交的配置条目。Leader首先在其日志中创建*C_old_new*配置条目，并将其提交给*C_old_new*（大多数*C_old*和*C_new*）。 然后，它创建*C_new*条目并将其提交给大多数*C_new*。*C_old*和*C_new*都无法独立做出决策。\n\n一旦提交了*C_old_new，C_old*和*C_new*都无法在未经另一方批准的情况下做出决策，并且Leader 完备性确保只有具有*C_old_new*日志条目的服务器才能被选为Leader。现在，Leader可以安全地创建描述*C_new*的日志条目并将其复制到集群。同样，该配置将在看到每台服务器后立即生效。在*C_new*的规则下提交新配置后，旧配置将不相关，并且可以关闭不在新配置中的服务器。如图11所示，*C_old*和*C_new*都没有时间可以单方面做出决定。这样可以保证安全。\n还需要解决三个问题以进行重新配置。第一个问题是新服务器最初可能不会存储任何日志条目。如果以这种状态将它们添加到群集，则它们可能要花很长时间才能赶上，在此期间可能无法提交新的日志条目.为了避免可用性差距，Raft在配置更改之前引入了一个附加阶段，在该阶段中，新服务器以无表决权的成员的身份加入群集（领导者将日志条目复制到它们，但多数情况下不考虑它们）。一旦新服务器赶上了群集的其余部分，重新配置就可以如上所述进行。\n第二个问题是集群Leader可能不属于新配置。在这种情况下，Leader一旦提交了*C_new*日志条目，便会下台（返回到Follower状态）。这意味着在一段时间（*C_new*提交）时，Leader将管理一个不包含自身的集群。它复制日志条目，但不占多数。提交*C_new*时会发生Leader转换，因为这是新配置可以独立运行的第一步（始终可以从*C_new*中选择一个领导者）。 在此之前，可能只有*C_old*中的服务器可以被选为Leader。\n第三个问题是，删除的服务器（那些不在*C_new*中的服务器）会破坏群集。这些服务器不会接收心跳，因此它们将超时并开始新的选择。然后，他们将使用新的任期编号发送RequestVote RPC，这将导致当前Leader恢复为Follower状态。最终将选举新的Leader，但是被删除的服务器将再次超时，并且该过程将重复进行，从而导致可用性降低。\n为防止此问题，服务器在认为当前的Leader存在时会忽略RequestVote RPC。具体来说，如果服务器在当前Leader的最小选举超时时间内收到RequestVote RPC，则该服务器不会更新其任期或授予其投票权。这不会影响正常的选举，在正常的选举中，每个服务器在开始选举之前至少等待最小选举超时。但是，它有助于避免因移动服务器而造成的中断：如果Leader能够对其集群发出心跳信号，那么它将不会被更高任期的成员所取代。\n\n# 7 日志压缩\n在正常运行期间，Raft的日志会增长，以合并更多的客户请求，但在实际系统中，它无法无限制地增长。随着日志的增长，它会占用更多空间，并需要更多时间来重放。 如果没有某种机制来丢弃日志中累积的过时信息，这最终将导致可用性问题。\n快照是最简单的压缩方法。 在快照中，将整个当前系统状态写入稳定存储上的快照，然后丢弃该点之前的整个日志。 快照在Chubby和ZooKeeper中使用，本节的其余部分介绍在Raft中进行快照。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145907980-1608425176.png\" width = \"300\" height = \"150\" alt=\"图12\" align=center />\n#### 图12:服务器用新快照替换其日志（索引1至5）中已提交的条目，该快照仅存储当前状态（在此示例中为变量x和y）。快照的最后一个包含索引和任期的作用是将快照放置在条目6之前的日志中。\n诸如日志清理和日志结构的合并树之类的递增压缩方法也是可能的。它们一次处理一部分数据，因此它们会随着时间的推移更均匀地分散压缩负载。他们首先选择一个累积了许多已删除和覆盖的对象的数据区域，然后再更紧凑地重写该区域中的活动对象并释放该区域。与快照相比，这需要大量的附加机制和复杂性，从而通过始终对整个数据集进行操作来简化问题。 尽管清除日志需要修改Raft，但是状态机可以使用与快照相同的接口来实现LSM树。\n图12显示了Raft中快照的基本概念。每个服务器独立地拍摄快照，仅覆盖其日志中的已提交条目。状态机的大部分工作都由状态机组成，将其当前状态写入快照。Raft在快照中还包含少量元数据：最后包含的索引是快照替换的日志中最后一个条目的索引（状态机已应用的最后一个条目），最后包含的任期是该日志的任期。保留这些内容是为了支持快照之后的第一个日志条目的AppendEntries一致性检查，因为该条目需要先前的日志索引和任期。为了启用集群成员资格更改（第6节），快照还包括自上次包含索引起的日志中的最新配置。服务器完成快照的写入后，它可能会删除最后一个包含的索引中的所有日志条目以及所有先前的快照。\n\n\n|**安装快照RPC**  |**由Leader调用并按顺序发送打包的快照到Follower。**  |\n| --- | --- |\n|**参数：**  |  |\n|term  |Leader的任期  |\n|leaderId  |用以Follower可以重定向客户端的请求到Leader  |\n|lastIncludedIndex  |快照将替换直到该索引的所有日志条目  |\n|lastIncludedTerm  |lastIncludedIndex的任期  |\n|offset  |快照文件的偏移量  |\n|data[]  |从offset开始，快照内的数据数组  |\n|done  |如果这是最后一个快照则为true  |\n|**结果:**  |  |\n|term  |当前任期，用以Leader更新自己  |\n|**接收者实现:**  |  |\n|1.如果任期小于当前任期则立即回复  |  |\n|2.如果第一次打包创建新的快照文件(offset为0)  |  |\n|3.从给予的offset写数据到快照中  |  |\n|4.如果失败的话回复并等待更多打包的数据  |  |\n|5.存储快照文件，抛弃所有存在的或并行的相同的索引快照文件。  |  |\n|6.如果在快照文件中包含的实体最后存在日志实体具有相同的索引与任期，保持日志实体并回复  |  |\n|7.抛弃整个日志  |  |\n|8.重置状态机并使用快照内容(加载快照集群配置)  |  |\n\n尽管服务器通常通常独立拍摄快照，但Leader有时必须将快照发送给落后的Follower。当Leader已经丢弃了需要发送给Follower的下一个日志条目时，就会发生这种情况。幸运的是，在正常操作中这种情况不太可能发生：与Leader保持同步的Follower将已经有此条目。但是，异常慢的Follower或加入集群的新服务器（第6节）则不会。 使此类Follower保持最新状态的方法是，Leader可以通过网络向其发送快照。\nLeader使用一个称为InstallSnapshot的新RPC将快照发送给落后的Follower。请参见图13。当Follower收到带有此RPC的快照时，它必须决定如何处理其现有的日志记录。通常，快照将包含收件人日志中尚未包含的新信息。在这种情况下，Follower将丢弃其整个日志；它全部被快照取代，并且可能具有与快照冲突的未提交条目。相反，如果Follower收到描述其日志前缀的快照（由于重新传输或错误操作），则快照所覆盖的日志条目将被删除，但快照之后的条目仍然有效并且必须保留。\n这种快照方法背离了Raft强大的Leader原则，因为Follower可以在不了解Leader的情况下进行快照。但是，我们认为这种偏离是合理的。尽管拥有Leader可以避免在达成共识时发生决策冲突，但快照时已经达成共识，因此没有决策冲突。数据仍然仅从Leader流向Follower，只是Follower现在可以重组其数据。\n我们考虑了另一种基于Leader的方法，其中只有Leader将创建快照，然后将快照发送给其每个Follower。但是，这有两个缺点。首先，将快照发送给每个Follower会浪费网络带宽并减慢快照过程。每个Follower已经具有生成自己的快照所需的信息，通常，服务器从其本地状态生成快照要比通过网络发送和接收快照便宜得多。其次，Leader的实施将更加复杂。例如，Leader将需要同时向跟随者发送快照，同时向其复制新的日志条目，以免阻塞新的客户要求。\n还有另外两个问题会影响每个快照形式。首先，服务器必须决定何时进行快照。如果服务器过于频繁地进行快照，则会浪费磁盘带宽和能量。如果快照太不频繁，则有耗尽其存储容量的风险，并且会增加重新启动期间重放日志所需的时间。一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果此大小设置为明显大于快照的预期大小，则用于快照的磁盘带宽开销将很小。\n第二个性能问题是写快照可能要花费大量时间，我们不希望这样做会延迟正常操作。解决方案是使用写时复制技术，以便可以接受新的更新而不会影响快照的写入。例如，使用功能性数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如，Linux上的fork）来创建整个状态机的内存快照（我们的实现使用这种方法）。\n\n# 8 客户端内部交互\n本节描述客户端如何与Raft交互，包括客户端如何找到集群领导者以及Raft如何支持线性化语义。这些问题适用于所有基于共识的系统，并且Raft的解决方案与其他系统类似。\nRaft的客户将所有请求发送给Leader。客户端首次启动时，它会连接到随机选择的服务器。如果客户的首选不是Leader，则该服务器将拒绝客户的请求，并提供有关其最近听到的Leader的信息（AppendEntries请求包括Leader的网络地址）。如果Leader崩溃，客户请求将超时；客户端，然后使用随机选择的服务器重试。\n我们对Raft的目标是实现线性化的语义（每个操作似乎在调用和响应之间的某个时刻立即执行一次，恰好一次）。但是，到目前为止，Raft可以多次执行命令：例如，如果Leader在提交日志条目之后但在响应客户端之前崩溃，则客户端将使用新的Leader重试该命令，从而导致该失败再次执行。解决方案是让客户端为每个命令分配唯一的序列号。然后，状态机将跟踪为每个客户端处理的最新序列号以及相关的响应。如果收到序列号已被执行的命令，它将立即响应而无需重新执行该请求。\n可以执行只读操作，而无需在日志中写入任何内容。但是，如果没有其他措施，这将存在返回陈旧数据的风险，因为响应请求的Leader可能已经受到了一个不知道的新Leader的支持。可读的读取一定不能返回陈旧的数据，并且Raft需要采取两项额外的预防措施来保证不使用日志就可以做到这一点。首先，Leader必须掌握提交条目的最新信息。Leader完整性属性可以保证Leader具有所有承担的职责，但是在任期开始之初，它可能不知道是谁。为了找出答案，它需要从其任期中提交一个条目。Raft通过让每个Leader在任期开始时在日志中输入一个空白的禁止操作条目来解决此问题。其次，Leader必须在处理只读请求之前检查其是否已处置（如果选择了新的Leader，其信息可能会过时）。Raft通过让Leader在响应只读请求之前与大多数集群交换心跳消息来解决此问题。或者，Leader可以依靠心跳机制来提供某种形式的租约，但这将依赖于时序以确保安全性（它假设时钟范围有界）。","slug":"blog/consensus/raft","published":1,"updated":"2020-05-11T03:44:37.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqygh000rk0vq07hhep8h","content":"<p>原文地址-&gt;<a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft算法</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>Raft是用于管理被复制的日志的共识算法。它与multi-Paxos算法产生的效果相同，并且和Paxos算法一样高效。但是结构与Paxos不同。这使得Raft算法比Paxos算法更容易理解。也为构建实际系统提供了更好的基础。为了加强理解，Raft将几个关键元素分离，比如leader选举，日志复制，安全性。并增强了一致性，以减少必须考虑的状态数。一项用户研究的结果表明，与Paxos相比，Raft算法更易于学生学习。Raft也提供了用于更新集群成员关系的新的机制。它使用重叠的多数来保证安全。</p>\n<h1 id=\"1-介绍\"><a href=\"#1-介绍\" class=\"headerlink\" title=\"1 介绍\"></a>1 介绍</h1><p>共识算法允许一组计算机的集合作为一个一致的的小组工作，这些小组可以承受某些成员的故障。正因为如此，共识机制在构建可信的大规模软件系统中起着至关重要的作用。在过去的十年中，Paxos一直主导着共识算法的讨论。很多共识算法都是基于Paxos或者受它的影响。Paxos称为了教受学生关于共识算法的主要工具。<br>不幸的是，尽管进行了许多尝试以使Paxos更加平易近人，Paxos仍然非常难以理解。此外，其体系结构需要复杂的更改以支持实际系统。结果，系统构建者和学生都与Paxos斗争。<br>在我们与Paxos斗争之后，我们着手寻找一种新的共识算法，该算法可以为系统构建和教育提供更好的基础。我们的方法与众不同，因为我们的主要目标是易于理解：我们能否为实际系统定义共识算法，并以比Paxos容易学习的方式对其进行描述？此外，我们希望该算法有助于系统开发人员必不可少的直觉的发展。重要的不仅是算法能起作用，而且要很清除它为什么起作用。<br>这项工作的结果是一个称为Raft的共识算法。在设计Raft时，我们应用了特定的技术来提高可理解性，包括分解（Raft分离了领导者选举，日志复制和安全性）以及状态空间减少（相对于Paxos，Raft减少了不确定性的程度以及服务器之间可能不一致的方式）。 一项对两所大学的43名学生进行的用户研究表明，Raft比Paxos容易理解得多：在学习了两种算法之后，其中33位学生比Rax更好地回答了有关Raft的问题。<br>Raft在许多方面与现有的共识算法相似（最著名的是Oki和Liskov的Viewstamped复制），但是它具有几个新颖的功能：</p>\n<ul>\n<li>强壮的leader:与其他共识算法相比，Raft使用更强大的领导方式。例如，日志条目仅从leader者流向其他服务器。 这简化了复制日志的管理，并使Raft更易于理解。</li>\n<li>Leader选举:Raft使用随机计时器选举leader。这可以为任何共识算法已经要求的心跳添加少量机制，同时可以快速而轻松地解决冲突。</li>\n<li>成员关系变化:Raft更改集群中服务器组的机制使用了一种新的联合共识方法，其中，两种不同配置的大多数在转换过程中会重叠。 这允许群集在配置更改期间继续正常运行。</li>\n</ul>\n<p>我们认为Raft在教育目的和实施基础上均优于Paxos和其他共识算法。它比其他算法更简单，更易懂。本文对其进行了足够详尽的描述以满足实际系统的需求。它具有多种开源实现，并被多家公司使用；其安全性能已得到正式规定和证明；并且其效率可与其他算法相比。<br>本文的其余部分介绍了复制状态机问题(第2节)，讨论了Paxos的优缺点(第3节),描述了我们对可理解性的一般方法(第4节)，介绍了Raft共识算法(第5–8节),评估Raft(第9节),并讨论相关工作(第10节)。</p>\n<h1 id=\"2-复制状态机\"><a href=\"#2-复制状态机\" class=\"headerlink\" title=\"2 复制状态机\"></a>2 复制状态机</h1><p>共识算法通常出现在复制状态机的环境中。通过这种方法，服务器集合上的状态机可以计算相同状态的相同副本，即使某些服务器宕机也可以继续运行。 复制状态机用于解决分布式系统中的各种容错问题。例如，具有单个集群领导者的大型系统，例如GFS，HDFS和RAMCloud，通常使用单独的复制状态机来管理领导者选举并存储配置信息，这些信息必须在领导者崩溃中幸存。复制状态机的示例包括Chubby和ZooKeeper。<br>复制状态机通常使用复制日志来实现，如图1所示。每个服务器都存储一个包含一系列命令的日志，其状态机按顺序执行这些命令。每个日志以相同的顺序包含相同的命令，因此每个状态机处理相同的命令序列。由于状态机是确定性的，每个计算相同的状态和输出的顺序相同。<br>保持复制日志的一致性是共识算法的工作。服务器上的共识模块从客户端接收命令，并将其添加到其日志中。它与其他服务器上的共识模块进行通信，以确保即使某些服务器发生故障，每个日志最终仍会以相同顺序包含相同的请求。正确复制命令后，每台服务器的状态机都将以日志顺序对其进行处理，然后将输出返回给客户端。服务器似乎形成了单个高度可靠的状态机。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145431490-1698442829.png\" srcset=\"undefined\" width = \"300\" height = \"180\" alt=\"图1\" align=center />\n\n<h4 id=\"图1：复制状态机架构。-共识算法管理包含来自客户端的状态机命令的复制日志。-状态机处理来自日志的相同命令序列，因此它们产生相同的输出。\"><a href=\"#图1：复制状态机架构。-共识算法管理包含来自客户端的状态机命令的复制日志。-状态机处理来自日志的相同命令序列，因此它们产生相同的输出。\" class=\"headerlink\" title=\"图1：复制状态机架构。 共识算法管理包含来自客户端的状态机命令的复制日志。 状态机处理来自日志的相同命令序列，因此它们产生相同的输出。\"></a>图1：复制状态机架构。 共识算法管理包含来自客户端的状态机命令的复制日志。 状态机处理来自日志的相同命令序列，因此它们产生相同的输出。</h4><p>实际系统的共识算法通常具有以下属性：</p>\n<ul>\n<li>它们可确保在所有非拜占庭条件下的安全性（绝不会返回错误的结果），包括网络延迟，分区，数据包丢失，复制和重新排序。</li>\n<li>只要大多数服务器都可以运行并且可以相互通信并与客户端进行通信，它们就可以正常运行（可用）。因此，由五个服务器组成的典型集群可以容忍任何两个服务器的故障。假定服务器因停止而发生故障；它们稍后可能会从稳定存储上的状态中恢复并重新加入群集。</li>\n<li>它们不依赖于时序来确保日志的一致性：错误的时钟和极端的消息延迟可能在最坏的情况下导致可用性问题。</li>\n<li>在通常情况下，只要集群的大多数都响应了一次远程过程调用，命令就可以完成。少数速度较慢的服务器不必影响整体系统性能。</li>\n</ul>\n<h1 id=\"3-Paxos算法怎么了\"><a href=\"#3-Paxos算法怎么了\" class=\"headerlink\" title=\"3 Paxos算法怎么了\"></a>3 Paxos算法怎么了</h1><p>…</p>\n<h1 id=\"4-可理解性的设计\"><a href=\"#4-可理解性的设计\" class=\"headerlink\" title=\"4 可理解性的设计\"></a>4 可理解性的设计</h1><p>…</p>\n<h1 id=\"5-Raft共识算法\"><a href=\"#5-Raft共识算法\" class=\"headerlink\" title=\"5 Raft共识算法\"></a>5 Raft共识算法</h1><p>Raft是一个如第二部分描述的对被复制的日志进行管理的算法。图2以简明形式总结了该算法以供参考，图3列出了该算法的关键属性。这些图的元素将在本节的其余部分中进行分段讨论。<br>Raft通过首先选举一位杰出的Leader，然后赋予Leader完全的责任来管理复制日志来实现共识。Leader接受来自客户端的日志条目，将其复制到其他服务器上，并告诉服务器何时可以安全地将日志条目应用于其状态机。拥有一个Leader可以简化复制日志的管理。例如，Leader可以决定在何处放置新条目而无需咨询其他服务器，并且数据以简单的方式从Leader流向其他服务器。Leader可能会失败或与其他服务器断开连接，在这种情况下，将选出新的Leader。<br>使用Leader方法，Raft将共识问题分解为三个相对独立的子问题，这些子问题将在以下小节中进行讨论：</p>\n<ul>\n<li>Leader选举:当存在的Leader失败后必须选出一个新的Leader(5.2部分)。</li>\n<li>日志复制:Leader必须接受客户端发送的日志条目并通过集群复制他们，强制其他日志接受自己的(5.3部分)。</li>\n<li>安全性:Raft的关键安全属性是图3中的状态机安全属性：如果任何服务器已将特定的日志条目应用于其状态机，则没有其他服务器可以在同一日志索引下应用不同的命令。5.4节介绍了Raft如何确保此属性；解决方案包括对第5.2节所述的选举机制的附加限制。</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>状态</th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>所有服务器上的一致状态</strong></td>\n<td><strong>在响应RPCs前稳定更新存储</strong></td>\n</tr>\n<tr>\n<td>currentTerm</td>\n<td>最新的服务器任期(第一次引导启动时初始化为0)单调递增</td>\n</tr>\n<tr>\n<td>votedFor</td>\n<td>当前任期投票的candidateId(如果没有则为null),为谁投票则对应的值为谁</td>\n</tr>\n<tr>\n<td>log[]</td>\n<td>日志条目集合，被Leader接收到的每一条日志包含状态机的命令和任期。(第一条索引为1)</td>\n</tr>\n<tr>\n<td><strong>所有服务器上的隔离状态</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>commitIndex</td>\n<td>已知的被提交的被最高的日志索引(初始为0，单调递增)</td>\n</tr>\n<tr>\n<td>lastApplied</td>\n<td>被应用到状态机的最高的日志条目索引(初始为0，单调递增)</td>\n</tr>\n<tr>\n<td><strong>Leader上的隔离状态</strong></td>\n<td><strong>在选举后重新初始化</strong></td>\n</tr>\n<tr>\n<td>nextIndex[]</td>\n<td>Leader对于每一台服务器，将要发送的下一条日志条目(被Leader初始化为最后一条日志索引+1)</td>\n</tr>\n<tr>\n<td>matchIndex[]</td>\n<td>Leader对于每一台服务器，已知的被复制到服务器上的最高的日志条目索引(初始为0，单调递增)</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th><strong>追加日志条目RPC</strong></th>\n<th><strong>由Leader调用完成日志复制(5.3节)，也可以用于心跳信息(5.2节)</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>参数:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>Leader的任期</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>follower可以重定向客户端</td>\n</tr>\n<tr>\n<td>prevLogIndex</td>\n<td>紧接新记录之前的日志条目索引(即上一条日志条目索引)</td>\n</tr>\n<tr>\n<td>prevLogTerm</td>\n<td>上一条日志条目索引的任期</td>\n</tr>\n<tr>\n<td>entries[]</td>\n<td>用于存储的日志实体(心跳信息为空，以至于更高效的发送)</td>\n</tr>\n<tr>\n<td>leaderCommit</td>\n<td>Leader的提交的索引</td>\n</tr>\n<tr>\n<td><strong>结果:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>当前任期，用于Leader更新自己的任期</td>\n</tr>\n<tr>\n<td>success</td>\n<td>如果follower包含的日志实体匹配到prevLogIndex和PrevLogTerm</td>\n</tr>\n<tr>\n<td><strong>接收者实现:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>1.如果任期小于当前任期回复false</td>\n<td>5.1节</td>\n</tr>\n<tr>\n<td>2.如果包含的日志实体没有匹配到prevLogIndex和PrevLogTerm回复false</td>\n<td>5.3节</td>\n</tr>\n<tr>\n<td>3.如果存在日志实体与新的日志实体冲突(相同的索引但任期不同),删除存在的日志实体并选择新的</td>\n<td>5.3节</td>\n</tr>\n<tr>\n<td>4.追加日志中尚未存在的任何新条目</td>\n<td></td>\n</tr>\n<tr>\n<td>5.如果leaderCommit大于commintIndex,将commitIndex设置为(leaderCommit,最后一条新的日志实体索引)中最小的那个</td>\n<td></td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th><strong>请求投票RPC</strong></th>\n<th><strong>由candidates调用用于收集投票数(5.2节)</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>参数:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>candidate的任期</td>\n</tr>\n<tr>\n<td>candidateId</td>\n<td>请求投票的candidateID</td>\n</tr>\n<tr>\n<td>lastLogIndex</td>\n<td>candidate的最后一条日志条目索引 (5.4节)</td>\n</tr>\n<tr>\n<td>lastLogTerm</td>\n<td>candidate的最后一条日志条目的任期(5.4节)</td>\n</tr>\n<tr>\n<td><strong>结果:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>当前任期，用以candidate更新自己的任期</td>\n</tr>\n<tr>\n<td>voteGranted</td>\n<td>如果candidate接受了投票则为true</td>\n</tr>\n<tr>\n<td><strong>接收者实现</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>1.如果任期小于当前任期回复false</td>\n<td>5.1节</td>\n</tr>\n<tr>\n<td>2.如果votedFor为空或者candidateId,且candidate的日志</td>\n<td></td>\n</tr>\n<tr>\n<td>至少与接收者的日志一样新，同意投票</td>\n<td>5.2节 5.4节</td>\n</tr>\n</tbody></table>\n<h4 id=\"服务器的规则\"><a href=\"#服务器的规则\" class=\"headerlink\" title=\"服务器的规则:\"></a>服务器的规则:</h4><p><strong>所有服务器:</strong></p>\n<ul>\n<li>如果commitIndex大于lastApplied;增加lastApplied,应用log[lastApplied]到状态机(5.3节)</li>\n<li>如果RPC请求或响应中的任期T大于currentTerm；设置currentTerm为T，变为follower(5.1节)</li>\n</ul>\n<p><strong>所有Follower(5.2节)</strong></p>\n<ul>\n<li>响应来自candidates和Leader的RPC消息。</li>\n<li>如果直到选举超时也没有接受到由当前Leader发送的追加日志条目RPC消息或者对candidate的投票，变为candidate</li>\n</ul>\n<p><strong>所有Candidate(5.2节)</strong></p>\n<ul>\n<li>当转换为Candidate后，启动选举过程：<ul>\n<li>增加currentTerm</li>\n<li>为自己投票</li>\n<li>重置选举计时器</li>\n<li>发送请求投票RPC消息到其他所有服务器</li>\n</ul>\n</li>\n<li>如果接收到大多数成员的投票信息，变为Leader</li>\n<li>如果接收到来自新的Leader的追加日志条目RPC消息，转换为follower</li>\n<li>如果选举超时，启动新的选举过程</li>\n</ul>\n<p><strong>Leader</strong></p>\n<ul>\n<li>选举过后：将初始的空追加日志条目RP(心跳)消息发送到每个服务器；在空闲时间重复此操作以防止选举超时(5.2节)</li>\n<li>如果接收到来自客户端的命令，追加日志到本地，在日志条目应用到状态机后回复客户端。</li>\n<li>如果对于follower，lastLogIndex大于等于nextIndex，发送带有从nextIndex开始的日志条目的追加日志条目RPC消息。<ul>\n<li>如果响应成功，更新对于该follower的nextIndex和matchIndex</li>\n<li>如果因为日志的不一致性导致追加日志实体消息失败，递减nextIndex并重试</li>\n</ul>\n</li>\n<li>如果存在N并且N大于commitIndex,并且大多数matchIndex[i]大于等于N，且log[N]的任期与currentTerm相等，将commitIndex设置为N(5.3 5.4节)</li>\n</ul>\n<h4 id=\"图2\"><a href=\"#图2\" class=\"headerlink\" title=\"图2\"></a>图2</h4><h4 id=\"选举安全\"><a href=\"#选举安全\" class=\"headerlink\" title=\"选举安全\"></a>选举安全</h4><p>在给定的任期，最多只能选举一个Leader。5.2节<br><strong>Leader只追加特性:</strong> Leader从不覆盖或删除它的日志条目，只追加新的。5.3节<br><strong>日志匹配:</strong> 如果两个日志包含的实体具有相同的索引和任期，那么直到给定索引为止，所有条目中的日志都是相同的。<br><strong>Leader完备性:</strong> 如果一个日志提示在给定的任期内被提交，那么该条目将出现在领导者的日志中，显示所有编号较高的条目。<br><strong>状态机安全:</strong> 如果服务器应用一条给定索引的日志实体到它的状态机，那么没有其他服务器可以应用一条不同的日志到相同的索引位置。</p>\n<h4 id=\"图3：Raft保证任何时刻这里的每一条属性都是成立的。\"><a href=\"#图3：Raft保证任何时刻这里的每一条属性都是成立的。\" class=\"headerlink\" title=\"图3：Raft保证任何时刻这里的每一条属性都是成立的。\"></a>图3：Raft保证任何时刻这里的每一条属性都是成立的。</h4><h2 id=\"5-1-Raft基础\"><a href=\"#5-1-Raft基础\" class=\"headerlink\" title=\"5.1 Raft基础\"></a>5.1 Raft基础</h2><p>一个Raft集群包含多个服务器；五是一个典型数字，它允许系统容忍两个服务器故障。在任何给定时间，每个服务器都处于以下三种状态之一：Leader，Follower或Candidate。在正常操作中，只有一个Leader，而其他所有服务器都是Follower。 Follower是被动的：他们自己不发出请求，而只是响应Leader和Candidate的请求。Leader处理所有客户请求(如果客户联系Follower，则Follower将其重定向到Leader)。第3种状态Candidate用于选举新的Leader。图4显示了状态及其转换。 过渡将在下面讨论。<br>Raft将时间划分为任意长度的项，如图5所示。项用连续的整数编号。每个任期都以选举开始，在选举中，一个或多个Candidate试图按照5.2节中的描述成为Leader。 如果Candidate在选举中获胜，则它将在剩余任期中担任Leader。在某些情况下，选举将导致投票分裂。在这种情况下，任期将以无Leader结束；新任期(以新的选举)将很快开始。Raft确保给定任期内最多有一位Leader。<br>不同的服务器可能会在不同时间观察任期之间的转换，并且在某些情况下，服务器可能不会观察到选举甚至整个任期。任期在Raft中充当逻辑时钟，它们使服务器能够检测过时的信息，例如陈旧的Leader。每个服务器存储一个当前的任期号，该任期号随时间单调增加。只要服务器进行通信，就会交换当前任期；如果一台服务器的当前任期小于另一台服务器，则它将其当前任期更新为较大的值。如果Candidate或Leader发现其任期已过时，它将立即恢复为Follower状态。如果服务器收到带有过期条款编号的请求，则服务器将拒绝该请求。<br>Raft式服务器使用远程过程调用（RPC）进行通信，并且基本共识算法仅需要两种类型的RPC。RequestVote RPC由Candidate在选举期间启动（第5.2节），而AppendEntries RPC由Leader启动以复制日志条目并提供心跳形式(第5.3节)的AppendEntries RPC消息。第7节添加了第三个RPC，用于在服务器之间传输快照。如果服务器未及时收到响应，则服务器会重试RPC，并且它们并行发出RPC消息以获得最佳性能。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145525858-271973058.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图4\" align=center />\n#### 图4：服务器状态。Follower仅响应来自其他服务器的请求。如果Follower未收到任何通讯，它将成为Candidate并发起选举。从整个集群的大多数中获得选票的Candidate将成为新的Leader。Leader通常会运作直到失败。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145554351-712530469.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图5\" align=center />\n#### 图5:时间分为几个任期，每个任期都以选举开始。选举成功后，由一位Leader管理集群，直到任期结束。在这种情况下可能选举失败导致任期届满而未选出Leader。任期之间的转换可以在不同的服务器上的不同时间观察到。\n## 5.2 Leader选举\nRaft使用心跳机制触发Leader选举。当服务器启动的角色为Follower。服务器保持Follower状态一直到接收到来自Leader或者Candidate的有效的RPC消息。Leader为了维护它的权利将发送周期性的心跳消息(不带有日志实体的AppendEntries RPCs消息)到所有的Follower。如果一个Follower在一整个周期时间内没有接收到任何通信消息则称为选举超时。他们将假设没有可以访问的Leader并开始新的投票选举新的Leader。\n为了开始一次选举，Follower递增它的当前任期并将状态转换为Candidate。然后为自己投一票并并行发送RequestVote RPC消息到集群中其他的所有服务器。Candidate的状态将会一直保持一直到这三种情况中其中一个发生:\n\n<ol>\n<li>赢得选举</li>\n<li>另外一个服务器称为了Leader</li>\n<li>在当前投票周期内没有赢得选举的服务器。</li>\n</ol>\n<p>将会在下面分别讨论这三种情况。<br>如果Candidate在同一任期内从整个集群中获得大多数服务器的票数，则将赢得选举。在给定的期限内，每台服务器将按先到先得的原则为最多一个Candidate投票(注：第5.4节增加了投票的其他限制)。多数规则确保最多只有一名Candidate可以赢得特定任期的选举(图3中的选举安全属性)。Candidate赢得选举后，便成为Leader。然后，它将心跳消息发送到所有其他服务器以建立其权限并阻止新的选举。<br>在等待投票时，Candidate可能会从声称是Leader的另一台服务器收到AppendEntries RPC消息。如果Leader的任期(在其RPC消息中可以获得)至少与Candidate当前任期一样大，则Candidate将Leader视为合法，并返回到Follower状态。 如果RPC中的任期小于Candidate当前的任期，则Candidate将拒绝RPC并继续处于Candidate状态。<br>第三种可能的结果是，Candidate既不会赢得选举也不会输掉选举：如果同时有许多Follower成为Candidate，那么票数可能会分散，从而任何Candidate都不会获得多数投票。当这种情况发生时，每个Candidate都将超时，并通过增加其任期并启动另一轮RequestVote RPC来开始新的选举。但是，如果不采取额外措施，分散投票可以无限期地重复。<br>Raft使用随机的选举超时来确保分散票很少发生，并且可以快速解决。为了避免投票分散，首先从固定间隔(例如150-300毫秒)中随机选择选举超时。这会分散服务器超时的时间，因此在大多数情况下，只有一台服务器会超时。它会赢得选举并在其他任何服务器超时之前发送心跳信号。使用相同的机制来处理分散投票。每位候选人在选举开始时都会重新启动其随机选举超时时间，并等待该超时时间过去后才开始下一次选举。这减少了在新选举中再次进行分散投票的可能性。第9.3节显示，这种方法可以迅速选举出一位Leader。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145629826-958115128.png\" srcset=\"undefined\" width = \"300\" height = \"180\" alt=\"图6\" align=center />\n#### 图6:日志由条目组成，这些条目按顺序编号。每个条目包含创建它的任期（每个框中的数字）和状态机的命令。如果可以安全地将条目应用于状态机，则认为该条目已提交。\n选举是如何理解指导我们在设计备选方案之间进行选择的一个示例。最初，我们计划使用排名系统：为每个Candidate分配一个唯一的排名，该排名用于在竞争Candidate之间进行选择。如果某个Candidate发现了另一名更高级别的Candidate，它将返回到Follower状态，以便更高级别的Candidate可以更轻松地赢得下一次选举。我们发现，这种方法在可用性方面产生了一些细微的问题(排名较低的服务器可能需要超时，如果排名较高的服务器出现故障，则可能再次成为Candidate，但是如果这样做过早，则可以重置选举Leader的进度)。我们对算法进行了数次调整，但每次调整后都会出现新的极端情况。 最终，我们得出结论，随机重试方法更加明显和易于理解。\n\n<h2 id=\"5-3-日志复制\"><a href=\"#5-3-日志复制\" class=\"headerlink\" title=\"5.3 日志复制\"></a>5.3 日志复制</h2><p>选举Leader后，便开始为客户的请求提供服务。每个客户端请求都包含可以由复制状态机执行的命令。Leader将命令作为新条目添加到其日志中，然后与其他每个服务器并行发出AppendEntries RPC消息，以复制该条目。在安全地复制了条目之后（如下所述），Leader将该条目应用于其状态机，并将执行结果返回给客户端。如果Follower崩溃或运行缓慢，或者丢失了网络数据包，则领导者会无限次（即使在响应客户端之后）重试附加该RPC消息，直到所有Follower最终存储所有日志条目为止。<br>日志的组织结构如图6所示。当Leader收到条目时，每个日志条目都会存储一个状态机命令以及任期号。日志条目中的任期号用于检测日志之间的不一致并确保图3中的某些属性。每个日志条目还具有一个整数索引，用于标识其在日志中的位置。<br>Leader决定什么时候可以安全地对状态机进行日志记录。这样的条目称为已提交。Raft保证提交的条目是持久的，并且最终将由所有可用状态机执行。一旦创建条目的Leader已在大多数服务器上复制了该日志条目（例如，图6中的条目7），则提交该日志条目。这还将提交Leader日志中的所有先前条目，包括先前Leader创建的条目。第5.4节讨论了Leader变更后应用此规则时的一些细微之处，并且还表明了对提交的定义是安全的。Leader保持记录被提交的日志的最高索引，并将该索引包括在将来的AppendEntries RPC（包括心跳）中，以便其他服务器发现。跟随者得知日志条目已提交后，便将该条目应用于其本地状态机（按日志顺序）。<br>我们设计了Raft日志机制来维持不同服务器上的日志之间的高度一致性。这不仅简化了系统的行为并使其更具可预测性，而且是确保安全的重要组成部分。Raft维护以下属性，它们共同构成了图3中的Log Matching属性：</p>\n<ul>\n<li>如果两个不同的日志实体具有相同的索引和任期号，那么他们存储有相同的命令。</li>\n<li>如果两个不同的日志实体具有相同的索引和任期号，则所有先前条目中的日志都相同。</li>\n</ul>\n<p>第一个属性来自以下事实：Leader在给定期限内最多创建一个具有给定日志索引的条目，并且日志条目从不更改其在日志中的位置。第二个属性由AppendEntries执行的简单一致性检查保证。在发送AppendEntries RPC时，Leader将在其日志中紧接新条目之前包含条目的索引和任期号。如果Follower在其日志中找不到具有相同索引和任期的条目，则它拒绝新条目。一致性检查是一个归纳步骤：日志的初始空状态满足Log Matching属性，并且只要扩展日志，一致性检查都会保留Log Matching属性。 因此，只要AppendEntries成功返回，Leader就会知道Follower的日志与它自己通过新条目记录的日志相同。<br>在正常操作期间，Leader和Follower的日志保持一致，因此AppendEntries一致性检查永远不会失败。但是，Leader崩溃可能会使日志不一致（旧的Leader可能没有完全复制其日志中的所有条目）。这些不一致会加剧一系列的Leader和Follower崩溃。图7说明了Follower的日志与新Follower的日志可能不同的方式。 Follower可能缺少Leader上存在的条目，它可能具有Leader上不存在的额外条目，或者两者都有。日志中的缺失条目和多余条目可能跨越多个任期。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145703880-1796421720.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图7\" align=center />\n#### 图7:当最上面的为Leader时，Follower日志中可能会出现任何情况（a–f）。每个框代表一个日志条目；框中的数字是其用语。Follower可能缺少条目（a–b），可能有多余的未提交条目（c–d），或者两者都有（e–f）。 例如，如果该服务器是第2任期的Leader，则可能会发生场景（f）。它迅速重启，成为第三学期的Leader，并在其日志中添加了更多条目； 在提交任期2或任期3中的任何条目之前，服务器再次崩溃并保持关闭状态。\n\n<p>在Raft中，Leader通过强迫Follower的日志重复自己的日志来处理不一致之处。这意味着Follower日志中的冲突条目将被Leader日志中的条目覆盖。第5.4节将说明，当再加上一个限制条件时则是安全的。<br>为了使Follower的日志与自己的日志保持一致，Leader必须找到两个日志一致的最新日志条目的位置，在该位置之后删除Follower日志中的所有条目，并将该位置之后的Leader的所有条目发送给Follower。所有这些操作都是响应AppendEntries RPC执行的一致性检查而发生的。Leader为每个关注者维护一个nextIndex，这是Leader将发送给该Follower的下一个日志条目的索引。当成功选举为Leader时，它将所有nextIndex值初始化为刚好在其日志中的最后一个索引之后的索引（图7中的11）。如果Follower的日志与Leader的日志不一致，则下一个AppendEntries RPC中的AppendEntries一致性检查将失败。在拒绝之后，Leader递减nextIndex并重试AppendEntries RPC。最终nextIndex将到达Leader和Follower日志匹配的位置。发生这种情况时，AppendEntries将成功执行，这将删除Follower日志中的所有冲突条目，并添加Leader的日志中的条目（如果有）。一旦AppendEntries成功通过，Follower的日志便与Leader的日志保持一致，并且在本任期的其余时间中都将保持这种状态。<br>如果需要，可以优化协议以减少拒绝的AppendEntries RPC的数量。例如，当发送拒绝AppendEntries请求的消息时，Follower可以将冲突条目的任期以及该任期存储的第一个索引包括在内。有了这些信息，Leader可以递减nextIndex来绕过该任期中所有冲突的条目。每个具有冲突条目的任期都需要一个AppendEntries RPC消息，而不是每个条目一个RPC。 在实践中，我们怀疑这种优化是否必要，因为故障很少发生，并且不太可能出现许多不一致的情况。<br>通过这种机制，Leader在启动时无需采取任何特殊措施即可恢复日志的一致性。它只是开始正常运行，并且响应于AppendEntries一致性检查的失败，日志自动收敛。Leader永远不会覆盖或删除其自己的日志中的条目（图3中的Leader Append-Only属性）。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145725706-1972318156.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图8\" align=center />\n#### 图8:一个时序显示Leader为何无法使用较早任期的日志条目来确定提交。 在（a）中，S1是Leader，并部分复制索引2处的日志条目。S5在S3，S4及其本身的投票下当选为任期3的Leader，并在日志索引2处接受不同的条目。S1重新启动，被选为Leader，然后继续复制。至此，任期2的日志条目已在大多数服务器上复制，但尚未提交。如果S1像（d）中那样崩溃，则S5可以被选为Leader（来自S2，S3和S4的投票），并用任期3中的条目覆盖该条目。但是，如果S1在崩溃之前在大多数服务器上其当前任期复制了一个条目，如（e）所示，该条目已提交（S5无法赢得选举）。此时，日志中的所有先前条目也将被提交。\n这种日志复制机制展现了第2节中描述的理想的共识属性：只要大多数服务器都在运行，Raft可以接受，复制和应用新的日志条目。通常情况下，可以通过一轮RPC将新条目复制到大多数集群中。一个慢速Follower不会影响性能。\n\n<h2 id=\"5-4-安全性\"><a href=\"#5-4-安全性\" class=\"headerlink\" title=\"5.4 安全性\"></a>5.4 安全性</h2><p>前面的章节描述了Raft如何选择Leader并复制日志条目。但是，到目前为止描述的机制还不足以确保每个状态机以相同的顺序执行完全相同的命令。例如，当Leader提交多个日志条目时，Follower可能不可用。然后可以当选为Leader并用新的日志条目覆盖这些条目。结果，不同的状态机可能执行不同的命令序列。<br>本节通过添加限制哪些服务器可以当选领导者来完善Raft算法。该限制可确保任何给定任期的Leader都包含先前任期中提交的所有条目（图3中的“领导者完整性”属性）。给定选举限制，我们便使承诺规则更加精确。最后，我们为Leader完整性属性提供了一个证明草图，并显示了它如何导致复制状态机的正确行为。</p>\n<h3 id=\"5-4-1-选举限制\"><a href=\"#5-4-1-选举限制\" class=\"headerlink\" title=\"5.4.1 选举限制\"></a>5.4.1 选举限制</h3><p>在任何基于Leader的共识算法中，Leader最终必须存储所有提交的日志条目。在某些共识算法中，例如“加盖时间戳的复制” ，即使最初并不包含所有提交的条目，也可以选择一个Leader。这些算法包含其他机制，无论是在选举过程中还是选举后不久，可以识别丢失的条目并将其发送给新的Leader。不幸的是，这导致了相当大的额外机制和复杂性。Raft使用一种更简单的方法来保证自新任Leader选举之日都具有所有先前提交的条目，而无需将这些条目转移给Leader。这意味着日志条目仅在一个方向上（从Leader到Follower）流动，并且Leader永远不会覆盖其日志中的现有条目。<br>Raft使用投票程序来防止Candidate赢得选举，除非其日志中包含所有已提交的条目。Candidate必须联系集群的大多数才能被选举，这意味着每个提交的条目都必须存在于其中至少一台服务器中。如果Candidate的日志至少与该多数服务器日志中的日志一样最新（以下精确定义了“最新”），则它将保存所有已提交的条目。RequestVote RPC实施了此限制：RPC包含有关Candidate日志的信息，如果投票者自己的日志比Candidate的日志最新，则投票者将拒绝投票。<br>Raft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新的。 如果日志中的最后一个条目具有不同的任期，则带有较新任期的日志是最新的。如果日志以相同的任期结尾，则以索引更大的日志为准。</p>\n<h3 id=\"5-4-2-提交之前任期的日志\"><a href=\"#5-4-2-提交之前任期的日志\" class=\"headerlink\" title=\"5.4.2 提交之前任期的日志\"></a>5.4.2 提交之前任期的日志</h3><p>如第5.3节所述，Leader知道，一旦该条目存储在大多数服务器上，就会提交当前项的输入。如果Leader在提交条目之前崩溃，将来的Leader将尝试完成复制条目。但是，Leader不能立即得出以下结论：一旦上一个任期的条目存储在大多数服务器上，就将其提交。 图-8说明了一种情况，其中旧的日志条目存储在大多数服务器上，但将来的Leader仍可以覆盖。<br>为了消除如图8所示的问题，Raft决不通过计算副本数来提交前项的日志条目。只有Leader当前任期的日志条目才通过计算副本数来提交；一旦以这种方式提交了当前任期的条目，则由于“日志匹配”属性而间接提交了所有先前的条目。在某些情况下，Leader可以安全地断定已提交了较旧的日志输入（例如，如果该条目存储在每个服务器上），但是Raft为简化起见采取了更为保守的方法。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145750026-1894945621.png\" srcset=\"undefined\" width = \"300\" height = \"140\" alt=\"图9\" align=center />\n#### 图9:如果S1（任期T的Leader）从其任期中提交了新的日志条目，并且S5当选为下一任期U的Leader，那么必须至少有一个服务器（S3）接受了该日志条目并投票支持S5。\nRaft会在承诺规则中带来额外的复杂性，因为当Leader从先前条款中复制条目时，日志条目将保留其原始任期号。在其他共识算法中，如果新的Leader从先前的“任期号”中复制条目，则必须使用其新的“任期号”来进行复制。Raft的方法使推理日志条目变得更加容易，因为它们在整个周期和跨日志期间保持相同的任期编号。此外，与其他算法相比，Raft中的新Leader发送的先前条目的日志条目要少（其他算法必须发送冗余日志条目以对其重新编号，然后才能提交）。\n\n<h3 id=\"5-4-3-安全性讨论\"><a href=\"#5-4-3-安全性讨论\" class=\"headerlink\" title=\"5.4.3 安全性讨论\"></a>5.4.3 安全性讨论</h3><p>给定完整的Raft算法，我们现在可以更精确地论证“领导者完整性属性”成立（此论点基于安全性证明；请参见9.2节）。 我们假设Leader完整性属性不成立，那么我们证明了一个矛盾。 假设任期<em>T</em>的Leader提交了其任期的日志条目，但该日志条目未由某个将来任期的Leader存储。考虑最小项任期<em>U&gt;T<em>，其领导者</em>leaderU</em> 不存储该条目。</p>\n<ol>\n<li>提交时，必须在<em>LeaderU</em>的日志中没有已提交的条目（Leader绝不会删除或覆盖条目）。</li>\n<li><em>LeaderT</em>在大多数集群中复制了该条目，<em>leaderU</em>从大多数集群中获得了投票。因此，如图9所示，至少有一个服务器（“投票者”）既接受了<em>LeaderT</em>的条目又投票给<em>LeaderU</em>，投票者是达成矛盾的关键。</li>\n<li>投票者必须在接受<em>LeaderU</em>投票之前已经接受了<em>LeaderT</em>提交的条目；否则，它将拒绝来自<em>LeaderT</em>的AppendEntries请求（其当前期限将高于<em>T</em>）。</li>\n<li>当每个投票者对<em>leaderU</em>投票时，投票者仍然存储该条目，因为每个居间的Leader都包含该条目（通过假设），Leader从不删除条目，而Follower仅在与Leader冲突时才删除条目。</li>\n<li>投票者将投票结果授予了<em>LeaderU</em>，因此<em>LeaderU</em>的日志必须与投票者的日志一样最新。这导致两个矛盾之一。</li>\n<li>首先，如果投票者和<em>LeaderU</em>共享相同的最后一个日志条款，那么<em>LeaderU</em>的日志必须至少与投票者的日志一样长，因此其日志包含投票者日志中的每个条目。这是一个矛盾，因为投票者中包含已承诺的条目，而<em>leaderU</em>被假定为不包含。</li>\n<li>否则，<em>leaderU</em>的最后一条日志的任期号必须大于投票者的最后一条日志的任期号。而且，它比<em>T</em>大，因为投票者的上一个日志任期号至少为<em>T</em>（它包含任期<em>T</em>中的所有已提交的条目）。创建<em>leaderU</em>的最后一个日志条目的较早的Leader必须在其日志中（通过假设）包含已提交的条目。然后，通过日志匹配属性，<em>leaderU</em>的日志还必须包含已提交的条目，这是矛盾的。</li>\n<li>这样就完成了矛盾。因此，所有任期大于<em>T</em>的Leader都必须包含在任期<em>T</em>中提交的所有任期为<em>T</em>的日志。</li>\n<li>日志匹配属性保证未来的Leader还将包含间接提交的条目，例如图8（d）中的索引2。</li>\n</ol>\n<p>给定“Leader完整性”属性，我们可以从图3证明“状态机安全性”属性，该状态表明，如果服务器已将给定索引的日志条目应用于其状态机，则其他任何服务器都不会为该状态机在相同的索引应用不同的日志条目。服务器在将日志条目应用于其状态机时，其日志必须与通过该条目的Leader的日志相同，并且必须提交该条目。现在考虑任何服务器应用给定日志索引的最低任期；日志完整性属性可确保Leader将存储所有较高任期的相同的日志条目，因此，服务器将在最新的任期中应用的索引将应用相同的值。 因此，状态机安全属性成立。<br>最后，Raft要求服务器以日志索引顺序应用条目。结合状态机安全属性，这意味着所有服务器将以相同的顺序将完全相同的日志条目集应用于其状态机。</p>\n<h2 id=\"5-5-Follower与Candidate崩溃\"><a href=\"#5-5-Follower与Candidate崩溃\" class=\"headerlink\" title=\"5.5 Follower与Candidate崩溃\"></a>5.5 Follower与Candidate崩溃</h2><p>到目前为止，我们只关注Leader的失败。Follower与Candidate崩溃比Leader崩溃要容易得多，并且两者的处理方式相同。 如果Follower与Candidate崩溃，则将来发送给它的RequestVote和AppendEntries RPC将失败。 Raft通过无限期重试来处理这些故障；如果崩溃的服务器重新启动，则RPC将成功完成。如果服务器在完成RPC之后但在响应之前崩溃，则在重新启动后它将再次收到相同的RPC。Raft中的RPC是幂等的，因此不会造成伤害。例如，如果Follower收到一个AppendEntries请求，其中包括其日志中已经存在的日志条目，则它会忽略新请求中的那些相同的条目。</p>\n<h2 id=\"5-6-时间和可用性\"><a href=\"#5-6-时间和可用性\" class=\"headerlink\" title=\"5.6 时间和可用性\"></a>5.6 时间和可用性</h2><p>我们对Raft的要求之一是安全不得取决于时间安排：系统不得仅由于某些事件比预期的快或慢发生而产生不正确的结果。 但是，可用性（系统及时响应客户端的能力）必定取决于时间。 例如，如果消息交换花费的时间比服务器崩溃之间的典型时间长，则Candidate将不会停留足够长的时间来赢得选举。 没有稳定的Leader，Raft无法取得进步。<br>领导人选举是Raft最重要的方面，时机至关重要。只要系统满足以下时间要求，Raft将能够选举和维持稳定的Leader：<br><strong>broadcastTime ≪ electionTimeout ≪ MTBF</strong><br>在这种不平等的情况下，<em>broadcastTime</em>是服务器将RPC并行发送到集群中的每个服务器并接收其响应所花费的平均时间。 <em>electionTimeout</em>是第5.2节所述的选举超时；<em>MTBF</em>是单个服务器两次故障之间的平均时间。广播时间应比选举超时小一个数量级，以便Leader能够可靠地发送所需的心跳消息，以防止Follower开始选举。考虑到用于选举超时的随机方法，这种不平等也使得分散投票变得不太可能。选举超时应该比<em>MTBF</em>小几个数量级，以便系统稳步前进。当Leader崩溃时，该系统将在大约选举超时时间内不可用；我们希望这只代表总时间的一小部分。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145827202-614379881.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图10\" align=center />\n#### 图10:直接从一种配置切换到另一种配置是不安全的，因为不同的服务器将在不同的时间进行切换。在此示例中，群集从三台服务器增长到五台。不幸的是，在某个时间点上，可以为同一任期选举两名不同的Leader，其中一位拥有多数旧配置（C_old），另一位拥有多数新配置（C_new）。\n\n<p>广播时间和<em>MTBF</em>是基础系统的属性，而选举超时是我们必须选择的东西。Raft的RPC通常需要接收者将信息持久存储到稳定的存储中，因此根据存储技术的不同，转换时间可能从0.5毫秒到20毫秒不等。选举超时可能在10毫秒至500毫秒之间。 典型服务器的<em>MTBF</em>长达数月或更长时间，可以轻松满足计时要求。</p>\n<h1 id=\"6-集群成员关系变化\"><a href=\"#6-集群成员关系变化\" class=\"headerlink\" title=\"6 集群成员关系变化\"></a>6 集群成员关系变化</h1><p>到现在为止，我们还假设集群配置（参与共识算法的服务器集合）是固定的。实际上，有时需要更改配置，例如在服务器出现故障时更换服务器或更改复制程度。尽管可以通过使整个群集脱机，更新配置文件，然后重新启动群集来完成此操作，但这将使群集在转换期间不可用。此外，如果有任何手动步骤，则可能会导致操作员出错。为了避免这些问题，我们决定自动进行配置更改，并将其合并到Raft共识算法中。<br>为了确保配置更改机制的安全，在过渡期间必须没有任何可能在同一任期内选举两名领导者的意义。不幸的是，任何将服务器直接从旧配置切换到新配置的方法都是不安全的。不可能一次自动切换所有服务器，因此群集在过渡期间可能会分成两个独立的多数（见图10）。<br>为了确保安全，更改配置必须使用两阶段方法。有两种方法可以实现两个阶段。例如，某些系统使用第一阶段来禁用旧配置，因此它无法处理客户端请求；然后第二阶段启用新配置。在Raft中，集群首先切换到过渡配置，我们称为联合共识；提交联合共识后，系统将过渡到新配置。联合共识将新旧配置结合在一起：</p>\n<ul>\n<li>两种配置中的日志条目均复制到所有服务器。</li>\n<li>来自任一配置的任何服务器都可以充当领导者。</li>\n<li>协议（用于选举和入职承诺）需要分别来自新旧配置的大多数人。</li>\n</ul>\n<p>联合共识允许单个服务器在不同时间在配置之间转换，而不会影响安全性。此外，联合共识允许群集在整个配置更改期间继续为客户请求提供服务。<br>群集配置使用复制日志中的特殊条目进行存储和通信。图11说明了配置更改过程。当Leader收到将配置从<em>C_old</em>更改为<em>C_new</em>的请求时，它将存储用于联合共识的配置（<em>C_old</em>，图中的<em>new</em>）作为日志条目，并使用前述机制复制该条目。给定服务器将新配置条目添加到其日志后，它将使用该配置进行所有将来的决策（服务器始终使用其日志中的最新配置，而不管该条目是否被提交）。这意味着Leader将使用<em>C_old_new</em>规则来确定何时提交<em>C_old_new</em>的日志条目。如果Leader崩溃，则可以根据获胜的Candidate是否收到了<em>C_old_new</em>来在<em>C_old</em>或<em>C_old_new</em>下选择新的Leader。无论如何，<em>C_new</em>不能在此期间做出单方面决定。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145845730-1927441378.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图11\" align=center />\n#### 图11:配置更改的时间表。虚线表示已创建但尚未提交的配置条目，实线表示最新的提交的配置条目。Leader首先在其日志中创建*C_old_new*配置条目，并将其提交给*C_old_new*（大多数*C_old*和*C_new*）。 然后，它创建*C_new*条目并将其提交给大多数*C_new*。*C_old*和*C_new*都无法独立做出决策。\n\n<p>一旦提交了<em>C_old_new，C_old</em>和<em>C_new</em>都无法在未经另一方批准的情况下做出决策，并且Leader 完备性确保只有具有<em>C_old_new</em>日志条目的服务器才能被选为Leader。现在，Leader可以安全地创建描述<em>C_new</em>的日志条目并将其复制到集群。同样，该配置将在看到每台服务器后立即生效。在<em>C_new</em>的规则下提交新配置后，旧配置将不相关，并且可以关闭不在新配置中的服务器。如图11所示，<em>C_old</em>和<em>C_new</em>都没有时间可以单方面做出决定。这样可以保证安全。<br>还需要解决三个问题以进行重新配置。第一个问题是新服务器最初可能不会存储任何日志条目。如果以这种状态将它们添加到群集，则它们可能要花很长时间才能赶上，在此期间可能无法提交新的日志条目.为了避免可用性差距，Raft在配置更改之前引入了一个附加阶段，在该阶段中，新服务器以无表决权的成员的身份加入群集（领导者将日志条目复制到它们，但多数情况下不考虑它们）。一旦新服务器赶上了群集的其余部分，重新配置就可以如上所述进行。<br>第二个问题是集群Leader可能不属于新配置。在这种情况下，Leader一旦提交了<em>C_new</em>日志条目，便会下台（返回到Follower状态）。这意味着在一段时间（<em>C_new</em>提交）时，Leader将管理一个不包含自身的集群。它复制日志条目，但不占多数。提交<em>C_new</em>时会发生Leader转换，因为这是新配置可以独立运行的第一步（始终可以从<em>C_new</em>中选择一个领导者）。 在此之前，可能只有<em>C_old</em>中的服务器可以被选为Leader。<br>第三个问题是，删除的服务器（那些不在<em>C_new</em>中的服务器）会破坏群集。这些服务器不会接收心跳，因此它们将超时并开始新的选择。然后，他们将使用新的任期编号发送RequestVote RPC，这将导致当前Leader恢复为Follower状态。最终将选举新的Leader，但是被删除的服务器将再次超时，并且该过程将重复进行，从而导致可用性降低。<br>为防止此问题，服务器在认为当前的Leader存在时会忽略RequestVote RPC。具体来说，如果服务器在当前Leader的最小选举超时时间内收到RequestVote RPC，则该服务器不会更新其任期或授予其投票权。这不会影响正常的选举，在正常的选举中，每个服务器在开始选举之前至少等待最小选举超时。但是，它有助于避免因移动服务器而造成的中断：如果Leader能够对其集群发出心跳信号，那么它将不会被更高任期的成员所取代。</p>\n<h1 id=\"7-日志压缩\"><a href=\"#7-日志压缩\" class=\"headerlink\" title=\"7 日志压缩\"></a>7 日志压缩</h1><p>在正常运行期间，Raft的日志会增长，以合并更多的客户请求，但在实际系统中，它无法无限制地增长。随着日志的增长，它会占用更多空间，并需要更多时间来重放。 如果没有某种机制来丢弃日志中累积的过时信息，这最终将导致可用性问题。<br>快照是最简单的压缩方法。 在快照中，将整个当前系统状态写入稳定存储上的快照，然后丢弃该点之前的整个日志。 快照在Chubby和ZooKeeper中使用，本节的其余部分介绍在Raft中进行快照。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145907980-1608425176.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图12\" align=center />\n#### 图12:服务器用新快照替换其日志（索引1至5）中已提交的条目，该快照仅存储当前状态（在此示例中为变量x和y）。快照的最后一个包含索引和任期的作用是将快照放置在条目6之前的日志中。\n诸如日志清理和日志结构的合并树之类的递增压缩方法也是可能的。它们一次处理一部分数据，因此它们会随着时间的推移更均匀地分散压缩负载。他们首先选择一个累积了许多已删除和覆盖的对象的数据区域，然后再更紧凑地重写该区域中的活动对象并释放该区域。与快照相比，这需要大量的附加机制和复杂性，从而通过始终对整个数据集进行操作来简化问题。 尽管清除日志需要修改Raft，但是状态机可以使用与快照相同的接口来实现LSM树。\n图12显示了Raft中快照的基本概念。每个服务器独立地拍摄快照，仅覆盖其日志中的已提交条目。状态机的大部分工作都由状态机组成，将其当前状态写入快照。Raft在快照中还包含少量元数据：最后包含的索引是快照替换的日志中最后一个条目的索引（状态机已应用的最后一个条目），最后包含的任期是该日志的任期。保留这些内容是为了支持快照之后的第一个日志条目的AppendEntries一致性检查，因为该条目需要先前的日志索引和任期。为了启用集群成员资格更改（第6节），快照还包括自上次包含索引起的日志中的最新配置。服务器完成快照的写入后，它可能会删除最后一个包含的索引中的所有日志条目以及所有先前的快照。\n\n\n<table>\n<thead>\n<tr>\n<th><strong>安装快照RPC</strong></th>\n<th><strong>由Leader调用并按顺序发送打包的快照到Follower。</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>参数：</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>Leader的任期</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>用以Follower可以重定向客户端的请求到Leader</td>\n</tr>\n<tr>\n<td>lastIncludedIndex</td>\n<td>快照将替换直到该索引的所有日志条目</td>\n</tr>\n<tr>\n<td>lastIncludedTerm</td>\n<td>lastIncludedIndex的任期</td>\n</tr>\n<tr>\n<td>offset</td>\n<td>快照文件的偏移量</td>\n</tr>\n<tr>\n<td>data[]</td>\n<td>从offset开始，快照内的数据数组</td>\n</tr>\n<tr>\n<td>done</td>\n<td>如果这是最后一个快照则为true</td>\n</tr>\n<tr>\n<td><strong>结果:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>当前任期，用以Leader更新自己</td>\n</tr>\n<tr>\n<td><strong>接收者实现:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>1.如果任期小于当前任期则立即回复</td>\n<td></td>\n</tr>\n<tr>\n<td>2.如果第一次打包创建新的快照文件(offset为0)</td>\n<td></td>\n</tr>\n<tr>\n<td>3.从给予的offset写数据到快照中</td>\n<td></td>\n</tr>\n<tr>\n<td>4.如果失败的话回复并等待更多打包的数据</td>\n<td></td>\n</tr>\n<tr>\n<td>5.存储快照文件，抛弃所有存在的或并行的相同的索引快照文件。</td>\n<td></td>\n</tr>\n<tr>\n<td>6.如果在快照文件中包含的实体最后存在日志实体具有相同的索引与任期，保持日志实体并回复</td>\n<td></td>\n</tr>\n<tr>\n<td>7.抛弃整个日志</td>\n<td></td>\n</tr>\n<tr>\n<td>8.重置状态机并使用快照内容(加载快照集群配置)</td>\n<td></td>\n</tr>\n</tbody></table>\n<p>尽管服务器通常通常独立拍摄快照，但Leader有时必须将快照发送给落后的Follower。当Leader已经丢弃了需要发送给Follower的下一个日志条目时，就会发生这种情况。幸运的是，在正常操作中这种情况不太可能发生：与Leader保持同步的Follower将已经有此条目。但是，异常慢的Follower或加入集群的新服务器（第6节）则不会。 使此类Follower保持最新状态的方法是，Leader可以通过网络向其发送快照。<br>Leader使用一个称为InstallSnapshot的新RPC将快照发送给落后的Follower。请参见图13。当Follower收到带有此RPC的快照时，它必须决定如何处理其现有的日志记录。通常，快照将包含收件人日志中尚未包含的新信息。在这种情况下，Follower将丢弃其整个日志；它全部被快照取代，并且可能具有与快照冲突的未提交条目。相反，如果Follower收到描述其日志前缀的快照（由于重新传输或错误操作），则快照所覆盖的日志条目将被删除，但快照之后的条目仍然有效并且必须保留。<br>这种快照方法背离了Raft强大的Leader原则，因为Follower可以在不了解Leader的情况下进行快照。但是，我们认为这种偏离是合理的。尽管拥有Leader可以避免在达成共识时发生决策冲突，但快照时已经达成共识，因此没有决策冲突。数据仍然仅从Leader流向Follower，只是Follower现在可以重组其数据。<br>我们考虑了另一种基于Leader的方法，其中只有Leader将创建快照，然后将快照发送给其每个Follower。但是，这有两个缺点。首先，将快照发送给每个Follower会浪费网络带宽并减慢快照过程。每个Follower已经具有生成自己的快照所需的信息，通常，服务器从其本地状态生成快照要比通过网络发送和接收快照便宜得多。其次，Leader的实施将更加复杂。例如，Leader将需要同时向跟随者发送快照，同时向其复制新的日志条目，以免阻塞新的客户要求。<br>还有另外两个问题会影响每个快照形式。首先，服务器必须决定何时进行快照。如果服务器过于频繁地进行快照，则会浪费磁盘带宽和能量。如果快照太不频繁，则有耗尽其存储容量的风险，并且会增加重新启动期间重放日志所需的时间。一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果此大小设置为明显大于快照的预期大小，则用于快照的磁盘带宽开销将很小。<br>第二个性能问题是写快照可能要花费大量时间，我们不希望这样做会延迟正常操作。解决方案是使用写时复制技术，以便可以接受新的更新而不会影响快照的写入。例如，使用功能性数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如，Linux上的fork）来创建整个状态机的内存快照（我们的实现使用这种方法）。</p>\n<h1 id=\"8-客户端内部交互\"><a href=\"#8-客户端内部交互\" class=\"headerlink\" title=\"8 客户端内部交互\"></a>8 客户端内部交互</h1><p>本节描述客户端如何与Raft交互，包括客户端如何找到集群领导者以及Raft如何支持线性化语义。这些问题适用于所有基于共识的系统，并且Raft的解决方案与其他系统类似。<br>Raft的客户将所有请求发送给Leader。客户端首次启动时，它会连接到随机选择的服务器。如果客户的首选不是Leader，则该服务器将拒绝客户的请求，并提供有关其最近听到的Leader的信息（AppendEntries请求包括Leader的网络地址）。如果Leader崩溃，客户请求将超时；客户端，然后使用随机选择的服务器重试。<br>我们对Raft的目标是实现线性化的语义（每个操作似乎在调用和响应之间的某个时刻立即执行一次，恰好一次）。但是，到目前为止，Raft可以多次执行命令：例如，如果Leader在提交日志条目之后但在响应客户端之前崩溃，则客户端将使用新的Leader重试该命令，从而导致该失败再次执行。解决方案是让客户端为每个命令分配唯一的序列号。然后，状态机将跟踪为每个客户端处理的最新序列号以及相关的响应。如果收到序列号已被执行的命令，它将立即响应而无需重新执行该请求。<br>可以执行只读操作，而无需在日志中写入任何内容。但是，如果没有其他措施，这将存在返回陈旧数据的风险，因为响应请求的Leader可能已经受到了一个不知道的新Leader的支持。可读的读取一定不能返回陈旧的数据，并且Raft需要采取两项额外的预防措施来保证不使用日志就可以做到这一点。首先，Leader必须掌握提交条目的最新信息。Leader完整性属性可以保证Leader具有所有承担的职责，但是在任期开始之初，它可能不知道是谁。为了找出答案，它需要从其任期中提交一个条目。Raft通过让每个Leader在任期开始时在日志中输入一个空白的禁止操作条目来解决此问题。其次，Leader必须在处理只读请求之前检查其是否已处置（如果选择了新的Leader，其信息可能会过时）。Raft通过让Leader在响应只读请求之前与大多数集群交换心跳消息来解决此问题。或者，Leader可以依靠心跳机制来提供某种形式的租约，但这将依赖于时序以确保安全性（它假设时钟范围有界）。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址-&gt;<a href=\"https://raft.github.io/raft.pdf\" target=\"_blank\" rel=\"noopener\">Raft算法</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>Raft是用于管理被复制的日志的共识算法。它与multi-Paxos算法产生的效果相同，并且和Paxos算法一样高效。但是结构与Paxos不同。这使得Raft算法比Paxos算法更容易理解。也为构建实际系统提供了更好的基础。为了加强理解，Raft将几个关键元素分离，比如leader选举，日志复制，安全性。并增强了一致性，以减少必须考虑的状态数。一项用户研究的结果表明，与Paxos相比，Raft算法更易于学生学习。Raft也提供了用于更新集群成员关系的新的机制。它使用重叠的多数来保证安全。</p>\n<h1 id=\"1-介绍\"><a href=\"#1-介绍\" class=\"headerlink\" title=\"1 介绍\"></a>1 介绍</h1><p>共识算法允许一组计算机的集合作为一个一致的的小组工作，这些小组可以承受某些成员的故障。正因为如此，共识机制在构建可信的大规模软件系统中起着至关重要的作用。在过去的十年中，Paxos一直主导着共识算法的讨论。很多共识算法都是基于Paxos或者受它的影响。Paxos称为了教受学生关于共识算法的主要工具。<br>不幸的是，尽管进行了许多尝试以使Paxos更加平易近人，Paxos仍然非常难以理解。此外，其体系结构需要复杂的更改以支持实际系统。结果，系统构建者和学生都与Paxos斗争。<br>在我们与Paxos斗争之后，我们着手寻找一种新的共识算法，该算法可以为系统构建和教育提供更好的基础。我们的方法与众不同，因为我们的主要目标是易于理解：我们能否为实际系统定义共识算法，并以比Paxos容易学习的方式对其进行描述？此外，我们希望该算法有助于系统开发人员必不可少的直觉的发展。重要的不仅是算法能起作用，而且要很清除它为什么起作用。<br>这项工作的结果是一个称为Raft的共识算法。在设计Raft时，我们应用了特定的技术来提高可理解性，包括分解（Raft分离了领导者选举，日志复制和安全性）以及状态空间减少（相对于Paxos，Raft减少了不确定性的程度以及服务器之间可能不一致的方式）。 一项对两所大学的43名学生进行的用户研究表明，Raft比Paxos容易理解得多：在学习了两种算法之后，其中33位学生比Rax更好地回答了有关Raft的问题。<br>Raft在许多方面与现有的共识算法相似（最著名的是Oki和Liskov的Viewstamped复制），但是它具有几个新颖的功能：</p>\n<ul>\n<li>强壮的leader:与其他共识算法相比，Raft使用更强大的领导方式。例如，日志条目仅从leader者流向其他服务器。 这简化了复制日志的管理，并使Raft更易于理解。</li>\n<li>Leader选举:Raft使用随机计时器选举leader。这可以为任何共识算法已经要求的心跳添加少量机制，同时可以快速而轻松地解决冲突。</li>\n<li>成员关系变化:Raft更改集群中服务器组的机制使用了一种新的联合共识方法，其中，两种不同配置的大多数在转换过程中会重叠。 这允许群集在配置更改期间继续正常运行。</li>\n</ul>\n<p>我们认为Raft在教育目的和实施基础上均优于Paxos和其他共识算法。它比其他算法更简单，更易懂。本文对其进行了足够详尽的描述以满足实际系统的需求。它具有多种开源实现，并被多家公司使用；其安全性能已得到正式规定和证明；并且其效率可与其他算法相比。<br>本文的其余部分介绍了复制状态机问题(第2节)，讨论了Paxos的优缺点(第3节),描述了我们对可理解性的一般方法(第4节)，介绍了Raft共识算法(第5–8节),评估Raft(第9节),并讨论相关工作(第10节)。</p>\n<h1 id=\"2-复制状态机\"><a href=\"#2-复制状态机\" class=\"headerlink\" title=\"2 复制状态机\"></a>2 复制状态机</h1><p>共识算法通常出现在复制状态机的环境中。通过这种方法，服务器集合上的状态机可以计算相同状态的相同副本，即使某些服务器宕机也可以继续运行。 复制状态机用于解决分布式系统中的各种容错问题。例如，具有单个集群领导者的大型系统，例如GFS，HDFS和RAMCloud，通常使用单独的复制状态机来管理领导者选举并存储配置信息，这些信息必须在领导者崩溃中幸存。复制状态机的示例包括Chubby和ZooKeeper。<br>复制状态机通常使用复制日志来实现，如图1所示。每个服务器都存储一个包含一系列命令的日志，其状态机按顺序执行这些命令。每个日志以相同的顺序包含相同的命令，因此每个状态机处理相同的命令序列。由于状态机是确定性的，每个计算相同的状态和输出的顺序相同。<br>保持复制日志的一致性是共识算法的工作。服务器上的共识模块从客户端接收命令，并将其添加到其日志中。它与其他服务器上的共识模块进行通信，以确保即使某些服务器发生故障，每个日志最终仍会以相同顺序包含相同的请求。正确复制命令后，每台服务器的状态机都将以日志顺序对其进行处理，然后将输出返回给客户端。服务器似乎形成了单个高度可靠的状态机。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145431490-1698442829.png\" srcset=\"undefined\" width = \"300\" height = \"180\" alt=\"图1\" align=center />\n\n<h4 id=\"图1：复制状态机架构。-共识算法管理包含来自客户端的状态机命令的复制日志。-状态机处理来自日志的相同命令序列，因此它们产生相同的输出。\"><a href=\"#图1：复制状态机架构。-共识算法管理包含来自客户端的状态机命令的复制日志。-状态机处理来自日志的相同命令序列，因此它们产生相同的输出。\" class=\"headerlink\" title=\"图1：复制状态机架构。 共识算法管理包含来自客户端的状态机命令的复制日志。 状态机处理来自日志的相同命令序列，因此它们产生相同的输出。\"></a>图1：复制状态机架构。 共识算法管理包含来自客户端的状态机命令的复制日志。 状态机处理来自日志的相同命令序列，因此它们产生相同的输出。</h4><p>实际系统的共识算法通常具有以下属性：</p>\n<ul>\n<li>它们可确保在所有非拜占庭条件下的安全性（绝不会返回错误的结果），包括网络延迟，分区，数据包丢失，复制和重新排序。</li>\n<li>只要大多数服务器都可以运行并且可以相互通信并与客户端进行通信，它们就可以正常运行（可用）。因此，由五个服务器组成的典型集群可以容忍任何两个服务器的故障。假定服务器因停止而发生故障；它们稍后可能会从稳定存储上的状态中恢复并重新加入群集。</li>\n<li>它们不依赖于时序来确保日志的一致性：错误的时钟和极端的消息延迟可能在最坏的情况下导致可用性问题。</li>\n<li>在通常情况下，只要集群的大多数都响应了一次远程过程调用，命令就可以完成。少数速度较慢的服务器不必影响整体系统性能。</li>\n</ul>\n<h1 id=\"3-Paxos算法怎么了\"><a href=\"#3-Paxos算法怎么了\" class=\"headerlink\" title=\"3 Paxos算法怎么了\"></a>3 Paxos算法怎么了</h1><p>…</p>\n<h1 id=\"4-可理解性的设计\"><a href=\"#4-可理解性的设计\" class=\"headerlink\" title=\"4 可理解性的设计\"></a>4 可理解性的设计</h1><p>…</p>\n<h1 id=\"5-Raft共识算法\"><a href=\"#5-Raft共识算法\" class=\"headerlink\" title=\"5 Raft共识算法\"></a>5 Raft共识算法</h1><p>Raft是一个如第二部分描述的对被复制的日志进行管理的算法。图2以简明形式总结了该算法以供参考，图3列出了该算法的关键属性。这些图的元素将在本节的其余部分中进行分段讨论。<br>Raft通过首先选举一位杰出的Leader，然后赋予Leader完全的责任来管理复制日志来实现共识。Leader接受来自客户端的日志条目，将其复制到其他服务器上，并告诉服务器何时可以安全地将日志条目应用于其状态机。拥有一个Leader可以简化复制日志的管理。例如，Leader可以决定在何处放置新条目而无需咨询其他服务器，并且数据以简单的方式从Leader流向其他服务器。Leader可能会失败或与其他服务器断开连接，在这种情况下，将选出新的Leader。<br>使用Leader方法，Raft将共识问题分解为三个相对独立的子问题，这些子问题将在以下小节中进行讨论：</p>\n<ul>\n<li>Leader选举:当存在的Leader失败后必须选出一个新的Leader(5.2部分)。</li>\n<li>日志复制:Leader必须接受客户端发送的日志条目并通过集群复制他们，强制其他日志接受自己的(5.3部分)。</li>\n<li>安全性:Raft的关键安全属性是图3中的状态机安全属性：如果任何服务器已将特定的日志条目应用于其状态机，则没有其他服务器可以在同一日志索引下应用不同的命令。5.4节介绍了Raft如何确保此属性；解决方案包括对第5.2节所述的选举机制的附加限制。</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>状态</th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>所有服务器上的一致状态</strong></td>\n<td><strong>在响应RPCs前稳定更新存储</strong></td>\n</tr>\n<tr>\n<td>currentTerm</td>\n<td>最新的服务器任期(第一次引导启动时初始化为0)单调递增</td>\n</tr>\n<tr>\n<td>votedFor</td>\n<td>当前任期投票的candidateId(如果没有则为null),为谁投票则对应的值为谁</td>\n</tr>\n<tr>\n<td>log[]</td>\n<td>日志条目集合，被Leader接收到的每一条日志包含状态机的命令和任期。(第一条索引为1)</td>\n</tr>\n<tr>\n<td><strong>所有服务器上的隔离状态</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>commitIndex</td>\n<td>已知的被提交的被最高的日志索引(初始为0，单调递增)</td>\n</tr>\n<tr>\n<td>lastApplied</td>\n<td>被应用到状态机的最高的日志条目索引(初始为0，单调递增)</td>\n</tr>\n<tr>\n<td><strong>Leader上的隔离状态</strong></td>\n<td><strong>在选举后重新初始化</strong></td>\n</tr>\n<tr>\n<td>nextIndex[]</td>\n<td>Leader对于每一台服务器，将要发送的下一条日志条目(被Leader初始化为最后一条日志索引+1)</td>\n</tr>\n<tr>\n<td>matchIndex[]</td>\n<td>Leader对于每一台服务器，已知的被复制到服务器上的最高的日志条目索引(初始为0，单调递增)</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th><strong>追加日志条目RPC</strong></th>\n<th><strong>由Leader调用完成日志复制(5.3节)，也可以用于心跳信息(5.2节)</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>参数:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>Leader的任期</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>follower可以重定向客户端</td>\n</tr>\n<tr>\n<td>prevLogIndex</td>\n<td>紧接新记录之前的日志条目索引(即上一条日志条目索引)</td>\n</tr>\n<tr>\n<td>prevLogTerm</td>\n<td>上一条日志条目索引的任期</td>\n</tr>\n<tr>\n<td>entries[]</td>\n<td>用于存储的日志实体(心跳信息为空，以至于更高效的发送)</td>\n</tr>\n<tr>\n<td>leaderCommit</td>\n<td>Leader的提交的索引</td>\n</tr>\n<tr>\n<td><strong>结果:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>当前任期，用于Leader更新自己的任期</td>\n</tr>\n<tr>\n<td>success</td>\n<td>如果follower包含的日志实体匹配到prevLogIndex和PrevLogTerm</td>\n</tr>\n<tr>\n<td><strong>接收者实现:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>1.如果任期小于当前任期回复false</td>\n<td>5.1节</td>\n</tr>\n<tr>\n<td>2.如果包含的日志实体没有匹配到prevLogIndex和PrevLogTerm回复false</td>\n<td>5.3节</td>\n</tr>\n<tr>\n<td>3.如果存在日志实体与新的日志实体冲突(相同的索引但任期不同),删除存在的日志实体并选择新的</td>\n<td>5.3节</td>\n</tr>\n<tr>\n<td>4.追加日志中尚未存在的任何新条目</td>\n<td></td>\n</tr>\n<tr>\n<td>5.如果leaderCommit大于commintIndex,将commitIndex设置为(leaderCommit,最后一条新的日志实体索引)中最小的那个</td>\n<td></td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th><strong>请求投票RPC</strong></th>\n<th><strong>由candidates调用用于收集投票数(5.2节)</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>参数:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>candidate的任期</td>\n</tr>\n<tr>\n<td>candidateId</td>\n<td>请求投票的candidateID</td>\n</tr>\n<tr>\n<td>lastLogIndex</td>\n<td>candidate的最后一条日志条目索引 (5.4节)</td>\n</tr>\n<tr>\n<td>lastLogTerm</td>\n<td>candidate的最后一条日志条目的任期(5.4节)</td>\n</tr>\n<tr>\n<td><strong>结果:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>当前任期，用以candidate更新自己的任期</td>\n</tr>\n<tr>\n<td>voteGranted</td>\n<td>如果candidate接受了投票则为true</td>\n</tr>\n<tr>\n<td><strong>接收者实现</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>1.如果任期小于当前任期回复false</td>\n<td>5.1节</td>\n</tr>\n<tr>\n<td>2.如果votedFor为空或者candidateId,且candidate的日志</td>\n<td></td>\n</tr>\n<tr>\n<td>至少与接收者的日志一样新，同意投票</td>\n<td>5.2节 5.4节</td>\n</tr>\n</tbody></table>\n<h4 id=\"服务器的规则\"><a href=\"#服务器的规则\" class=\"headerlink\" title=\"服务器的规则:\"></a>服务器的规则:</h4><p><strong>所有服务器:</strong></p>\n<ul>\n<li>如果commitIndex大于lastApplied;增加lastApplied,应用log[lastApplied]到状态机(5.3节)</li>\n<li>如果RPC请求或响应中的任期T大于currentTerm；设置currentTerm为T，变为follower(5.1节)</li>\n</ul>\n<p><strong>所有Follower(5.2节)</strong></p>\n<ul>\n<li>响应来自candidates和Leader的RPC消息。</li>\n<li>如果直到选举超时也没有接受到由当前Leader发送的追加日志条目RPC消息或者对candidate的投票，变为candidate</li>\n</ul>\n<p><strong>所有Candidate(5.2节)</strong></p>\n<ul>\n<li>当转换为Candidate后，启动选举过程：<ul>\n<li>增加currentTerm</li>\n<li>为自己投票</li>\n<li>重置选举计时器</li>\n<li>发送请求投票RPC消息到其他所有服务器</li>\n</ul>\n</li>\n<li>如果接收到大多数成员的投票信息，变为Leader</li>\n<li>如果接收到来自新的Leader的追加日志条目RPC消息，转换为follower</li>\n<li>如果选举超时，启动新的选举过程</li>\n</ul>\n<p><strong>Leader</strong></p>\n<ul>\n<li>选举过后：将初始的空追加日志条目RP(心跳)消息发送到每个服务器；在空闲时间重复此操作以防止选举超时(5.2节)</li>\n<li>如果接收到来自客户端的命令，追加日志到本地，在日志条目应用到状态机后回复客户端。</li>\n<li>如果对于follower，lastLogIndex大于等于nextIndex，发送带有从nextIndex开始的日志条目的追加日志条目RPC消息。<ul>\n<li>如果响应成功，更新对于该follower的nextIndex和matchIndex</li>\n<li>如果因为日志的不一致性导致追加日志实体消息失败，递减nextIndex并重试</li>\n</ul>\n</li>\n<li>如果存在N并且N大于commitIndex,并且大多数matchIndex[i]大于等于N，且log[N]的任期与currentTerm相等，将commitIndex设置为N(5.3 5.4节)</li>\n</ul>\n<h4 id=\"图2\"><a href=\"#图2\" class=\"headerlink\" title=\"图2\"></a>图2</h4><h4 id=\"选举安全\"><a href=\"#选举安全\" class=\"headerlink\" title=\"选举安全\"></a>选举安全</h4><p>在给定的任期，最多只能选举一个Leader。5.2节<br><strong>Leader只追加特性:</strong> Leader从不覆盖或删除它的日志条目，只追加新的。5.3节<br><strong>日志匹配:</strong> 如果两个日志包含的实体具有相同的索引和任期，那么直到给定索引为止，所有条目中的日志都是相同的。<br><strong>Leader完备性:</strong> 如果一个日志提示在给定的任期内被提交，那么该条目将出现在领导者的日志中，显示所有编号较高的条目。<br><strong>状态机安全:</strong> 如果服务器应用一条给定索引的日志实体到它的状态机，那么没有其他服务器可以应用一条不同的日志到相同的索引位置。</p>\n<h4 id=\"图3：Raft保证任何时刻这里的每一条属性都是成立的。\"><a href=\"#图3：Raft保证任何时刻这里的每一条属性都是成立的。\" class=\"headerlink\" title=\"图3：Raft保证任何时刻这里的每一条属性都是成立的。\"></a>图3：Raft保证任何时刻这里的每一条属性都是成立的。</h4><h2 id=\"5-1-Raft基础\"><a href=\"#5-1-Raft基础\" class=\"headerlink\" title=\"5.1 Raft基础\"></a>5.1 Raft基础</h2><p>一个Raft集群包含多个服务器；五是一个典型数字，它允许系统容忍两个服务器故障。在任何给定时间，每个服务器都处于以下三种状态之一：Leader，Follower或Candidate。在正常操作中，只有一个Leader，而其他所有服务器都是Follower。 Follower是被动的：他们自己不发出请求，而只是响应Leader和Candidate的请求。Leader处理所有客户请求(如果客户联系Follower，则Follower将其重定向到Leader)。第3种状态Candidate用于选举新的Leader。图4显示了状态及其转换。 过渡将在下面讨论。<br>Raft将时间划分为任意长度的项，如图5所示。项用连续的整数编号。每个任期都以选举开始，在选举中，一个或多个Candidate试图按照5.2节中的描述成为Leader。 如果Candidate在选举中获胜，则它将在剩余任期中担任Leader。在某些情况下，选举将导致投票分裂。在这种情况下，任期将以无Leader结束；新任期(以新的选举)将很快开始。Raft确保给定任期内最多有一位Leader。<br>不同的服务器可能会在不同时间观察任期之间的转换，并且在某些情况下，服务器可能不会观察到选举甚至整个任期。任期在Raft中充当逻辑时钟，它们使服务器能够检测过时的信息，例如陈旧的Leader。每个服务器存储一个当前的任期号，该任期号随时间单调增加。只要服务器进行通信，就会交换当前任期；如果一台服务器的当前任期小于另一台服务器，则它将其当前任期更新为较大的值。如果Candidate或Leader发现其任期已过时，它将立即恢复为Follower状态。如果服务器收到带有过期条款编号的请求，则服务器将拒绝该请求。<br>Raft式服务器使用远程过程调用（RPC）进行通信，并且基本共识算法仅需要两种类型的RPC。RequestVote RPC由Candidate在选举期间启动（第5.2节），而AppendEntries RPC由Leader启动以复制日志条目并提供心跳形式(第5.3节)的AppendEntries RPC消息。第7节添加了第三个RPC，用于在服务器之间传输快照。如果服务器未及时收到响应，则服务器会重试RPC，并且它们并行发出RPC消息以获得最佳性能。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145525858-271973058.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图4\" align=center />\n#### 图4：服务器状态。Follower仅响应来自其他服务器的请求。如果Follower未收到任何通讯，它将成为Candidate并发起选举。从整个集群的大多数中获得选票的Candidate将成为新的Leader。Leader通常会运作直到失败。\n\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145554351-712530469.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图5\" align=center />\n#### 图5:时间分为几个任期，每个任期都以选举开始。选举成功后，由一位Leader管理集群，直到任期结束。在这种情况下可能选举失败导致任期届满而未选出Leader。任期之间的转换可以在不同的服务器上的不同时间观察到。\n## 5.2 Leader选举\nRaft使用心跳机制触发Leader选举。当服务器启动的角色为Follower。服务器保持Follower状态一直到接收到来自Leader或者Candidate的有效的RPC消息。Leader为了维护它的权利将发送周期性的心跳消息(不带有日志实体的AppendEntries RPCs消息)到所有的Follower。如果一个Follower在一整个周期时间内没有接收到任何通信消息则称为选举超时。他们将假设没有可以访问的Leader并开始新的投票选举新的Leader。\n为了开始一次选举，Follower递增它的当前任期并将状态转换为Candidate。然后为自己投一票并并行发送RequestVote RPC消息到集群中其他的所有服务器。Candidate的状态将会一直保持一直到这三种情况中其中一个发生:\n\n<ol>\n<li>赢得选举</li>\n<li>另外一个服务器称为了Leader</li>\n<li>在当前投票周期内没有赢得选举的服务器。</li>\n</ol>\n<p>将会在下面分别讨论这三种情况。<br>如果Candidate在同一任期内从整个集群中获得大多数服务器的票数，则将赢得选举。在给定的期限内，每台服务器将按先到先得的原则为最多一个Candidate投票(注：第5.4节增加了投票的其他限制)。多数规则确保最多只有一名Candidate可以赢得特定任期的选举(图3中的选举安全属性)。Candidate赢得选举后，便成为Leader。然后，它将心跳消息发送到所有其他服务器以建立其权限并阻止新的选举。<br>在等待投票时，Candidate可能会从声称是Leader的另一台服务器收到AppendEntries RPC消息。如果Leader的任期(在其RPC消息中可以获得)至少与Candidate当前任期一样大，则Candidate将Leader视为合法，并返回到Follower状态。 如果RPC中的任期小于Candidate当前的任期，则Candidate将拒绝RPC并继续处于Candidate状态。<br>第三种可能的结果是，Candidate既不会赢得选举也不会输掉选举：如果同时有许多Follower成为Candidate，那么票数可能会分散，从而任何Candidate都不会获得多数投票。当这种情况发生时，每个Candidate都将超时，并通过增加其任期并启动另一轮RequestVote RPC来开始新的选举。但是，如果不采取额外措施，分散投票可以无限期地重复。<br>Raft使用随机的选举超时来确保分散票很少发生，并且可以快速解决。为了避免投票分散，首先从固定间隔(例如150-300毫秒)中随机选择选举超时。这会分散服务器超时的时间，因此在大多数情况下，只有一台服务器会超时。它会赢得选举并在其他任何服务器超时之前发送心跳信号。使用相同的机制来处理分散投票。每位候选人在选举开始时都会重新启动其随机选举超时时间，并等待该超时时间过去后才开始下一次选举。这减少了在新选举中再次进行分散投票的可能性。第9.3节显示，这种方法可以迅速选举出一位Leader。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145629826-958115128.png\" srcset=\"undefined\" width = \"300\" height = \"180\" alt=\"图6\" align=center />\n#### 图6:日志由条目组成，这些条目按顺序编号。每个条目包含创建它的任期（每个框中的数字）和状态机的命令。如果可以安全地将条目应用于状态机，则认为该条目已提交。\n选举是如何理解指导我们在设计备选方案之间进行选择的一个示例。最初，我们计划使用排名系统：为每个Candidate分配一个唯一的排名，该排名用于在竞争Candidate之间进行选择。如果某个Candidate发现了另一名更高级别的Candidate，它将返回到Follower状态，以便更高级别的Candidate可以更轻松地赢得下一次选举。我们发现，这种方法在可用性方面产生了一些细微的问题(排名较低的服务器可能需要超时，如果排名较高的服务器出现故障，则可能再次成为Candidate，但是如果这样做过早，则可以重置选举Leader的进度)。我们对算法进行了数次调整，但每次调整后都会出现新的极端情况。 最终，我们得出结论，随机重试方法更加明显和易于理解。\n\n<h2 id=\"5-3-日志复制\"><a href=\"#5-3-日志复制\" class=\"headerlink\" title=\"5.3 日志复制\"></a>5.3 日志复制</h2><p>选举Leader后，便开始为客户的请求提供服务。每个客户端请求都包含可以由复制状态机执行的命令。Leader将命令作为新条目添加到其日志中，然后与其他每个服务器并行发出AppendEntries RPC消息，以复制该条目。在安全地复制了条目之后（如下所述），Leader将该条目应用于其状态机，并将执行结果返回给客户端。如果Follower崩溃或运行缓慢，或者丢失了网络数据包，则领导者会无限次（即使在响应客户端之后）重试附加该RPC消息，直到所有Follower最终存储所有日志条目为止。<br>日志的组织结构如图6所示。当Leader收到条目时，每个日志条目都会存储一个状态机命令以及任期号。日志条目中的任期号用于检测日志之间的不一致并确保图3中的某些属性。每个日志条目还具有一个整数索引，用于标识其在日志中的位置。<br>Leader决定什么时候可以安全地对状态机进行日志记录。这样的条目称为已提交。Raft保证提交的条目是持久的，并且最终将由所有可用状态机执行。一旦创建条目的Leader已在大多数服务器上复制了该日志条目（例如，图6中的条目7），则提交该日志条目。这还将提交Leader日志中的所有先前条目，包括先前Leader创建的条目。第5.4节讨论了Leader变更后应用此规则时的一些细微之处，并且还表明了对提交的定义是安全的。Leader保持记录被提交的日志的最高索引，并将该索引包括在将来的AppendEntries RPC（包括心跳）中，以便其他服务器发现。跟随者得知日志条目已提交后，便将该条目应用于其本地状态机（按日志顺序）。<br>我们设计了Raft日志机制来维持不同服务器上的日志之间的高度一致性。这不仅简化了系统的行为并使其更具可预测性，而且是确保安全的重要组成部分。Raft维护以下属性，它们共同构成了图3中的Log Matching属性：</p>\n<ul>\n<li>如果两个不同的日志实体具有相同的索引和任期号，那么他们存储有相同的命令。</li>\n<li>如果两个不同的日志实体具有相同的索引和任期号，则所有先前条目中的日志都相同。</li>\n</ul>\n<p>第一个属性来自以下事实：Leader在给定期限内最多创建一个具有给定日志索引的条目，并且日志条目从不更改其在日志中的位置。第二个属性由AppendEntries执行的简单一致性检查保证。在发送AppendEntries RPC时，Leader将在其日志中紧接新条目之前包含条目的索引和任期号。如果Follower在其日志中找不到具有相同索引和任期的条目，则它拒绝新条目。一致性检查是一个归纳步骤：日志的初始空状态满足Log Matching属性，并且只要扩展日志，一致性检查都会保留Log Matching属性。 因此，只要AppendEntries成功返回，Leader就会知道Follower的日志与它自己通过新条目记录的日志相同。<br>在正常操作期间，Leader和Follower的日志保持一致，因此AppendEntries一致性检查永远不会失败。但是，Leader崩溃可能会使日志不一致（旧的Leader可能没有完全复制其日志中的所有条目）。这些不一致会加剧一系列的Leader和Follower崩溃。图7说明了Follower的日志与新Follower的日志可能不同的方式。 Follower可能缺少Leader上存在的条目，它可能具有Leader上不存在的额外条目，或者两者都有。日志中的缺失条目和多余条目可能跨越多个任期。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145703880-1796421720.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图7\" align=center />\n#### 图7:当最上面的为Leader时，Follower日志中可能会出现任何情况（a–f）。每个框代表一个日志条目；框中的数字是其用语。Follower可能缺少条目（a–b），可能有多余的未提交条目（c–d），或者两者都有（e–f）。 例如，如果该服务器是第2任期的Leader，则可能会发生场景（f）。它迅速重启，成为第三学期的Leader，并在其日志中添加了更多条目； 在提交任期2或任期3中的任何条目之前，服务器再次崩溃并保持关闭状态。\n\n<p>在Raft中，Leader通过强迫Follower的日志重复自己的日志来处理不一致之处。这意味着Follower日志中的冲突条目将被Leader日志中的条目覆盖。第5.4节将说明，当再加上一个限制条件时则是安全的。<br>为了使Follower的日志与自己的日志保持一致，Leader必须找到两个日志一致的最新日志条目的位置，在该位置之后删除Follower日志中的所有条目，并将该位置之后的Leader的所有条目发送给Follower。所有这些操作都是响应AppendEntries RPC执行的一致性检查而发生的。Leader为每个关注者维护一个nextIndex，这是Leader将发送给该Follower的下一个日志条目的索引。当成功选举为Leader时，它将所有nextIndex值初始化为刚好在其日志中的最后一个索引之后的索引（图7中的11）。如果Follower的日志与Leader的日志不一致，则下一个AppendEntries RPC中的AppendEntries一致性检查将失败。在拒绝之后，Leader递减nextIndex并重试AppendEntries RPC。最终nextIndex将到达Leader和Follower日志匹配的位置。发生这种情况时，AppendEntries将成功执行，这将删除Follower日志中的所有冲突条目，并添加Leader的日志中的条目（如果有）。一旦AppendEntries成功通过，Follower的日志便与Leader的日志保持一致，并且在本任期的其余时间中都将保持这种状态。<br>如果需要，可以优化协议以减少拒绝的AppendEntries RPC的数量。例如，当发送拒绝AppendEntries请求的消息时，Follower可以将冲突条目的任期以及该任期存储的第一个索引包括在内。有了这些信息，Leader可以递减nextIndex来绕过该任期中所有冲突的条目。每个具有冲突条目的任期都需要一个AppendEntries RPC消息，而不是每个条目一个RPC。 在实践中，我们怀疑这种优化是否必要，因为故障很少发生，并且不太可能出现许多不一致的情况。<br>通过这种机制，Leader在启动时无需采取任何特殊措施即可恢复日志的一致性。它只是开始正常运行，并且响应于AppendEntries一致性检查的失败，日志自动收敛。Leader永远不会覆盖或删除其自己的日志中的条目（图3中的Leader Append-Only属性）。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145725706-1972318156.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图8\" align=center />\n#### 图8:一个时序显示Leader为何无法使用较早任期的日志条目来确定提交。 在（a）中，S1是Leader，并部分复制索引2处的日志条目。S5在S3，S4及其本身的投票下当选为任期3的Leader，并在日志索引2处接受不同的条目。S1重新启动，被选为Leader，然后继续复制。至此，任期2的日志条目已在大多数服务器上复制，但尚未提交。如果S1像（d）中那样崩溃，则S5可以被选为Leader（来自S2，S3和S4的投票），并用任期3中的条目覆盖该条目。但是，如果S1在崩溃之前在大多数服务器上其当前任期复制了一个条目，如（e）所示，该条目已提交（S5无法赢得选举）。此时，日志中的所有先前条目也将被提交。\n这种日志复制机制展现了第2节中描述的理想的共识属性：只要大多数服务器都在运行，Raft可以接受，复制和应用新的日志条目。通常情况下，可以通过一轮RPC将新条目复制到大多数集群中。一个慢速Follower不会影响性能。\n\n<h2 id=\"5-4-安全性\"><a href=\"#5-4-安全性\" class=\"headerlink\" title=\"5.4 安全性\"></a>5.4 安全性</h2><p>前面的章节描述了Raft如何选择Leader并复制日志条目。但是，到目前为止描述的机制还不足以确保每个状态机以相同的顺序执行完全相同的命令。例如，当Leader提交多个日志条目时，Follower可能不可用。然后可以当选为Leader并用新的日志条目覆盖这些条目。结果，不同的状态机可能执行不同的命令序列。<br>本节通过添加限制哪些服务器可以当选领导者来完善Raft算法。该限制可确保任何给定任期的Leader都包含先前任期中提交的所有条目（图3中的“领导者完整性”属性）。给定选举限制，我们便使承诺规则更加精确。最后，我们为Leader完整性属性提供了一个证明草图，并显示了它如何导致复制状态机的正确行为。</p>\n<h3 id=\"5-4-1-选举限制\"><a href=\"#5-4-1-选举限制\" class=\"headerlink\" title=\"5.4.1 选举限制\"></a>5.4.1 选举限制</h3><p>在任何基于Leader的共识算法中，Leader最终必须存储所有提交的日志条目。在某些共识算法中，例如“加盖时间戳的复制” ，即使最初并不包含所有提交的条目，也可以选择一个Leader。这些算法包含其他机制，无论是在选举过程中还是选举后不久，可以识别丢失的条目并将其发送给新的Leader。不幸的是，这导致了相当大的额外机制和复杂性。Raft使用一种更简单的方法来保证自新任Leader选举之日都具有所有先前提交的条目，而无需将这些条目转移给Leader。这意味着日志条目仅在一个方向上（从Leader到Follower）流动，并且Leader永远不会覆盖其日志中的现有条目。<br>Raft使用投票程序来防止Candidate赢得选举，除非其日志中包含所有已提交的条目。Candidate必须联系集群的大多数才能被选举，这意味着每个提交的条目都必须存在于其中至少一台服务器中。如果Candidate的日志至少与该多数服务器日志中的日志一样最新（以下精确定义了“最新”），则它将保存所有已提交的条目。RequestVote RPC实施了此限制：RPC包含有关Candidate日志的信息，如果投票者自己的日志比Candidate的日志最新，则投票者将拒绝投票。<br>Raft通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个是最新的。 如果日志中的最后一个条目具有不同的任期，则带有较新任期的日志是最新的。如果日志以相同的任期结尾，则以索引更大的日志为准。</p>\n<h3 id=\"5-4-2-提交之前任期的日志\"><a href=\"#5-4-2-提交之前任期的日志\" class=\"headerlink\" title=\"5.4.2 提交之前任期的日志\"></a>5.4.2 提交之前任期的日志</h3><p>如第5.3节所述，Leader知道，一旦该条目存储在大多数服务器上，就会提交当前项的输入。如果Leader在提交条目之前崩溃，将来的Leader将尝试完成复制条目。但是，Leader不能立即得出以下结论：一旦上一个任期的条目存储在大多数服务器上，就将其提交。 图-8说明了一种情况，其中旧的日志条目存储在大多数服务器上，但将来的Leader仍可以覆盖。<br>为了消除如图8所示的问题，Raft决不通过计算副本数来提交前项的日志条目。只有Leader当前任期的日志条目才通过计算副本数来提交；一旦以这种方式提交了当前任期的条目，则由于“日志匹配”属性而间接提交了所有先前的条目。在某些情况下，Leader可以安全地断定已提交了较旧的日志输入（例如，如果该条目存储在每个服务器上），但是Raft为简化起见采取了更为保守的方法。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145750026-1894945621.png\" srcset=\"undefined\" width = \"300\" height = \"140\" alt=\"图9\" align=center />\n#### 图9:如果S1（任期T的Leader）从其任期中提交了新的日志条目，并且S5当选为下一任期U的Leader，那么必须至少有一个服务器（S3）接受了该日志条目并投票支持S5。\nRaft会在承诺规则中带来额外的复杂性，因为当Leader从先前条款中复制条目时，日志条目将保留其原始任期号。在其他共识算法中，如果新的Leader从先前的“任期号”中复制条目，则必须使用其新的“任期号”来进行复制。Raft的方法使推理日志条目变得更加容易，因为它们在整个周期和跨日志期间保持相同的任期编号。此外，与其他算法相比，Raft中的新Leader发送的先前条目的日志条目要少（其他算法必须发送冗余日志条目以对其重新编号，然后才能提交）。\n\n<h3 id=\"5-4-3-安全性讨论\"><a href=\"#5-4-3-安全性讨论\" class=\"headerlink\" title=\"5.4.3 安全性讨论\"></a>5.4.3 安全性讨论</h3><p>给定完整的Raft算法，我们现在可以更精确地论证“领导者完整性属性”成立（此论点基于安全性证明；请参见9.2节）。 我们假设Leader完整性属性不成立，那么我们证明了一个矛盾。 假设任期<em>T</em>的Leader提交了其任期的日志条目，但该日志条目未由某个将来任期的Leader存储。考虑最小项任期<em>U&gt;T<em>，其领导者</em>leaderU</em> 不存储该条目。</p>\n<ol>\n<li>提交时，必须在<em>LeaderU</em>的日志中没有已提交的条目（Leader绝不会删除或覆盖条目）。</li>\n<li><em>LeaderT</em>在大多数集群中复制了该条目，<em>leaderU</em>从大多数集群中获得了投票。因此，如图9所示，至少有一个服务器（“投票者”）既接受了<em>LeaderT</em>的条目又投票给<em>LeaderU</em>，投票者是达成矛盾的关键。</li>\n<li>投票者必须在接受<em>LeaderU</em>投票之前已经接受了<em>LeaderT</em>提交的条目；否则，它将拒绝来自<em>LeaderT</em>的AppendEntries请求（其当前期限将高于<em>T</em>）。</li>\n<li>当每个投票者对<em>leaderU</em>投票时，投票者仍然存储该条目，因为每个居间的Leader都包含该条目（通过假设），Leader从不删除条目，而Follower仅在与Leader冲突时才删除条目。</li>\n<li>投票者将投票结果授予了<em>LeaderU</em>，因此<em>LeaderU</em>的日志必须与投票者的日志一样最新。这导致两个矛盾之一。</li>\n<li>首先，如果投票者和<em>LeaderU</em>共享相同的最后一个日志条款，那么<em>LeaderU</em>的日志必须至少与投票者的日志一样长，因此其日志包含投票者日志中的每个条目。这是一个矛盾，因为投票者中包含已承诺的条目，而<em>leaderU</em>被假定为不包含。</li>\n<li>否则，<em>leaderU</em>的最后一条日志的任期号必须大于投票者的最后一条日志的任期号。而且，它比<em>T</em>大，因为投票者的上一个日志任期号至少为<em>T</em>（它包含任期<em>T</em>中的所有已提交的条目）。创建<em>leaderU</em>的最后一个日志条目的较早的Leader必须在其日志中（通过假设）包含已提交的条目。然后，通过日志匹配属性，<em>leaderU</em>的日志还必须包含已提交的条目，这是矛盾的。</li>\n<li>这样就完成了矛盾。因此，所有任期大于<em>T</em>的Leader都必须包含在任期<em>T</em>中提交的所有任期为<em>T</em>的日志。</li>\n<li>日志匹配属性保证未来的Leader还将包含间接提交的条目，例如图8（d）中的索引2。</li>\n</ol>\n<p>给定“Leader完整性”属性，我们可以从图3证明“状态机安全性”属性，该状态表明，如果服务器已将给定索引的日志条目应用于其状态机，则其他任何服务器都不会为该状态机在相同的索引应用不同的日志条目。服务器在将日志条目应用于其状态机时，其日志必须与通过该条目的Leader的日志相同，并且必须提交该条目。现在考虑任何服务器应用给定日志索引的最低任期；日志完整性属性可确保Leader将存储所有较高任期的相同的日志条目，因此，服务器将在最新的任期中应用的索引将应用相同的值。 因此，状态机安全属性成立。<br>最后，Raft要求服务器以日志索引顺序应用条目。结合状态机安全属性，这意味着所有服务器将以相同的顺序将完全相同的日志条目集应用于其状态机。</p>\n<h2 id=\"5-5-Follower与Candidate崩溃\"><a href=\"#5-5-Follower与Candidate崩溃\" class=\"headerlink\" title=\"5.5 Follower与Candidate崩溃\"></a>5.5 Follower与Candidate崩溃</h2><p>到目前为止，我们只关注Leader的失败。Follower与Candidate崩溃比Leader崩溃要容易得多，并且两者的处理方式相同。 如果Follower与Candidate崩溃，则将来发送给它的RequestVote和AppendEntries RPC将失败。 Raft通过无限期重试来处理这些故障；如果崩溃的服务器重新启动，则RPC将成功完成。如果服务器在完成RPC之后但在响应之前崩溃，则在重新启动后它将再次收到相同的RPC。Raft中的RPC是幂等的，因此不会造成伤害。例如，如果Follower收到一个AppendEntries请求，其中包括其日志中已经存在的日志条目，则它会忽略新请求中的那些相同的条目。</p>\n<h2 id=\"5-6-时间和可用性\"><a href=\"#5-6-时间和可用性\" class=\"headerlink\" title=\"5.6 时间和可用性\"></a>5.6 时间和可用性</h2><p>我们对Raft的要求之一是安全不得取决于时间安排：系统不得仅由于某些事件比预期的快或慢发生而产生不正确的结果。 但是，可用性（系统及时响应客户端的能力）必定取决于时间。 例如，如果消息交换花费的时间比服务器崩溃之间的典型时间长，则Candidate将不会停留足够长的时间来赢得选举。 没有稳定的Leader，Raft无法取得进步。<br>领导人选举是Raft最重要的方面，时机至关重要。只要系统满足以下时间要求，Raft将能够选举和维持稳定的Leader：<br><strong>broadcastTime ≪ electionTimeout ≪ MTBF</strong><br>在这种不平等的情况下，<em>broadcastTime</em>是服务器将RPC并行发送到集群中的每个服务器并接收其响应所花费的平均时间。 <em>electionTimeout</em>是第5.2节所述的选举超时；<em>MTBF</em>是单个服务器两次故障之间的平均时间。广播时间应比选举超时小一个数量级，以便Leader能够可靠地发送所需的心跳消息，以防止Follower开始选举。考虑到用于选举超时的随机方法，这种不平等也使得分散投票变得不太可能。选举超时应该比<em>MTBF</em>小几个数量级，以便系统稳步前进。当Leader崩溃时，该系统将在大约选举超时时间内不可用；我们希望这只代表总时间的一小部分。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145827202-614379881.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图10\" align=center />\n#### 图10:直接从一种配置切换到另一种配置是不安全的，因为不同的服务器将在不同的时间进行切换。在此示例中，群集从三台服务器增长到五台。不幸的是，在某个时间点上，可以为同一任期选举两名不同的Leader，其中一位拥有多数旧配置（C_old），另一位拥有多数新配置（C_new）。\n\n<p>广播时间和<em>MTBF</em>是基础系统的属性，而选举超时是我们必须选择的东西。Raft的RPC通常需要接收者将信息持久存储到稳定的存储中，因此根据存储技术的不同，转换时间可能从0.5毫秒到20毫秒不等。选举超时可能在10毫秒至500毫秒之间。 典型服务器的<em>MTBF</em>长达数月或更长时间，可以轻松满足计时要求。</p>\n<h1 id=\"6-集群成员关系变化\"><a href=\"#6-集群成员关系变化\" class=\"headerlink\" title=\"6 集群成员关系变化\"></a>6 集群成员关系变化</h1><p>到现在为止，我们还假设集群配置（参与共识算法的服务器集合）是固定的。实际上，有时需要更改配置，例如在服务器出现故障时更换服务器或更改复制程度。尽管可以通过使整个群集脱机，更新配置文件，然后重新启动群集来完成此操作，但这将使群集在转换期间不可用。此外，如果有任何手动步骤，则可能会导致操作员出错。为了避免这些问题，我们决定自动进行配置更改，并将其合并到Raft共识算法中。<br>为了确保配置更改机制的安全，在过渡期间必须没有任何可能在同一任期内选举两名领导者的意义。不幸的是，任何将服务器直接从旧配置切换到新配置的方法都是不安全的。不可能一次自动切换所有服务器，因此群集在过渡期间可能会分成两个独立的多数（见图10）。<br>为了确保安全，更改配置必须使用两阶段方法。有两种方法可以实现两个阶段。例如，某些系统使用第一阶段来禁用旧配置，因此它无法处理客户端请求；然后第二阶段启用新配置。在Raft中，集群首先切换到过渡配置，我们称为联合共识；提交联合共识后，系统将过渡到新配置。联合共识将新旧配置结合在一起：</p>\n<ul>\n<li>两种配置中的日志条目均复制到所有服务器。</li>\n<li>来自任一配置的任何服务器都可以充当领导者。</li>\n<li>协议（用于选举和入职承诺）需要分别来自新旧配置的大多数人。</li>\n</ul>\n<p>联合共识允许单个服务器在不同时间在配置之间转换，而不会影响安全性。此外，联合共识允许群集在整个配置更改期间继续为客户请求提供服务。<br>群集配置使用复制日志中的特殊条目进行存储和通信。图11说明了配置更改过程。当Leader收到将配置从<em>C_old</em>更改为<em>C_new</em>的请求时，它将存储用于联合共识的配置（<em>C_old</em>，图中的<em>new</em>）作为日志条目，并使用前述机制复制该条目。给定服务器将新配置条目添加到其日志后，它将使用该配置进行所有将来的决策（服务器始终使用其日志中的最新配置，而不管该条目是否被提交）。这意味着Leader将使用<em>C_old_new</em>规则来确定何时提交<em>C_old_new</em>的日志条目。如果Leader崩溃，则可以根据获胜的Candidate是否收到了<em>C_old_new</em>来在<em>C_old</em>或<em>C_old_new</em>下选择新的Leader。无论如何，<em>C_new</em>不能在此期间做出单方面决定。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145845730-1927441378.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图11\" align=center />\n#### 图11:配置更改的时间表。虚线表示已创建但尚未提交的配置条目，实线表示最新的提交的配置条目。Leader首先在其日志中创建*C_old_new*配置条目，并将其提交给*C_old_new*（大多数*C_old*和*C_new*）。 然后，它创建*C_new*条目并将其提交给大多数*C_new*。*C_old*和*C_new*都无法独立做出决策。\n\n<p>一旦提交了<em>C_old_new，C_old</em>和<em>C_new</em>都无法在未经另一方批准的情况下做出决策，并且Leader 完备性确保只有具有<em>C_old_new</em>日志条目的服务器才能被选为Leader。现在，Leader可以安全地创建描述<em>C_new</em>的日志条目并将其复制到集群。同样，该配置将在看到每台服务器后立即生效。在<em>C_new</em>的规则下提交新配置后，旧配置将不相关，并且可以关闭不在新配置中的服务器。如图11所示，<em>C_old</em>和<em>C_new</em>都没有时间可以单方面做出决定。这样可以保证安全。<br>还需要解决三个问题以进行重新配置。第一个问题是新服务器最初可能不会存储任何日志条目。如果以这种状态将它们添加到群集，则它们可能要花很长时间才能赶上，在此期间可能无法提交新的日志条目.为了避免可用性差距，Raft在配置更改之前引入了一个附加阶段，在该阶段中，新服务器以无表决权的成员的身份加入群集（领导者将日志条目复制到它们，但多数情况下不考虑它们）。一旦新服务器赶上了群集的其余部分，重新配置就可以如上所述进行。<br>第二个问题是集群Leader可能不属于新配置。在这种情况下，Leader一旦提交了<em>C_new</em>日志条目，便会下台（返回到Follower状态）。这意味着在一段时间（<em>C_new</em>提交）时，Leader将管理一个不包含自身的集群。它复制日志条目，但不占多数。提交<em>C_new</em>时会发生Leader转换，因为这是新配置可以独立运行的第一步（始终可以从<em>C_new</em>中选择一个领导者）。 在此之前，可能只有<em>C_old</em>中的服务器可以被选为Leader。<br>第三个问题是，删除的服务器（那些不在<em>C_new</em>中的服务器）会破坏群集。这些服务器不会接收心跳，因此它们将超时并开始新的选择。然后，他们将使用新的任期编号发送RequestVote RPC，这将导致当前Leader恢复为Follower状态。最终将选举新的Leader，但是被删除的服务器将再次超时，并且该过程将重复进行，从而导致可用性降低。<br>为防止此问题，服务器在认为当前的Leader存在时会忽略RequestVote RPC。具体来说，如果服务器在当前Leader的最小选举超时时间内收到RequestVote RPC，则该服务器不会更新其任期或授予其投票权。这不会影响正常的选举，在正常的选举中，每个服务器在开始选举之前至少等待最小选举超时。但是，它有助于避免因移动服务器而造成的中断：如果Leader能够对其集群发出心跳信号，那么它将不会被更高任期的成员所取代。</p>\n<h1 id=\"7-日志压缩\"><a href=\"#7-日志压缩\" class=\"headerlink\" title=\"7 日志压缩\"></a>7 日志压缩</h1><p>在正常运行期间，Raft的日志会增长，以合并更多的客户请求，但在实际系统中，它无法无限制地增长。随着日志的增长，它会占用更多空间，并需要更多时间来重放。 如果没有某种机制来丢弃日志中累积的过时信息，这最终将导致可用性问题。<br>快照是最简单的压缩方法。 在快照中，将整个当前系统状态写入稳定存储上的快照，然后丢弃该点之前的整个日志。 快照在Chubby和ZooKeeper中使用，本节的其余部分介绍在Raft中进行快照。</p>\n <img src=\"https://img2018.cnblogs.com/blog/1652302/202001/1652302-20200104145907980-1608425176.png\" srcset=\"undefined\" width = \"300\" height = \"150\" alt=\"图12\" align=center />\n#### 图12:服务器用新快照替换其日志（索引1至5）中已提交的条目，该快照仅存储当前状态（在此示例中为变量x和y）。快照的最后一个包含索引和任期的作用是将快照放置在条目6之前的日志中。\n诸如日志清理和日志结构的合并树之类的递增压缩方法也是可能的。它们一次处理一部分数据，因此它们会随着时间的推移更均匀地分散压缩负载。他们首先选择一个累积了许多已删除和覆盖的对象的数据区域，然后再更紧凑地重写该区域中的活动对象并释放该区域。与快照相比，这需要大量的附加机制和复杂性，从而通过始终对整个数据集进行操作来简化问题。 尽管清除日志需要修改Raft，但是状态机可以使用与快照相同的接口来实现LSM树。\n图12显示了Raft中快照的基本概念。每个服务器独立地拍摄快照，仅覆盖其日志中的已提交条目。状态机的大部分工作都由状态机组成，将其当前状态写入快照。Raft在快照中还包含少量元数据：最后包含的索引是快照替换的日志中最后一个条目的索引（状态机已应用的最后一个条目），最后包含的任期是该日志的任期。保留这些内容是为了支持快照之后的第一个日志条目的AppendEntries一致性检查，因为该条目需要先前的日志索引和任期。为了启用集群成员资格更改（第6节），快照还包括自上次包含索引起的日志中的最新配置。服务器完成快照的写入后，它可能会删除最后一个包含的索引中的所有日志条目以及所有先前的快照。\n\n\n<table>\n<thead>\n<tr>\n<th><strong>安装快照RPC</strong></th>\n<th><strong>由Leader调用并按顺序发送打包的快照到Follower。</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>参数：</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>Leader的任期</td>\n</tr>\n<tr>\n<td>leaderId</td>\n<td>用以Follower可以重定向客户端的请求到Leader</td>\n</tr>\n<tr>\n<td>lastIncludedIndex</td>\n<td>快照将替换直到该索引的所有日志条目</td>\n</tr>\n<tr>\n<td>lastIncludedTerm</td>\n<td>lastIncludedIndex的任期</td>\n</tr>\n<tr>\n<td>offset</td>\n<td>快照文件的偏移量</td>\n</tr>\n<tr>\n<td>data[]</td>\n<td>从offset开始，快照内的数据数组</td>\n</tr>\n<tr>\n<td>done</td>\n<td>如果这是最后一个快照则为true</td>\n</tr>\n<tr>\n<td><strong>结果:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>term</td>\n<td>当前任期，用以Leader更新自己</td>\n</tr>\n<tr>\n<td><strong>接收者实现:</strong></td>\n<td></td>\n</tr>\n<tr>\n<td>1.如果任期小于当前任期则立即回复</td>\n<td></td>\n</tr>\n<tr>\n<td>2.如果第一次打包创建新的快照文件(offset为0)</td>\n<td></td>\n</tr>\n<tr>\n<td>3.从给予的offset写数据到快照中</td>\n<td></td>\n</tr>\n<tr>\n<td>4.如果失败的话回复并等待更多打包的数据</td>\n<td></td>\n</tr>\n<tr>\n<td>5.存储快照文件，抛弃所有存在的或并行的相同的索引快照文件。</td>\n<td></td>\n</tr>\n<tr>\n<td>6.如果在快照文件中包含的实体最后存在日志实体具有相同的索引与任期，保持日志实体并回复</td>\n<td></td>\n</tr>\n<tr>\n<td>7.抛弃整个日志</td>\n<td></td>\n</tr>\n<tr>\n<td>8.重置状态机并使用快照内容(加载快照集群配置)</td>\n<td></td>\n</tr>\n</tbody></table>\n<p>尽管服务器通常通常独立拍摄快照，但Leader有时必须将快照发送给落后的Follower。当Leader已经丢弃了需要发送给Follower的下一个日志条目时，就会发生这种情况。幸运的是，在正常操作中这种情况不太可能发生：与Leader保持同步的Follower将已经有此条目。但是，异常慢的Follower或加入集群的新服务器（第6节）则不会。 使此类Follower保持最新状态的方法是，Leader可以通过网络向其发送快照。<br>Leader使用一个称为InstallSnapshot的新RPC将快照发送给落后的Follower。请参见图13。当Follower收到带有此RPC的快照时，它必须决定如何处理其现有的日志记录。通常，快照将包含收件人日志中尚未包含的新信息。在这种情况下，Follower将丢弃其整个日志；它全部被快照取代，并且可能具有与快照冲突的未提交条目。相反，如果Follower收到描述其日志前缀的快照（由于重新传输或错误操作），则快照所覆盖的日志条目将被删除，但快照之后的条目仍然有效并且必须保留。<br>这种快照方法背离了Raft强大的Leader原则，因为Follower可以在不了解Leader的情况下进行快照。但是，我们认为这种偏离是合理的。尽管拥有Leader可以避免在达成共识时发生决策冲突，但快照时已经达成共识，因此没有决策冲突。数据仍然仅从Leader流向Follower，只是Follower现在可以重组其数据。<br>我们考虑了另一种基于Leader的方法，其中只有Leader将创建快照，然后将快照发送给其每个Follower。但是，这有两个缺点。首先，将快照发送给每个Follower会浪费网络带宽并减慢快照过程。每个Follower已经具有生成自己的快照所需的信息，通常，服务器从其本地状态生成快照要比通过网络发送和接收快照便宜得多。其次，Leader的实施将更加复杂。例如，Leader将需要同时向跟随者发送快照，同时向其复制新的日志条目，以免阻塞新的客户要求。<br>还有另外两个问题会影响每个快照形式。首先，服务器必须决定何时进行快照。如果服务器过于频繁地进行快照，则会浪费磁盘带宽和能量。如果快照太不频繁，则有耗尽其存储容量的风险，并且会增加重新启动期间重放日志所需的时间。一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果此大小设置为明显大于快照的预期大小，则用于快照的磁盘带宽开销将很小。<br>第二个性能问题是写快照可能要花费大量时间，我们不希望这样做会延迟正常操作。解决方案是使用写时复制技术，以便可以接受新的更新而不会影响快照的写入。例如，使用功能性数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如，Linux上的fork）来创建整个状态机的内存快照（我们的实现使用这种方法）。</p>\n<h1 id=\"8-客户端内部交互\"><a href=\"#8-客户端内部交互\" class=\"headerlink\" title=\"8 客户端内部交互\"></a>8 客户端内部交互</h1><p>本节描述客户端如何与Raft交互，包括客户端如何找到集群领导者以及Raft如何支持线性化语义。这些问题适用于所有基于共识的系统，并且Raft的解决方案与其他系统类似。<br>Raft的客户将所有请求发送给Leader。客户端首次启动时，它会连接到随机选择的服务器。如果客户的首选不是Leader，则该服务器将拒绝客户的请求，并提供有关其最近听到的Leader的信息（AppendEntries请求包括Leader的网络地址）。如果Leader崩溃，客户请求将超时；客户端，然后使用随机选择的服务器重试。<br>我们对Raft的目标是实现线性化的语义（每个操作似乎在调用和响应之间的某个时刻立即执行一次，恰好一次）。但是，到目前为止，Raft可以多次执行命令：例如，如果Leader在提交日志条目之后但在响应客户端之前崩溃，则客户端将使用新的Leader重试该命令，从而导致该失败再次执行。解决方案是让客户端为每个命令分配唯一的序列号。然后，状态机将跟踪为每个客户端处理的最新序列号以及相关的响应。如果收到序列号已被执行的命令，它将立即响应而无需重新执行该请求。<br>可以执行只读操作，而无需在日志中写入任何内容。但是，如果没有其他措施，这将存在返回陈旧数据的风险，因为响应请求的Leader可能已经受到了一个不知道的新Leader的支持。可读的读取一定不能返回陈旧的数据，并且Raft需要采取两项额外的预防措施来保证不使用日志就可以做到这一点。首先，Leader必须掌握提交条目的最新信息。Leader完整性属性可以保证Leader具有所有承担的职责，但是在任期开始之初，它可能不知道是谁。为了找出答案，它需要从其任期中提交一个条目。Raft通过让每个Leader在任期开始时在日志中输入一个空白的禁止操作条目来解决此问题。其次，Leader必须在处理只读请求之前检查其是否已处置（如果选择了新的Leader，其信息可能会过时）。Raft通过让Leader在响应只读请求之前与大多数集群交换心跳消息来解决此问题。或者，Leader可以依靠心跳机制来提供某种形式的租约，但这将依赖于时序以确保安全性（它假设时钟范围有界）。</p>\n"},{"title":"CouchDB学习-维护","date":"2019-12-22T04:26:41.000Z","_content":"[官方文档](https://docs.couchdb.org/en/stable/maintenance/index.html)\n# 1 压缩\n* * *\n压缩操作是通过从数据库或者视图索引文件中移除无用的和老的数据减少硬盘使用空间.操作非常简单类似于其他数据库(SQLite等)管理系统。\n在压缩目标期间，CouchDB将创建扩展名为.compact的新文件，并将仅实际数据传输到该文件中。 因此，CouchDB首先检查可用磁盘空间-它应比压缩文件的数据大两倍。\n当所有实际数据都成功传输到压缩文件后，CouchDB用目标文件替换目标文件。\n## 1.1 数据库压缩\n数据库压缩通过删除更新期间创建的未使用的文件部分来压缩数据库文件。 旧文档修订版被少量称为`tombstone`的元数据代替，该元数据用于复制期间的冲突解决。 可以使用`_revs_limit`URL配置存储的修订版本（以及`tombstone`）的数量。\n压缩是每个数据库手动触发的操作，并作为后台任务运行。要针对特定的数据库启动它，需要发送目标数据库的HTTP POST`/{db}/_compact`子资源：\n```\ncurl -H \"Content-Type: application/json\" -X POST http://localhost:5984/my_db/_compact\n```\n成功的话，将立即返回HTTP 状态码`202 Accepted`。\n```\nHTTP/1.1 202 Accepted\nCache-Control: must-revalidate Content-Length: 12\nContent-Type: text/plain; charset=utf-8 Date: Wed, 19 Jun 2013 09:43:52 GMT Server: CouchDB (Erlang/OTP)\n```\n```\n{\"ok\":true}\n```\n尽管未使用请求主体，但仍必须为请求指定带有`application/json`值的`Content-Type`标头。否则，您会知道HTTP状态`415不支持的媒体类型响应`：\n```\nHTTP/1.1 415 Unsupported Media Type\nCache-Control: must-revalidate \nContent-Length: 78\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 09:43:44 GMT \nServer: CouchDB (Erlang/OTP)\n{\"error\":\"bad_content_type\",\"reason\":\"Content-Type must be application/json\"}\n```\n当压缩成功启动并运行时，可以通过数据库信息资源获取有关压缩的信息：\n```\ncurl http://localhost:5984/my_db\n```\n```\nHTTP/1.1 200 OK\nCache-Control: must-revalidate \nContent-Length: 246\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 16:51:20 GMT \nServer: CouchDB (Erlang/OTP)\n{\n    \"committed_update_seq\": 76215, \n    \"compact_running\": true, \n    \"data_size\": 3787996, \n    \"db_name\": \"my_db\", \n    \"disk_format_version\": 6, \n    \"disk_size\": 17703025, \n    \"doc_count\": 5091, \n    \"doc_del_count\": 0, \n    \"instance_start_time\": \"0\", \n    \"purge_seq\": 0,\n    \"update_seq\": 76215\n}\n```\n请注意，`compaction_running`字段为`true`，指示压缩实际上正在运行。 要跟踪压缩进度，可以查询`_active_tasks`资源：\n```\ncurl http://localhost:5984/_active_tasks\n```\n```\nHTTP/1.1 200 OK\nCache-Control: must-revalidate\nContent-Length: 175\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 16:27:23 GMT \nServer: CouchDB (Erlang/OTP)\n[\n    {\n        \"changes_done\": 44461, \n        \"database\": \"my_db\",\n        \"pid\": \"<0.218.0>\", \n        \"progress\": 58,\n        \"started_on\": 1371659228, \n        \"total_changes\": 76215, \n        \"type\": \"database_compaction\", \n        \"updated_on\": 1371659241\n    }\n]\n```\n## 1.2 视图压缩\n与数据库视图不同，视图也需要像数据库一样进行压缩，这与按每个设计文档按组对数据库视图进行压缩不同。要启动其压缩，需要发送HTTP POST`/{db}/_compact/{ddoc}`请求：\n```\ncurl -H \"Content-Type: application/json\" -X POST http://localhost:5984/dbname/_compact/designname\n```\n```\n{\"ok\":true}\n```\n这将从指定设计文档的当前版本压缩视图索引。 HTTP响应代码`202 Accepted`(类似于数据库的压缩)，并且将创建压缩后台任务。\n### 1.2.1视图清理\n磁盘上的视图索引以视图定义的MD5哈希命名。更改视图时，旧索引仍保留在磁盘上。要清除所有过时的视图索引（以视图的MD5表示形式命名的文件，该文件不再存在），可以触发视图清除：\n```\ncurl -H \"Content-Type: application/json\" -X POST http://localhost:5984/dbname/_view_cleanup\n```\n```\n{\"ok\":true}\n```\n## 1.3 自动压缩\n虽然需要手动触发数据库和视图的压缩，但也可以配置自动压缩，以便基于各种条件自动触发数据库和视图的压缩。 在CouchDB的配置文件中配置了自动压缩。\n守护程序`/compaction_daemon`负责触发压缩。 默认情况下启用它并自动启动。在压缩部分中配置了触发压缩的条件。\n# 2 性能\n* * *\n无论您如何编写代码，即使有了成千上万的文档，通常都会发现CouchDB可以很好地执行。但是一旦开始阅读数百万个文档，您需要更加小心。\n## 2.1 硬盘IO\n### 2.1.1 文件大小\n文件大小越小，I/O操作将越少，CouchDB和操作系统可以缓存的文件越多，复制，备份等的速度就越快。因此，您应该仔细检查所要存储的数据储存。例如，使用长度为数百个字符的键会很愚蠢，但是如果仅使用单个字符键，则程序将很难维护。通过将其放在视图中来仔细考虑重复的数据。\n### 2.1.2 硬盘和文件系统性能\n使用更快的磁盘，条带化RAID阵列和现代文件系统都可以加快CouchDB部署。但是，当磁盘性能成为瓶颈时，有一种方法可以提高CouchDB服务器的响应速度。 从文件模块的Erlang文档中：\n在具有线程支持的操作系统上，可以让文件操作在其自己的线程中执行，从而允许其他Erlang进程继续与文件操作并行执行。 请参阅erl(1)中的命令行标志+A。\n将此参数设置为大于零的数字可以使您的CouchDB安装保持响应状态，即使在磁盘使用率很高的时期也是如此。设置此选项的最简单方法是通过`ERL_FLAGS`环境变量。例如，要为Erlang提供执行I/O操作的四个线程，请将以下内容添加到`(prefix)/etc/defaults/couchdb`（或等效项）中：\n```\nexport ERL_FLAGS=\"+A 4\"\n```\n## 2.2 系统资源限制\n管理员在其部署变大时会遇到的问题之一是系统和应用程序配置施加的资源限制。 提高这些限制可以使您的部署超出默认配置所支持的范围。\n### 2.2.1 CouchDB配置选项\n#### `delayed_commits`\n延迟的提交允许在某些工作负载下实现更好的写入性能，同时牺牲少量的持久性。 该设置使CouchDB在更新后提交新数据之前要等待一整秒。如果服务器在写入标头之前崩溃，则自上次提交以来的所有写入都将丢失。 启用此选项需要您自担风险。\n#### `max_dbs_open`\n在配置(`local.ini`或类似版本)中,或者地址`couchdb/max_dbs_open`：\n```\n[couchdb]\nmax_dbs_open = 100\n```\n此选项将一次可以打开的数据库数量设置为上限。CouchDB引用对内部数据库访问进行计数，并在必要时关闭空闲数据库。有时有必要一次保持超过默认值的速度，例如在许多数据库将被连续复制的部署中。\n#### `Erlang`\n即使增加了CouchDB允许的最大连接数，默认情况下，Erlang运行时系统也将不允许超过1024个连接。 将以下指令添加到`(prefix)/etc/default/couchdb`(或等效文件)将增加此限制(在这种情况下，增加到4096)：\n```\nexport ERL_MAX_PORTS=4096\n```\n高达1.1.x的CouchDB版本还会为每个复制创建`Erlang Term Storage`(ETS)表。如果您使用的CouchDB版本早于1.2，并且必须支持许多复制，则还应设置`ERL_MAX_ETS_TABLES`变量。 默认值是大约1400表。\n请注意，在Mac OS X上，Erlang实际上不会将文件描述符限制增加到超过1024（即系统标头定义的FD_SETSIZE值）。 \n#### 打开文件描述的最大数量(无限制)\n大多数`*nix`操作系统在每个进程上都有各种资源限制。增加这些限制的方法因初始化系统和特定的OS版本而异。许多操作系统的默认值为1024或4096。在具有许多数据库或视图的系统上，CouchDB可以非常迅速地达到此限制。\n如果您的系统设置为使用可插拔身份验证模块(`PAM`)系统（几乎所有现代Linux都是这种情况），则增加此限制很简单。例如，创建具有以下内容的名为`/etc/security/limits.d/100-couchdb.conf`的文件将确保CouchDB可以一次打开多达10000个文件描述符：\n```\n#<domain>  <type>    <item>  <value>\ncouchdb    hard      nofile  10000 \ncouchdb    soft      nofile  10000\n```\n如果使用的是Debian/Ubuntu sysvinit脚本(`/etc/init.d/couchdb`，则还需要提高root用户的限制：\n```\n#<domain>    <type>   <item>  <value>\nroot         hard    nofile   10000\nroot         soft    nofile   10000\n```\n您可能还需要编辑`/etc/pam.d/common-session`和`/etc/pam.d/common-session-noninteractive`文件以添加以下行：\n```\nsession required pam_limits.so\n```\n如果还不存在。\n对于基于系统的Linux（例如CentOS/RHEL 7，Ubuntu 16.04 +，Debian 8或更高版本）,假设您要从systemd启动CouchDB，则还必须通过创建文件`/etc/systemd/system/<servicename>.d/override.conf`添加以下内容：\n```\n[Service]\nLimitNOFILE=#######\n```\n并将`#######`替换为文件描述符的上限CouchDB允许立即保持打开状态。\n如果您的系统不使用`PAM`，通常可以在自定义脚本中使用`ulimit`命令来启动\nCouchDB具有增加的资源限制。 典型的语法类似于`ulimit -n 10000`。\n通常，类似UNIX的现代系统每个进程可以处理大量文件句柄（例如100000）\n没有问题。 不要害怕增加系统限制。\n## 2.3 网络\n产生和接收每个请求/响应都有延迟开销。通常，您应该分批执行请求。大多数API具有某种批处理机制，通常是通过在请求正文中提供文档或键的列表来进行的。请注意为批次选择的大小。较大的批处理需要更多的时间来使客户将项目编码为  `JSON`，并将更多的时间用于解码该数量的响应。使用您自己的配置和典型数据进行一些基准测试，以找到最佳位置。 它可能在一到一万个文档之间。\n如果您拥有快速的I/O系统，那么您也可以使用并发-同时具有多个请求/响应。这减轻了组装JSON，进行网络连接和解码`JSON`所涉及的延迟。\n从CouchDB 1.1.0开始，与旧版本相比，用户经常报告文档的写入性能较低。主要原因是此版本随附HTTP服务器库`MochiWeb`的最新版本，该库默认情况下将`TCP`套接字选项`SO_NODELAY`设置为`false`。这意味着发送到TCP套接字的小数据（例如对文档写请求的答复（或读取非常小的文档）的答复）不会立即发送到网络`TCP`将对其缓冲一会儿，希望它会被询问通过同一套接字发送更多数据，然后一次发送所有数据以提高性能。 可以通过`httpd/socket_options`禁用此`TCP`缓冲行为：\n```\n[httpd]\nsocket_options = [{nodelay, true}]\n```\n### 2.3.1 连接限制\n`MochiWeb`处理`CouchDB`请求。默认最大连接数为2048。要更改此限制，请使用`server_options`配置变量。 `max`表示最大连接数。\n```\n[chttpd]\nserver_options = [{backlog, 128}, {acceptor_pool_size, 16}, {max, 4096}]\n```\n## 2.4 CouchDB\n### 2.4.1 删除操作\n当您删除文档时，数据库将创建一个新的修订版，其中包含`_id`和`_rev`字段以及`_deleted`标志。即使在数据库压缩后，此修订版仍将保留，以便可以复制删除内容。像未删除的文档一样，已删除的文档可能会影响视图生成时间，`PUT`和`DELETE`请求时间以及数据库的大小，因为它们会增加`B+Tree`的大小。您可以在数据库信息中看到已删除文档的数量。如果您的用例创建了许多已删除的文档（例如，如果您存储日志条目，消息队列等短期数据），则可能需要定期切换到新数据库并删除已过期的旧数据库）。\n### 2.4.2 文档ID\n数据库文件的大小源自您的文档和视图大小，但也取决于您的`_id`大小的倍数。`_id`不仅存在于文档中，而且它及其部分内容在`CouchDB`用来导航文件以首先找到文档的二叉树结构中也是重复的。作为一个现实世界的例子，一个用户从16个字节的ID切换到4个字节的ID，使数据库从21GB变为4GB，包含1000万个文档（从2.5GB到2GB的原始JSON文本）。\n插入具有顺序（至少已排序）的ID的速度要比随机ID快。 因此，您应该考虑自己生成id，按顺序分配它们，并使用消耗更少字节的编码方案。例如，可以用4个基数62个数字（用10个数字，26个小写字母，26个大写字母）来完成需要16个十六进制数字表示的内容。\n## 2.5 视图\n### 2.5.1 视图生成\n当要处理的文档数量非常少时，使用JavaScript查询服务器生成的视图非常慢。生成过程甚至不会使单个CPU饱和，更不用说您的I/O了。原因是CouchDB服务器和单独的`couchjs`查询服务器中涉及的延迟，这显着表明了从实施中消除延迟的重要性。\n您可以让视图访问权限“过时”，但要确定何时发生会给您带来快速响应以及何时更新视图会花费很长时间，这是不切实际的。(一个拥有1000万个文档的数据库大约需要10分钟才能加载到CouchDB中，而生成视图需要大约4个小时)。\n在集群中，“陈旧的”请求由一组固定的分片服务，以便为用户提供请求之间的一致结果。这需要进行可用性权衡-固定的分片集可能不是集群中响应最快的/可用的。如果不需要这种一致性(例如，索引相对静态)，则可以通过指定`stable = false＆update = false`代替`stale = ok`或`stable = false＆update = lazy`代替`stale = update_after`。\n视图信息不会被复制-它会在每个数据库上重建，因此您无法在单独的服务器上生成视图。\n### 2.5.2 内置缩小功能\n如果您使用的是非常简单的视图函数，仅执行求和或计数减少，则可以通过简单地编写`_sum`或`_count`代替函数声明来调用它们的本机`Erlang`实现。 这将大大加快速度，因为它减少了CouchDB和JavaScript查询服务器之间的IO。 例如，如邮件列表中所述，用于输出（已索引和缓存的）视图的时间大约为78,000个项目，时间从60秒减少到4秒。\n之前：\n```\n{\n    \"_id\": \"_design/foo\",\n    \"views\": {\n        \"bar\": {\n            \"map\": \"function (doc) { emit(doc.author, 1); }\",\n            \"reduce\": \"function (keys, values, rereduce) { return sum(values); }\"\n        }   \n    }\n}\n```\n之后：\n```\n{\n    \"_id\": \"_design/foo\",\n    \"views\": {\n        \"bar\": {\n            \"map\": \"function (doc) { emit(doc.author, 1); }\",\n            \"reduce\": \"_sum\"\n        }\n    }\n}\n```\n# 3 CouchDB备份\n* * *\nCouchDB在运行时可以创建三种不同类型的文件：\n\n* 数据库文件（包括二级索引）\n* 配置文件(`* .ini`)\n* 日志文件（如果配置为记录到磁盘）\n\n以下是确保所有这些文件的备份一致的策略。\n## 3.1 数据库备份\nCouchDB备份的最简单，最简单的方法是使用CouchDB复制到另一个CouchDB安装。您可以根据需要在普通（单次）复制或连续复制之间进行选择。\n但是，您也可以随时从CouchDB数据目录(默认为`data/`)中复制实际的`.couch`文件，而不会出现问题。 CouchDB的数据库和二级索引的仅追加存储格式可确保这种方法可以正常工作。\n为了确保备份的可靠性，建议先备份二级索引(存储在`data/.shards`下)，然后再备份主数据库文件(存储在`data/ shards`以及父级`data/`下的系统级数据库)目录)。这是因为CouchDB将在下一次读取访问时通过更新视图/二级索引来自动处理稍微过时的视图/二级索引，但是比其关联数据库新的视图或二级索引将触发索引的完全重建。这可能是一项非常昂贵且耗时的操作，并且会影响您在灾难情况下快速恢复的能力。\n在受支持的操作系统/存储环境上，您还可以使用存储快照。这些优点是在使用块存储系统(例如`ZFS`或`LVM`或`Amazon EBS`)时几乎是即时的。在块存储级别使用快照时，请确保在必要时使用OS级实用程序(例如Linux的`fsfreeze`)使文件系统停顿。如果不确定，请查阅操作系统或云提供商的文档以获取更多详细信息。\n## 3.2 配置备份\nCouchDB的配置系统将数据存储在配置目录(默认为`etc/`)下的`.ini`文件中。 如果在运行时对配置进行了更改，则配置链中的最后一个文件将使用更改进行更新。\n从备份还原后，简单地备份整个`etc/`目录，以确保配置一致。\n如果在运行时未通过HTTP API对配置进行任何更改，并且所有配置文件都由配置管理系统(例如`Ansible`或`Chef`)管理，则无需备份配置目录。\n## 3.3 日志备份\n如果配置为记录到文件，则可能要备份CouchDB编写的日志文件。这些文件的任何备份解决方案都可以使用。\n在类似UNIX的系统上，如果使用日志轮换软件，则必须采用“复制然后截断”的方法。创建副本后，这会将原始日志文件截断为零。CouchDB无法识别要关闭其日志文件并创建一个新信号的任何信号。因此，并且由于文件处理功能的差异，除了定期重新启动CouchDB进程外，在Microsoft Windows下没有简单的日志轮换解决方案。","source":"_posts/blog/couchDB/CouchDB学习-维护.md","raw":"---\ntitle: CouchDB学习-维护\ndate: 2019-12-22 12:26:41\ntags: CouchDb\ncategories: CouchDb学习\n---\n[官方文档](https://docs.couchdb.org/en/stable/maintenance/index.html)\n# 1 压缩\n* * *\n压缩操作是通过从数据库或者视图索引文件中移除无用的和老的数据减少硬盘使用空间.操作非常简单类似于其他数据库(SQLite等)管理系统。\n在压缩目标期间，CouchDB将创建扩展名为.compact的新文件，并将仅实际数据传输到该文件中。 因此，CouchDB首先检查可用磁盘空间-它应比压缩文件的数据大两倍。\n当所有实际数据都成功传输到压缩文件后，CouchDB用目标文件替换目标文件。\n## 1.1 数据库压缩\n数据库压缩通过删除更新期间创建的未使用的文件部分来压缩数据库文件。 旧文档修订版被少量称为`tombstone`的元数据代替，该元数据用于复制期间的冲突解决。 可以使用`_revs_limit`URL配置存储的修订版本（以及`tombstone`）的数量。\n压缩是每个数据库手动触发的操作，并作为后台任务运行。要针对特定的数据库启动它，需要发送目标数据库的HTTP POST`/{db}/_compact`子资源：\n```\ncurl -H \"Content-Type: application/json\" -X POST http://localhost:5984/my_db/_compact\n```\n成功的话，将立即返回HTTP 状态码`202 Accepted`。\n```\nHTTP/1.1 202 Accepted\nCache-Control: must-revalidate Content-Length: 12\nContent-Type: text/plain; charset=utf-8 Date: Wed, 19 Jun 2013 09:43:52 GMT Server: CouchDB (Erlang/OTP)\n```\n```\n{\"ok\":true}\n```\n尽管未使用请求主体，但仍必须为请求指定带有`application/json`值的`Content-Type`标头。否则，您会知道HTTP状态`415不支持的媒体类型响应`：\n```\nHTTP/1.1 415 Unsupported Media Type\nCache-Control: must-revalidate \nContent-Length: 78\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 09:43:44 GMT \nServer: CouchDB (Erlang/OTP)\n{\"error\":\"bad_content_type\",\"reason\":\"Content-Type must be application/json\"}\n```\n当压缩成功启动并运行时，可以通过数据库信息资源获取有关压缩的信息：\n```\ncurl http://localhost:5984/my_db\n```\n```\nHTTP/1.1 200 OK\nCache-Control: must-revalidate \nContent-Length: 246\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 16:51:20 GMT \nServer: CouchDB (Erlang/OTP)\n{\n    \"committed_update_seq\": 76215, \n    \"compact_running\": true, \n    \"data_size\": 3787996, \n    \"db_name\": \"my_db\", \n    \"disk_format_version\": 6, \n    \"disk_size\": 17703025, \n    \"doc_count\": 5091, \n    \"doc_del_count\": 0, \n    \"instance_start_time\": \"0\", \n    \"purge_seq\": 0,\n    \"update_seq\": 76215\n}\n```\n请注意，`compaction_running`字段为`true`，指示压缩实际上正在运行。 要跟踪压缩进度，可以查询`_active_tasks`资源：\n```\ncurl http://localhost:5984/_active_tasks\n```\n```\nHTTP/1.1 200 OK\nCache-Control: must-revalidate\nContent-Length: 175\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 16:27:23 GMT \nServer: CouchDB (Erlang/OTP)\n[\n    {\n        \"changes_done\": 44461, \n        \"database\": \"my_db\",\n        \"pid\": \"<0.218.0>\", \n        \"progress\": 58,\n        \"started_on\": 1371659228, \n        \"total_changes\": 76215, \n        \"type\": \"database_compaction\", \n        \"updated_on\": 1371659241\n    }\n]\n```\n## 1.2 视图压缩\n与数据库视图不同，视图也需要像数据库一样进行压缩，这与按每个设计文档按组对数据库视图进行压缩不同。要启动其压缩，需要发送HTTP POST`/{db}/_compact/{ddoc}`请求：\n```\ncurl -H \"Content-Type: application/json\" -X POST http://localhost:5984/dbname/_compact/designname\n```\n```\n{\"ok\":true}\n```\n这将从指定设计文档的当前版本压缩视图索引。 HTTP响应代码`202 Accepted`(类似于数据库的压缩)，并且将创建压缩后台任务。\n### 1.2.1视图清理\n磁盘上的视图索引以视图定义的MD5哈希命名。更改视图时，旧索引仍保留在磁盘上。要清除所有过时的视图索引（以视图的MD5表示形式命名的文件，该文件不再存在），可以触发视图清除：\n```\ncurl -H \"Content-Type: application/json\" -X POST http://localhost:5984/dbname/_view_cleanup\n```\n```\n{\"ok\":true}\n```\n## 1.3 自动压缩\n虽然需要手动触发数据库和视图的压缩，但也可以配置自动压缩，以便基于各种条件自动触发数据库和视图的压缩。 在CouchDB的配置文件中配置了自动压缩。\n守护程序`/compaction_daemon`负责触发压缩。 默认情况下启用它并自动启动。在压缩部分中配置了触发压缩的条件。\n# 2 性能\n* * *\n无论您如何编写代码，即使有了成千上万的文档，通常都会发现CouchDB可以很好地执行。但是一旦开始阅读数百万个文档，您需要更加小心。\n## 2.1 硬盘IO\n### 2.1.1 文件大小\n文件大小越小，I/O操作将越少，CouchDB和操作系统可以缓存的文件越多，复制，备份等的速度就越快。因此，您应该仔细检查所要存储的数据储存。例如，使用长度为数百个字符的键会很愚蠢，但是如果仅使用单个字符键，则程序将很难维护。通过将其放在视图中来仔细考虑重复的数据。\n### 2.1.2 硬盘和文件系统性能\n使用更快的磁盘，条带化RAID阵列和现代文件系统都可以加快CouchDB部署。但是，当磁盘性能成为瓶颈时，有一种方法可以提高CouchDB服务器的响应速度。 从文件模块的Erlang文档中：\n在具有线程支持的操作系统上，可以让文件操作在其自己的线程中执行，从而允许其他Erlang进程继续与文件操作并行执行。 请参阅erl(1)中的命令行标志+A。\n将此参数设置为大于零的数字可以使您的CouchDB安装保持响应状态，即使在磁盘使用率很高的时期也是如此。设置此选项的最简单方法是通过`ERL_FLAGS`环境变量。例如，要为Erlang提供执行I/O操作的四个线程，请将以下内容添加到`(prefix)/etc/defaults/couchdb`（或等效项）中：\n```\nexport ERL_FLAGS=\"+A 4\"\n```\n## 2.2 系统资源限制\n管理员在其部署变大时会遇到的问题之一是系统和应用程序配置施加的资源限制。 提高这些限制可以使您的部署超出默认配置所支持的范围。\n### 2.2.1 CouchDB配置选项\n#### `delayed_commits`\n延迟的提交允许在某些工作负载下实现更好的写入性能，同时牺牲少量的持久性。 该设置使CouchDB在更新后提交新数据之前要等待一整秒。如果服务器在写入标头之前崩溃，则自上次提交以来的所有写入都将丢失。 启用此选项需要您自担风险。\n#### `max_dbs_open`\n在配置(`local.ini`或类似版本)中,或者地址`couchdb/max_dbs_open`：\n```\n[couchdb]\nmax_dbs_open = 100\n```\n此选项将一次可以打开的数据库数量设置为上限。CouchDB引用对内部数据库访问进行计数，并在必要时关闭空闲数据库。有时有必要一次保持超过默认值的速度，例如在许多数据库将被连续复制的部署中。\n#### `Erlang`\n即使增加了CouchDB允许的最大连接数，默认情况下，Erlang运行时系统也将不允许超过1024个连接。 将以下指令添加到`(prefix)/etc/default/couchdb`(或等效文件)将增加此限制(在这种情况下，增加到4096)：\n```\nexport ERL_MAX_PORTS=4096\n```\n高达1.1.x的CouchDB版本还会为每个复制创建`Erlang Term Storage`(ETS)表。如果您使用的CouchDB版本早于1.2，并且必须支持许多复制，则还应设置`ERL_MAX_ETS_TABLES`变量。 默认值是大约1400表。\n请注意，在Mac OS X上，Erlang实际上不会将文件描述符限制增加到超过1024（即系统标头定义的FD_SETSIZE值）。 \n#### 打开文件描述的最大数量(无限制)\n大多数`*nix`操作系统在每个进程上都有各种资源限制。增加这些限制的方法因初始化系统和特定的OS版本而异。许多操作系统的默认值为1024或4096。在具有许多数据库或视图的系统上，CouchDB可以非常迅速地达到此限制。\n如果您的系统设置为使用可插拔身份验证模块(`PAM`)系统（几乎所有现代Linux都是这种情况），则增加此限制很简单。例如，创建具有以下内容的名为`/etc/security/limits.d/100-couchdb.conf`的文件将确保CouchDB可以一次打开多达10000个文件描述符：\n```\n#<domain>  <type>    <item>  <value>\ncouchdb    hard      nofile  10000 \ncouchdb    soft      nofile  10000\n```\n如果使用的是Debian/Ubuntu sysvinit脚本(`/etc/init.d/couchdb`，则还需要提高root用户的限制：\n```\n#<domain>    <type>   <item>  <value>\nroot         hard    nofile   10000\nroot         soft    nofile   10000\n```\n您可能还需要编辑`/etc/pam.d/common-session`和`/etc/pam.d/common-session-noninteractive`文件以添加以下行：\n```\nsession required pam_limits.so\n```\n如果还不存在。\n对于基于系统的Linux（例如CentOS/RHEL 7，Ubuntu 16.04 +，Debian 8或更高版本）,假设您要从systemd启动CouchDB，则还必须通过创建文件`/etc/systemd/system/<servicename>.d/override.conf`添加以下内容：\n```\n[Service]\nLimitNOFILE=#######\n```\n并将`#######`替换为文件描述符的上限CouchDB允许立即保持打开状态。\n如果您的系统不使用`PAM`，通常可以在自定义脚本中使用`ulimit`命令来启动\nCouchDB具有增加的资源限制。 典型的语法类似于`ulimit -n 10000`。\n通常，类似UNIX的现代系统每个进程可以处理大量文件句柄（例如100000）\n没有问题。 不要害怕增加系统限制。\n## 2.3 网络\n产生和接收每个请求/响应都有延迟开销。通常，您应该分批执行请求。大多数API具有某种批处理机制，通常是通过在请求正文中提供文档或键的列表来进行的。请注意为批次选择的大小。较大的批处理需要更多的时间来使客户将项目编码为  `JSON`，并将更多的时间用于解码该数量的响应。使用您自己的配置和典型数据进行一些基准测试，以找到最佳位置。 它可能在一到一万个文档之间。\n如果您拥有快速的I/O系统，那么您也可以使用并发-同时具有多个请求/响应。这减轻了组装JSON，进行网络连接和解码`JSON`所涉及的延迟。\n从CouchDB 1.1.0开始，与旧版本相比，用户经常报告文档的写入性能较低。主要原因是此版本随附HTTP服务器库`MochiWeb`的最新版本，该库默认情况下将`TCP`套接字选项`SO_NODELAY`设置为`false`。这意味着发送到TCP套接字的小数据（例如对文档写请求的答复（或读取非常小的文档）的答复）不会立即发送到网络`TCP`将对其缓冲一会儿，希望它会被询问通过同一套接字发送更多数据，然后一次发送所有数据以提高性能。 可以通过`httpd/socket_options`禁用此`TCP`缓冲行为：\n```\n[httpd]\nsocket_options = [{nodelay, true}]\n```\n### 2.3.1 连接限制\n`MochiWeb`处理`CouchDB`请求。默认最大连接数为2048。要更改此限制，请使用`server_options`配置变量。 `max`表示最大连接数。\n```\n[chttpd]\nserver_options = [{backlog, 128}, {acceptor_pool_size, 16}, {max, 4096}]\n```\n## 2.4 CouchDB\n### 2.4.1 删除操作\n当您删除文档时，数据库将创建一个新的修订版，其中包含`_id`和`_rev`字段以及`_deleted`标志。即使在数据库压缩后，此修订版仍将保留，以便可以复制删除内容。像未删除的文档一样，已删除的文档可能会影响视图生成时间，`PUT`和`DELETE`请求时间以及数据库的大小，因为它们会增加`B+Tree`的大小。您可以在数据库信息中看到已删除文档的数量。如果您的用例创建了许多已删除的文档（例如，如果您存储日志条目，消息队列等短期数据），则可能需要定期切换到新数据库并删除已过期的旧数据库）。\n### 2.4.2 文档ID\n数据库文件的大小源自您的文档和视图大小，但也取决于您的`_id`大小的倍数。`_id`不仅存在于文档中，而且它及其部分内容在`CouchDB`用来导航文件以首先找到文档的二叉树结构中也是重复的。作为一个现实世界的例子，一个用户从16个字节的ID切换到4个字节的ID，使数据库从21GB变为4GB，包含1000万个文档（从2.5GB到2GB的原始JSON文本）。\n插入具有顺序（至少已排序）的ID的速度要比随机ID快。 因此，您应该考虑自己生成id，按顺序分配它们，并使用消耗更少字节的编码方案。例如，可以用4个基数62个数字（用10个数字，26个小写字母，26个大写字母）来完成需要16个十六进制数字表示的内容。\n## 2.5 视图\n### 2.5.1 视图生成\n当要处理的文档数量非常少时，使用JavaScript查询服务器生成的视图非常慢。生成过程甚至不会使单个CPU饱和，更不用说您的I/O了。原因是CouchDB服务器和单独的`couchjs`查询服务器中涉及的延迟，这显着表明了从实施中消除延迟的重要性。\n您可以让视图访问权限“过时”，但要确定何时发生会给您带来快速响应以及何时更新视图会花费很长时间，这是不切实际的。(一个拥有1000万个文档的数据库大约需要10分钟才能加载到CouchDB中，而生成视图需要大约4个小时)。\n在集群中，“陈旧的”请求由一组固定的分片服务，以便为用户提供请求之间的一致结果。这需要进行可用性权衡-固定的分片集可能不是集群中响应最快的/可用的。如果不需要这种一致性(例如，索引相对静态)，则可以通过指定`stable = false＆update = false`代替`stale = ok`或`stable = false＆update = lazy`代替`stale = update_after`。\n视图信息不会被复制-它会在每个数据库上重建，因此您无法在单独的服务器上生成视图。\n### 2.5.2 内置缩小功能\n如果您使用的是非常简单的视图函数，仅执行求和或计数减少，则可以通过简单地编写`_sum`或`_count`代替函数声明来调用它们的本机`Erlang`实现。 这将大大加快速度，因为它减少了CouchDB和JavaScript查询服务器之间的IO。 例如，如邮件列表中所述，用于输出（已索引和缓存的）视图的时间大约为78,000个项目，时间从60秒减少到4秒。\n之前：\n```\n{\n    \"_id\": \"_design/foo\",\n    \"views\": {\n        \"bar\": {\n            \"map\": \"function (doc) { emit(doc.author, 1); }\",\n            \"reduce\": \"function (keys, values, rereduce) { return sum(values); }\"\n        }   \n    }\n}\n```\n之后：\n```\n{\n    \"_id\": \"_design/foo\",\n    \"views\": {\n        \"bar\": {\n            \"map\": \"function (doc) { emit(doc.author, 1); }\",\n            \"reduce\": \"_sum\"\n        }\n    }\n}\n```\n# 3 CouchDB备份\n* * *\nCouchDB在运行时可以创建三种不同类型的文件：\n\n* 数据库文件（包括二级索引）\n* 配置文件(`* .ini`)\n* 日志文件（如果配置为记录到磁盘）\n\n以下是确保所有这些文件的备份一致的策略。\n## 3.1 数据库备份\nCouchDB备份的最简单，最简单的方法是使用CouchDB复制到另一个CouchDB安装。您可以根据需要在普通（单次）复制或连续复制之间进行选择。\n但是，您也可以随时从CouchDB数据目录(默认为`data/`)中复制实际的`.couch`文件，而不会出现问题。 CouchDB的数据库和二级索引的仅追加存储格式可确保这种方法可以正常工作。\n为了确保备份的可靠性，建议先备份二级索引(存储在`data/.shards`下)，然后再备份主数据库文件(存储在`data/ shards`以及父级`data/`下的系统级数据库)目录)。这是因为CouchDB将在下一次读取访问时通过更新视图/二级索引来自动处理稍微过时的视图/二级索引，但是比其关联数据库新的视图或二级索引将触发索引的完全重建。这可能是一项非常昂贵且耗时的操作，并且会影响您在灾难情况下快速恢复的能力。\n在受支持的操作系统/存储环境上，您还可以使用存储快照。这些优点是在使用块存储系统(例如`ZFS`或`LVM`或`Amazon EBS`)时几乎是即时的。在块存储级别使用快照时，请确保在必要时使用OS级实用程序(例如Linux的`fsfreeze`)使文件系统停顿。如果不确定，请查阅操作系统或云提供商的文档以获取更多详细信息。\n## 3.2 配置备份\nCouchDB的配置系统将数据存储在配置目录(默认为`etc/`)下的`.ini`文件中。 如果在运行时对配置进行了更改，则配置链中的最后一个文件将使用更改进行更新。\n从备份还原后，简单地备份整个`etc/`目录，以确保配置一致。\n如果在运行时未通过HTTP API对配置进行任何更改，并且所有配置文件都由配置管理系统(例如`Ansible`或`Chef`)管理，则无需备份配置目录。\n## 3.3 日志备份\n如果配置为记录到文件，则可能要备份CouchDB编写的日志文件。这些文件的任何备份解决方案都可以使用。\n在类似UNIX的系统上，如果使用日志轮换软件，则必须采用“复制然后截断”的方法。创建副本后，这会将原始日志文件截断为零。CouchDB无法识别要关闭其日志文件并创建一个新信号的任何信号。因此，并且由于文件处理功能的差异，除了定期重新启动CouchDB进程外，在Microsoft Windows下没有简单的日志轮换解决方案。","slug":"blog/couchDB/CouchDB学习-维护","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqygl000vk0vqakilajuk","content":"<p><a href=\"https://docs.couchdb.org/en/stable/maintenance/index.html\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1 id=\"1-压缩\"><a href=\"#1-压缩\" class=\"headerlink\" title=\"1 压缩\"></a>1 压缩</h1><hr>\n<p>压缩操作是通过从数据库或者视图索引文件中移除无用的和老的数据减少硬盘使用空间.操作非常简单类似于其他数据库(SQLite等)管理系统。<br>在压缩目标期间，CouchDB将创建扩展名为.compact的新文件，并将仅实际数据传输到该文件中。 因此，CouchDB首先检查可用磁盘空间-它应比压缩文件的数据大两倍。<br>当所有实际数据都成功传输到压缩文件后，CouchDB用目标文件替换目标文件。</p>\n<h2 id=\"1-1-数据库压缩\"><a href=\"#1-1-数据库压缩\" class=\"headerlink\" title=\"1.1 数据库压缩\"></a>1.1 数据库压缩</h2><p>数据库压缩通过删除更新期间创建的未使用的文件部分来压缩数据库文件。 旧文档修订版被少量称为<code>tombstone</code>的元数据代替，该元数据用于复制期间的冲突解决。 可以使用<code>_revs_limit</code>URL配置存储的修订版本（以及<code>tombstone</code>）的数量。<br>压缩是每个数据库手动触发的操作，并作为后台任务运行。要针对特定的数据库启动它，需要发送目标数据库的HTTP POST<code>/{db}/_compact</code>子资源：</p>\n<pre><code>curl -H &quot;Content-Type: application/json&quot; -X POST http://localhost:5984/my_db/_compact</code></pre><p>成功的话，将立即返回HTTP 状态码<code>202 Accepted</code>。</p>\n<pre><code>HTTP/1.1 202 Accepted\nCache-Control: must-revalidate Content-Length: 12\nContent-Type: text/plain; charset=utf-8 Date: Wed, 19 Jun 2013 09:43:52 GMT Server: CouchDB (Erlang/OTP)</code></pre><pre><code>{&quot;ok&quot;:true}</code></pre><p>尽管未使用请求主体，但仍必须为请求指定带有<code>application/json</code>值的<code>Content-Type</code>标头。否则，您会知道HTTP状态<code>415不支持的媒体类型响应</code>：</p>\n<pre><code>HTTP/1.1 415 Unsupported Media Type\nCache-Control: must-revalidate \nContent-Length: 78\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 09:43:44 GMT \nServer: CouchDB (Erlang/OTP)\n{&quot;error&quot;:&quot;bad_content_type&quot;,&quot;reason&quot;:&quot;Content-Type must be application/json&quot;}</code></pre><p>当压缩成功启动并运行时，可以通过数据库信息资源获取有关压缩的信息：</p>\n<pre><code>curl http://localhost:5984/my_db</code></pre><pre><code>HTTP/1.1 200 OK\nCache-Control: must-revalidate \nContent-Length: 246\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 16:51:20 GMT \nServer: CouchDB (Erlang/OTP)\n{\n    &quot;committed_update_seq&quot;: 76215, \n    &quot;compact_running&quot;: true, \n    &quot;data_size&quot;: 3787996, \n    &quot;db_name&quot;: &quot;my_db&quot;, \n    &quot;disk_format_version&quot;: 6, \n    &quot;disk_size&quot;: 17703025, \n    &quot;doc_count&quot;: 5091, \n    &quot;doc_del_count&quot;: 0, \n    &quot;instance_start_time&quot;: &quot;0&quot;, \n    &quot;purge_seq&quot;: 0,\n    &quot;update_seq&quot;: 76215\n}</code></pre><p>请注意，<code>compaction_running</code>字段为<code>true</code>，指示压缩实际上正在运行。 要跟踪压缩进度，可以查询<code>_active_tasks</code>资源：</p>\n<pre><code>curl http://localhost:5984/_active_tasks</code></pre><pre><code>HTTP/1.1 200 OK\nCache-Control: must-revalidate\nContent-Length: 175\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 16:27:23 GMT \nServer: CouchDB (Erlang/OTP)\n[\n    {\n        &quot;changes_done&quot;: 44461, \n        &quot;database&quot;: &quot;my_db&quot;,\n        &quot;pid&quot;: &quot;&lt;0.218.0&gt;&quot;, \n        &quot;progress&quot;: 58,\n        &quot;started_on&quot;: 1371659228, \n        &quot;total_changes&quot;: 76215, \n        &quot;type&quot;: &quot;database_compaction&quot;, \n        &quot;updated_on&quot;: 1371659241\n    }\n]</code></pre><h2 id=\"1-2-视图压缩\"><a href=\"#1-2-视图压缩\" class=\"headerlink\" title=\"1.2 视图压缩\"></a>1.2 视图压缩</h2><p>与数据库视图不同，视图也需要像数据库一样进行压缩，这与按每个设计文档按组对数据库视图进行压缩不同。要启动其压缩，需要发送HTTP POST<code>/{db}/_compact/{ddoc}</code>请求：</p>\n<pre><code>curl -H &quot;Content-Type: application/json&quot; -X POST http://localhost:5984/dbname/_compact/designname</code></pre><pre><code>{&quot;ok&quot;:true}</code></pre><p>这将从指定设计文档的当前版本压缩视图索引。 HTTP响应代码<code>202 Accepted</code>(类似于数据库的压缩)，并且将创建压缩后台任务。</p>\n<h3 id=\"1-2-1视图清理\"><a href=\"#1-2-1视图清理\" class=\"headerlink\" title=\"1.2.1视图清理\"></a>1.2.1视图清理</h3><p>磁盘上的视图索引以视图定义的MD5哈希命名。更改视图时，旧索引仍保留在磁盘上。要清除所有过时的视图索引（以视图的MD5表示形式命名的文件，该文件不再存在），可以触发视图清除：</p>\n<pre><code>curl -H &quot;Content-Type: application/json&quot; -X POST http://localhost:5984/dbname/_view_cleanup</code></pre><pre><code>{&quot;ok&quot;:true}</code></pre><h2 id=\"1-3-自动压缩\"><a href=\"#1-3-自动压缩\" class=\"headerlink\" title=\"1.3 自动压缩\"></a>1.3 自动压缩</h2><p>虽然需要手动触发数据库和视图的压缩，但也可以配置自动压缩，以便基于各种条件自动触发数据库和视图的压缩。 在CouchDB的配置文件中配置了自动压缩。<br>守护程序<code>/compaction_daemon</code>负责触发压缩。 默认情况下启用它并自动启动。在压缩部分中配置了触发压缩的条件。</p>\n<h1 id=\"2-性能\"><a href=\"#2-性能\" class=\"headerlink\" title=\"2 性能\"></a>2 性能</h1><hr>\n<p>无论您如何编写代码，即使有了成千上万的文档，通常都会发现CouchDB可以很好地执行。但是一旦开始阅读数百万个文档，您需要更加小心。</p>\n<h2 id=\"2-1-硬盘IO\"><a href=\"#2-1-硬盘IO\" class=\"headerlink\" title=\"2.1 硬盘IO\"></a>2.1 硬盘IO</h2><h3 id=\"2-1-1-文件大小\"><a href=\"#2-1-1-文件大小\" class=\"headerlink\" title=\"2.1.1 文件大小\"></a>2.1.1 文件大小</h3><p>文件大小越小，I/O操作将越少，CouchDB和操作系统可以缓存的文件越多，复制，备份等的速度就越快。因此，您应该仔细检查所要存储的数据储存。例如，使用长度为数百个字符的键会很愚蠢，但是如果仅使用单个字符键，则程序将很难维护。通过将其放在视图中来仔细考虑重复的数据。</p>\n<h3 id=\"2-1-2-硬盘和文件系统性能\"><a href=\"#2-1-2-硬盘和文件系统性能\" class=\"headerlink\" title=\"2.1.2 硬盘和文件系统性能\"></a>2.1.2 硬盘和文件系统性能</h3><p>使用更快的磁盘，条带化RAID阵列和现代文件系统都可以加快CouchDB部署。但是，当磁盘性能成为瓶颈时，有一种方法可以提高CouchDB服务器的响应速度。 从文件模块的Erlang文档中：<br>在具有线程支持的操作系统上，可以让文件操作在其自己的线程中执行，从而允许其他Erlang进程继续与文件操作并行执行。 请参阅erl(1)中的命令行标志+A。<br>将此参数设置为大于零的数字可以使您的CouchDB安装保持响应状态，即使在磁盘使用率很高的时期也是如此。设置此选项的最简单方法是通过<code>ERL_FLAGS</code>环境变量。例如，要为Erlang提供执行I/O操作的四个线程，请将以下内容添加到<code>(prefix)/etc/defaults/couchdb</code>（或等效项）中：</p>\n<pre><code>export ERL_FLAGS=&quot;+A 4&quot;</code></pre><h2 id=\"2-2-系统资源限制\"><a href=\"#2-2-系统资源限制\" class=\"headerlink\" title=\"2.2 系统资源限制\"></a>2.2 系统资源限制</h2><p>管理员在其部署变大时会遇到的问题之一是系统和应用程序配置施加的资源限制。 提高这些限制可以使您的部署超出默认配置所支持的范围。</p>\n<h3 id=\"2-2-1-CouchDB配置选项\"><a href=\"#2-2-1-CouchDB配置选项\" class=\"headerlink\" title=\"2.2.1 CouchDB配置选项\"></a>2.2.1 CouchDB配置选项</h3><h4 id=\"delayed-commits\"><a href=\"#delayed-commits\" class=\"headerlink\" title=\"delayed_commits\"></a><code>delayed_commits</code></h4><p>延迟的提交允许在某些工作负载下实现更好的写入性能，同时牺牲少量的持久性。 该设置使CouchDB在更新后提交新数据之前要等待一整秒。如果服务器在写入标头之前崩溃，则自上次提交以来的所有写入都将丢失。 启用此选项需要您自担风险。</p>\n<h4 id=\"max-dbs-open\"><a href=\"#max-dbs-open\" class=\"headerlink\" title=\"max_dbs_open\"></a><code>max_dbs_open</code></h4><p>在配置(<code>local.ini</code>或类似版本)中,或者地址<code>couchdb/max_dbs_open</code>：</p>\n<pre><code>[couchdb]\nmax_dbs_open = 100</code></pre><p>此选项将一次可以打开的数据库数量设置为上限。CouchDB引用对内部数据库访问进行计数，并在必要时关闭空闲数据库。有时有必要一次保持超过默认值的速度，例如在许多数据库将被连续复制的部署中。</p>\n<h4 id=\"Erlang\"><a href=\"#Erlang\" class=\"headerlink\" title=\"Erlang\"></a><code>Erlang</code></h4><p>即使增加了CouchDB允许的最大连接数，默认情况下，Erlang运行时系统也将不允许超过1024个连接。 将以下指令添加到<code>(prefix)/etc/default/couchdb</code>(或等效文件)将增加此限制(在这种情况下，增加到4096)：</p>\n<pre><code>export ERL_MAX_PORTS=4096</code></pre><p>高达1.1.x的CouchDB版本还会为每个复制创建<code>Erlang Term Storage</code>(ETS)表。如果您使用的CouchDB版本早于1.2，并且必须支持许多复制，则还应设置<code>ERL_MAX_ETS_TABLES</code>变量。 默认值是大约1400表。<br>请注意，在Mac OS X上，Erlang实际上不会将文件描述符限制增加到超过1024（即系统标头定义的FD_SETSIZE值）。 </p>\n<h4 id=\"打开文件描述的最大数量-无限制\"><a href=\"#打开文件描述的最大数量-无限制\" class=\"headerlink\" title=\"打开文件描述的最大数量(无限制)\"></a>打开文件描述的最大数量(无限制)</h4><p>大多数<code>*nix</code>操作系统在每个进程上都有各种资源限制。增加这些限制的方法因初始化系统和特定的OS版本而异。许多操作系统的默认值为1024或4096。在具有许多数据库或视图的系统上，CouchDB可以非常迅速地达到此限制。<br>如果您的系统设置为使用可插拔身份验证模块(<code>PAM</code>)系统（几乎所有现代Linux都是这种情况），则增加此限制很简单。例如，创建具有以下内容的名为<code>/etc/security/limits.d/100-couchdb.conf</code>的文件将确保CouchDB可以一次打开多达10000个文件描述符：</p>\n<pre><code>#&lt;domain&gt;  &lt;type&gt;    &lt;item&gt;  &lt;value&gt;\ncouchdb    hard      nofile  10000 \ncouchdb    soft      nofile  10000</code></pre><p>如果使用的是Debian/Ubuntu sysvinit脚本(<code>/etc/init.d/couchdb</code>，则还需要提高root用户的限制：</p>\n<pre><code>#&lt;domain&gt;    &lt;type&gt;   &lt;item&gt;  &lt;value&gt;\nroot         hard    nofile   10000\nroot         soft    nofile   10000</code></pre><p>您可能还需要编辑<code>/etc/pam.d/common-session</code>和<code>/etc/pam.d/common-session-noninteractive</code>文件以添加以下行：</p>\n<pre><code>session required pam_limits.so</code></pre><p>如果还不存在。<br>对于基于系统的Linux（例如CentOS/RHEL 7，Ubuntu 16.04 +，Debian 8或更高版本）,假设您要从systemd启动CouchDB，则还必须通过创建文件<code>/etc/systemd/system/&lt;servicename&gt;.d/override.conf</code>添加以下内容：</p>\n<pre><code>[Service]\nLimitNOFILE=#######</code></pre><p>并将<code>#######</code>替换为文件描述符的上限CouchDB允许立即保持打开状态。<br>如果您的系统不使用<code>PAM</code>，通常可以在自定义脚本中使用<code>ulimit</code>命令来启动<br>CouchDB具有增加的资源限制。 典型的语法类似于<code>ulimit -n 10000</code>。<br>通常，类似UNIX的现代系统每个进程可以处理大量文件句柄（例如100000）<br>没有问题。 不要害怕增加系统限制。</p>\n<h2 id=\"2-3-网络\"><a href=\"#2-3-网络\" class=\"headerlink\" title=\"2.3 网络\"></a>2.3 网络</h2><p>产生和接收每个请求/响应都有延迟开销。通常，您应该分批执行请求。大多数API具有某种批处理机制，通常是通过在请求正文中提供文档或键的列表来进行的。请注意为批次选择的大小。较大的批处理需要更多的时间来使客户将项目编码为  <code>JSON</code>，并将更多的时间用于解码该数量的响应。使用您自己的配置和典型数据进行一些基准测试，以找到最佳位置。 它可能在一到一万个文档之间。<br>如果您拥有快速的I/O系统，那么您也可以使用并发-同时具有多个请求/响应。这减轻了组装JSON，进行网络连接和解码<code>JSON</code>所涉及的延迟。<br>从CouchDB 1.1.0开始，与旧版本相比，用户经常报告文档的写入性能较低。主要原因是此版本随附HTTP服务器库<code>MochiWeb</code>的最新版本，该库默认情况下将<code>TCP</code>套接字选项<code>SO_NODELAY</code>设置为<code>false</code>。这意味着发送到TCP套接字的小数据（例如对文档写请求的答复（或读取非常小的文档）的答复）不会立即发送到网络<code>TCP</code>将对其缓冲一会儿，希望它会被询问通过同一套接字发送更多数据，然后一次发送所有数据以提高性能。 可以通过<code>httpd/socket_options</code>禁用此<code>TCP</code>缓冲行为：</p>\n<pre><code>[httpd]\nsocket_options = [{nodelay, true}]</code></pre><h3 id=\"2-3-1-连接限制\"><a href=\"#2-3-1-连接限制\" class=\"headerlink\" title=\"2.3.1 连接限制\"></a>2.3.1 连接限制</h3><p><code>MochiWeb</code>处理<code>CouchDB</code>请求。默认最大连接数为2048。要更改此限制，请使用<code>server_options</code>配置变量。 <code>max</code>表示最大连接数。</p>\n<pre><code>[chttpd]\nserver_options = [{backlog, 128}, {acceptor_pool_size, 16}, {max, 4096}]</code></pre><h2 id=\"2-4-CouchDB\"><a href=\"#2-4-CouchDB\" class=\"headerlink\" title=\"2.4 CouchDB\"></a>2.4 CouchDB</h2><h3 id=\"2-4-1-删除操作\"><a href=\"#2-4-1-删除操作\" class=\"headerlink\" title=\"2.4.1 删除操作\"></a>2.4.1 删除操作</h3><p>当您删除文档时，数据库将创建一个新的修订版，其中包含<code>_id</code>和<code>_rev</code>字段以及<code>_deleted</code>标志。即使在数据库压缩后，此修订版仍将保留，以便可以复制删除内容。像未删除的文档一样，已删除的文档可能会影响视图生成时间，<code>PUT</code>和<code>DELETE</code>请求时间以及数据库的大小，因为它们会增加<code>B+Tree</code>的大小。您可以在数据库信息中看到已删除文档的数量。如果您的用例创建了许多已删除的文档（例如，如果您存储日志条目，消息队列等短期数据），则可能需要定期切换到新数据库并删除已过期的旧数据库）。</p>\n<h3 id=\"2-4-2-文档ID\"><a href=\"#2-4-2-文档ID\" class=\"headerlink\" title=\"2.4.2 文档ID\"></a>2.4.2 文档ID</h3><p>数据库文件的大小源自您的文档和视图大小，但也取决于您的<code>_id</code>大小的倍数。<code>_id</code>不仅存在于文档中，而且它及其部分内容在<code>CouchDB</code>用来导航文件以首先找到文档的二叉树结构中也是重复的。作为一个现实世界的例子，一个用户从16个字节的ID切换到4个字节的ID，使数据库从21GB变为4GB，包含1000万个文档（从2.5GB到2GB的原始JSON文本）。<br>插入具有顺序（至少已排序）的ID的速度要比随机ID快。 因此，您应该考虑自己生成id，按顺序分配它们，并使用消耗更少字节的编码方案。例如，可以用4个基数62个数字（用10个数字，26个小写字母，26个大写字母）来完成需要16个十六进制数字表示的内容。</p>\n<h2 id=\"2-5-视图\"><a href=\"#2-5-视图\" class=\"headerlink\" title=\"2.5 视图\"></a>2.5 视图</h2><h3 id=\"2-5-1-视图生成\"><a href=\"#2-5-1-视图生成\" class=\"headerlink\" title=\"2.5.1 视图生成\"></a>2.5.1 视图生成</h3><p>当要处理的文档数量非常少时，使用JavaScript查询服务器生成的视图非常慢。生成过程甚至不会使单个CPU饱和，更不用说您的I/O了。原因是CouchDB服务器和单独的<code>couchjs</code>查询服务器中涉及的延迟，这显着表明了从实施中消除延迟的重要性。<br>您可以让视图访问权限“过时”，但要确定何时发生会给您带来快速响应以及何时更新视图会花费很长时间，这是不切实际的。(一个拥有1000万个文档的数据库大约需要10分钟才能加载到CouchDB中，而生成视图需要大约4个小时)。<br>在集群中，“陈旧的”请求由一组固定的分片服务，以便为用户提供请求之间的一致结果。这需要进行可用性权衡-固定的分片集可能不是集群中响应最快的/可用的。如果不需要这种一致性(例如，索引相对静态)，则可以通过指定<code>stable = false＆update = false</code>代替<code>stale = ok</code>或<code>stable = false＆update = lazy</code>代替<code>stale = update_after</code>。<br>视图信息不会被复制-它会在每个数据库上重建，因此您无法在单独的服务器上生成视图。</p>\n<h3 id=\"2-5-2-内置缩小功能\"><a href=\"#2-5-2-内置缩小功能\" class=\"headerlink\" title=\"2.5.2 内置缩小功能\"></a>2.5.2 内置缩小功能</h3><p>如果您使用的是非常简单的视图函数，仅执行求和或计数减少，则可以通过简单地编写<code>_sum</code>或<code>_count</code>代替函数声明来调用它们的本机<code>Erlang</code>实现。 这将大大加快速度，因为它减少了CouchDB和JavaScript查询服务器之间的IO。 例如，如邮件列表中所述，用于输出（已索引和缓存的）视图的时间大约为78,000个项目，时间从60秒减少到4秒。<br>之前：</p>\n<pre><code>{\n    &quot;_id&quot;: &quot;_design/foo&quot;,\n    &quot;views&quot;: {\n        &quot;bar&quot;: {\n            &quot;map&quot;: &quot;function (doc) { emit(doc.author, 1); }&quot;,\n            &quot;reduce&quot;: &quot;function (keys, values, rereduce) { return sum(values); }&quot;\n        }   \n    }\n}</code></pre><p>之后：</p>\n<pre><code>{\n    &quot;_id&quot;: &quot;_design/foo&quot;,\n    &quot;views&quot;: {\n        &quot;bar&quot;: {\n            &quot;map&quot;: &quot;function (doc) { emit(doc.author, 1); }&quot;,\n            &quot;reduce&quot;: &quot;_sum&quot;\n        }\n    }\n}</code></pre><h1 id=\"3-CouchDB备份\"><a href=\"#3-CouchDB备份\" class=\"headerlink\" title=\"3 CouchDB备份\"></a>3 CouchDB备份</h1><hr>\n<p>CouchDB在运行时可以创建三种不同类型的文件：</p>\n<ul>\n<li>数据库文件（包括二级索引）</li>\n<li>配置文件(<code>* .ini</code>)</li>\n<li>日志文件（如果配置为记录到磁盘）</li>\n</ul>\n<p>以下是确保所有这些文件的备份一致的策略。</p>\n<h2 id=\"3-1-数据库备份\"><a href=\"#3-1-数据库备份\" class=\"headerlink\" title=\"3.1 数据库备份\"></a>3.1 数据库备份</h2><p>CouchDB备份的最简单，最简单的方法是使用CouchDB复制到另一个CouchDB安装。您可以根据需要在普通（单次）复制或连续复制之间进行选择。<br>但是，您也可以随时从CouchDB数据目录(默认为<code>data/</code>)中复制实际的<code>.couch</code>文件，而不会出现问题。 CouchDB的数据库和二级索引的仅追加存储格式可确保这种方法可以正常工作。<br>为了确保备份的可靠性，建议先备份二级索引(存储在<code>data/.shards</code>下)，然后再备份主数据库文件(存储在<code>data/ shards</code>以及父级<code>data/</code>下的系统级数据库)目录)。这是因为CouchDB将在下一次读取访问时通过更新视图/二级索引来自动处理稍微过时的视图/二级索引，但是比其关联数据库新的视图或二级索引将触发索引的完全重建。这可能是一项非常昂贵且耗时的操作，并且会影响您在灾难情况下快速恢复的能力。<br>在受支持的操作系统/存储环境上，您还可以使用存储快照。这些优点是在使用块存储系统(例如<code>ZFS</code>或<code>LVM</code>或<code>Amazon EBS</code>)时几乎是即时的。在块存储级别使用快照时，请确保在必要时使用OS级实用程序(例如Linux的<code>fsfreeze</code>)使文件系统停顿。如果不确定，请查阅操作系统或云提供商的文档以获取更多详细信息。</p>\n<h2 id=\"3-2-配置备份\"><a href=\"#3-2-配置备份\" class=\"headerlink\" title=\"3.2 配置备份\"></a>3.2 配置备份</h2><p>CouchDB的配置系统将数据存储在配置目录(默认为<code>etc/</code>)下的<code>.ini</code>文件中。 如果在运行时对配置进行了更改，则配置链中的最后一个文件将使用更改进行更新。<br>从备份还原后，简单地备份整个<code>etc/</code>目录，以确保配置一致。<br>如果在运行时未通过HTTP API对配置进行任何更改，并且所有配置文件都由配置管理系统(例如<code>Ansible</code>或<code>Chef</code>)管理，则无需备份配置目录。</p>\n<h2 id=\"3-3-日志备份\"><a href=\"#3-3-日志备份\" class=\"headerlink\" title=\"3.3 日志备份\"></a>3.3 日志备份</h2><p>如果配置为记录到文件，则可能要备份CouchDB编写的日志文件。这些文件的任何备份解决方案都可以使用。<br>在类似UNIX的系统上，如果使用日志轮换软件，则必须采用“复制然后截断”的方法。创建副本后，这会将原始日志文件截断为零。CouchDB无法识别要关闭其日志文件并创建一个新信号的任何信号。因此，并且由于文件处理功能的差异，除了定期重新启动CouchDB进程外，在Microsoft Windows下没有简单的日志轮换解决方案。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://docs.couchdb.org/en/stable/maintenance/index.html\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1 id=\"1-压缩\"><a href=\"#1-压缩\" class=\"headerlink\" title=\"1 压缩\"></a>1 压缩</h1><hr>\n<p>压缩操作是通过从数据库或者视图索引文件中移除无用的和老的数据减少硬盘使用空间.操作非常简单类似于其他数据库(SQLite等)管理系统。<br>在压缩目标期间，CouchDB将创建扩展名为.compact的新文件，并将仅实际数据传输到该文件中。 因此，CouchDB首先检查可用磁盘空间-它应比压缩文件的数据大两倍。<br>当所有实际数据都成功传输到压缩文件后，CouchDB用目标文件替换目标文件。</p>\n<h2 id=\"1-1-数据库压缩\"><a href=\"#1-1-数据库压缩\" class=\"headerlink\" title=\"1.1 数据库压缩\"></a>1.1 数据库压缩</h2><p>数据库压缩通过删除更新期间创建的未使用的文件部分来压缩数据库文件。 旧文档修订版被少量称为<code>tombstone</code>的元数据代替，该元数据用于复制期间的冲突解决。 可以使用<code>_revs_limit</code>URL配置存储的修订版本（以及<code>tombstone</code>）的数量。<br>压缩是每个数据库手动触发的操作，并作为后台任务运行。要针对特定的数据库启动它，需要发送目标数据库的HTTP POST<code>/{db}/_compact</code>子资源：</p>\n<pre><code>curl -H &quot;Content-Type: application/json&quot; -X POST http://localhost:5984/my_db/_compact</code></pre><p>成功的话，将立即返回HTTP 状态码<code>202 Accepted</code>。</p>\n<pre><code>HTTP/1.1 202 Accepted\nCache-Control: must-revalidate Content-Length: 12\nContent-Type: text/plain; charset=utf-8 Date: Wed, 19 Jun 2013 09:43:52 GMT Server: CouchDB (Erlang/OTP)</code></pre><pre><code>{&quot;ok&quot;:true}</code></pre><p>尽管未使用请求主体，但仍必须为请求指定带有<code>application/json</code>值的<code>Content-Type</code>标头。否则，您会知道HTTP状态<code>415不支持的媒体类型响应</code>：</p>\n<pre><code>HTTP/1.1 415 Unsupported Media Type\nCache-Control: must-revalidate \nContent-Length: 78\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 09:43:44 GMT \nServer: CouchDB (Erlang/OTP)\n{&quot;error&quot;:&quot;bad_content_type&quot;,&quot;reason&quot;:&quot;Content-Type must be application/json&quot;}</code></pre><p>当压缩成功启动并运行时，可以通过数据库信息资源获取有关压缩的信息：</p>\n<pre><code>curl http://localhost:5984/my_db</code></pre><pre><code>HTTP/1.1 200 OK\nCache-Control: must-revalidate \nContent-Length: 246\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 16:51:20 GMT \nServer: CouchDB (Erlang/OTP)\n{\n    &quot;committed_update_seq&quot;: 76215, \n    &quot;compact_running&quot;: true, \n    &quot;data_size&quot;: 3787996, \n    &quot;db_name&quot;: &quot;my_db&quot;, \n    &quot;disk_format_version&quot;: 6, \n    &quot;disk_size&quot;: 17703025, \n    &quot;doc_count&quot;: 5091, \n    &quot;doc_del_count&quot;: 0, \n    &quot;instance_start_time&quot;: &quot;0&quot;, \n    &quot;purge_seq&quot;: 0,\n    &quot;update_seq&quot;: 76215\n}</code></pre><p>请注意，<code>compaction_running</code>字段为<code>true</code>，指示压缩实际上正在运行。 要跟踪压缩进度，可以查询<code>_active_tasks</code>资源：</p>\n<pre><code>curl http://localhost:5984/_active_tasks</code></pre><pre><code>HTTP/1.1 200 OK\nCache-Control: must-revalidate\nContent-Length: 175\nContent-Type: application/json \nDate: Wed, 19 Jun 2013 16:27:23 GMT \nServer: CouchDB (Erlang/OTP)\n[\n    {\n        &quot;changes_done&quot;: 44461, \n        &quot;database&quot;: &quot;my_db&quot;,\n        &quot;pid&quot;: &quot;&lt;0.218.0&gt;&quot;, \n        &quot;progress&quot;: 58,\n        &quot;started_on&quot;: 1371659228, \n        &quot;total_changes&quot;: 76215, \n        &quot;type&quot;: &quot;database_compaction&quot;, \n        &quot;updated_on&quot;: 1371659241\n    }\n]</code></pre><h2 id=\"1-2-视图压缩\"><a href=\"#1-2-视图压缩\" class=\"headerlink\" title=\"1.2 视图压缩\"></a>1.2 视图压缩</h2><p>与数据库视图不同，视图也需要像数据库一样进行压缩，这与按每个设计文档按组对数据库视图进行压缩不同。要启动其压缩，需要发送HTTP POST<code>/{db}/_compact/{ddoc}</code>请求：</p>\n<pre><code>curl -H &quot;Content-Type: application/json&quot; -X POST http://localhost:5984/dbname/_compact/designname</code></pre><pre><code>{&quot;ok&quot;:true}</code></pre><p>这将从指定设计文档的当前版本压缩视图索引。 HTTP响应代码<code>202 Accepted</code>(类似于数据库的压缩)，并且将创建压缩后台任务。</p>\n<h3 id=\"1-2-1视图清理\"><a href=\"#1-2-1视图清理\" class=\"headerlink\" title=\"1.2.1视图清理\"></a>1.2.1视图清理</h3><p>磁盘上的视图索引以视图定义的MD5哈希命名。更改视图时，旧索引仍保留在磁盘上。要清除所有过时的视图索引（以视图的MD5表示形式命名的文件，该文件不再存在），可以触发视图清除：</p>\n<pre><code>curl -H &quot;Content-Type: application/json&quot; -X POST http://localhost:5984/dbname/_view_cleanup</code></pre><pre><code>{&quot;ok&quot;:true}</code></pre><h2 id=\"1-3-自动压缩\"><a href=\"#1-3-自动压缩\" class=\"headerlink\" title=\"1.3 自动压缩\"></a>1.3 自动压缩</h2><p>虽然需要手动触发数据库和视图的压缩，但也可以配置自动压缩，以便基于各种条件自动触发数据库和视图的压缩。 在CouchDB的配置文件中配置了自动压缩。<br>守护程序<code>/compaction_daemon</code>负责触发压缩。 默认情况下启用它并自动启动。在压缩部分中配置了触发压缩的条件。</p>\n<h1 id=\"2-性能\"><a href=\"#2-性能\" class=\"headerlink\" title=\"2 性能\"></a>2 性能</h1><hr>\n<p>无论您如何编写代码，即使有了成千上万的文档，通常都会发现CouchDB可以很好地执行。但是一旦开始阅读数百万个文档，您需要更加小心。</p>\n<h2 id=\"2-1-硬盘IO\"><a href=\"#2-1-硬盘IO\" class=\"headerlink\" title=\"2.1 硬盘IO\"></a>2.1 硬盘IO</h2><h3 id=\"2-1-1-文件大小\"><a href=\"#2-1-1-文件大小\" class=\"headerlink\" title=\"2.1.1 文件大小\"></a>2.1.1 文件大小</h3><p>文件大小越小，I/O操作将越少，CouchDB和操作系统可以缓存的文件越多，复制，备份等的速度就越快。因此，您应该仔细检查所要存储的数据储存。例如，使用长度为数百个字符的键会很愚蠢，但是如果仅使用单个字符键，则程序将很难维护。通过将其放在视图中来仔细考虑重复的数据。</p>\n<h3 id=\"2-1-2-硬盘和文件系统性能\"><a href=\"#2-1-2-硬盘和文件系统性能\" class=\"headerlink\" title=\"2.1.2 硬盘和文件系统性能\"></a>2.1.2 硬盘和文件系统性能</h3><p>使用更快的磁盘，条带化RAID阵列和现代文件系统都可以加快CouchDB部署。但是，当磁盘性能成为瓶颈时，有一种方法可以提高CouchDB服务器的响应速度。 从文件模块的Erlang文档中：<br>在具有线程支持的操作系统上，可以让文件操作在其自己的线程中执行，从而允许其他Erlang进程继续与文件操作并行执行。 请参阅erl(1)中的命令行标志+A。<br>将此参数设置为大于零的数字可以使您的CouchDB安装保持响应状态，即使在磁盘使用率很高的时期也是如此。设置此选项的最简单方法是通过<code>ERL_FLAGS</code>环境变量。例如，要为Erlang提供执行I/O操作的四个线程，请将以下内容添加到<code>(prefix)/etc/defaults/couchdb</code>（或等效项）中：</p>\n<pre><code>export ERL_FLAGS=&quot;+A 4&quot;</code></pre><h2 id=\"2-2-系统资源限制\"><a href=\"#2-2-系统资源限制\" class=\"headerlink\" title=\"2.2 系统资源限制\"></a>2.2 系统资源限制</h2><p>管理员在其部署变大时会遇到的问题之一是系统和应用程序配置施加的资源限制。 提高这些限制可以使您的部署超出默认配置所支持的范围。</p>\n<h3 id=\"2-2-1-CouchDB配置选项\"><a href=\"#2-2-1-CouchDB配置选项\" class=\"headerlink\" title=\"2.2.1 CouchDB配置选项\"></a>2.2.1 CouchDB配置选项</h3><h4 id=\"delayed-commits\"><a href=\"#delayed-commits\" class=\"headerlink\" title=\"delayed_commits\"></a><code>delayed_commits</code></h4><p>延迟的提交允许在某些工作负载下实现更好的写入性能，同时牺牲少量的持久性。 该设置使CouchDB在更新后提交新数据之前要等待一整秒。如果服务器在写入标头之前崩溃，则自上次提交以来的所有写入都将丢失。 启用此选项需要您自担风险。</p>\n<h4 id=\"max-dbs-open\"><a href=\"#max-dbs-open\" class=\"headerlink\" title=\"max_dbs_open\"></a><code>max_dbs_open</code></h4><p>在配置(<code>local.ini</code>或类似版本)中,或者地址<code>couchdb/max_dbs_open</code>：</p>\n<pre><code>[couchdb]\nmax_dbs_open = 100</code></pre><p>此选项将一次可以打开的数据库数量设置为上限。CouchDB引用对内部数据库访问进行计数，并在必要时关闭空闲数据库。有时有必要一次保持超过默认值的速度，例如在许多数据库将被连续复制的部署中。</p>\n<h4 id=\"Erlang\"><a href=\"#Erlang\" class=\"headerlink\" title=\"Erlang\"></a><code>Erlang</code></h4><p>即使增加了CouchDB允许的最大连接数，默认情况下，Erlang运行时系统也将不允许超过1024个连接。 将以下指令添加到<code>(prefix)/etc/default/couchdb</code>(或等效文件)将增加此限制(在这种情况下，增加到4096)：</p>\n<pre><code>export ERL_MAX_PORTS=4096</code></pre><p>高达1.1.x的CouchDB版本还会为每个复制创建<code>Erlang Term Storage</code>(ETS)表。如果您使用的CouchDB版本早于1.2，并且必须支持许多复制，则还应设置<code>ERL_MAX_ETS_TABLES</code>变量。 默认值是大约1400表。<br>请注意，在Mac OS X上，Erlang实际上不会将文件描述符限制增加到超过1024（即系统标头定义的FD_SETSIZE值）。 </p>\n<h4 id=\"打开文件描述的最大数量-无限制\"><a href=\"#打开文件描述的最大数量-无限制\" class=\"headerlink\" title=\"打开文件描述的最大数量(无限制)\"></a>打开文件描述的最大数量(无限制)</h4><p>大多数<code>*nix</code>操作系统在每个进程上都有各种资源限制。增加这些限制的方法因初始化系统和特定的OS版本而异。许多操作系统的默认值为1024或4096。在具有许多数据库或视图的系统上，CouchDB可以非常迅速地达到此限制。<br>如果您的系统设置为使用可插拔身份验证模块(<code>PAM</code>)系统（几乎所有现代Linux都是这种情况），则增加此限制很简单。例如，创建具有以下内容的名为<code>/etc/security/limits.d/100-couchdb.conf</code>的文件将确保CouchDB可以一次打开多达10000个文件描述符：</p>\n<pre><code>#&lt;domain&gt;  &lt;type&gt;    &lt;item&gt;  &lt;value&gt;\ncouchdb    hard      nofile  10000 \ncouchdb    soft      nofile  10000</code></pre><p>如果使用的是Debian/Ubuntu sysvinit脚本(<code>/etc/init.d/couchdb</code>，则还需要提高root用户的限制：</p>\n<pre><code>#&lt;domain&gt;    &lt;type&gt;   &lt;item&gt;  &lt;value&gt;\nroot         hard    nofile   10000\nroot         soft    nofile   10000</code></pre><p>您可能还需要编辑<code>/etc/pam.d/common-session</code>和<code>/etc/pam.d/common-session-noninteractive</code>文件以添加以下行：</p>\n<pre><code>session required pam_limits.so</code></pre><p>如果还不存在。<br>对于基于系统的Linux（例如CentOS/RHEL 7，Ubuntu 16.04 +，Debian 8或更高版本）,假设您要从systemd启动CouchDB，则还必须通过创建文件<code>/etc/systemd/system/&lt;servicename&gt;.d/override.conf</code>添加以下内容：</p>\n<pre><code>[Service]\nLimitNOFILE=#######</code></pre><p>并将<code>#######</code>替换为文件描述符的上限CouchDB允许立即保持打开状态。<br>如果您的系统不使用<code>PAM</code>，通常可以在自定义脚本中使用<code>ulimit</code>命令来启动<br>CouchDB具有增加的资源限制。 典型的语法类似于<code>ulimit -n 10000</code>。<br>通常，类似UNIX的现代系统每个进程可以处理大量文件句柄（例如100000）<br>没有问题。 不要害怕增加系统限制。</p>\n<h2 id=\"2-3-网络\"><a href=\"#2-3-网络\" class=\"headerlink\" title=\"2.3 网络\"></a>2.3 网络</h2><p>产生和接收每个请求/响应都有延迟开销。通常，您应该分批执行请求。大多数API具有某种批处理机制，通常是通过在请求正文中提供文档或键的列表来进行的。请注意为批次选择的大小。较大的批处理需要更多的时间来使客户将项目编码为  <code>JSON</code>，并将更多的时间用于解码该数量的响应。使用您自己的配置和典型数据进行一些基准测试，以找到最佳位置。 它可能在一到一万个文档之间。<br>如果您拥有快速的I/O系统，那么您也可以使用并发-同时具有多个请求/响应。这减轻了组装JSON，进行网络连接和解码<code>JSON</code>所涉及的延迟。<br>从CouchDB 1.1.0开始，与旧版本相比，用户经常报告文档的写入性能较低。主要原因是此版本随附HTTP服务器库<code>MochiWeb</code>的最新版本，该库默认情况下将<code>TCP</code>套接字选项<code>SO_NODELAY</code>设置为<code>false</code>。这意味着发送到TCP套接字的小数据（例如对文档写请求的答复（或读取非常小的文档）的答复）不会立即发送到网络<code>TCP</code>将对其缓冲一会儿，希望它会被询问通过同一套接字发送更多数据，然后一次发送所有数据以提高性能。 可以通过<code>httpd/socket_options</code>禁用此<code>TCP</code>缓冲行为：</p>\n<pre><code>[httpd]\nsocket_options = [{nodelay, true}]</code></pre><h3 id=\"2-3-1-连接限制\"><a href=\"#2-3-1-连接限制\" class=\"headerlink\" title=\"2.3.1 连接限制\"></a>2.3.1 连接限制</h3><p><code>MochiWeb</code>处理<code>CouchDB</code>请求。默认最大连接数为2048。要更改此限制，请使用<code>server_options</code>配置变量。 <code>max</code>表示最大连接数。</p>\n<pre><code>[chttpd]\nserver_options = [{backlog, 128}, {acceptor_pool_size, 16}, {max, 4096}]</code></pre><h2 id=\"2-4-CouchDB\"><a href=\"#2-4-CouchDB\" class=\"headerlink\" title=\"2.4 CouchDB\"></a>2.4 CouchDB</h2><h3 id=\"2-4-1-删除操作\"><a href=\"#2-4-1-删除操作\" class=\"headerlink\" title=\"2.4.1 删除操作\"></a>2.4.1 删除操作</h3><p>当您删除文档时，数据库将创建一个新的修订版，其中包含<code>_id</code>和<code>_rev</code>字段以及<code>_deleted</code>标志。即使在数据库压缩后，此修订版仍将保留，以便可以复制删除内容。像未删除的文档一样，已删除的文档可能会影响视图生成时间，<code>PUT</code>和<code>DELETE</code>请求时间以及数据库的大小，因为它们会增加<code>B+Tree</code>的大小。您可以在数据库信息中看到已删除文档的数量。如果您的用例创建了许多已删除的文档（例如，如果您存储日志条目，消息队列等短期数据），则可能需要定期切换到新数据库并删除已过期的旧数据库）。</p>\n<h3 id=\"2-4-2-文档ID\"><a href=\"#2-4-2-文档ID\" class=\"headerlink\" title=\"2.4.2 文档ID\"></a>2.4.2 文档ID</h3><p>数据库文件的大小源自您的文档和视图大小，但也取决于您的<code>_id</code>大小的倍数。<code>_id</code>不仅存在于文档中，而且它及其部分内容在<code>CouchDB</code>用来导航文件以首先找到文档的二叉树结构中也是重复的。作为一个现实世界的例子，一个用户从16个字节的ID切换到4个字节的ID，使数据库从21GB变为4GB，包含1000万个文档（从2.5GB到2GB的原始JSON文本）。<br>插入具有顺序（至少已排序）的ID的速度要比随机ID快。 因此，您应该考虑自己生成id，按顺序分配它们，并使用消耗更少字节的编码方案。例如，可以用4个基数62个数字（用10个数字，26个小写字母，26个大写字母）来完成需要16个十六进制数字表示的内容。</p>\n<h2 id=\"2-5-视图\"><a href=\"#2-5-视图\" class=\"headerlink\" title=\"2.5 视图\"></a>2.5 视图</h2><h3 id=\"2-5-1-视图生成\"><a href=\"#2-5-1-视图生成\" class=\"headerlink\" title=\"2.5.1 视图生成\"></a>2.5.1 视图生成</h3><p>当要处理的文档数量非常少时，使用JavaScript查询服务器生成的视图非常慢。生成过程甚至不会使单个CPU饱和，更不用说您的I/O了。原因是CouchDB服务器和单独的<code>couchjs</code>查询服务器中涉及的延迟，这显着表明了从实施中消除延迟的重要性。<br>您可以让视图访问权限“过时”，但要确定何时发生会给您带来快速响应以及何时更新视图会花费很长时间，这是不切实际的。(一个拥有1000万个文档的数据库大约需要10分钟才能加载到CouchDB中，而生成视图需要大约4个小时)。<br>在集群中，“陈旧的”请求由一组固定的分片服务，以便为用户提供请求之间的一致结果。这需要进行可用性权衡-固定的分片集可能不是集群中响应最快的/可用的。如果不需要这种一致性(例如，索引相对静态)，则可以通过指定<code>stable = false＆update = false</code>代替<code>stale = ok</code>或<code>stable = false＆update = lazy</code>代替<code>stale = update_after</code>。<br>视图信息不会被复制-它会在每个数据库上重建，因此您无法在单独的服务器上生成视图。</p>\n<h3 id=\"2-5-2-内置缩小功能\"><a href=\"#2-5-2-内置缩小功能\" class=\"headerlink\" title=\"2.5.2 内置缩小功能\"></a>2.5.2 内置缩小功能</h3><p>如果您使用的是非常简单的视图函数，仅执行求和或计数减少，则可以通过简单地编写<code>_sum</code>或<code>_count</code>代替函数声明来调用它们的本机<code>Erlang</code>实现。 这将大大加快速度，因为它减少了CouchDB和JavaScript查询服务器之间的IO。 例如，如邮件列表中所述，用于输出（已索引和缓存的）视图的时间大约为78,000个项目，时间从60秒减少到4秒。<br>之前：</p>\n<pre><code>{\n    &quot;_id&quot;: &quot;_design/foo&quot;,\n    &quot;views&quot;: {\n        &quot;bar&quot;: {\n            &quot;map&quot;: &quot;function (doc) { emit(doc.author, 1); }&quot;,\n            &quot;reduce&quot;: &quot;function (keys, values, rereduce) { return sum(values); }&quot;\n        }   \n    }\n}</code></pre><p>之后：</p>\n<pre><code>{\n    &quot;_id&quot;: &quot;_design/foo&quot;,\n    &quot;views&quot;: {\n        &quot;bar&quot;: {\n            &quot;map&quot;: &quot;function (doc) { emit(doc.author, 1); }&quot;,\n            &quot;reduce&quot;: &quot;_sum&quot;\n        }\n    }\n}</code></pre><h1 id=\"3-CouchDB备份\"><a href=\"#3-CouchDB备份\" class=\"headerlink\" title=\"3 CouchDB备份\"></a>3 CouchDB备份</h1><hr>\n<p>CouchDB在运行时可以创建三种不同类型的文件：</p>\n<ul>\n<li>数据库文件（包括二级索引）</li>\n<li>配置文件(<code>* .ini</code>)</li>\n<li>日志文件（如果配置为记录到磁盘）</li>\n</ul>\n<p>以下是确保所有这些文件的备份一致的策略。</p>\n<h2 id=\"3-1-数据库备份\"><a href=\"#3-1-数据库备份\" class=\"headerlink\" title=\"3.1 数据库备份\"></a>3.1 数据库备份</h2><p>CouchDB备份的最简单，最简单的方法是使用CouchDB复制到另一个CouchDB安装。您可以根据需要在普通（单次）复制或连续复制之间进行选择。<br>但是，您也可以随时从CouchDB数据目录(默认为<code>data/</code>)中复制实际的<code>.couch</code>文件，而不会出现问题。 CouchDB的数据库和二级索引的仅追加存储格式可确保这种方法可以正常工作。<br>为了确保备份的可靠性，建议先备份二级索引(存储在<code>data/.shards</code>下)，然后再备份主数据库文件(存储在<code>data/ shards</code>以及父级<code>data/</code>下的系统级数据库)目录)。这是因为CouchDB将在下一次读取访问时通过更新视图/二级索引来自动处理稍微过时的视图/二级索引，但是比其关联数据库新的视图或二级索引将触发索引的完全重建。这可能是一项非常昂贵且耗时的操作，并且会影响您在灾难情况下快速恢复的能力。<br>在受支持的操作系统/存储环境上，您还可以使用存储快照。这些优点是在使用块存储系统(例如<code>ZFS</code>或<code>LVM</code>或<code>Amazon EBS</code>)时几乎是即时的。在块存储级别使用快照时，请确保在必要时使用OS级实用程序(例如Linux的<code>fsfreeze</code>)使文件系统停顿。如果不确定，请查阅操作系统或云提供商的文档以获取更多详细信息。</p>\n<h2 id=\"3-2-配置备份\"><a href=\"#3-2-配置备份\" class=\"headerlink\" title=\"3.2 配置备份\"></a>3.2 配置备份</h2><p>CouchDB的配置系统将数据存储在配置目录(默认为<code>etc/</code>)下的<code>.ini</code>文件中。 如果在运行时对配置进行了更改，则配置链中的最后一个文件将使用更改进行更新。<br>从备份还原后，简单地备份整个<code>etc/</code>目录，以确保配置一致。<br>如果在运行时未通过HTTP API对配置进行任何更改，并且所有配置文件都由配置管理系统(例如<code>Ansible</code>或<code>Chef</code>)管理，则无需备份配置目录。</p>\n<h2 id=\"3-3-日志备份\"><a href=\"#3-3-日志备份\" class=\"headerlink\" title=\"3.3 日志备份\"></a>3.3 日志备份</h2><p>如果配置为记录到文件，则可能要备份CouchDB编写的日志文件。这些文件的任何备份解决方案都可以使用。<br>在类似UNIX的系统上，如果使用日志轮换软件，则必须采用“复制然后截断”的方法。创建副本后，这会将原始日志文件截断为零。CouchDB无法识别要关闭其日志文件并创建一个新信号的任何信号。因此，并且由于文件处理功能的差异，除了定期重新启动CouchDB进程外，在Microsoft Windows下没有简单的日志轮换解决方案。</p>\n"},{"title":"CouchDB学习-API","date":"2019-12-24T02:52:40.000Z","_content":"# API\n\nAPI URL路径可以指定访问CouchDB服务器的某个组件。URL请求结果包括标识和访问的数据库中的高效的描述字段。\n与所有URL一样，各个组件之间用正斜杠分隔。\n通常，以_（下划线）字符开头的URL组件和JSON字段表示服务器或返回的对象中的特殊组件或实体。例如，URL片段`/_all_dbs`获取CouchDB实例中所有数据库的列表。\n该引用根据URL结构进行构造，如下所示。\n\n## 1 基本API\n\nCouchDB API是与CouchDB实例接口的主要方法。使用HTTP发出请求，请求用于从数据库请求信息，存储新数据以及对文档中存储的信息进行查看和格式化。\n对API的请求可以按您正在访问的CouchDB系统的不同区域以及用于发送请求的HTTP方法进行分类。不同的方法意味着不同的操作，例如，从数据库中检索信息通常由`GET`操作处理，而更新则由`POST`或`PUT`请求处理。不同方法必须提供的信息之间存在一些差异。有关基本HTTP方法和请求结构的指南，请参见请求格式和响应。\n对于几乎所有操作，都在JavaScript对象表示法（JSON）对象中定义了提交的数据和返回的数据结构。 JSON基础知识中提供了有关JSON内容和数据类型的基本信息。\n使用标准HTTP状态代码报告访问CouchDB API时的错误。 HTTP状态代码中提供了有关CouchDB返回的通用代码的指南。\n访问CouchDB API的特定区域时，将提供有关HTTP方法和请求，JSON结构和错误代码的特定信息和示例。\n\n### 1.1 请求格式和响应\n\nCouchDB支持以下HTTP请求方法：\n\n* `GET`:请求指定的条目。\n* `HEAD`：获取HTTP请求头部信息\n* `POST`：上传数据\n* `PUT`：用于PUT指定的资源，如创建新的对象(数据库，文档，视图，设计文档)\n* `DELETE`：删除指定的资源\n* `COPY`：特殊的方法，用于拷贝文档和对象\n\n如果使用CouchDB不支持的HTTP请求类型，将会返回状态码`405-Method Not Allowed`.\n\n### 1.2 HTTP请求头\n\nCouchDB使用HTTP进行所有通信。所以需要确保正确的并且是被支持的HTTP头部信息。\n\n#### 1.2.1 请求头\n\n* `Accept`:由服务器返回指定的被接受的数据类型列表\n* `Content-type`：指定请求中提供的信息的内容类型\n\n#### 1.2.2 响应头\n\n* `Cache-control`：缓存控制\n* `Content-length`：返回的内容的长度\n* `Content-type`：指定的返回数据的MIME类型\n* `Etag`：显示文档或者视图的修订版本\n* `Transfer-Encoding`：如果响应使用编码，那么则在该字段中指定\n\n\n### 1.3 JSON基础\n### 1.4 HTTP状态码\n\n* 200 - `OK`：请求完全成功\n* 201 - `Created`：文档成功创建\n* 202 - `Accepted`：请求被接受，但是相关的操作可能不完整，经常用于后台操作如数据库压缩。\n* 304 - `Not Modified`：请求的其他内容尚未修改。\n* 400 - `Bad Request`：坏的请求结构，如错误的请求URL，路径或请求头。\n* 401 - `Unauthorized`：不具备获取指定数据的权限，或者权限不支持\n* 403 - `Forbidden`：请求被服务器拒绝\n* 404 - `Not Found`：没有找到请求的数据\n* 405 - `Method Not Allowed`：请求方法不被支持\n* 406 - `Not Acceptable`：请求的内容类型不被支持\n* 409 - `Conflict`：更新数据冲突\n* 412 - `Preconfition Failed`：客户端的请求头和服务器的兼容性不匹配\n* 413 - `Request Entity Too Large`：请求的数据过大\n* 415 - `Unsupported Media Type`：支持的内容类型以及正在请求或提交的信息的内容类型表示不支持该内容类型。\n* 416 - `Requested Range Not Satisfiable`：服务器无法满足请求标头中指定的范围。\n* 417 - `Expectation Failed`：批量发送文档时，批量加载操作失败。\n* 500 - `Internal Server Error`：请求无效\n\n\n## 2 服务器\n### 2.1 `/`\n`GET /`:访问CouchDB实例的Root并返回关于实例的元数据。\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n\n### 2.2 `/_active_tasks`\n\n`GET /_active_tasks`:列出运行中的任务，包括任务类型，名字，状态和进程ID。\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**响应JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `changes_done(number):`处理的变更\n&emsp;&emsp;&emsp;&emsp;    * `database(string):`源数据库\n&emsp;&emsp;&emsp;&emsp;    * `pid(string):`进程ID\n&emsp;&emsp;&emsp;&emsp;    * `progress(number):`当前进度百分比\n&emsp;&emsp;&emsp;&emsp;    * `started_on(number):`任务开始时间\n&emsp;&emsp;&emsp;&emsp;    * `status(string):`任务状态信息\n&emsp;&emsp;&emsp;&emsp;    * `task(string):`任务名称\n&emsp;&emsp;&emsp;&emsp;    * `total_changes(number):`进程总的改变\n&emsp;&emsp;&emsp;&emsp;    * `type(string):`操作类型\n&emsp;&emsp;&emsp;&emsp;    * `update_on(number):`最后一次更新时间\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 401 `Unauthorized`\n\n### 2.3 `/_all_dbs`\n\n`GET /_all_dbs`:返回CouchDB实例所有数据库列表\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**请求参数：**\n&emsp;&emsp;&emsp;&emsp;    * `descending(boolean):`按键降序返回数据库。 默认为false。\n&emsp;&emsp;&emsp;&emsp;    * `endkey(json):`到达指定的键时，停止返回数据库。\n&emsp;&emsp;&emsp;&emsp;    * `end_key(json):`endkey的别名\n&emsp;&emsp;&emsp;&emsp;    * `limit(number):`返回数据库数量的限制\n&emsp;&emsp;&emsp;&emsp;    * `skip(number):`跳过此值得数据库，默认为0\n&emsp;&emsp;&emsp;&emsp;    * `startkey(json):`从指定的键返回数据库\n&emsp;&emsp;&emsp;&emsp;    * `start_key(json):`startkey的别名\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * `200 OK`\n\n### 2.4 `/_dbs_info`\n\n`GET /_dbs_info`:返回CouchDB实例所有数据库列表的数据库信息\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**请求JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `keys(array):`被请求的数据库名字(数组)\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n\n### 2.5 `/_cluster_setup`\n\n`GET /_cluster_setup`:根据群集设置向导返回节点或群集的状态。\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**请求参数：**\n&emsp;&emsp;&emsp;&emsp;    * `ensure_dbs_exist(array):`列出系统数据库确保在节点/集群上存在,默认：`[\"_users\",\"_replicator\", \"_global_changes\"]`.\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**响应JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `state(string):`节点/集群的当前状态(见下面)\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n\n**STATE**\n\n* `cluster_disabled`:当前节点完全没有被配置。\n* `single_node_disabled`：当前节点被配置为单节点，不具备服务器基本的管理员用户定义并且完整的标准系统数据库没有被创建。如果指定了`ensure_dbs_exist`查询参数，则提供的数据库列表将覆盖标准系统数据库的默认列表。\n* `single_node_enabled`：当前节点被配置为单节点，具有服务器基本的管理员用户定义并且`ensure_dbs_exist`列表中的数据库已经被创建。\n* `cluster_enabled`：当前节点集群数量大于1，没有绑定在`127.0.0.1`，具有服务器基本的管理员用户定义但是完整的标准系统数据库没有被创建。如果指定了`ensure_dbs_exist`查询参数，则提供的数据库列表将覆盖标准系统数据库的默认列表。\n* `cluster_finished`：当前节点集群数量大于1，没有绑定在`127.0.0.1`，具有服务器基本的管理员用户定义并且`ensure_dbs_exist`列表中的数据库已经被创建。\n\n`POST /_cluster_setup`:配置一个节点作为单节点，或者是集群的一部分，或者完成集群。\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**请求JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `action(string):`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `enable_single_node:`配置当前节点为单节点\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `enable_cluster:`配置本地或远程节点，准备将它添加到新的CouchDB集群\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `add_node:`添加一个指定的远程节点到该集群列表中\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `finish_cluster:`完成集群的创建并创建标准的系统数据库\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `bind_address(string):`当前节点绑定的IP地址\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `username(string):`服务器级别的管理员用户名\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `password(string):`服务器级别的管理员密码\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `port(number):`该节点或远程节点(只用于`add_node`)绑定的TCP端口\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `node_count(number):`集群中节点总数\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `remote_node(string):`配置集群中的远程该节点的IP地址\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `remote_current_user(string):`服务器级别的管理员授权到远程节点的管理员用户名\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `remote_current_password(string):`服务器级别的管理员授权到远程节点的管理员密码\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `host(string):`添加到集群的远程节点的IP地址\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `ensure_dbs_exist(array):`列出系统数据库确保在该节点上存在。\n\n### 2.6 `/_db_updates`\n\n`GET /_db_updates`:返回CouchDB实例上所有数据库事件列表\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**请求参数：**\n&emsp;&emsp;&emsp;&emsp;    * `feed(string)`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `normal`:返回所有数据库历史变化，然后关闭连接。\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `longpoll`:在第一次事件后关闭连接\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `continuous`:每个事件发送一行JSON，一直保持socket开启直到超时。\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `eventsource`:与`continuous`相似，只是以`EventSource`格式发送。\n&emsp;&emsp;&emsp;&emsp;    * `timeout(number)`：指定多少秒后关闭CouchDB连接\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `heartbeat(number)`:每隔多少周期毫秒发送空行保持连接，默认60000\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `since(string)`:仅返回指定的序列ID更新\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**响应JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `results(array)`:数据库事件的数组\n&emsp;&emsp;&emsp;&emsp;    * `last_seq(string)`:记录的最后一个序列ID\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 404 `Unauthorized`\n**JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `db_name(string):`数据库名称\n&emsp;&emsp;&emsp;&emsp;    * `type(string):`数据库事件类型(`created,updated,deleted`)\n&emsp;&emsp;&emsp;&emsp;    * `seq(json):`事件更新序列\n\n### 2.7 `/_membership`\n\n`GET /_membership`:显示`cluster_nodes`集群中的部分节点。 `all_nodes`字段显示该节点知道的所有节点，包括属于集群的节点。在设置集群时非常有用，\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n### 2.8 `/_replicate`\n`GET /_replicate`:请求，配置或停止复制操作\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type:` `application/json`\n**请求JSON对象:**\n&emsp;&emsp;&emsp;&emsp;        * `cancel(boolean):`取消复制\n&emsp;&emsp;&emsp;&emsp;        * `continuous(boolean):`继续复制\n&emsp;&emsp;&emsp;&emsp;        * `create_target(boolean):`创建目标数据库，要求管理员权限\n&emsp;&emsp;&emsp;&emsp;        * `doc_ids(array):`同步的文档ID的数组\n&emsp;&emsp;&emsp;&emsp;        * `filter(string):`过滤器函数的名字\n&emsp;&emsp;&emsp;&emsp;        * `proxy(string):`代理服务器的地址\n&emsp;&emsp;&emsp;&emsp;        * `source(string/object):`源数据库名字或URL或对象，包含完整的源数据库URL以及参数例如头部信息\n&emsp;&emsp;&emsp;&emsp;        * `target(string/object):`目标数据库名字或URL或对象，包含完整的目标数据库URL以及参数例如头部信息\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**响应JSON对象:**\n&emsp;&emsp;&emsp;&emsp;        * `history(array):`复制历史(见下面)\n&emsp;&emsp;&emsp;&emsp;        * `ok(boolean):`复制状态\n&emsp;&emsp;&emsp;&emsp;        * `replication_id_version(number):`复制协议版本\n&emsp;&emsp;&emsp;&emsp;        * `session_id(string):`唯一的sessionID\n&emsp;&emsp;&emsp;&emsp;        * `source_last_seq(number):`从源数据库读取的最后的序列号\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 202 `Accepted`\n&emsp;&emsp;&emsp;&emsp;    * 400 `Bad Request`\n&emsp;&emsp;&emsp;&emsp;    * 401 `Unauthorized`\n&emsp;&emsp;&emsp;&emsp;    * 404 `Not Found`\n&emsp;&emsp;&emsp;&emsp;    * 500 `Internal Server Error`\n**JSON对象:**\n&emsp;&emsp;&emsp;&emsp;        * `doc_write_failures(number):`文档写失败的数量\n&emsp;&emsp;&emsp;&emsp;        * `docs_read(number):`文档读取的数量\n&emsp;&emsp;&emsp;&emsp;        * `docs_written (number):`文档成功写到目标的数量\n&emsp;&emsp;&emsp;&emsp;        * `end_last_seq (number):`更新流中最后的序列号\n&emsp;&emsp;&emsp;&emsp;        * `end_time (string):`复制操作完成的时间\n&emsp;&emsp;&emsp;&emsp;        * `missing_checked (number):`检查到的缺失的文档数量\n&emsp;&emsp;&emsp;&emsp;        * `missing_found (number):`缺失的文档被发现的数量\n&emsp;&emsp;&emsp;&emsp;        * `recorded_seq (number):`记录的最后的序列号\n&emsp;&emsp;&emsp;&emsp;        * `session_id (string):`这次复制操作的SessionID\n&emsp;&emsp;&emsp;&emsp;        * `start_last_seq (number):`更新流中第一个序列号\n&emsp;&emsp;&emsp;&emsp;        * `start_time (string):`复制操作开始的时间\n\n#### 2.8.1 复制操作\n\n复制的目的是，在过程结束时，源数据库上的所有活动文档也都在目标数据库中，并且在源数据库中删除的所有文档也都在目标数据库上被删除（如果存在）。\n复制可以描述为推式或拉式复制：\n&emsp;&emsp;&emsp;&emsp; * 拉复制是当远程CouchDB实例是源数据库，目标是本地数据库的位置。\n如果源数据库具有永久IP地址，而目标（本地）数据库可能具有动态分配的IP地址（例如，通过DHCP），则Pull复制是最有用的解决方案。 如果要从中央服务器复制到移动设备或其他设备，则这尤其重要。\n&emsp;&emsp;&emsp;&emsp; * 推复制是当本地数据库为源数据库，远程数据库为目标数据库。\n\n#### 2.8.2 指定源和目标数据库\n\n如果要在以下两种情况之一中执行复制，则必须使用CouchDB数据库的URL规范：\n&emsp;&emsp;&emsp;&emsp; * 使用远程数据库进行复制（即CouchDB的另一个实例在同一主机或其他主机上）\n&emsp;&emsp;&emsp;&emsp; * 数据库复制需要权限认证。\n例如，要请求向其发送请求的CouchDB实例本地数据库和远程数据库之间复制，可以使用以下请求：\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    \"source\" : \"recipes\",\n    \"target\" : \"http://coucdb-remote:5984/recipes\",\n}\n```\n在所有情况下，源规范和目标规范中所请求的数据库都必须存在。 如果不这样做，则将在JSON对象内返回错误：\n```\n{\n    \"reason\" : \"could not open http://couchdb-remote:5984/ol1ka/\",\n}\n```\n您可以通过将`create_target`字段添加到请求对象来创建目标数据库（只要您的用户凭据允许）：\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    \"create_target\" : true\n    \"source\" : \"recipes\",\n    \"target\" : \"http://couchdb-remote:5984/recipes\",\n}\n```\n`create_target`字段不是破坏性的。 如果数据库已经存在，则复制将正常进行。\n\n#### 2.8.3 单个复制\n\n您可以请求复制数据库，以便可以同步两个数据库。 默认情况下，复制过程会发生一次，并将两个数据库同步在一起。 例如，您可以通过在请求JSON内容中提供`source`和`target`字段来请求两个数据库之间的单个同步。\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    \"source\" : \"recipes\",\n    \"target\" : \"recipes-snapshot\",\n}\n```\n在上面的示例中，数据库配方和配方快照将被同步。 这些数据库是发出请求的CouchDB实例的本地数据库。响应将是一个JSON结构，其中包含同步过程的成功（或失败），以及有关该过程的统计信息：\n```\n{\n    \"ok\" : true,\n    \"history\" : [\n        {\n            \"docs_read\" : 1000,\n            \"session_id\" : \"52c2370f5027043d286daca4de247db0\",\n            \"recorded_seq\" : 1000,\n            \"end_last_seq\" : 1000,\n            \"doc_write_failures\" : 0,\n            \"start_time\" : \"Thu, 28 Oct 2010 10:24:13 GMT\",\n            \"start_last_seq\" : 0,\n            \"end_time\" : \"Thu, 28 Oct 2010 10:24:14 GMT\",\n            \"missing_checked\" : 0,\n            \"docs_written\" : 1000,\n            \"missing_found\" : 1000\n        }\n    ],\n    \"session_id\" : \"52c2370f5027043d286daca4de247db0\",\n    \"source_last_seq\" : 1000\n}\n```\n\n#### 2.8.4 继续复制\n\n在执行复制请求时，数据库与以前提到的方法的同步仅发生一次。 要使目标数据库从源永久复制，您必须将请求内JSON对象的Continuous字段设置为true。\n通过连续复制，源数据库中的更改将永久复制到目标数据库，直到您明确要求停止复制为止。\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    \"continuous\" : true\n    \"source\" : \"recipes\",\n    \"target\" : \"http://couchdb-remote:5984/recipes\",\n}\n```\n只要两个实例之间存在网络连接，更改就会在两个数据库之间复制。\n两个保持彼此同步的数据库，您需要在两个方向上设置复制；也就是说，您必须从源复制到目标，并且必须从目标复制到另一个。\n\n#### 2.8.5 取消继续复制\n\n您可以通过将`cancel`字段添加到JSON请求对象并将值设置为`true`来取消连续复制。请注意，请求的结构必须与原始结构相同，才能兑现取消请求。例如，如果您请求连续复制，则取消请求还必须包含连续字段。\n例如，复制请求：\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    \"source\" : \"recipes\",\n    \"target\" : \"http://couchdb-remote:5984/recipes\", \"create_target\" : true,\n    \"continuous\" : true\n}\n```\n必须使用请求取消复制：\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    \"cancel\" : true,\n    \"continuous\" : true\n    \"create_target\" : true,\n    \"source\" : \"recipes\",\n    \"target\" : \"http://couchdb-remote:5984/recipes\",\n}\n```\n请求取消不存在的复制将导致404错误。\n\n### 2.9 `/_scheduler/jobs`\n\n`GET /_scheduler/jobs`:列出复制任务\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**请求参数**\n&emsp;&emsp;&emsp;&emsp;    * `limit(number)：`返回多少数量的结果\n&emsp;&emsp;&emsp;&emsp;    * `skip(number):`跳过多少数量的结果，以复制ID排序\n**响应JSON对象**\n&emsp;&emsp;&emsp;&emsp;    * `offset (number)：`多少数量的结果被跳过\n&emsp;&emsp;&emsp;&emsp;    * `total_rows (number) ：`复制任务的总数量\n&emsp;&emsp;&emsp;&emsp;    * `id (string)：`复制ID\n&emsp;&emsp;&emsp;&emsp;    * `database (string)：`复制文档数据库\n&emsp;&emsp;&emsp;&emsp;    * `doc_id (string)：`复制文档ID\n&emsp;&emsp;&emsp;&emsp;    * `history (list)：`事件的时间戳历史以对象列表展示\n&emsp;&emsp;&emsp;&emsp;    * `pid (string)：`复制进程ID\n&emsp;&emsp;&emsp;&emsp;    * `node (string)：`运行任务的集群中的节点\n&emsp;&emsp;&emsp;&emsp;    * `source (string)：`复制源头\n&emsp;&emsp;&emsp;&emsp;    * `target (string)：`复制目标\n&emsp;&emsp;&emsp;&emsp;    * `start_time (string)：`开始复制的时间戳\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 401 `Unauthorized`\n\n### 2.10 `/_scheduler/docs`\n\n`GET /_scheduler/docs`:列出复制文档的状态\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**请求参数：**\n&emsp;&emsp;&emsp;&emsp;    * `limit(number):`返回多少结果\n&emsp;&emsp;&emsp;&emsp;    * `skip(number):`跳过多少数量的结果，以文档ID排序\n**响应JSON对象**\n&emsp;&emsp;&emsp;&emsp;    * `offset (number)：`多少数量的结果被跳过\n&emsp;&emsp;&emsp;&emsp;    * `total_rows (number) ：`复制文档的总数量\n&emsp;&emsp;&emsp;&emsp;    * `id (string)：`复制ID或者当复制状态为完成或失败时为空\n&emsp;&emsp;&emsp;&emsp;    * `state(string)：`复制状态(`initializing,running,completed,pending,crashing,error,failed`)\n&emsp;&emsp;&emsp;&emsp;    * `database (string)：`复制文档的目标数据库\n&emsp;&emsp;&emsp;&emsp;    * `doc_id (string)：`复制文档ID\n&emsp;&emsp;&emsp;&emsp;    * `last_update(string)：`最后一次更新的时间\n&emsp;&emsp;&emsp;&emsp;    * `info(object)：`关于状态的可能的额外信息\n&emsp;&emsp;&emsp;&emsp;    * `node (string)：`运行任务的集群中的节点\n&emsp;&emsp;&emsp;&emsp;    * `source (string)：`复制源头\n&emsp;&emsp;&emsp;&emsp;    * `target (string)：`复制目标\n&emsp;&emsp;&emsp;&emsp;    * `start_time (string)：`开始复制的时间戳\n&emsp;&emsp;&emsp;&emsp;    * `error_count(number) ：`复制出现错误的数量\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 401 `Unauthorized`\n**JSON对象**\n&emsp;&emsp;&emsp;&emsp;    * `revisions_checked (number):`在复制开始时被检查的修订版本的数量\n&emsp;&emsp;&emsp;&emsp;    * `missing_revisions_found(number):`在源处有而目标处没有的修订版本的数量\n&emsp;&emsp;&emsp;&emsp;    * `docs_read (number) :`从源处读取的文档的数量\n&emsp;&emsp;&emsp;&emsp;    * `docs_written (number) :`写到目标的文档的数量\n&emsp;&emsp;&emsp;&emsp;    * `changes_pending (number):`还没有复制完成的数量\n&emsp;&emsp;&emsp;&emsp;    * `doc_write_failures (number) :`写目标失败的数量\n&emsp;&emsp;&emsp;&emsp;    * `checkpointed_source_seq (object):`最后一个从源处成功复制的序列ID\n\n### 2.11 `/_node/{node-name}/_stats`\n\n\n### 2.12 `/_node/{node-name}/_system`\n\n\n### 2.13 `/_node/{node-name}/_restart`\n\n\n### 2.14 `/_utils`\n\n\n### 2.15 `/_up`\n\n\n\n### 2.16 `/_uuids`\n\n\n\n### 2.17 `/favicon.ico`\n\n\n### 2.18 权限认证\n\n### 2.19 配置\n","source":"_posts/blog/couchDB/CouchDB学习-API.md","raw":"---\ntitle: CouchDB学习-API\ndate: 2019-12-24 10:52:40\ntags: CouchDb\ncategories: CouchDb学习\n---\n# API\n\nAPI URL路径可以指定访问CouchDB服务器的某个组件。URL请求结果包括标识和访问的数据库中的高效的描述字段。\n与所有URL一样，各个组件之间用正斜杠分隔。\n通常，以_（下划线）字符开头的URL组件和JSON字段表示服务器或返回的对象中的特殊组件或实体。例如，URL片段`/_all_dbs`获取CouchDB实例中所有数据库的列表。\n该引用根据URL结构进行构造，如下所示。\n\n## 1 基本API\n\nCouchDB API是与CouchDB实例接口的主要方法。使用HTTP发出请求，请求用于从数据库请求信息，存储新数据以及对文档中存储的信息进行查看和格式化。\n对API的请求可以按您正在访问的CouchDB系统的不同区域以及用于发送请求的HTTP方法进行分类。不同的方法意味着不同的操作，例如，从数据库中检索信息通常由`GET`操作处理，而更新则由`POST`或`PUT`请求处理。不同方法必须提供的信息之间存在一些差异。有关基本HTTP方法和请求结构的指南，请参见请求格式和响应。\n对于几乎所有操作，都在JavaScript对象表示法（JSON）对象中定义了提交的数据和返回的数据结构。 JSON基础知识中提供了有关JSON内容和数据类型的基本信息。\n使用标准HTTP状态代码报告访问CouchDB API时的错误。 HTTP状态代码中提供了有关CouchDB返回的通用代码的指南。\n访问CouchDB API的特定区域时，将提供有关HTTP方法和请求，JSON结构和错误代码的特定信息和示例。\n\n### 1.1 请求格式和响应\n\nCouchDB支持以下HTTP请求方法：\n\n* `GET`:请求指定的条目。\n* `HEAD`：获取HTTP请求头部信息\n* `POST`：上传数据\n* `PUT`：用于PUT指定的资源，如创建新的对象(数据库，文档，视图，设计文档)\n* `DELETE`：删除指定的资源\n* `COPY`：特殊的方法，用于拷贝文档和对象\n\n如果使用CouchDB不支持的HTTP请求类型，将会返回状态码`405-Method Not Allowed`.\n\n### 1.2 HTTP请求头\n\nCouchDB使用HTTP进行所有通信。所以需要确保正确的并且是被支持的HTTP头部信息。\n\n#### 1.2.1 请求头\n\n* `Accept`:由服务器返回指定的被接受的数据类型列表\n* `Content-type`：指定请求中提供的信息的内容类型\n\n#### 1.2.2 响应头\n\n* `Cache-control`：缓存控制\n* `Content-length`：返回的内容的长度\n* `Content-type`：指定的返回数据的MIME类型\n* `Etag`：显示文档或者视图的修订版本\n* `Transfer-Encoding`：如果响应使用编码，那么则在该字段中指定\n\n\n### 1.3 JSON基础\n### 1.4 HTTP状态码\n\n* 200 - `OK`：请求完全成功\n* 201 - `Created`：文档成功创建\n* 202 - `Accepted`：请求被接受，但是相关的操作可能不完整，经常用于后台操作如数据库压缩。\n* 304 - `Not Modified`：请求的其他内容尚未修改。\n* 400 - `Bad Request`：坏的请求结构，如错误的请求URL，路径或请求头。\n* 401 - `Unauthorized`：不具备获取指定数据的权限，或者权限不支持\n* 403 - `Forbidden`：请求被服务器拒绝\n* 404 - `Not Found`：没有找到请求的数据\n* 405 - `Method Not Allowed`：请求方法不被支持\n* 406 - `Not Acceptable`：请求的内容类型不被支持\n* 409 - `Conflict`：更新数据冲突\n* 412 - `Preconfition Failed`：客户端的请求头和服务器的兼容性不匹配\n* 413 - `Request Entity Too Large`：请求的数据过大\n* 415 - `Unsupported Media Type`：支持的内容类型以及正在请求或提交的信息的内容类型表示不支持该内容类型。\n* 416 - `Requested Range Not Satisfiable`：服务器无法满足请求标头中指定的范围。\n* 417 - `Expectation Failed`：批量发送文档时，批量加载操作失败。\n* 500 - `Internal Server Error`：请求无效\n\n\n## 2 服务器\n### 2.1 `/`\n`GET /`:访问CouchDB实例的Root并返回关于实例的元数据。\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n\n### 2.2 `/_active_tasks`\n\n`GET /_active_tasks`:列出运行中的任务，包括任务类型，名字，状态和进程ID。\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**响应JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `changes_done(number):`处理的变更\n&emsp;&emsp;&emsp;&emsp;    * `database(string):`源数据库\n&emsp;&emsp;&emsp;&emsp;    * `pid(string):`进程ID\n&emsp;&emsp;&emsp;&emsp;    * `progress(number):`当前进度百分比\n&emsp;&emsp;&emsp;&emsp;    * `started_on(number):`任务开始时间\n&emsp;&emsp;&emsp;&emsp;    * `status(string):`任务状态信息\n&emsp;&emsp;&emsp;&emsp;    * `task(string):`任务名称\n&emsp;&emsp;&emsp;&emsp;    * `total_changes(number):`进程总的改变\n&emsp;&emsp;&emsp;&emsp;    * `type(string):`操作类型\n&emsp;&emsp;&emsp;&emsp;    * `update_on(number):`最后一次更新时间\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 401 `Unauthorized`\n\n### 2.3 `/_all_dbs`\n\n`GET /_all_dbs`:返回CouchDB实例所有数据库列表\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**请求参数：**\n&emsp;&emsp;&emsp;&emsp;    * `descending(boolean):`按键降序返回数据库。 默认为false。\n&emsp;&emsp;&emsp;&emsp;    * `endkey(json):`到达指定的键时，停止返回数据库。\n&emsp;&emsp;&emsp;&emsp;    * `end_key(json):`endkey的别名\n&emsp;&emsp;&emsp;&emsp;    * `limit(number):`返回数据库数量的限制\n&emsp;&emsp;&emsp;&emsp;    * `skip(number):`跳过此值得数据库，默认为0\n&emsp;&emsp;&emsp;&emsp;    * `startkey(json):`从指定的键返回数据库\n&emsp;&emsp;&emsp;&emsp;    * `start_key(json):`startkey的别名\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * `200 OK`\n\n### 2.4 `/_dbs_info`\n\n`GET /_dbs_info`:返回CouchDB实例所有数据库列表的数据库信息\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**请求JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `keys(array):`被请求的数据库名字(数组)\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n\n### 2.5 `/_cluster_setup`\n\n`GET /_cluster_setup`:根据群集设置向导返回节点或群集的状态。\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**请求参数：**\n&emsp;&emsp;&emsp;&emsp;    * `ensure_dbs_exist(array):`列出系统数据库确保在节点/集群上存在,默认：`[\"_users\",\"_replicator\", \"_global_changes\"]`.\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**响应JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `state(string):`节点/集群的当前状态(见下面)\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n\n**STATE**\n\n* `cluster_disabled`:当前节点完全没有被配置。\n* `single_node_disabled`：当前节点被配置为单节点，不具备服务器基本的管理员用户定义并且完整的标准系统数据库没有被创建。如果指定了`ensure_dbs_exist`查询参数，则提供的数据库列表将覆盖标准系统数据库的默认列表。\n* `single_node_enabled`：当前节点被配置为单节点，具有服务器基本的管理员用户定义并且`ensure_dbs_exist`列表中的数据库已经被创建。\n* `cluster_enabled`：当前节点集群数量大于1，没有绑定在`127.0.0.1`，具有服务器基本的管理员用户定义但是完整的标准系统数据库没有被创建。如果指定了`ensure_dbs_exist`查询参数，则提供的数据库列表将覆盖标准系统数据库的默认列表。\n* `cluster_finished`：当前节点集群数量大于1，没有绑定在`127.0.0.1`，具有服务器基本的管理员用户定义并且`ensure_dbs_exist`列表中的数据库已经被创建。\n\n`POST /_cluster_setup`:配置一个节点作为单节点，或者是集群的一部分，或者完成集群。\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**请求JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `action(string):`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `enable_single_node:`配置当前节点为单节点\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `enable_cluster:`配置本地或远程节点，准备将它添加到新的CouchDB集群\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `add_node:`添加一个指定的远程节点到该集群列表中\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `finish_cluster:`完成集群的创建并创建标准的系统数据库\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `bind_address(string):`当前节点绑定的IP地址\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `username(string):`服务器级别的管理员用户名\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `password(string):`服务器级别的管理员密码\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `port(number):`该节点或远程节点(只用于`add_node`)绑定的TCP端口\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `node_count(number):`集群中节点总数\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `remote_node(string):`配置集群中的远程该节点的IP地址\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `remote_current_user(string):`服务器级别的管理员授权到远程节点的管理员用户名\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `remote_current_password(string):`服务器级别的管理员授权到远程节点的管理员密码\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `host(string):`添加到集群的远程节点的IP地址\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `ensure_dbs_exist(array):`列出系统数据库确保在该节点上存在。\n\n### 2.6 `/_db_updates`\n\n`GET /_db_updates`:返回CouchDB实例上所有数据库事件列表\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**请求参数：**\n&emsp;&emsp;&emsp;&emsp;    * `feed(string)`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `normal`:返回所有数据库历史变化，然后关闭连接。\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `longpoll`:在第一次事件后关闭连接\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `continuous`:每个事件发送一行JSON，一直保持socket开启直到超时。\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `eventsource`:与`continuous`相似，只是以`EventSource`格式发送。\n&emsp;&emsp;&emsp;&emsp;    * `timeout(number)`：指定多少秒后关闭CouchDB连接\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `heartbeat(number)`:每隔多少周期毫秒发送空行保持连接，默认60000\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `since(string)`:仅返回指定的序列ID更新\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**响应JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `results(array)`:数据库事件的数组\n&emsp;&emsp;&emsp;&emsp;    * `last_seq(string)`:记录的最后一个序列ID\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 404 `Unauthorized`\n**JSON对象：**\n&emsp;&emsp;&emsp;&emsp;    * `db_name(string):`数据库名称\n&emsp;&emsp;&emsp;&emsp;    * `type(string):`数据库事件类型(`created,updated,deleted`)\n&emsp;&emsp;&emsp;&emsp;    * `seq(json):`事件更新序列\n\n### 2.7 `/_membership`\n\n`GET /_membership`:显示`cluster_nodes`集群中的部分节点。 `all_nodes`字段显示该节点知道的所有节点，包括属于集群的节点。在设置集群时非常有用，\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n### 2.8 `/_replicate`\n`GET /_replicate`:请求，配置或停止复制操作\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain`\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type:` `application/json`\n**请求JSON对象:**\n&emsp;&emsp;&emsp;&emsp;        * `cancel(boolean):`取消复制\n&emsp;&emsp;&emsp;&emsp;        * `continuous(boolean):`继续复制\n&emsp;&emsp;&emsp;&emsp;        * `create_target(boolean):`创建目标数据库，要求管理员权限\n&emsp;&emsp;&emsp;&emsp;        * `doc_ids(array):`同步的文档ID的数组\n&emsp;&emsp;&emsp;&emsp;        * `filter(string):`过滤器函数的名字\n&emsp;&emsp;&emsp;&emsp;        * `proxy(string):`代理服务器的地址\n&emsp;&emsp;&emsp;&emsp;        * `source(string/object):`源数据库名字或URL或对象，包含完整的源数据库URL以及参数例如头部信息\n&emsp;&emsp;&emsp;&emsp;        * `target(string/object):`目标数据库名字或URL或对象，包含完整的目标数据库URL以及参数例如头部信息\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `text/plain;charset=utf-8`\n**响应JSON对象:**\n&emsp;&emsp;&emsp;&emsp;        * `history(array):`复制历史(见下面)\n&emsp;&emsp;&emsp;&emsp;        * `ok(boolean):`复制状态\n&emsp;&emsp;&emsp;&emsp;        * `replication_id_version(number):`复制协议版本\n&emsp;&emsp;&emsp;&emsp;        * `session_id(string):`唯一的sessionID\n&emsp;&emsp;&emsp;&emsp;        * `source_last_seq(number):`从源数据库读取的最后的序列号\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 202 `Accepted`\n&emsp;&emsp;&emsp;&emsp;    * 400 `Bad Request`\n&emsp;&emsp;&emsp;&emsp;    * 401 `Unauthorized`\n&emsp;&emsp;&emsp;&emsp;    * 404 `Not Found`\n&emsp;&emsp;&emsp;&emsp;    * 500 `Internal Server Error`\n**JSON对象:**\n&emsp;&emsp;&emsp;&emsp;        * `doc_write_failures(number):`文档写失败的数量\n&emsp;&emsp;&emsp;&emsp;        * `docs_read(number):`文档读取的数量\n&emsp;&emsp;&emsp;&emsp;        * `docs_written (number):`文档成功写到目标的数量\n&emsp;&emsp;&emsp;&emsp;        * `end_last_seq (number):`更新流中最后的序列号\n&emsp;&emsp;&emsp;&emsp;        * `end_time (string):`复制操作完成的时间\n&emsp;&emsp;&emsp;&emsp;        * `missing_checked (number):`检查到的缺失的文档数量\n&emsp;&emsp;&emsp;&emsp;        * `missing_found (number):`缺失的文档被发现的数量\n&emsp;&emsp;&emsp;&emsp;        * `recorded_seq (number):`记录的最后的序列号\n&emsp;&emsp;&emsp;&emsp;        * `session_id (string):`这次复制操作的SessionID\n&emsp;&emsp;&emsp;&emsp;        * `start_last_seq (number):`更新流中第一个序列号\n&emsp;&emsp;&emsp;&emsp;        * `start_time (string):`复制操作开始的时间\n\n#### 2.8.1 复制操作\n\n复制的目的是，在过程结束时，源数据库上的所有活动文档也都在目标数据库中，并且在源数据库中删除的所有文档也都在目标数据库上被删除（如果存在）。\n复制可以描述为推式或拉式复制：\n&emsp;&emsp;&emsp;&emsp; * 拉复制是当远程CouchDB实例是源数据库，目标是本地数据库的位置。\n如果源数据库具有永久IP地址，而目标（本地）数据库可能具有动态分配的IP地址（例如，通过DHCP），则Pull复制是最有用的解决方案。 如果要从中央服务器复制到移动设备或其他设备，则这尤其重要。\n&emsp;&emsp;&emsp;&emsp; * 推复制是当本地数据库为源数据库，远程数据库为目标数据库。\n\n#### 2.8.2 指定源和目标数据库\n\n如果要在以下两种情况之一中执行复制，则必须使用CouchDB数据库的URL规范：\n&emsp;&emsp;&emsp;&emsp; * 使用远程数据库进行复制（即CouchDB的另一个实例在同一主机或其他主机上）\n&emsp;&emsp;&emsp;&emsp; * 数据库复制需要权限认证。\n例如，要请求向其发送请求的CouchDB实例本地数据库和远程数据库之间复制，可以使用以下请求：\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    \"source\" : \"recipes\",\n    \"target\" : \"http://coucdb-remote:5984/recipes\",\n}\n```\n在所有情况下，源规范和目标规范中所请求的数据库都必须存在。 如果不这样做，则将在JSON对象内返回错误：\n```\n{\n    \"reason\" : \"could not open http://couchdb-remote:5984/ol1ka/\",\n}\n```\n您可以通过将`create_target`字段添加到请求对象来创建目标数据库（只要您的用户凭据允许）：\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    \"create_target\" : true\n    \"source\" : \"recipes\",\n    \"target\" : \"http://couchdb-remote:5984/recipes\",\n}\n```\n`create_target`字段不是破坏性的。 如果数据库已经存在，则复制将正常进行。\n\n#### 2.8.3 单个复制\n\n您可以请求复制数据库，以便可以同步两个数据库。 默认情况下，复制过程会发生一次，并将两个数据库同步在一起。 例如，您可以通过在请求JSON内容中提供`source`和`target`字段来请求两个数据库之间的单个同步。\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    \"source\" : \"recipes\",\n    \"target\" : \"recipes-snapshot\",\n}\n```\n在上面的示例中，数据库配方和配方快照将被同步。 这些数据库是发出请求的CouchDB实例的本地数据库。响应将是一个JSON结构，其中包含同步过程的成功（或失败），以及有关该过程的统计信息：\n```\n{\n    \"ok\" : true,\n    \"history\" : [\n        {\n            \"docs_read\" : 1000,\n            \"session_id\" : \"52c2370f5027043d286daca4de247db0\",\n            \"recorded_seq\" : 1000,\n            \"end_last_seq\" : 1000,\n            \"doc_write_failures\" : 0,\n            \"start_time\" : \"Thu, 28 Oct 2010 10:24:13 GMT\",\n            \"start_last_seq\" : 0,\n            \"end_time\" : \"Thu, 28 Oct 2010 10:24:14 GMT\",\n            \"missing_checked\" : 0,\n            \"docs_written\" : 1000,\n            \"missing_found\" : 1000\n        }\n    ],\n    \"session_id\" : \"52c2370f5027043d286daca4de247db0\",\n    \"source_last_seq\" : 1000\n}\n```\n\n#### 2.8.4 继续复制\n\n在执行复制请求时，数据库与以前提到的方法的同步仅发生一次。 要使目标数据库从源永久复制，您必须将请求内JSON对象的Continuous字段设置为true。\n通过连续复制，源数据库中的更改将永久复制到目标数据库，直到您明确要求停止复制为止。\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    \"continuous\" : true\n    \"source\" : \"recipes\",\n    \"target\" : \"http://couchdb-remote:5984/recipes\",\n}\n```\n只要两个实例之间存在网络连接，更改就会在两个数据库之间复制。\n两个保持彼此同步的数据库，您需要在两个方向上设置复制；也就是说，您必须从源复制到目标，并且必须从目标复制到另一个。\n\n#### 2.8.5 取消继续复制\n\n您可以通过将`cancel`字段添加到JSON请求对象并将值设置为`true`来取消连续复制。请注意，请求的结构必须与原始结构相同，才能兑现取消请求。例如，如果您请求连续复制，则取消请求还必须包含连续字段。\n例如，复制请求：\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    \"source\" : \"recipes\",\n    \"target\" : \"http://couchdb-remote:5984/recipes\", \"create_target\" : true,\n    \"continuous\" : true\n}\n```\n必须使用请求取消复制：\n```\nPOST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    \"cancel\" : true,\n    \"continuous\" : true\n    \"create_target\" : true,\n    \"source\" : \"recipes\",\n    \"target\" : \"http://couchdb-remote:5984/recipes\",\n}\n```\n请求取消不存在的复制将导致404错误。\n\n### 2.9 `/_scheduler/jobs`\n\n`GET /_scheduler/jobs`:列出复制任务\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**请求参数**\n&emsp;&emsp;&emsp;&emsp;    * `limit(number)：`返回多少数量的结果\n&emsp;&emsp;&emsp;&emsp;    * `skip(number):`跳过多少数量的结果，以复制ID排序\n**响应JSON对象**\n&emsp;&emsp;&emsp;&emsp;    * `offset (number)：`多少数量的结果被跳过\n&emsp;&emsp;&emsp;&emsp;    * `total_rows (number) ：`复制任务的总数量\n&emsp;&emsp;&emsp;&emsp;    * `id (string)：`复制ID\n&emsp;&emsp;&emsp;&emsp;    * `database (string)：`复制文档数据库\n&emsp;&emsp;&emsp;&emsp;    * `doc_id (string)：`复制文档ID\n&emsp;&emsp;&emsp;&emsp;    * `history (list)：`事件的时间戳历史以对象列表展示\n&emsp;&emsp;&emsp;&emsp;    * `pid (string)：`复制进程ID\n&emsp;&emsp;&emsp;&emsp;    * `node (string)：`运行任务的集群中的节点\n&emsp;&emsp;&emsp;&emsp;    * `source (string)：`复制源头\n&emsp;&emsp;&emsp;&emsp;    * `target (string)：`复制目标\n&emsp;&emsp;&emsp;&emsp;    * `start_time (string)：`开始复制的时间戳\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 401 `Unauthorized`\n\n### 2.10 `/_scheduler/docs`\n\n`GET /_scheduler/docs`:列出复制文档的状态\n**请求头：**\n&emsp;&emsp;&emsp;&emsp;    * `Accept-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**响应头：**\n&emsp;&emsp;&emsp;&emsp;    * `Content-Type-`\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * `application/json`\n**请求参数：**\n&emsp;&emsp;&emsp;&emsp;    * `limit(number):`返回多少结果\n&emsp;&emsp;&emsp;&emsp;    * `skip(number):`跳过多少数量的结果，以文档ID排序\n**响应JSON对象**\n&emsp;&emsp;&emsp;&emsp;    * `offset (number)：`多少数量的结果被跳过\n&emsp;&emsp;&emsp;&emsp;    * `total_rows (number) ：`复制文档的总数量\n&emsp;&emsp;&emsp;&emsp;    * `id (string)：`复制ID或者当复制状态为完成或失败时为空\n&emsp;&emsp;&emsp;&emsp;    * `state(string)：`复制状态(`initializing,running,completed,pending,crashing,error,failed`)\n&emsp;&emsp;&emsp;&emsp;    * `database (string)：`复制文档的目标数据库\n&emsp;&emsp;&emsp;&emsp;    * `doc_id (string)：`复制文档ID\n&emsp;&emsp;&emsp;&emsp;    * `last_update(string)：`最后一次更新的时间\n&emsp;&emsp;&emsp;&emsp;    * `info(object)：`关于状态的可能的额外信息\n&emsp;&emsp;&emsp;&emsp;    * `node (string)：`运行任务的集群中的节点\n&emsp;&emsp;&emsp;&emsp;    * `source (string)：`复制源头\n&emsp;&emsp;&emsp;&emsp;    * `target (string)：`复制目标\n&emsp;&emsp;&emsp;&emsp;    * `start_time (string)：`开始复制的时间戳\n&emsp;&emsp;&emsp;&emsp;    * `error_count(number) ：`复制出现错误的数量\n**状态码：**\n&emsp;&emsp;&emsp;&emsp;    * 200 `OK`\n&emsp;&emsp;&emsp;&emsp;    * 401 `Unauthorized`\n**JSON对象**\n&emsp;&emsp;&emsp;&emsp;    * `revisions_checked (number):`在复制开始时被检查的修订版本的数量\n&emsp;&emsp;&emsp;&emsp;    * `missing_revisions_found(number):`在源处有而目标处没有的修订版本的数量\n&emsp;&emsp;&emsp;&emsp;    * `docs_read (number) :`从源处读取的文档的数量\n&emsp;&emsp;&emsp;&emsp;    * `docs_written (number) :`写到目标的文档的数量\n&emsp;&emsp;&emsp;&emsp;    * `changes_pending (number):`还没有复制完成的数量\n&emsp;&emsp;&emsp;&emsp;    * `doc_write_failures (number) :`写目标失败的数量\n&emsp;&emsp;&emsp;&emsp;    * `checkpointed_source_seq (object):`最后一个从源处成功复制的序列ID\n\n### 2.11 `/_node/{node-name}/_stats`\n\n\n### 2.12 `/_node/{node-name}/_system`\n\n\n### 2.13 `/_node/{node-name}/_restart`\n\n\n### 2.14 `/_utils`\n\n\n### 2.15 `/_up`\n\n\n\n### 2.16 `/_uuids`\n\n\n\n### 2.17 `/favicon.ico`\n\n\n### 2.18 权限认证\n\n### 2.19 配置\n","slug":"blog/couchDB/CouchDB学习-API","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqygo000xk0vqdfztgyfz","content":"<h1 id=\"API\"><a href=\"#API\" class=\"headerlink\" title=\"API\"></a>API</h1><p>API URL路径可以指定访问CouchDB服务器的某个组件。URL请求结果包括标识和访问的数据库中的高效的描述字段。<br>与所有URL一样，各个组件之间用正斜杠分隔。<br>通常，以_（下划线）字符开头的URL组件和JSON字段表示服务器或返回的对象中的特殊组件或实体。例如，URL片段<code>/_all_dbs</code>获取CouchDB实例中所有数据库的列表。<br>该引用根据URL结构进行构造，如下所示。</p>\n<h2 id=\"1-基本API\"><a href=\"#1-基本API\" class=\"headerlink\" title=\"1 基本API\"></a>1 基本API</h2><p>CouchDB API是与CouchDB实例接口的主要方法。使用HTTP发出请求，请求用于从数据库请求信息，存储新数据以及对文档中存储的信息进行查看和格式化。<br>对API的请求可以按您正在访问的CouchDB系统的不同区域以及用于发送请求的HTTP方法进行分类。不同的方法意味着不同的操作，例如，从数据库中检索信息通常由<code>GET</code>操作处理，而更新则由<code>POST</code>或<code>PUT</code>请求处理。不同方法必须提供的信息之间存在一些差异。有关基本HTTP方法和请求结构的指南，请参见请求格式和响应。<br>对于几乎所有操作，都在JavaScript对象表示法（JSON）对象中定义了提交的数据和返回的数据结构。 JSON基础知识中提供了有关JSON内容和数据类型的基本信息。<br>使用标准HTTP状态代码报告访问CouchDB API时的错误。 HTTP状态代码中提供了有关CouchDB返回的通用代码的指南。<br>访问CouchDB API的特定区域时，将提供有关HTTP方法和请求，JSON结构和错误代码的特定信息和示例。</p>\n<h3 id=\"1-1-请求格式和响应\"><a href=\"#1-1-请求格式和响应\" class=\"headerlink\" title=\"1.1 请求格式和响应\"></a>1.1 请求格式和响应</h3><p>CouchDB支持以下HTTP请求方法：</p>\n<ul>\n<li><code>GET</code>:请求指定的条目。</li>\n<li><code>HEAD</code>：获取HTTP请求头部信息</li>\n<li><code>POST</code>：上传数据</li>\n<li><code>PUT</code>：用于PUT指定的资源，如创建新的对象(数据库，文档，视图，设计文档)</li>\n<li><code>DELETE</code>：删除指定的资源</li>\n<li><code>COPY</code>：特殊的方法，用于拷贝文档和对象</li>\n</ul>\n<p>如果使用CouchDB不支持的HTTP请求类型，将会返回状态码<code>405-Method Not Allowed</code>.</p>\n<h3 id=\"1-2-HTTP请求头\"><a href=\"#1-2-HTTP请求头\" class=\"headerlink\" title=\"1.2 HTTP请求头\"></a>1.2 HTTP请求头</h3><p>CouchDB使用HTTP进行所有通信。所以需要确保正确的并且是被支持的HTTP头部信息。</p>\n<h4 id=\"1-2-1-请求头\"><a href=\"#1-2-1-请求头\" class=\"headerlink\" title=\"1.2.1 请求头\"></a>1.2.1 请求头</h4><ul>\n<li><code>Accept</code>:由服务器返回指定的被接受的数据类型列表</li>\n<li><code>Content-type</code>：指定请求中提供的信息的内容类型</li>\n</ul>\n<h4 id=\"1-2-2-响应头\"><a href=\"#1-2-2-响应头\" class=\"headerlink\" title=\"1.2.2 响应头\"></a>1.2.2 响应头</h4><ul>\n<li><code>Cache-control</code>：缓存控制</li>\n<li><code>Content-length</code>：返回的内容的长度</li>\n<li><code>Content-type</code>：指定的返回数据的MIME类型</li>\n<li><code>Etag</code>：显示文档或者视图的修订版本</li>\n<li><code>Transfer-Encoding</code>：如果响应使用编码，那么则在该字段中指定</li>\n</ul>\n<h3 id=\"1-3-JSON基础\"><a href=\"#1-3-JSON基础\" class=\"headerlink\" title=\"1.3 JSON基础\"></a>1.3 JSON基础</h3><h3 id=\"1-4-HTTP状态码\"><a href=\"#1-4-HTTP状态码\" class=\"headerlink\" title=\"1.4 HTTP状态码\"></a>1.4 HTTP状态码</h3><ul>\n<li>200 - <code>OK</code>：请求完全成功</li>\n<li>201 - <code>Created</code>：文档成功创建</li>\n<li>202 - <code>Accepted</code>：请求被接受，但是相关的操作可能不完整，经常用于后台操作如数据库压缩。</li>\n<li>304 - <code>Not Modified</code>：请求的其他内容尚未修改。</li>\n<li>400 - <code>Bad Request</code>：坏的请求结构，如错误的请求URL，路径或请求头。</li>\n<li>401 - <code>Unauthorized</code>：不具备获取指定数据的权限，或者权限不支持</li>\n<li>403 - <code>Forbidden</code>：请求被服务器拒绝</li>\n<li>404 - <code>Not Found</code>：没有找到请求的数据</li>\n<li>405 - <code>Method Not Allowed</code>：请求方法不被支持</li>\n<li>406 - <code>Not Acceptable</code>：请求的内容类型不被支持</li>\n<li>409 - <code>Conflict</code>：更新数据冲突</li>\n<li>412 - <code>Preconfition Failed</code>：客户端的请求头和服务器的兼容性不匹配</li>\n<li>413 - <code>Request Entity Too Large</code>：请求的数据过大</li>\n<li>415 - <code>Unsupported Media Type</code>：支持的内容类型以及正在请求或提交的信息的内容类型表示不支持该内容类型。</li>\n<li>416 - <code>Requested Range Not Satisfiable</code>：服务器无法满足请求标头中指定的范围。</li>\n<li>417 - <code>Expectation Failed</code>：批量发送文档时，批量加载操作失败。</li>\n<li>500 - <code>Internal Server Error</code>：请求无效</li>\n</ul>\n<h2 id=\"2-服务器\"><a href=\"#2-服务器\" class=\"headerlink\" title=\"2 服务器\"></a>2 服务器</h2><h3 id=\"2-1\"><a href=\"#2-1\" class=\"headerlink\" title=\"2.1 /\"></a>2.1 <code>/</code></h3><p><code>GET /</code>:访问CouchDB实例的Root并返回关于实例的元数据。<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code></p>\n<h3 id=\"2-2-active-tasks\"><a href=\"#2-2-active-tasks\" class=\"headerlink\" title=\"2.2 /_active_tasks\"></a>2.2 <code>/_active_tasks</code></h3><p><code>GET /_active_tasks</code>:列出运行中的任务，包括任务类型，名字，状态和进程ID。<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>响应JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>changes_done(number):</code>处理的变更<br>&emsp;&emsp;&emsp;&emsp;    * <code>database(string):</code>源数据库<br>&emsp;&emsp;&emsp;&emsp;    * <code>pid(string):</code>进程ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>progress(number):</code>当前进度百分比<br>&emsp;&emsp;&emsp;&emsp;    * <code>started_on(number):</code>任务开始时间<br>&emsp;&emsp;&emsp;&emsp;    * <code>status(string):</code>任务状态信息<br>&emsp;&emsp;&emsp;&emsp;    * <code>task(string):</code>任务名称<br>&emsp;&emsp;&emsp;&emsp;    * <code>total_changes(number):</code>进程总的改变<br>&emsp;&emsp;&emsp;&emsp;    * <code>type(string):</code>操作类型<br>&emsp;&emsp;&emsp;&emsp;    * <code>update_on(number):</code>最后一次更新时间<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 401 <code>Unauthorized</code></p>\n<h3 id=\"2-3-all-dbs\"><a href=\"#2-3-all-dbs\" class=\"headerlink\" title=\"2.3 /_all_dbs\"></a>2.3 <code>/_all_dbs</code></h3><p><code>GET /_all_dbs</code>:返回CouchDB实例所有数据库列表<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>请求参数：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>descending(boolean):</code>按键降序返回数据库。 默认为false。<br>&emsp;&emsp;&emsp;&emsp;    * <code>endkey(json):</code>到达指定的键时，停止返回数据库。<br>&emsp;&emsp;&emsp;&emsp;    * <code>end_key(json):</code>endkey的别名<br>&emsp;&emsp;&emsp;&emsp;    * <code>limit(number):</code>返回数据库数量的限制<br>&emsp;&emsp;&emsp;&emsp;    * <code>skip(number):</code>跳过此值得数据库，默认为0<br>&emsp;&emsp;&emsp;&emsp;    * <code>startkey(json):</code>从指定的键返回数据库<br>&emsp;&emsp;&emsp;&emsp;    * <code>start_key(json):</code>startkey的别名<br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>200 OK</code></p>\n<h3 id=\"2-4-dbs-info\"><a href=\"#2-4-dbs-info\" class=\"headerlink\" title=\"2.4 /_dbs_info\"></a>2.4 <code>/_dbs_info</code></h3><p><code>GET /_dbs_info</code>:返回CouchDB实例所有数据库列表的数据库信息<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>请求JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>keys(array):</code>被请求的数据库名字(数组)<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code></p>\n<h3 id=\"2-5-cluster-setup\"><a href=\"#2-5-cluster-setup\" class=\"headerlink\" title=\"2.5 /_cluster_setup\"></a>2.5 <code>/_cluster_setup</code></h3><p><code>GET /_cluster_setup</code>:根据群集设置向导返回节点或群集的状态。<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>请求参数：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>ensure_dbs_exist(array):</code>列出系统数据库确保在节点/集群上存在,默认：<code>[&quot;_users&quot;,&quot;_replicator&quot;, &quot;_global_changes&quot;]</code>.<br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>响应JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>state(string):</code>节点/集群的当前状态(见下面)<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code></p>\n<p><strong>STATE</strong></p>\n<ul>\n<li><code>cluster_disabled</code>:当前节点完全没有被配置。</li>\n<li><code>single_node_disabled</code>：当前节点被配置为单节点，不具备服务器基本的管理员用户定义并且完整的标准系统数据库没有被创建。如果指定了<code>ensure_dbs_exist</code>查询参数，则提供的数据库列表将覆盖标准系统数据库的默认列表。</li>\n<li><code>single_node_enabled</code>：当前节点被配置为单节点，具有服务器基本的管理员用户定义并且<code>ensure_dbs_exist</code>列表中的数据库已经被创建。</li>\n<li><code>cluster_enabled</code>：当前节点集群数量大于1，没有绑定在<code>127.0.0.1</code>，具有服务器基本的管理员用户定义但是完整的标准系统数据库没有被创建。如果指定了<code>ensure_dbs_exist</code>查询参数，则提供的数据库列表将覆盖标准系统数据库的默认列表。</li>\n<li><code>cluster_finished</code>：当前节点集群数量大于1，没有绑定在<code>127.0.0.1</code>，具有服务器基本的管理员用户定义并且<code>ensure_dbs_exist</code>列表中的数据库已经被创建。</li>\n</ul>\n<p><code>POST /_cluster_setup</code>:配置一个节点作为单节点，或者是集群的一部分，或者完成集群。<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>请求JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>action(string):</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>enable_single_node:</code>配置当前节点为单节点<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>enable_cluster:</code>配置本地或远程节点，准备将它添加到新的CouchDB集群<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>add_node:</code>添加一个指定的远程节点到该集群列表中<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>finish_cluster:</code>完成集群的创建并创建标准的系统数据库<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>bind_address(string):</code>当前节点绑定的IP地址<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>username(string):</code>服务器级别的管理员用户名<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>password(string):</code>服务器级别的管理员密码<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>port(number):</code>该节点或远程节点(只用于<code>add_node</code>)绑定的TCP端口<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>node_count(number):</code>集群中节点总数<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>remote_node(string):</code>配置集群中的远程该节点的IP地址<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>remote_current_user(string):</code>服务器级别的管理员授权到远程节点的管理员用户名<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>remote_current_password(string):</code>服务器级别的管理员授权到远程节点的管理员密码<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>host(string):</code>添加到集群的远程节点的IP地址<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>ensure_dbs_exist(array):</code>列出系统数据库确保在该节点上存在。</p>\n<h3 id=\"2-6-db-updates\"><a href=\"#2-6-db-updates\" class=\"headerlink\" title=\"2.6 /_db_updates\"></a>2.6 <code>/_db_updates</code></h3><p><code>GET /_db_updates</code>:返回CouchDB实例上所有数据库事件列表<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>请求参数：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>feed(string)</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>normal</code>:返回所有数据库历史变化，然后关闭连接。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>longpoll</code>:在第一次事件后关闭连接<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>continuous</code>:每个事件发送一行JSON，一直保持socket开启直到超时。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>eventsource</code>:与<code>continuous</code>相似，只是以<code>EventSource</code>格式发送。<br>&emsp;&emsp;&emsp;&emsp;    * <code>timeout(number)</code>：指定多少秒后关闭CouchDB连接<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>heartbeat(number)</code>:每隔多少周期毫秒发送空行保持连接，默认60000<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>since(string)</code>:仅返回指定的序列ID更新<br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>响应JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>results(array)</code>:数据库事件的数组<br>&emsp;&emsp;&emsp;&emsp;    * <code>last_seq(string)</code>:记录的最后一个序列ID<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 404 <code>Unauthorized</code><br><strong>JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>db_name(string):</code>数据库名称<br>&emsp;&emsp;&emsp;&emsp;    * <code>type(string):</code>数据库事件类型(<code>created,updated,deleted</code>)<br>&emsp;&emsp;&emsp;&emsp;    * <code>seq(json):</code>事件更新序列</p>\n<h3 id=\"2-7-membership\"><a href=\"#2-7-membership\" class=\"headerlink\" title=\"2.7 /_membership\"></a>2.7 <code>/_membership</code></h3><p><code>GET /_membership</code>:显示<code>cluster_nodes</code>集群中的部分节点。 <code>all_nodes</code>字段显示该节点知道的所有节点，包括属于集群的节点。在设置集群时非常有用，<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code></p>\n<h3 id=\"2-8-replicate\"><a href=\"#2-8-replicate\" class=\"headerlink\" title=\"2.8 /_replicate\"></a>2.8 <code>/_replicate</code></h3><p><code>GET /_replicate</code>:请求，配置或停止复制操作<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type:</code> <code>application/json</code><br><strong>请求JSON对象:</strong><br>&emsp;&emsp;&emsp;&emsp;        * <code>cancel(boolean):</code>取消复制<br>&emsp;&emsp;&emsp;&emsp;        * <code>continuous(boolean):</code>继续复制<br>&emsp;&emsp;&emsp;&emsp;        * <code>create_target(boolean):</code>创建目标数据库，要求管理员权限<br>&emsp;&emsp;&emsp;&emsp;        * <code>doc_ids(array):</code>同步的文档ID的数组<br>&emsp;&emsp;&emsp;&emsp;        * <code>filter(string):</code>过滤器函数的名字<br>&emsp;&emsp;&emsp;&emsp;        * <code>proxy(string):</code>代理服务器的地址<br>&emsp;&emsp;&emsp;&emsp;        * <code>source(string/object):</code>源数据库名字或URL或对象，包含完整的源数据库URL以及参数例如头部信息<br>&emsp;&emsp;&emsp;&emsp;        * <code>target(string/object):</code>目标数据库名字或URL或对象，包含完整的目标数据库URL以及参数例如头部信息<br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>响应JSON对象:</strong><br>&emsp;&emsp;&emsp;&emsp;        * <code>history(array):</code>复制历史(见下面)<br>&emsp;&emsp;&emsp;&emsp;        * <code>ok(boolean):</code>复制状态<br>&emsp;&emsp;&emsp;&emsp;        * <code>replication_id_version(number):</code>复制协议版本<br>&emsp;&emsp;&emsp;&emsp;        * <code>session_id(string):</code>唯一的sessionID<br>&emsp;&emsp;&emsp;&emsp;        * <code>source_last_seq(number):</code>从源数据库读取的最后的序列号<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 202 <code>Accepted</code><br>&emsp;&emsp;&emsp;&emsp;    * 400 <code>Bad Request</code><br>&emsp;&emsp;&emsp;&emsp;    * 401 <code>Unauthorized</code><br>&emsp;&emsp;&emsp;&emsp;    * 404 <code>Not Found</code><br>&emsp;&emsp;&emsp;&emsp;    * 500 <code>Internal Server Error</code><br><strong>JSON对象:</strong><br>&emsp;&emsp;&emsp;&emsp;        * <code>doc_write_failures(number):</code>文档写失败的数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>docs_read(number):</code>文档读取的数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>docs_written (number):</code>文档成功写到目标的数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>end_last_seq (number):</code>更新流中最后的序列号<br>&emsp;&emsp;&emsp;&emsp;        * <code>end_time (string):</code>复制操作完成的时间<br>&emsp;&emsp;&emsp;&emsp;        * <code>missing_checked (number):</code>检查到的缺失的文档数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>missing_found (number):</code>缺失的文档被发现的数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>recorded_seq (number):</code>记录的最后的序列号<br>&emsp;&emsp;&emsp;&emsp;        * <code>session_id (string):</code>这次复制操作的SessionID<br>&emsp;&emsp;&emsp;&emsp;        * <code>start_last_seq (number):</code>更新流中第一个序列号<br>&emsp;&emsp;&emsp;&emsp;        * <code>start_time (string):</code>复制操作开始的时间</p>\n<h4 id=\"2-8-1-复制操作\"><a href=\"#2-8-1-复制操作\" class=\"headerlink\" title=\"2.8.1 复制操作\"></a>2.8.1 复制操作</h4><p>复制的目的是，在过程结束时，源数据库上的所有活动文档也都在目标数据库中，并且在源数据库中删除的所有文档也都在目标数据库上被删除（如果存在）。<br>复制可以描述为推式或拉式复制：<br>&emsp;&emsp;&emsp;&emsp; * 拉复制是当远程CouchDB实例是源数据库，目标是本地数据库的位置。<br>如果源数据库具有永久IP地址，而目标（本地）数据库可能具有动态分配的IP地址（例如，通过DHCP），则Pull复制是最有用的解决方案。 如果要从中央服务器复制到移动设备或其他设备，则这尤其重要。<br>&emsp;&emsp;&emsp;&emsp; * 推复制是当本地数据库为源数据库，远程数据库为目标数据库。</p>\n<h4 id=\"2-8-2-指定源和目标数据库\"><a href=\"#2-8-2-指定源和目标数据库\" class=\"headerlink\" title=\"2.8.2 指定源和目标数据库\"></a>2.8.2 指定源和目标数据库</h4><p>如果要在以下两种情况之一中执行复制，则必须使用CouchDB数据库的URL规范：<br>&emsp;&emsp;&emsp;&emsp; * 使用远程数据库进行复制（即CouchDB的另一个实例在同一主机或其他主机上）<br>&emsp;&emsp;&emsp;&emsp; * 数据库复制需要权限认证。<br>例如，要请求向其发送请求的CouchDB实例本地数据库和远程数据库之间复制，可以使用以下请求：</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://coucdb-remote:5984/recipes&quot;,\n}</code></pre><p>在所有情况下，源规范和目标规范中所请求的数据库都必须存在。 如果不这样做，则将在JSON对象内返回错误：</p>\n<pre><code>{\n    &quot;reason&quot; : &quot;could not open http://couchdb-remote:5984/ol1ka/&quot;,\n}</code></pre><p>您可以通过将<code>create_target</code>字段添加到请求对象来创建目标数据库（只要您的用户凭据允许）：</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    &quot;create_target&quot; : true\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://couchdb-remote:5984/recipes&quot;,\n}</code></pre><p><code>create_target</code>字段不是破坏性的。 如果数据库已经存在，则复制将正常进行。</p>\n<h4 id=\"2-8-3-单个复制\"><a href=\"#2-8-3-单个复制\" class=\"headerlink\" title=\"2.8.3 单个复制\"></a>2.8.3 单个复制</h4><p>您可以请求复制数据库，以便可以同步两个数据库。 默认情况下，复制过程会发生一次，并将两个数据库同步在一起。 例如，您可以通过在请求JSON内容中提供<code>source</code>和<code>target</code>字段来请求两个数据库之间的单个同步。</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;recipes-snapshot&quot;,\n}</code></pre><p>在上面的示例中，数据库配方和配方快照将被同步。 这些数据库是发出请求的CouchDB实例的本地数据库。响应将是一个JSON结构，其中包含同步过程的成功（或失败），以及有关该过程的统计信息：</p>\n<pre><code>{\n    &quot;ok&quot; : true,\n    &quot;history&quot; : [\n        {\n            &quot;docs_read&quot; : 1000,\n            &quot;session_id&quot; : &quot;52c2370f5027043d286daca4de247db0&quot;,\n            &quot;recorded_seq&quot; : 1000,\n            &quot;end_last_seq&quot; : 1000,\n            &quot;doc_write_failures&quot; : 0,\n            &quot;start_time&quot; : &quot;Thu, 28 Oct 2010 10:24:13 GMT&quot;,\n            &quot;start_last_seq&quot; : 0,\n            &quot;end_time&quot; : &quot;Thu, 28 Oct 2010 10:24:14 GMT&quot;,\n            &quot;missing_checked&quot; : 0,\n            &quot;docs_written&quot; : 1000,\n            &quot;missing_found&quot; : 1000\n        }\n    ],\n    &quot;session_id&quot; : &quot;52c2370f5027043d286daca4de247db0&quot;,\n    &quot;source_last_seq&quot; : 1000\n}</code></pre><h4 id=\"2-8-4-继续复制\"><a href=\"#2-8-4-继续复制\" class=\"headerlink\" title=\"2.8.4 继续复制\"></a>2.8.4 继续复制</h4><p>在执行复制请求时，数据库与以前提到的方法的同步仅发生一次。 要使目标数据库从源永久复制，您必须将请求内JSON对象的Continuous字段设置为true。<br>通过连续复制，源数据库中的更改将永久复制到目标数据库，直到您明确要求停止复制为止。</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    &quot;continuous&quot; : true\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://couchdb-remote:5984/recipes&quot;,\n}</code></pre><p>只要两个实例之间存在网络连接，更改就会在两个数据库之间复制。<br>两个保持彼此同步的数据库，您需要在两个方向上设置复制；也就是说，您必须从源复制到目标，并且必须从目标复制到另一个。</p>\n<h4 id=\"2-8-5-取消继续复制\"><a href=\"#2-8-5-取消继续复制\" class=\"headerlink\" title=\"2.8.5 取消继续复制\"></a>2.8.5 取消继续复制</h4><p>您可以通过将<code>cancel</code>字段添加到JSON请求对象并将值设置为<code>true</code>来取消连续复制。请注意，请求的结构必须与原始结构相同，才能兑现取消请求。例如，如果您请求连续复制，则取消请求还必须包含连续字段。<br>例如，复制请求：</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://couchdb-remote:5984/recipes&quot;, &quot;create_target&quot; : true,\n    &quot;continuous&quot; : true\n}</code></pre><p>必须使用请求取消复制：</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    &quot;cancel&quot; : true,\n    &quot;continuous&quot; : true\n    &quot;create_target&quot; : true,\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://couchdb-remote:5984/recipes&quot;,\n}</code></pre><p>请求取消不存在的复制将导致404错误。</p>\n<h3 id=\"2-9-scheduler-jobs\"><a href=\"#2-9-scheduler-jobs\" class=\"headerlink\" title=\"2.9 /_scheduler/jobs\"></a>2.9 <code>/_scheduler/jobs</code></h3><p><code>GET /_scheduler/jobs</code>:列出复制任务<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>请求参数</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>limit(number)：</code>返回多少数量的结果<br>&emsp;&emsp;&emsp;&emsp;    * <code>skip(number):</code>跳过多少数量的结果，以复制ID排序<br><strong>响应JSON对象</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>offset (number)：</code>多少数量的结果被跳过<br>&emsp;&emsp;&emsp;&emsp;    * <code>total_rows (number) ：</code>复制任务的总数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>id (string)：</code>复制ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>database (string)：</code>复制文档数据库<br>&emsp;&emsp;&emsp;&emsp;    * <code>doc_id (string)：</code>复制文档ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>history (list)：</code>事件的时间戳历史以对象列表展示<br>&emsp;&emsp;&emsp;&emsp;    * <code>pid (string)：</code>复制进程ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>node (string)：</code>运行任务的集群中的节点<br>&emsp;&emsp;&emsp;&emsp;    * <code>source (string)：</code>复制源头<br>&emsp;&emsp;&emsp;&emsp;    * <code>target (string)：</code>复制目标<br>&emsp;&emsp;&emsp;&emsp;    * <code>start_time (string)：</code>开始复制的时间戳<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 401 <code>Unauthorized</code></p>\n<h3 id=\"2-10-scheduler-docs\"><a href=\"#2-10-scheduler-docs\" class=\"headerlink\" title=\"2.10 /_scheduler/docs\"></a>2.10 <code>/_scheduler/docs</code></h3><p><code>GET /_scheduler/docs</code>:列出复制文档的状态<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>请求参数：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>limit(number):</code>返回多少结果<br>&emsp;&emsp;&emsp;&emsp;    * <code>skip(number):</code>跳过多少数量的结果，以文档ID排序<br><strong>响应JSON对象</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>offset (number)：</code>多少数量的结果被跳过<br>&emsp;&emsp;&emsp;&emsp;    * <code>total_rows (number) ：</code>复制文档的总数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>id (string)：</code>复制ID或者当复制状态为完成或失败时为空<br>&emsp;&emsp;&emsp;&emsp;    * <code>state(string)：</code>复制状态(<code>initializing,running,completed,pending,crashing,error,failed</code>)<br>&emsp;&emsp;&emsp;&emsp;    * <code>database (string)：</code>复制文档的目标数据库<br>&emsp;&emsp;&emsp;&emsp;    * <code>doc_id (string)：</code>复制文档ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>last_update(string)：</code>最后一次更新的时间<br>&emsp;&emsp;&emsp;&emsp;    * <code>info(object)：</code>关于状态的可能的额外信息<br>&emsp;&emsp;&emsp;&emsp;    * <code>node (string)：</code>运行任务的集群中的节点<br>&emsp;&emsp;&emsp;&emsp;    * <code>source (string)：</code>复制源头<br>&emsp;&emsp;&emsp;&emsp;    * <code>target (string)：</code>复制目标<br>&emsp;&emsp;&emsp;&emsp;    * <code>start_time (string)：</code>开始复制的时间戳<br>&emsp;&emsp;&emsp;&emsp;    * <code>error_count(number) ：</code>复制出现错误的数量<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 401 <code>Unauthorized</code><br><strong>JSON对象</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>revisions_checked (number):</code>在复制开始时被检查的修订版本的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>missing_revisions_found(number):</code>在源处有而目标处没有的修订版本的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>docs_read (number) :</code>从源处读取的文档的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>docs_written (number) :</code>写到目标的文档的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>changes_pending (number):</code>还没有复制完成的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>doc_write_failures (number) :</code>写目标失败的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>checkpointed_source_seq (object):</code>最后一个从源处成功复制的序列ID</p>\n<h3 id=\"2-11-node-node-name-stats\"><a href=\"#2-11-node-node-name-stats\" class=\"headerlink\" title=\"2.11 /_node/{node-name}/_stats\"></a>2.11 <code>/_node/{node-name}/_stats</code></h3><h3 id=\"2-12-node-node-name-system\"><a href=\"#2-12-node-node-name-system\" class=\"headerlink\" title=\"2.12 /_node/{node-name}/_system\"></a>2.12 <code>/_node/{node-name}/_system</code></h3><h3 id=\"2-13-node-node-name-restart\"><a href=\"#2-13-node-node-name-restart\" class=\"headerlink\" title=\"2.13 /_node/{node-name}/_restart\"></a>2.13 <code>/_node/{node-name}/_restart</code></h3><h3 id=\"2-14-utils\"><a href=\"#2-14-utils\" class=\"headerlink\" title=\"2.14 /_utils\"></a>2.14 <code>/_utils</code></h3><h3 id=\"2-15-up\"><a href=\"#2-15-up\" class=\"headerlink\" title=\"2.15 /_up\"></a>2.15 <code>/_up</code></h3><h3 id=\"2-16-uuids\"><a href=\"#2-16-uuids\" class=\"headerlink\" title=\"2.16 /_uuids\"></a>2.16 <code>/_uuids</code></h3><h3 id=\"2-17-favicon-ico\"><a href=\"#2-17-favicon-ico\" class=\"headerlink\" title=\"2.17 /favicon.ico\"></a>2.17 <code>/favicon.ico</code></h3><h3 id=\"2-18-权限认证\"><a href=\"#2-18-权限认证\" class=\"headerlink\" title=\"2.18 权限认证\"></a>2.18 权限认证</h3><h3 id=\"2-19-配置\"><a href=\"#2-19-配置\" class=\"headerlink\" title=\"2.19 配置\"></a>2.19 配置</h3>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"API\"><a href=\"#API\" class=\"headerlink\" title=\"API\"></a>API</h1><p>API URL路径可以指定访问CouchDB服务器的某个组件。URL请求结果包括标识和访问的数据库中的高效的描述字段。<br>与所有URL一样，各个组件之间用正斜杠分隔。<br>通常，以_（下划线）字符开头的URL组件和JSON字段表示服务器或返回的对象中的特殊组件或实体。例如，URL片段<code>/_all_dbs</code>获取CouchDB实例中所有数据库的列表。<br>该引用根据URL结构进行构造，如下所示。</p>\n<h2 id=\"1-基本API\"><a href=\"#1-基本API\" class=\"headerlink\" title=\"1 基本API\"></a>1 基本API</h2><p>CouchDB API是与CouchDB实例接口的主要方法。使用HTTP发出请求，请求用于从数据库请求信息，存储新数据以及对文档中存储的信息进行查看和格式化。<br>对API的请求可以按您正在访问的CouchDB系统的不同区域以及用于发送请求的HTTP方法进行分类。不同的方法意味着不同的操作，例如，从数据库中检索信息通常由<code>GET</code>操作处理，而更新则由<code>POST</code>或<code>PUT</code>请求处理。不同方法必须提供的信息之间存在一些差异。有关基本HTTP方法和请求结构的指南，请参见请求格式和响应。<br>对于几乎所有操作，都在JavaScript对象表示法（JSON）对象中定义了提交的数据和返回的数据结构。 JSON基础知识中提供了有关JSON内容和数据类型的基本信息。<br>使用标准HTTP状态代码报告访问CouchDB API时的错误。 HTTP状态代码中提供了有关CouchDB返回的通用代码的指南。<br>访问CouchDB API的特定区域时，将提供有关HTTP方法和请求，JSON结构和错误代码的特定信息和示例。</p>\n<h3 id=\"1-1-请求格式和响应\"><a href=\"#1-1-请求格式和响应\" class=\"headerlink\" title=\"1.1 请求格式和响应\"></a>1.1 请求格式和响应</h3><p>CouchDB支持以下HTTP请求方法：</p>\n<ul>\n<li><code>GET</code>:请求指定的条目。</li>\n<li><code>HEAD</code>：获取HTTP请求头部信息</li>\n<li><code>POST</code>：上传数据</li>\n<li><code>PUT</code>：用于PUT指定的资源，如创建新的对象(数据库，文档，视图，设计文档)</li>\n<li><code>DELETE</code>：删除指定的资源</li>\n<li><code>COPY</code>：特殊的方法，用于拷贝文档和对象</li>\n</ul>\n<p>如果使用CouchDB不支持的HTTP请求类型，将会返回状态码<code>405-Method Not Allowed</code>.</p>\n<h3 id=\"1-2-HTTP请求头\"><a href=\"#1-2-HTTP请求头\" class=\"headerlink\" title=\"1.2 HTTP请求头\"></a>1.2 HTTP请求头</h3><p>CouchDB使用HTTP进行所有通信。所以需要确保正确的并且是被支持的HTTP头部信息。</p>\n<h4 id=\"1-2-1-请求头\"><a href=\"#1-2-1-请求头\" class=\"headerlink\" title=\"1.2.1 请求头\"></a>1.2.1 请求头</h4><ul>\n<li><code>Accept</code>:由服务器返回指定的被接受的数据类型列表</li>\n<li><code>Content-type</code>：指定请求中提供的信息的内容类型</li>\n</ul>\n<h4 id=\"1-2-2-响应头\"><a href=\"#1-2-2-响应头\" class=\"headerlink\" title=\"1.2.2 响应头\"></a>1.2.2 响应头</h4><ul>\n<li><code>Cache-control</code>：缓存控制</li>\n<li><code>Content-length</code>：返回的内容的长度</li>\n<li><code>Content-type</code>：指定的返回数据的MIME类型</li>\n<li><code>Etag</code>：显示文档或者视图的修订版本</li>\n<li><code>Transfer-Encoding</code>：如果响应使用编码，那么则在该字段中指定</li>\n</ul>\n<h3 id=\"1-3-JSON基础\"><a href=\"#1-3-JSON基础\" class=\"headerlink\" title=\"1.3 JSON基础\"></a>1.3 JSON基础</h3><h3 id=\"1-4-HTTP状态码\"><a href=\"#1-4-HTTP状态码\" class=\"headerlink\" title=\"1.4 HTTP状态码\"></a>1.4 HTTP状态码</h3><ul>\n<li>200 - <code>OK</code>：请求完全成功</li>\n<li>201 - <code>Created</code>：文档成功创建</li>\n<li>202 - <code>Accepted</code>：请求被接受，但是相关的操作可能不完整，经常用于后台操作如数据库压缩。</li>\n<li>304 - <code>Not Modified</code>：请求的其他内容尚未修改。</li>\n<li>400 - <code>Bad Request</code>：坏的请求结构，如错误的请求URL，路径或请求头。</li>\n<li>401 - <code>Unauthorized</code>：不具备获取指定数据的权限，或者权限不支持</li>\n<li>403 - <code>Forbidden</code>：请求被服务器拒绝</li>\n<li>404 - <code>Not Found</code>：没有找到请求的数据</li>\n<li>405 - <code>Method Not Allowed</code>：请求方法不被支持</li>\n<li>406 - <code>Not Acceptable</code>：请求的内容类型不被支持</li>\n<li>409 - <code>Conflict</code>：更新数据冲突</li>\n<li>412 - <code>Preconfition Failed</code>：客户端的请求头和服务器的兼容性不匹配</li>\n<li>413 - <code>Request Entity Too Large</code>：请求的数据过大</li>\n<li>415 - <code>Unsupported Media Type</code>：支持的内容类型以及正在请求或提交的信息的内容类型表示不支持该内容类型。</li>\n<li>416 - <code>Requested Range Not Satisfiable</code>：服务器无法满足请求标头中指定的范围。</li>\n<li>417 - <code>Expectation Failed</code>：批量发送文档时，批量加载操作失败。</li>\n<li>500 - <code>Internal Server Error</code>：请求无效</li>\n</ul>\n<h2 id=\"2-服务器\"><a href=\"#2-服务器\" class=\"headerlink\" title=\"2 服务器\"></a>2 服务器</h2><h3 id=\"2-1\"><a href=\"#2-1\" class=\"headerlink\" title=\"2.1 /\"></a>2.1 <code>/</code></h3><p><code>GET /</code>:访问CouchDB实例的Root并返回关于实例的元数据。<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code></p>\n<h3 id=\"2-2-active-tasks\"><a href=\"#2-2-active-tasks\" class=\"headerlink\" title=\"2.2 /_active_tasks\"></a>2.2 <code>/_active_tasks</code></h3><p><code>GET /_active_tasks</code>:列出运行中的任务，包括任务类型，名字，状态和进程ID。<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>响应JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>changes_done(number):</code>处理的变更<br>&emsp;&emsp;&emsp;&emsp;    * <code>database(string):</code>源数据库<br>&emsp;&emsp;&emsp;&emsp;    * <code>pid(string):</code>进程ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>progress(number):</code>当前进度百分比<br>&emsp;&emsp;&emsp;&emsp;    * <code>started_on(number):</code>任务开始时间<br>&emsp;&emsp;&emsp;&emsp;    * <code>status(string):</code>任务状态信息<br>&emsp;&emsp;&emsp;&emsp;    * <code>task(string):</code>任务名称<br>&emsp;&emsp;&emsp;&emsp;    * <code>total_changes(number):</code>进程总的改变<br>&emsp;&emsp;&emsp;&emsp;    * <code>type(string):</code>操作类型<br>&emsp;&emsp;&emsp;&emsp;    * <code>update_on(number):</code>最后一次更新时间<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 401 <code>Unauthorized</code></p>\n<h3 id=\"2-3-all-dbs\"><a href=\"#2-3-all-dbs\" class=\"headerlink\" title=\"2.3 /_all_dbs\"></a>2.3 <code>/_all_dbs</code></h3><p><code>GET /_all_dbs</code>:返回CouchDB实例所有数据库列表<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>请求参数：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>descending(boolean):</code>按键降序返回数据库。 默认为false。<br>&emsp;&emsp;&emsp;&emsp;    * <code>endkey(json):</code>到达指定的键时，停止返回数据库。<br>&emsp;&emsp;&emsp;&emsp;    * <code>end_key(json):</code>endkey的别名<br>&emsp;&emsp;&emsp;&emsp;    * <code>limit(number):</code>返回数据库数量的限制<br>&emsp;&emsp;&emsp;&emsp;    * <code>skip(number):</code>跳过此值得数据库，默认为0<br>&emsp;&emsp;&emsp;&emsp;    * <code>startkey(json):</code>从指定的键返回数据库<br>&emsp;&emsp;&emsp;&emsp;    * <code>start_key(json):</code>startkey的别名<br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>200 OK</code></p>\n<h3 id=\"2-4-dbs-info\"><a href=\"#2-4-dbs-info\" class=\"headerlink\" title=\"2.4 /_dbs_info\"></a>2.4 <code>/_dbs_info</code></h3><p><code>GET /_dbs_info</code>:返回CouchDB实例所有数据库列表的数据库信息<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>请求JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>keys(array):</code>被请求的数据库名字(数组)<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code></p>\n<h3 id=\"2-5-cluster-setup\"><a href=\"#2-5-cluster-setup\" class=\"headerlink\" title=\"2.5 /_cluster_setup\"></a>2.5 <code>/_cluster_setup</code></h3><p><code>GET /_cluster_setup</code>:根据群集设置向导返回节点或群集的状态。<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>请求参数：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>ensure_dbs_exist(array):</code>列出系统数据库确保在节点/集群上存在,默认：<code>[&quot;_users&quot;,&quot;_replicator&quot;, &quot;_global_changes&quot;]</code>.<br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>响应JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>state(string):</code>节点/集群的当前状态(见下面)<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code></p>\n<p><strong>STATE</strong></p>\n<ul>\n<li><code>cluster_disabled</code>:当前节点完全没有被配置。</li>\n<li><code>single_node_disabled</code>：当前节点被配置为单节点，不具备服务器基本的管理员用户定义并且完整的标准系统数据库没有被创建。如果指定了<code>ensure_dbs_exist</code>查询参数，则提供的数据库列表将覆盖标准系统数据库的默认列表。</li>\n<li><code>single_node_enabled</code>：当前节点被配置为单节点，具有服务器基本的管理员用户定义并且<code>ensure_dbs_exist</code>列表中的数据库已经被创建。</li>\n<li><code>cluster_enabled</code>：当前节点集群数量大于1，没有绑定在<code>127.0.0.1</code>，具有服务器基本的管理员用户定义但是完整的标准系统数据库没有被创建。如果指定了<code>ensure_dbs_exist</code>查询参数，则提供的数据库列表将覆盖标准系统数据库的默认列表。</li>\n<li><code>cluster_finished</code>：当前节点集群数量大于1，没有绑定在<code>127.0.0.1</code>，具有服务器基本的管理员用户定义并且<code>ensure_dbs_exist</code>列表中的数据库已经被创建。</li>\n</ul>\n<p><code>POST /_cluster_setup</code>:配置一个节点作为单节点，或者是集群的一部分，或者完成集群。<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>请求JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>action(string):</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>enable_single_node:</code>配置当前节点为单节点<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>enable_cluster:</code>配置本地或远程节点，准备将它添加到新的CouchDB集群<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>add_node:</code>添加一个指定的远程节点到该集群列表中<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>finish_cluster:</code>完成集群的创建并创建标准的系统数据库<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>bind_address(string):</code>当前节点绑定的IP地址<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>username(string):</code>服务器级别的管理员用户名<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>password(string):</code>服务器级别的管理员密码<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>port(number):</code>该节点或远程节点(只用于<code>add_node</code>)绑定的TCP端口<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>node_count(number):</code>集群中节点总数<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>remote_node(string):</code>配置集群中的远程该节点的IP地址<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>remote_current_user(string):</code>服务器级别的管理员授权到远程节点的管理员用户名<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>remote_current_password(string):</code>服务器级别的管理员授权到远程节点的管理员密码<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>host(string):</code>添加到集群的远程节点的IP地址<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>ensure_dbs_exist(array):</code>列出系统数据库确保在该节点上存在。</p>\n<h3 id=\"2-6-db-updates\"><a href=\"#2-6-db-updates\" class=\"headerlink\" title=\"2.6 /_db_updates\"></a>2.6 <code>/_db_updates</code></h3><p><code>GET /_db_updates</code>:返回CouchDB实例上所有数据库事件列表<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>请求参数：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>feed(string)</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>normal</code>:返回所有数据库历史变化，然后关闭连接。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>longpoll</code>:在第一次事件后关闭连接<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>continuous</code>:每个事件发送一行JSON，一直保持socket开启直到超时。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>eventsource</code>:与<code>continuous</code>相似，只是以<code>EventSource</code>格式发送。<br>&emsp;&emsp;&emsp;&emsp;    * <code>timeout(number)</code>：指定多少秒后关闭CouchDB连接<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>heartbeat(number)</code>:每隔多少周期毫秒发送空行保持连接，默认60000<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>since(string)</code>:仅返回指定的序列ID更新<br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>响应JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>results(array)</code>:数据库事件的数组<br>&emsp;&emsp;&emsp;&emsp;    * <code>last_seq(string)</code>:记录的最后一个序列ID<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 404 <code>Unauthorized</code><br><strong>JSON对象：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>db_name(string):</code>数据库名称<br>&emsp;&emsp;&emsp;&emsp;    * <code>type(string):</code>数据库事件类型(<code>created,updated,deleted</code>)<br>&emsp;&emsp;&emsp;&emsp;    * <code>seq(json):</code>事件更新序列</p>\n<h3 id=\"2-7-membership\"><a href=\"#2-7-membership\" class=\"headerlink\" title=\"2.7 /_membership\"></a>2.7 <code>/_membership</code></h3><p><code>GET /_membership</code>:显示<code>cluster_nodes</code>集群中的部分节点。 <code>all_nodes</code>字段显示该节点知道的所有节点，包括属于集群的节点。在设置集群时非常有用，<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code></p>\n<h3 id=\"2-8-replicate\"><a href=\"#2-8-replicate\" class=\"headerlink\" title=\"2.8 /_replicate\"></a>2.8 <code>/_replicate</code></h3><p><code>GET /_replicate</code>:请求，配置或停止复制操作<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain</code><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type:</code> <code>application/json</code><br><strong>请求JSON对象:</strong><br>&emsp;&emsp;&emsp;&emsp;        * <code>cancel(boolean):</code>取消复制<br>&emsp;&emsp;&emsp;&emsp;        * <code>continuous(boolean):</code>继续复制<br>&emsp;&emsp;&emsp;&emsp;        * <code>create_target(boolean):</code>创建目标数据库，要求管理员权限<br>&emsp;&emsp;&emsp;&emsp;        * <code>doc_ids(array):</code>同步的文档ID的数组<br>&emsp;&emsp;&emsp;&emsp;        * <code>filter(string):</code>过滤器函数的名字<br>&emsp;&emsp;&emsp;&emsp;        * <code>proxy(string):</code>代理服务器的地址<br>&emsp;&emsp;&emsp;&emsp;        * <code>source(string/object):</code>源数据库名字或URL或对象，包含完整的源数据库URL以及参数例如头部信息<br>&emsp;&emsp;&emsp;&emsp;        * <code>target(string/object):</code>目标数据库名字或URL或对象，包含完整的目标数据库URL以及参数例如头部信息<br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>text/plain;charset=utf-8</code><br><strong>响应JSON对象:</strong><br>&emsp;&emsp;&emsp;&emsp;        * <code>history(array):</code>复制历史(见下面)<br>&emsp;&emsp;&emsp;&emsp;        * <code>ok(boolean):</code>复制状态<br>&emsp;&emsp;&emsp;&emsp;        * <code>replication_id_version(number):</code>复制协议版本<br>&emsp;&emsp;&emsp;&emsp;        * <code>session_id(string):</code>唯一的sessionID<br>&emsp;&emsp;&emsp;&emsp;        * <code>source_last_seq(number):</code>从源数据库读取的最后的序列号<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 202 <code>Accepted</code><br>&emsp;&emsp;&emsp;&emsp;    * 400 <code>Bad Request</code><br>&emsp;&emsp;&emsp;&emsp;    * 401 <code>Unauthorized</code><br>&emsp;&emsp;&emsp;&emsp;    * 404 <code>Not Found</code><br>&emsp;&emsp;&emsp;&emsp;    * 500 <code>Internal Server Error</code><br><strong>JSON对象:</strong><br>&emsp;&emsp;&emsp;&emsp;        * <code>doc_write_failures(number):</code>文档写失败的数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>docs_read(number):</code>文档读取的数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>docs_written (number):</code>文档成功写到目标的数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>end_last_seq (number):</code>更新流中最后的序列号<br>&emsp;&emsp;&emsp;&emsp;        * <code>end_time (string):</code>复制操作完成的时间<br>&emsp;&emsp;&emsp;&emsp;        * <code>missing_checked (number):</code>检查到的缺失的文档数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>missing_found (number):</code>缺失的文档被发现的数量<br>&emsp;&emsp;&emsp;&emsp;        * <code>recorded_seq (number):</code>记录的最后的序列号<br>&emsp;&emsp;&emsp;&emsp;        * <code>session_id (string):</code>这次复制操作的SessionID<br>&emsp;&emsp;&emsp;&emsp;        * <code>start_last_seq (number):</code>更新流中第一个序列号<br>&emsp;&emsp;&emsp;&emsp;        * <code>start_time (string):</code>复制操作开始的时间</p>\n<h4 id=\"2-8-1-复制操作\"><a href=\"#2-8-1-复制操作\" class=\"headerlink\" title=\"2.8.1 复制操作\"></a>2.8.1 复制操作</h4><p>复制的目的是，在过程结束时，源数据库上的所有活动文档也都在目标数据库中，并且在源数据库中删除的所有文档也都在目标数据库上被删除（如果存在）。<br>复制可以描述为推式或拉式复制：<br>&emsp;&emsp;&emsp;&emsp; * 拉复制是当远程CouchDB实例是源数据库，目标是本地数据库的位置。<br>如果源数据库具有永久IP地址，而目标（本地）数据库可能具有动态分配的IP地址（例如，通过DHCP），则Pull复制是最有用的解决方案。 如果要从中央服务器复制到移动设备或其他设备，则这尤其重要。<br>&emsp;&emsp;&emsp;&emsp; * 推复制是当本地数据库为源数据库，远程数据库为目标数据库。</p>\n<h4 id=\"2-8-2-指定源和目标数据库\"><a href=\"#2-8-2-指定源和目标数据库\" class=\"headerlink\" title=\"2.8.2 指定源和目标数据库\"></a>2.8.2 指定源和目标数据库</h4><p>如果要在以下两种情况之一中执行复制，则必须使用CouchDB数据库的URL规范：<br>&emsp;&emsp;&emsp;&emsp; * 使用远程数据库进行复制（即CouchDB的另一个实例在同一主机或其他主机上）<br>&emsp;&emsp;&emsp;&emsp; * 数据库复制需要权限认证。<br>例如，要请求向其发送请求的CouchDB实例本地数据库和远程数据库之间复制，可以使用以下请求：</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://coucdb-remote:5984/recipes&quot;,\n}</code></pre><p>在所有情况下，源规范和目标规范中所请求的数据库都必须存在。 如果不这样做，则将在JSON对象内返回错误：</p>\n<pre><code>{\n    &quot;reason&quot; : &quot;could not open http://couchdb-remote:5984/ol1ka/&quot;,\n}</code></pre><p>您可以通过将<code>create_target</code>字段添加到请求对象来创建目标数据库（只要您的用户凭据允许）：</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    &quot;create_target&quot; : true\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://couchdb-remote:5984/recipes&quot;,\n}</code></pre><p><code>create_target</code>字段不是破坏性的。 如果数据库已经存在，则复制将正常进行。</p>\n<h4 id=\"2-8-3-单个复制\"><a href=\"#2-8-3-单个复制\" class=\"headerlink\" title=\"2.8.3 单个复制\"></a>2.8.3 单个复制</h4><p>您可以请求复制数据库，以便可以同步两个数据库。 默认情况下，复制过程会发生一次，并将两个数据库同步在一起。 例如，您可以通过在请求JSON内容中提供<code>source</code>和<code>target</code>字段来请求两个数据库之间的单个同步。</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;recipes-snapshot&quot;,\n}</code></pre><p>在上面的示例中，数据库配方和配方快照将被同步。 这些数据库是发出请求的CouchDB实例的本地数据库。响应将是一个JSON结构，其中包含同步过程的成功（或失败），以及有关该过程的统计信息：</p>\n<pre><code>{\n    &quot;ok&quot; : true,\n    &quot;history&quot; : [\n        {\n            &quot;docs_read&quot; : 1000,\n            &quot;session_id&quot; : &quot;52c2370f5027043d286daca4de247db0&quot;,\n            &quot;recorded_seq&quot; : 1000,\n            &quot;end_last_seq&quot; : 1000,\n            &quot;doc_write_failures&quot; : 0,\n            &quot;start_time&quot; : &quot;Thu, 28 Oct 2010 10:24:13 GMT&quot;,\n            &quot;start_last_seq&quot; : 0,\n            &quot;end_time&quot; : &quot;Thu, 28 Oct 2010 10:24:14 GMT&quot;,\n            &quot;missing_checked&quot; : 0,\n            &quot;docs_written&quot; : 1000,\n            &quot;missing_found&quot; : 1000\n        }\n    ],\n    &quot;session_id&quot; : &quot;52c2370f5027043d286daca4de247db0&quot;,\n    &quot;source_last_seq&quot; : 1000\n}</code></pre><h4 id=\"2-8-4-继续复制\"><a href=\"#2-8-4-继续复制\" class=\"headerlink\" title=\"2.8.4 继续复制\"></a>2.8.4 继续复制</h4><p>在执行复制请求时，数据库与以前提到的方法的同步仅发生一次。 要使目标数据库从源永久复制，您必须将请求内JSON对象的Continuous字段设置为true。<br>通过连续复制，源数据库中的更改将永久复制到目标数据库，直到您明确要求停止复制为止。</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    &quot;continuous&quot; : true\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://couchdb-remote:5984/recipes&quot;,\n}</code></pre><p>只要两个实例之间存在网络连接，更改就会在两个数据库之间复制。<br>两个保持彼此同步的数据库，您需要在两个方向上设置复制；也就是说，您必须从源复制到目标，并且必须从目标复制到另一个。</p>\n<h4 id=\"2-8-5-取消继续复制\"><a href=\"#2-8-5-取消继续复制\" class=\"headerlink\" title=\"2.8.5 取消继续复制\"></a>2.8.5 取消继续复制</h4><p>您可以通过将<code>cancel</code>字段添加到JSON请求对象并将值设置为<code>true</code>来取消连续复制。请注意，请求的结构必须与原始结构相同，才能兑现取消请求。例如，如果您请求连续复制，则取消请求还必须包含连续字段。<br>例如，复制请求：</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nContent-Type: application/json\nAccept: application/json\n{\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://couchdb-remote:5984/recipes&quot;, &quot;create_target&quot; : true,\n    &quot;continuous&quot; : true\n}</code></pre><p>必须使用请求取消复制：</p>\n<pre><code>POST http://couchdb:5984/_replicate HTTP/1.1 \nAccept: application/json\nContent-Type: application/json\n{\n    &quot;cancel&quot; : true,\n    &quot;continuous&quot; : true\n    &quot;create_target&quot; : true,\n    &quot;source&quot; : &quot;recipes&quot;,\n    &quot;target&quot; : &quot;http://couchdb-remote:5984/recipes&quot;,\n}</code></pre><p>请求取消不存在的复制将导致404错误。</p>\n<h3 id=\"2-9-scheduler-jobs\"><a href=\"#2-9-scheduler-jobs\" class=\"headerlink\" title=\"2.9 /_scheduler/jobs\"></a>2.9 <code>/_scheduler/jobs</code></h3><p><code>GET /_scheduler/jobs</code>:列出复制任务<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>请求参数</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>limit(number)：</code>返回多少数量的结果<br>&emsp;&emsp;&emsp;&emsp;    * <code>skip(number):</code>跳过多少数量的结果，以复制ID排序<br><strong>响应JSON对象</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>offset (number)：</code>多少数量的结果被跳过<br>&emsp;&emsp;&emsp;&emsp;    * <code>total_rows (number) ：</code>复制任务的总数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>id (string)：</code>复制ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>database (string)：</code>复制文档数据库<br>&emsp;&emsp;&emsp;&emsp;    * <code>doc_id (string)：</code>复制文档ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>history (list)：</code>事件的时间戳历史以对象列表展示<br>&emsp;&emsp;&emsp;&emsp;    * <code>pid (string)：</code>复制进程ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>node (string)：</code>运行任务的集群中的节点<br>&emsp;&emsp;&emsp;&emsp;    * <code>source (string)：</code>复制源头<br>&emsp;&emsp;&emsp;&emsp;    * <code>target (string)：</code>复制目标<br>&emsp;&emsp;&emsp;&emsp;    * <code>start_time (string)：</code>开始复制的时间戳<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 401 <code>Unauthorized</code></p>\n<h3 id=\"2-10-scheduler-docs\"><a href=\"#2-10-scheduler-docs\" class=\"headerlink\" title=\"2.10 /_scheduler/docs\"></a>2.10 <code>/_scheduler/docs</code></h3><p><code>GET /_scheduler/docs</code>:列出复制文档的状态<br><strong>请求头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Accept-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>响应头：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>Content-Type-</code><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;        * <code>application/json</code><br><strong>请求参数：</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>limit(number):</code>返回多少结果<br>&emsp;&emsp;&emsp;&emsp;    * <code>skip(number):</code>跳过多少数量的结果，以文档ID排序<br><strong>响应JSON对象</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>offset (number)：</code>多少数量的结果被跳过<br>&emsp;&emsp;&emsp;&emsp;    * <code>total_rows (number) ：</code>复制文档的总数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>id (string)：</code>复制ID或者当复制状态为完成或失败时为空<br>&emsp;&emsp;&emsp;&emsp;    * <code>state(string)：</code>复制状态(<code>initializing,running,completed,pending,crashing,error,failed</code>)<br>&emsp;&emsp;&emsp;&emsp;    * <code>database (string)：</code>复制文档的目标数据库<br>&emsp;&emsp;&emsp;&emsp;    * <code>doc_id (string)：</code>复制文档ID<br>&emsp;&emsp;&emsp;&emsp;    * <code>last_update(string)：</code>最后一次更新的时间<br>&emsp;&emsp;&emsp;&emsp;    * <code>info(object)：</code>关于状态的可能的额外信息<br>&emsp;&emsp;&emsp;&emsp;    * <code>node (string)：</code>运行任务的集群中的节点<br>&emsp;&emsp;&emsp;&emsp;    * <code>source (string)：</code>复制源头<br>&emsp;&emsp;&emsp;&emsp;    * <code>target (string)：</code>复制目标<br>&emsp;&emsp;&emsp;&emsp;    * <code>start_time (string)：</code>开始复制的时间戳<br>&emsp;&emsp;&emsp;&emsp;    * <code>error_count(number) ：</code>复制出现错误的数量<br><strong>状态码：</strong><br>&emsp;&emsp;&emsp;&emsp;    * 200 <code>OK</code><br>&emsp;&emsp;&emsp;&emsp;    * 401 <code>Unauthorized</code><br><strong>JSON对象</strong><br>&emsp;&emsp;&emsp;&emsp;    * <code>revisions_checked (number):</code>在复制开始时被检查的修订版本的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>missing_revisions_found(number):</code>在源处有而目标处没有的修订版本的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>docs_read (number) :</code>从源处读取的文档的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>docs_written (number) :</code>写到目标的文档的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>changes_pending (number):</code>还没有复制完成的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>doc_write_failures (number) :</code>写目标失败的数量<br>&emsp;&emsp;&emsp;&emsp;    * <code>checkpointed_source_seq (object):</code>最后一个从源处成功复制的序列ID</p>\n<h3 id=\"2-11-node-node-name-stats\"><a href=\"#2-11-node-node-name-stats\" class=\"headerlink\" title=\"2.11 /_node/{node-name}/_stats\"></a>2.11 <code>/_node/{node-name}/_stats</code></h3><h3 id=\"2-12-node-node-name-system\"><a href=\"#2-12-node-node-name-system\" class=\"headerlink\" title=\"2.12 /_node/{node-name}/_system\"></a>2.12 <code>/_node/{node-name}/_system</code></h3><h3 id=\"2-13-node-node-name-restart\"><a href=\"#2-13-node-node-name-restart\" class=\"headerlink\" title=\"2.13 /_node/{node-name}/_restart\"></a>2.13 <code>/_node/{node-name}/_restart</code></h3><h3 id=\"2-14-utils\"><a href=\"#2-14-utils\" class=\"headerlink\" title=\"2.14 /_utils\"></a>2.14 <code>/_utils</code></h3><h3 id=\"2-15-up\"><a href=\"#2-15-up\" class=\"headerlink\" title=\"2.15 /_up\"></a>2.15 <code>/_up</code></h3><h3 id=\"2-16-uuids\"><a href=\"#2-16-uuids\" class=\"headerlink\" title=\"2.16 /_uuids\"></a>2.16 <code>/_uuids</code></h3><h3 id=\"2-17-favicon-ico\"><a href=\"#2-17-favicon-ico\" class=\"headerlink\" title=\"2.17 /favicon.ico\"></a>2.17 <code>/favicon.ico</code></h3><h3 id=\"2-18-权限认证\"><a href=\"#2-18-权限认证\" class=\"headerlink\" title=\"2.18 权限认证\"></a>2.18 权限认证</h3><h3 id=\"2-19-配置\"><a href=\"#2-19-配置\" class=\"headerlink\" title=\"2.19 配置\"></a>2.19 配置</h3>"},{"title":"CouchDB学习-集群管理","date":"2019-12-21T07:24:56.000Z","_content":"[官方文档](https://docs.couchdb.org/en/stable/cluster/index.html)\n# 集群管理\n## 理论\n在`etc/fefault.ini`文件中有以下部分：\n```\n[cluster]\nq=8\nn=3\n```\n\n* q - 分片的数量\n* n - 每一份文档的拷贝数量(加上原文档一共几份副本)\n\n创建数据库时可以通过覆盖该值修改为自己的值。\n在集群操作中，获取操作中CouchDB返回状态码200或者是写操作返回状态码201即为大多数成员达成一致。大多数成员定义为相关拷贝的数量的一半。对于“读写”操作，“相关副本”的定义稍有不同。\n对于读操作，相关副本的数量是保存请求数据的当前可访问分片的数量，这意味着在发生故障或网络分区的情况下，相关副本的数量可能少于集群中的副本数量。 可以使用r参数设置读取份数。\n对于写操作，相关副本的数量始终为n，即集群中的副本数量。 对于写操作，可以使用w参数设置份数。 如果少于此数量的可用节点，则返回202。\n## 节点管理\n### 查看所有节点\n```\ncurl -u admin:adminpw -X GET http://localhost:5984/_membership\n{\n    \"all_nodes\":[   # 当前节点所知道的节点\n        \"node1@xxx.xxx.xxx.xxx\"],\n    \"cluster_nodes\":[ #当前节点所连接的节点\n        \"node1@xxx.xxx.xxx.xxx\"],\n}\n```\n### 添加一个节点\n```\ncurl -u admin:adminpw -X PUT http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy -d {}\n```\n### 删除一个节点\n```\n#首先获取关于文档的revision\ncurl -u admin:adminpw -X GET \"http://xxx.xxx.xxx.xxx:5986/_nodes/node2@yyy.yyy.yyy.yyy\"\n{\"_id\":\"node2@yyy.yyy.yyy.yyy\",\"_rev\":\"1-967a00dff5e02add41820138abb3284d\"}\t\n#删除节点\ncurl -u admin:adminpw -X DELETE http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy?rev=1-967a00dff5e02add41820138abb3284d\n```\n\n## 数据库管理\n### 创建数据库\n数据库名字不支持大写字符，只支持[a-z],[0-9],以及特殊字符:`_ $ ( ) + - /`\n```\n#创建一个数据库名字为db_name \ncurl -u admin:adminpw -X PUT http://localhost:5984/db_name?q=4&n=2\n```\n### 删除数据库\n```\ncurl -u admin:adminpw -X DELETE http://localhost:5984/db_name\n```\n### 在一个具体的节点放置数据库\n在CouchDB 2.0群集功能的前身BigCouch中，存在区域的概念。 CouchDB 2.0通过集群放置规则来实现这一目标。\n使用`placement`参数将覆盖分片副本基数的标准逻辑（由[cluster] `n`指定）。\n首先，每个节点必须标记有zone属性。 这定义了每个节点所在的区域。您可以通过编辑`/nodes`数据库中的节点文档来实现，该文档可通过“后门”（5986）端口进行访问。 添加以下形式的键值对：\n```\n\"zone\":\"metro-dc-a\"\n```\n在集群上所有节点上操作。\n在每一个节点的配置文件`local.ini`或者`default.ini`中，定义相同的集群设置：\n```\n[cluster]\nplacement = metro-dc-a:2,metro-dc-b:1\n```\n在此示例中，它将确保将一个分区的两个副本托管在将`zone`属性设置为`metro-dc-a`的节点上，并将一个副本副本托管在一个将`zone`属性设置为`metro-dc-b`的新副本上。\n请注意，您还可以使用该系统，通过为群集中的某些节点提供不出现在[cluster]放置字符串中的zone属性，来确保它们不承载新创建的数据库的任何副本。\n## 分片管理\n### 介绍\n本文档讨论了分片在CouchDB中的工作方式，以及如何安全地添加，移动，删除和创建分片和分片副本的放置规则。\n分片是数据库中数据的水平分区。将数据划分为多个碎片，并将每个碎片的副本（称为“碎片副本”或简称为“副本”）分布到群集中的不同节点，可以提高数据的持久性，防止节点丢失。CouchDB群集自动分片数据库，并在节点之间分配组成每个分片的文档子集。 修改群集成员身份和分片行为必须手动完成。\n### 分片和复制\n可以在全局级别或每个数据库的基础上设置每个数据库有多少个分片和副本。 相关参数是`q`和`n`。\n`q`是要维护的数据库分片数。`n`是要分发的每个文档的副本数。`n`的默认值为3,`q`的默认值为8。当`q`= 8时，数据库分为8个分片。在`n`=3的情况下，群集分发每个分片的三个副本。总共，一个数据库有24个分片副本。在默认的3节点群集中，每个节点将接收8个分片。在4节点群集中，每个节点将接收6个分片。 在一般情况下，我们建议群集中的节点数应为n的倍数，以使碎片均匀分布。\nCouchDB节点的`etc/local.ini`文件中的`cluster`部分：\n```\n[cluster]\nq=8\nn=3\n```\n可以修改这些设置以设置所有数据库的分片默认值，或者可以在创建数据库时通过指定q和n查询参数来针对每个数据库进行设置。 例如：\n```\ncurl -X PUT \"http://localhost:5984/database-name?q=4&n=2\"\n```\n这将创建一个数据库，该数据库分为4个分片和2个副本，从而产生8个分片副本分布在整个数据库中\n的集群上。\n### Quorum\n取决于集群的大小，每个数据库的分片数量以及分片副本的数量，并非每个节点都可以访问每个分片，但是每个节点都知道可以通过CouchDB的内部分片在哪里找到每个分片的所有副本。\n进入CouchDB集群的每个请求均由任意一个随机协调节点处理。该协调节点将请求代理给其他具有相关数据的节点，这些数据可能包含也可能不包含自身。一旦达到法定数量的数据库节点响应，协调节点就会向客户端发送响应。2 默认情况下,默认的法定仲裁大小等于`r=w=((n+1)/2)`，其中`r`表示读取仲裁的大小，`w`表示写入仲裁的大小，`n`表示数字每个分片的副本。在n为3的默认群集中，`((n+1)/2)`将为2。\n集群中的每个节点都可以作为任何请求的协调节点。集群内部没有专门的节点角色。\n可以在请求时通过设置文档和视图读取的`r`参数以及文档写入的`w`参数来配置所需仲裁的大小。例如，这是一个请求，一旦至少两个节点已响应，该请求便指示协调节点发送响应：\n```\ncurl \"$COUCH_URL:5984/<db>/<doc>?r=2\"\n```\n这是写文档的类似示例：\n```\ncurl -X PUT \"$COUCH_URL:5984/<db>/<doc>?w=2\" -d '{...}'\n```\n将`r`或`w`设置为等于n（副本数）意味着只有在所有具有相关分片的节点都响应或超时后，您才会收到响应，因此这种方法不能保证`ACID`的一致性。 将`r`或`w`设置为1意味着仅一个相关节点响应后，您将收到响应。\n### 数据库分片测试\n有一些API端点可以帮助您了解如何分片数据库。 首先，在集群上创建一个新数据库，然后将几个文档放入其中：\n```\n$ curl -X PUT $COUCH_URL:5984/mydb\n{\"ok\":true}\n$ curl -X PUT $COUCH_URL:5984/mydb/joan -d '{\"loves\":\"cats\"}'\n{\"ok\":true,\"id\":\"joan\",\"rev\":\"1-cc240d66a894a7ee7ad3160e69f9051f\"}\n$ curl -X PUT $COUCH_URL:5984/mydb/robert -d '{\"loves\":\"dogs\"}'\n{\"ok\":true,\"id\":\"robert\",\"rev\":\"1-4032b428c7574a85bc04f1f271be446e\"}\n```\n首先，`/db`将告诉您数据库的分片参数：\n```\ncurl -s $COUCH_URL:5984/db | jq .\n{\n  \"db_name\": \"mydb\",\n...\n  \"cluster\": {\n    \"q\": 8,\n    \"n\": 3,\n    \"w\": 2,\n    \"r\": 2\n}, ...\n}\n```\n因此，我们知道此数据库是由8个分片(`q`=8)建的，每个分片具有3个副本(`n`=3),集群中节点之间总共有24个分片副本。\n现在，让我们看一下这些分片副本如何通过`/db/_shards`端点放置在集群上：\n```\ncurl -s $COUCH_URL:5984/mydb/_shards | jq .\n{\n\"shards\": {\n    \"00000000-1fffffff\": [\n      \"node1@127.0.0.1\",\n      \"node2@127.0.0.1\",\n      \"node4@127.0.0.1\"\n    ],\n    \"20000000-3fffffff\": [\n      \"node1@127.0.0.1\",\n      \"node2@127.0.0.1\",\n      \"node3@127.0.0.1\"\n    ],\n    ...\n  }\n}\n```\n现在我们看到该集群中实际上有4个节点，并且CouchDB已将这24个分片副本均匀地分布在所有4个节点上。\n我们还可以确切地看到哪个分片包含具有`/db/_shards/doc`端点的给定文档：\n```\ncurl -s $COUCH_URL:5984/mydb/_shards/joan | jq .\n{\n  \"range\": \"e0000000-ffffffff\",\n  \"nodes\": [\n    \"node1@127.0.0.1\",\n    \"node3@127.0.0.1\",\n    \"node4@127.0.0.1\"\n] }\n$ curl -s $COUCH_URL:5984/mydb/_shards/robert | jq .\n{\n  \"range\": \"60000000-7fffffff\",\n  \"nodes\": [\n   \"node1@127.0.0.1\",\n    \"node3@127.0.0.1\",\n    \"node4@127.0.0.1\"\n   ] \n}\n```\nCouchDB向我们展示了两个示例文档中每个映射到的特定分片。\n### 移动一个分片\n本节介绍如何手动放置和更换碎片。 当您确定群集太大或太小，并且想要成功调整其大小，或者从服务器指标中注意到数据库/碎片布局不是最佳的，并且您需要一些“热点”时，这些活动是至关重要的步骤 解决。\n考虑一个`q`=8和`n`=3的三节点群集。每个数据库有24个分片，分布在三个节点上。如果将第四个节点添加到集群，则CouchDB不会将现有数据库分片重新分配给该集群。 这将导致负载不平衡，因为新节点将仅托管其加入集群后创建的数据库的分片。 为了平衡现有数据库中的分片分布，必须手动移动它们。\n在集群中的节点上移动分片涉及以下几个步骤：\n\n1. 确保目标节点已经加入集群\n2. 将分片和任何辅助索引分片复制到目标节点上。\n3. 设置目标节点为维护模式。\n4. 更新集群元数据反映新的目标分片。\n5. 监视内部复制以确保最新的分片。\n6. 清除目标节点的维护模式。\n7. 再次更新集群元数据移除原分片。\n8. 移除原节点的分片和任何辅助索引分片.\n\n#### 拷贝分片文件\n从技术上讲，复制数据库和辅助索引碎片是可选的。 如果在不执行此数据副本的情况下继续进行下一步，则CouchDB将使用内部复制来填充新添加的分片副本。 但是，复制文件的速度比内部复制快，尤其是在繁忙的群集上，这就是为什么我们建议首先执行此手动数据复制的原因。\n碎片文件位于CouchDB安装目录的`data/shards`目录中。这些子目录中包含分片文件本身。例如，对于一个名为`abc`的`q`=8数据库，这是其数据库分片文件：\n```\ndata/shards/00000000-1fffffff/abc.1529362187.couch\ndata/shards/20000000-3fffffff/abc.1529362187.couch\ndata/shards/40000000-5fffffff/abc.1529362187.couch\n...\n```\n辅助索引(包括`JavaScript`视图，`Erlang`视图和`Mango`索引)也被分片，并且应移动它们的分片以节省新节点重建视图的工作量。查看`data/.`中的分片。例如：\n```\ndata/.shards\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design/mrview\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design/mrview/3e823c2a4383ac0c18d4e574135a5b08.view\n...\n```\n由于它们是文件，因此可以使用`cp`，`rsync`，`scp`或其他文件复制命令将它们从一个节点复制到另一个节点。 例如：\n```\n# 1主机\n$ mkdir -p data/.shards/<range>\n$ mkdir -p data/shards/<range>\n# 2主机\n$ scp <couch-dir>/data/.shards/<range>/<database>.<datecode>* \\\n<node>:<couch-dir>/data/.shards/<range>/\n$ scp <couch-dir>/data/shards/<range>/<database>.<datecode>.couch \\\n  <node>:<couch-dir>/data/shards/<range>/\n```\n先移动视图文件再移动数据库文件！ 如果视图索引在其数据库之前，则数据库将从头开始重建它。\n#### 设置目标节点为维护模式\n在告诉CouchDB节点上的这些新分片之前，必须将节点置于维护模式。维护模式指示CouchDB返回`404 Not Found`响应在`/_up`端点，并确保其不参与其分片的常规交互式集群请求。使用GET`/_up`检查节点的运行状况的正确配置的负载均衡器将检测到此404并将该节点从循环中删除，从而阻止将请求发送到该节点。 例如，要将HAProxy配置为使用`/_up`端点，请使用：\n```\nhttp-check disable-on-404\noption httpchk GET /_up\n```\n如果未设置维护模式，或者负载平衡器忽略了此维护模式状态，则在执行下一步之后，群集在咨询相关节点时可能会返回错误的响应。不要这样做！在接下来的步骤中，我们将确保此分片是最新的，然后再允许其参与最终用户的请求。\n启用维护模式：\n```\n curl -X PUT -H \"Content-type: application/json\" \\ $COUCH_URL:5984/_node/<nodename>/_config/couchdb/maintenance_mode \\ -d \"\\\"true\\\"\"\n```\n然后，通过在该节点的单个端点上执行GET`/_up`来验证该节点是否处于维护模式：\n```\ncurl -v $COUCH_URL/_up\n...\n< HTTP/1.1 404 Object Not Found\n...\n{\"status\":\"maintenance_mode\"}\n```\n最后，检查负载均衡器是否已从可用后端节点池中删除了该节点。\n#### 更新集群元数据反映新的目标分片。\n现在我们需要告诉CouchDB，目标节点（必须已经加入集群）应该为给定数据库托管碎片副本。\n要更新群集元数据，请使用特殊的`/_dbs`数据库，该数据库是内部CouchDB数据库，它将数据库映射到分片和节点。该数据库在节点之间复制。它只能通过节点本地端口（通常是端口5986）进行访问。默认情况下，出于安全目的，此端口仅在localhost接口上可用。\n首先，检索数据库的当前元数据：\n```\ncurl http://localhost:5986/_dbs/{name}\n{\n  \"_id\": \"{name}\",\n  \"_rev\": \"1-e13fb7e79af3b3107ed62925058bfa3a\",\n  \"shard_suffix\": [46, 49, 53, 51, 48, 50, 51, 50, 53, 50, 54],\n  \"changelog\": [\n    [\"add\", \"00000000-1fffffff\", \"node1@xxx.xxx.xxx.xxx\"],\n    [\"add\", \"00000000-1fffffff\", \"node2@xxx.xxx.xxx.xxx\"],\n    [\"add\", \"00000000-1fffffff\", \"node3@xxx.xxx.xxx.xxx\"],\n    ...\n  ],\n  \"by_node\": {\n    \"node1@xxx.xxx.xxx.xxx\": [\n      \"00000000-1fffffff\",\n      ...\n    ],\n    ... \n  },\n  \"by_range\": {\n    \"00000000-1fffffff\": [\n      \"node1@xxx.xxx.xxx.xxx\",\n      \"node2@xxx.xxx.xxx.xxx\",\n      \"node3@xxx.xxx.xxx.xxx\"\n    ],\n    ...\n   } \n}\n```\n这是该文档的简要剖析：\n\n* `_id`:数据库的名字\n* `_rev`:元数据的当前版本\n* `shard_suffix`:数据库创建时的时间戳，在Unix时期映射到ASCII数字的代码点后的秒。\n* `changelog`:数据库分片的历史\n* `by_node`:每个节点的分片列表\n* `by_range`:每个分片由哪些节点持有。\n\n要反映元数据中的分片移动，请执行以下三个步骤：\n\n1. 添加合适的`changelog`实体。\n2. 更新`by_node`实体。\n3. 更新`by_range`实体。\n\n在修改时，此过程必须手动完成。\n要将分片添加到节点，请将以下条目添加到数据库元数据的`changelog`属性中：\n```\n[\"add\", \"<range>\", \"<node-name>\"]\n```\n`<range>`是特定的硬范围设置。`<node-name>`应该与集群中GET`/_membership`中显示的节点的名称和地址匹配。\n如果从节点移除一个分片，简单地将`add`替换为`remove`。\n找到新的变更日志条目后，将需要更新`by_node`和`by_range`以反映谁在存储哪些分片。 更改日志条目中的数据和这些属性必须匹配。 否则，数据库可能会损坏。\n继续我们的示例，这是上面的元数据的更新版本，该版本将分片添加到名为node4的其他节点中：\n```\n{\n\n  \"_id\": \"{name}\",\n  \"_rev\": \"1-e13fb7e79af3b3107ed62925058bfa3a\",\n  \"shard_suffix\": [46, 49, 53, 51, 48, 50, 51, 50, 53, 50, 54],\n  \"changelog\": [\n    [\"add\", \"00000000-1fffffff\", \"node1@xxx.xxx.xxx.xxx\"],\n    [\"add\", \"00000000-1fffffff\", \"node2@xxx.xxx.xxx.xxx\"],\n    [\"add\", \"00000000-1fffffff\", \"node3@xxx.xxx.xxx.xxx\"],\n    ...\n    [\"add\", \"00000000-1fffffff\", \"node4@xxx.xxx.xxx.xxx\"]\n  ],\n  \"by_node\": {\n    \"node1@xxx.xxx.xxx.xxx\": [\n      \"00000000-1fffffff\",\n    ... \n    ],\n    ...\n    \"node4@xxx.xxx.xxx.xxx\": [\n      \"00000000-1fffffff\"\n    ]\n  },\n  \"by_range\": {\n    \"00000000-1fffffff\": [\n      \"node1@xxx.xxx.xxx.xxx\",\n      \"node2@xxx.xxx.xxx.xxx\",\n      \"node3@xxx.xxx.xxx.xxx\",\n      \"node4@xxx.xxx.xxx.xxx\"\n    ],\n    ...\n  }\n}\n```\n现在可以`PUT`新元数据：\n```\ncurl -X PUT http://localhost:5986/_dbs/{name} -d '{...}'\n```\n#### 强制同步分片\n无论您是否将分片预先复制到新节点，都可以强制CouchDB同步所有分片的所有副本。\n具有`/db/_sync_shards`端点的数据库中的分片：\n```\ncurl -X POST $COUCH_URL:5984/{dbname}/_sync_shards\n{\"ok\":true}\n```\n这将启动同步过程。 请注意，这将给群集增加额外的负载，这可能会影响性能。\n通过写入存储在该分片中的文档，也可以在每个分片的基础上强制进行同步。\n#### 监视内部复制以确保最新的分片\n完成上一步后，CouchDB将开始同步分片。可以通过监视`/_node/<nodename>/_system`端点(包括`internal_replication_jobs`指标)来观察这种情况。\n一旦此指标从开始分片同步之前返回到基线，或者为0，分片副本就可以提供数据了，我们可以使节点退出维护模式。\n#### 清除目标节点的维护模式\n现在，可以像在步骤2中一样，通过在维护模式配置端点上放置`false`,使节点开始为数据请求提供服务。\n通过在该节点的单个端点上执行GET`/_up`来验证该节点是否不在维护模式下。最后，检查负载均衡器是否已将该节点返回到可用后端节点池中。\n#### 再次更新集群元数据移除原分片\n现在，以与在步骤2中将新目标分片添加到分片图中相同的方式，从分片图中删除源分片。确保将`[“ remove”，<range>，<source-shard>]`条目添加到 更改日志的末尾，以及修改数据库元数据文档的`by_node`和`by_range`部分。\n#### 移除原节点的分片和任何辅助索引分片\n最后，可以通过从源主机上的命令行中删除源碎片副本的文件以及任何视图碎片副本来删除源碎片副本：\n```\nrm <couch-dir>/data/shards/<range>/<dbname>.<datecode>.couch\nrm -r <couch-dir>/data/.shards/<range>/<dbname>.<datecode>*\n```\n恭喜你！ 您已经移动了数据库分片副本。通过以这种方式添加和删除数据库分片副本，您可以更改集群的分片布局，也称为分片映射。\n### 指定数据库放置位置\n您可以配置CouchDB，以使用放置规则在数据库创建时将碎片副本放置在某些节点上。\n首先，每个节点必须标记有zone属性。 这定义了每个节点所在的区域。您可以通过编辑`/_nodes`数据库中的节点文档来实现，该文档可通过本地节点端口进行访问。 添加以下形式的键值对：\n```\n\"zone\": \"{zone-name}\"\n```\n在集群中的每一个节点都这样做：\n```\ncurl -X PUT http://localhost:5986/_nodes/<node-name> \\ -d '{ \\\n        \"_id\": \"<node-name>\",\n        \"_rev\": \"<rev>\",\n        \"zone\": \"<zone-name>\"\n        }'\n```\n在每个节点的本地配置文件（local.ini）中，定义一个一致的群集范围设置，例如：\n```\n[cluster]\nplacement = <zone-name-1>:2,<zone-name-2>:1\n```\n在此示例中，CouchDB将确保将分区的两个副本托管在区域属性设置为`<zone-name-1>`的节点上，并将一个副本托管在新的区域属性设置为`<zone-name-2>`的节点上。\n这种方法非常灵活，因为您还可以在创建数据库时使用与ini文件相同的语法，通过将放置设置指定为查询参数来基于每个数据库指定区域：\n```\ncurl -X PUT $COUCH_URL:5984/<dbname>?zone=<zone>\n```\n也可以指定放置参数。 请注意，这将覆盖确定已创建副本的数量！\n请注意，您还可以使用此系统来确保群集中的某些节点不为新主机托管任何副本。\n通过为它们提供一个不会出现在`[cluster]`放置字符串中的`zone`属性，来创建数据库。\n### 修改数据库到一个新的`q`值\n数据库的q值只能在创建数据库时设置，不能进行实时重新分片。 相反，要重新分片数据库，必须重新生成它。 步骤如下：\n\n1. 通过在PUT操作期间将`q`值指定为查询参数来创建具有所需分片设置的临时数据库。\n2. 停止客户端访问数据库\n3. 将主数据库复制到临时数据库。 如果主数据库正在使用中，则可能需要多次复制。\n4. 删除主数据库，**确保没有人在使用!**\n5. 使用所需的分片设置重新创建主数据库。\n6. 客户端现在可以再次访问数据库。\n7. 将临时数据库复制回主数据库。\n8. 删除临时数据库.\n\n一旦完成所有步骤，即可再次使用该数据库。 集群将根据放置规则自动创建并分发其碎片。\n如果可以指示客户端应用程序使用新数据库而不是旧数据库，并且可以在非常短暂的中断窗口内进行切换，则可以避免生产中的停机时间。\n## 集群清除\n群集清除的主要目的是清除具有多个删除的逻辑删除或包含大量冲突的单个文档的数据库。 但是，它也可以用于清除具有任何修订版本的任何文档（已删除或未删除）。\n群集清除旨在维护最终的一致性并防止不必要的二级索引无效。 为此，每个数据库都会跟踪数据库中请求的一定数量的历史清除以及其当前的`purge_seq`。 内部复制和二级索引处理数据库的清除，并定期更新其相应的清除检查点文档以报告由其处理的`purge_seq`。 为了确保最终的一致性，数据库将仅在内部复制作业和二级索引处理了存储的历史清除请求之后，才删除它们。\n### 内部结构\n为了在节点和二级索引之间实现内部清除信息的复制，将两个内部清除树添加到数据库文件中以跟踪历史清除。\n```\npurge_tree: UUID -> {PurgeSeq, DocId, Revs}\npurge_seq_tree: PurgeSeq -> {UUID, DocId, Revs}\n```\n每次对`_purge API`的交互式请求，都会在增加`purge_seq`和`purge_request`时创建成对的有序集合，其中`purge_request`是一个包含`docid`和修订列表的元组。 对于每个`purge_request`都会生成`uuid`。清除请求将添加到内部清除树：将元组`{UUID-> {PurgeSeq，DocId，Revs}}`添加到`purge_tree`，元组 `{PurgeSeq-> {UUID，DocId，Revs}}`添加到`purge_seq_tree`。\n### 压缩清除\n在数据库压缩期间，最旧的清除请求将被删除，以仅在数据库中存储`purged_infos_limit`个清除数目。 但是，为了使数据库与索引和其他副本保持一致，我们只能删除索引和内部复制作业已处理的清除请求。因此，有时清除树可能存储的数据超过`purged_infos_limit`清除数目。 如果数据库中存储的清除数量超出`purged_infos_limit`某个阈值，则日志中会产生警告，表明数据库的清除与索引和其他副本的同步问题。\n### 本地清除检查点文档\n具有清除的数据库索引和内部复制会创建并定期更新本地检查点清除文档：`_local/purge-$type-$hash`。 这些文档报告了它们最后处理的`purge_seq`以及最后处理的时间戳。 本地检查点清除文档的示例：\n```\n{\n\"_id\": \"_local/purge-mrview-86cacdfbaf6968d4ebbc324dd3723fe7\", \"type\": \"mrview\",\n\"purge_seq\": 10,\n\"updated_on\": 1540541874,\n\"ddoc_id\": \"_design/foo\",\n\"signature\": \"5d10247925f826ae3e00966ec24b7bf6\"\n}\n```\n### 内部复制\n清除请求将以最终一致的方式在所有节点上重播。 清除的内部复制包括两个步骤：\n1.拉取复制。内部复制首先要从目标中清除并将其应用于源，以确保我们不会重新引入目标中已清除的源文档/修订版。 在这一步中，我们使用存储在目标上的清除检查点文档来跟踪源处理的最后一个目标的`purge_seq`。 我们发现清除请求在此`purge_seq`之后发生，并在源上重播它们。 通过使用最新进程`purge_seq`和时间戳更新目标的检查点清除文档来完成此步骤。\n2.推送复制。 然后，内部复制将照常进行，并插入一个额外的步骤以将源的清除请求推送到目标。 在此步骤中，我们使用本地内部复制检查点文档，这些文档在目标和源上均已更新。\n在正常情况下，交互式清除请求已发送到包含数据库碎片副本的每个节点，并应用于每个副本。节点之间清除的内部复制只是确保副本之间一致性的一个额外步骤，在此副本上，一个节点上的所有清除请求都会在另一个节点上重播。为了不在副本上重播相同的清除请求，每个交互式清除请求都用唯一的`uuid`标记。内部复制会过滤出副本的`purge_tree`中已存在的`UUID`的清除请求，并仅应用`purge_tree`中不存在的`UUID`的清除请求。 这就是为什么我们需要有两个内部清除树的原因：1）`purge_tree：{UUID-> {PurgeSeq，DocId，Revs}}`可以快速找到带有已存在的`UUID`的`purge requests`存在的副本； 2）`purge_seq_tree：{PurgeSeq-> {UUID，DocId，Revs }}`允许从给定的`purge_seq`进行迭代，以收集在此`purge_seq`之后发生的所有清除请求。\n### 索引\n每个清除请求将增加数据库的`update_seq`，以便还更新每个辅助索引，以便应用清除请求以维护主数据库内的一致性。\n### 配置设置\n这些设置可以在`default.ini`或`local.ini`中进行更新：\n\n|字段|描述|默认值|\n|---|---|---|\n|max_document_id_number|一个清除请求中允许的最大文档数|100|\n|max_revisions_number|一项清除请求中允许的最大累积修订版本数|1000|\n|allowed_purge_seq_lag|除了`purged_infos_limit`外，还允许其他缓冲区存储清除请求|100|\n|index_lag_warn_seconds|本地清除检查点文档的索引未更新时的允许持续时间|86400|\n\n在数据库压缩期间，我们检查所有检查点清除文档。 允许客户端（索引或内部复制作业）的上一次报告的`purge_seq`小于当前数据库碎片的`purge_seq`的值(`purged_infos_limit + allowed_purge_seq_lag`)。如果客户端的`purge_seq`甚至更小，并且客户端未在`index_lag_warn_seconds`内设置检查点，则它会阻止清除清除树，因此我们必须对此客户端发出以下日志警告：\n```\nPurge checkpoint '_local/purge-mrview-9152d15c12011288629bcffba7693fd4’\nnot updated in 86400 seconds in\n<<\"shards/00000000-1fffffff/testdb12.1491979089\">>\n```\n如果发生这种类型的日志警告，请检查客户端以查看为什么清除请求的处理停滞在其中。\n索引的设计文档和本地检查点文档之间存在映射关系。 如果更新或删除了索引的设计文档，则也应自动删除相应的本地检查点文档。 但是在意外情况下，当设计文档被更新/删除但其检查点文档仍然存在于数据库中时，将发出以下警告：\n```\n\"Invalid purge doc '<<\"_design/bar\">>' on database\n<<\"shards/00000000-1fffffff/testdb12.1491979089\">>\nwith purge_seq '50'\"\n```\n如果发生这种类型的日志警告，请从数据库中删除本地清除文档。","source":"_posts/blog/couchDB/CouchDB学习-集群管理.md","raw":"---\ntitle: CouchDB学习-集群管理\ndate: 2019-12-21 15:24:56\ntags: CouchDb\ncategories: CouchDb学习\n---\n[官方文档](https://docs.couchdb.org/en/stable/cluster/index.html)\n# 集群管理\n## 理论\n在`etc/fefault.ini`文件中有以下部分：\n```\n[cluster]\nq=8\nn=3\n```\n\n* q - 分片的数量\n* n - 每一份文档的拷贝数量(加上原文档一共几份副本)\n\n创建数据库时可以通过覆盖该值修改为自己的值。\n在集群操作中，获取操作中CouchDB返回状态码200或者是写操作返回状态码201即为大多数成员达成一致。大多数成员定义为相关拷贝的数量的一半。对于“读写”操作，“相关副本”的定义稍有不同。\n对于读操作，相关副本的数量是保存请求数据的当前可访问分片的数量，这意味着在发生故障或网络分区的情况下，相关副本的数量可能少于集群中的副本数量。 可以使用r参数设置读取份数。\n对于写操作，相关副本的数量始终为n，即集群中的副本数量。 对于写操作，可以使用w参数设置份数。 如果少于此数量的可用节点，则返回202。\n## 节点管理\n### 查看所有节点\n```\ncurl -u admin:adminpw -X GET http://localhost:5984/_membership\n{\n    \"all_nodes\":[   # 当前节点所知道的节点\n        \"node1@xxx.xxx.xxx.xxx\"],\n    \"cluster_nodes\":[ #当前节点所连接的节点\n        \"node1@xxx.xxx.xxx.xxx\"],\n}\n```\n### 添加一个节点\n```\ncurl -u admin:adminpw -X PUT http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy -d {}\n```\n### 删除一个节点\n```\n#首先获取关于文档的revision\ncurl -u admin:adminpw -X GET \"http://xxx.xxx.xxx.xxx:5986/_nodes/node2@yyy.yyy.yyy.yyy\"\n{\"_id\":\"node2@yyy.yyy.yyy.yyy\",\"_rev\":\"1-967a00dff5e02add41820138abb3284d\"}\t\n#删除节点\ncurl -u admin:adminpw -X DELETE http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy?rev=1-967a00dff5e02add41820138abb3284d\n```\n\n## 数据库管理\n### 创建数据库\n数据库名字不支持大写字符，只支持[a-z],[0-9],以及特殊字符:`_ $ ( ) + - /`\n```\n#创建一个数据库名字为db_name \ncurl -u admin:adminpw -X PUT http://localhost:5984/db_name?q=4&n=2\n```\n### 删除数据库\n```\ncurl -u admin:adminpw -X DELETE http://localhost:5984/db_name\n```\n### 在一个具体的节点放置数据库\n在CouchDB 2.0群集功能的前身BigCouch中，存在区域的概念。 CouchDB 2.0通过集群放置规则来实现这一目标。\n使用`placement`参数将覆盖分片副本基数的标准逻辑（由[cluster] `n`指定）。\n首先，每个节点必须标记有zone属性。 这定义了每个节点所在的区域。您可以通过编辑`/nodes`数据库中的节点文档来实现，该文档可通过“后门”（5986）端口进行访问。 添加以下形式的键值对：\n```\n\"zone\":\"metro-dc-a\"\n```\n在集群上所有节点上操作。\n在每一个节点的配置文件`local.ini`或者`default.ini`中，定义相同的集群设置：\n```\n[cluster]\nplacement = metro-dc-a:2,metro-dc-b:1\n```\n在此示例中，它将确保将一个分区的两个副本托管在将`zone`属性设置为`metro-dc-a`的节点上，并将一个副本副本托管在一个将`zone`属性设置为`metro-dc-b`的新副本上。\n请注意，您还可以使用该系统，通过为群集中的某些节点提供不出现在[cluster]放置字符串中的zone属性，来确保它们不承载新创建的数据库的任何副本。\n## 分片管理\n### 介绍\n本文档讨论了分片在CouchDB中的工作方式，以及如何安全地添加，移动，删除和创建分片和分片副本的放置规则。\n分片是数据库中数据的水平分区。将数据划分为多个碎片，并将每个碎片的副本（称为“碎片副本”或简称为“副本”）分布到群集中的不同节点，可以提高数据的持久性，防止节点丢失。CouchDB群集自动分片数据库，并在节点之间分配组成每个分片的文档子集。 修改群集成员身份和分片行为必须手动完成。\n### 分片和复制\n可以在全局级别或每个数据库的基础上设置每个数据库有多少个分片和副本。 相关参数是`q`和`n`。\n`q`是要维护的数据库分片数。`n`是要分发的每个文档的副本数。`n`的默认值为3,`q`的默认值为8。当`q`= 8时，数据库分为8个分片。在`n`=3的情况下，群集分发每个分片的三个副本。总共，一个数据库有24个分片副本。在默认的3节点群集中，每个节点将接收8个分片。在4节点群集中，每个节点将接收6个分片。 在一般情况下，我们建议群集中的节点数应为n的倍数，以使碎片均匀分布。\nCouchDB节点的`etc/local.ini`文件中的`cluster`部分：\n```\n[cluster]\nq=8\nn=3\n```\n可以修改这些设置以设置所有数据库的分片默认值，或者可以在创建数据库时通过指定q和n查询参数来针对每个数据库进行设置。 例如：\n```\ncurl -X PUT \"http://localhost:5984/database-name?q=4&n=2\"\n```\n这将创建一个数据库，该数据库分为4个分片和2个副本，从而产生8个分片副本分布在整个数据库中\n的集群上。\n### Quorum\n取决于集群的大小，每个数据库的分片数量以及分片副本的数量，并非每个节点都可以访问每个分片，但是每个节点都知道可以通过CouchDB的内部分片在哪里找到每个分片的所有副本。\n进入CouchDB集群的每个请求均由任意一个随机协调节点处理。该协调节点将请求代理给其他具有相关数据的节点，这些数据可能包含也可能不包含自身。一旦达到法定数量的数据库节点响应，协调节点就会向客户端发送响应。2 默认情况下,默认的法定仲裁大小等于`r=w=((n+1)/2)`，其中`r`表示读取仲裁的大小，`w`表示写入仲裁的大小，`n`表示数字每个分片的副本。在n为3的默认群集中，`((n+1)/2)`将为2。\n集群中的每个节点都可以作为任何请求的协调节点。集群内部没有专门的节点角色。\n可以在请求时通过设置文档和视图读取的`r`参数以及文档写入的`w`参数来配置所需仲裁的大小。例如，这是一个请求，一旦至少两个节点已响应，该请求便指示协调节点发送响应：\n```\ncurl \"$COUCH_URL:5984/<db>/<doc>?r=2\"\n```\n这是写文档的类似示例：\n```\ncurl -X PUT \"$COUCH_URL:5984/<db>/<doc>?w=2\" -d '{...}'\n```\n将`r`或`w`设置为等于n（副本数）意味着只有在所有具有相关分片的节点都响应或超时后，您才会收到响应，因此这种方法不能保证`ACID`的一致性。 将`r`或`w`设置为1意味着仅一个相关节点响应后，您将收到响应。\n### 数据库分片测试\n有一些API端点可以帮助您了解如何分片数据库。 首先，在集群上创建一个新数据库，然后将几个文档放入其中：\n```\n$ curl -X PUT $COUCH_URL:5984/mydb\n{\"ok\":true}\n$ curl -X PUT $COUCH_URL:5984/mydb/joan -d '{\"loves\":\"cats\"}'\n{\"ok\":true,\"id\":\"joan\",\"rev\":\"1-cc240d66a894a7ee7ad3160e69f9051f\"}\n$ curl -X PUT $COUCH_URL:5984/mydb/robert -d '{\"loves\":\"dogs\"}'\n{\"ok\":true,\"id\":\"robert\",\"rev\":\"1-4032b428c7574a85bc04f1f271be446e\"}\n```\n首先，`/db`将告诉您数据库的分片参数：\n```\ncurl -s $COUCH_URL:5984/db | jq .\n{\n  \"db_name\": \"mydb\",\n...\n  \"cluster\": {\n    \"q\": 8,\n    \"n\": 3,\n    \"w\": 2,\n    \"r\": 2\n}, ...\n}\n```\n因此，我们知道此数据库是由8个分片(`q`=8)建的，每个分片具有3个副本(`n`=3),集群中节点之间总共有24个分片副本。\n现在，让我们看一下这些分片副本如何通过`/db/_shards`端点放置在集群上：\n```\ncurl -s $COUCH_URL:5984/mydb/_shards | jq .\n{\n\"shards\": {\n    \"00000000-1fffffff\": [\n      \"node1@127.0.0.1\",\n      \"node2@127.0.0.1\",\n      \"node4@127.0.0.1\"\n    ],\n    \"20000000-3fffffff\": [\n      \"node1@127.0.0.1\",\n      \"node2@127.0.0.1\",\n      \"node3@127.0.0.1\"\n    ],\n    ...\n  }\n}\n```\n现在我们看到该集群中实际上有4个节点，并且CouchDB已将这24个分片副本均匀地分布在所有4个节点上。\n我们还可以确切地看到哪个分片包含具有`/db/_shards/doc`端点的给定文档：\n```\ncurl -s $COUCH_URL:5984/mydb/_shards/joan | jq .\n{\n  \"range\": \"e0000000-ffffffff\",\n  \"nodes\": [\n    \"node1@127.0.0.1\",\n    \"node3@127.0.0.1\",\n    \"node4@127.0.0.1\"\n] }\n$ curl -s $COUCH_URL:5984/mydb/_shards/robert | jq .\n{\n  \"range\": \"60000000-7fffffff\",\n  \"nodes\": [\n   \"node1@127.0.0.1\",\n    \"node3@127.0.0.1\",\n    \"node4@127.0.0.1\"\n   ] \n}\n```\nCouchDB向我们展示了两个示例文档中每个映射到的特定分片。\n### 移动一个分片\n本节介绍如何手动放置和更换碎片。 当您确定群集太大或太小，并且想要成功调整其大小，或者从服务器指标中注意到数据库/碎片布局不是最佳的，并且您需要一些“热点”时，这些活动是至关重要的步骤 解决。\n考虑一个`q`=8和`n`=3的三节点群集。每个数据库有24个分片，分布在三个节点上。如果将第四个节点添加到集群，则CouchDB不会将现有数据库分片重新分配给该集群。 这将导致负载不平衡，因为新节点将仅托管其加入集群后创建的数据库的分片。 为了平衡现有数据库中的分片分布，必须手动移动它们。\n在集群中的节点上移动分片涉及以下几个步骤：\n\n1. 确保目标节点已经加入集群\n2. 将分片和任何辅助索引分片复制到目标节点上。\n3. 设置目标节点为维护模式。\n4. 更新集群元数据反映新的目标分片。\n5. 监视内部复制以确保最新的分片。\n6. 清除目标节点的维护模式。\n7. 再次更新集群元数据移除原分片。\n8. 移除原节点的分片和任何辅助索引分片.\n\n#### 拷贝分片文件\n从技术上讲，复制数据库和辅助索引碎片是可选的。 如果在不执行此数据副本的情况下继续进行下一步，则CouchDB将使用内部复制来填充新添加的分片副本。 但是，复制文件的速度比内部复制快，尤其是在繁忙的群集上，这就是为什么我们建议首先执行此手动数据复制的原因。\n碎片文件位于CouchDB安装目录的`data/shards`目录中。这些子目录中包含分片文件本身。例如，对于一个名为`abc`的`q`=8数据库，这是其数据库分片文件：\n```\ndata/shards/00000000-1fffffff/abc.1529362187.couch\ndata/shards/20000000-3fffffff/abc.1529362187.couch\ndata/shards/40000000-5fffffff/abc.1529362187.couch\n...\n```\n辅助索引(包括`JavaScript`视图，`Erlang`视图和`Mango`索引)也被分片，并且应移动它们的分片以节省新节点重建视图的工作量。查看`data/.`中的分片。例如：\n```\ndata/.shards\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design/mrview\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design/mrview/3e823c2a4383ac0c18d4e574135a5b08.view\n...\n```\n由于它们是文件，因此可以使用`cp`，`rsync`，`scp`或其他文件复制命令将它们从一个节点复制到另一个节点。 例如：\n```\n# 1主机\n$ mkdir -p data/.shards/<range>\n$ mkdir -p data/shards/<range>\n# 2主机\n$ scp <couch-dir>/data/.shards/<range>/<database>.<datecode>* \\\n<node>:<couch-dir>/data/.shards/<range>/\n$ scp <couch-dir>/data/shards/<range>/<database>.<datecode>.couch \\\n  <node>:<couch-dir>/data/shards/<range>/\n```\n先移动视图文件再移动数据库文件！ 如果视图索引在其数据库之前，则数据库将从头开始重建它。\n#### 设置目标节点为维护模式\n在告诉CouchDB节点上的这些新分片之前，必须将节点置于维护模式。维护模式指示CouchDB返回`404 Not Found`响应在`/_up`端点，并确保其不参与其分片的常规交互式集群请求。使用GET`/_up`检查节点的运行状况的正确配置的负载均衡器将检测到此404并将该节点从循环中删除，从而阻止将请求发送到该节点。 例如，要将HAProxy配置为使用`/_up`端点，请使用：\n```\nhttp-check disable-on-404\noption httpchk GET /_up\n```\n如果未设置维护模式，或者负载平衡器忽略了此维护模式状态，则在执行下一步之后，群集在咨询相关节点时可能会返回错误的响应。不要这样做！在接下来的步骤中，我们将确保此分片是最新的，然后再允许其参与最终用户的请求。\n启用维护模式：\n```\n curl -X PUT -H \"Content-type: application/json\" \\ $COUCH_URL:5984/_node/<nodename>/_config/couchdb/maintenance_mode \\ -d \"\\\"true\\\"\"\n```\n然后，通过在该节点的单个端点上执行GET`/_up`来验证该节点是否处于维护模式：\n```\ncurl -v $COUCH_URL/_up\n...\n< HTTP/1.1 404 Object Not Found\n...\n{\"status\":\"maintenance_mode\"}\n```\n最后，检查负载均衡器是否已从可用后端节点池中删除了该节点。\n#### 更新集群元数据反映新的目标分片。\n现在我们需要告诉CouchDB，目标节点（必须已经加入集群）应该为给定数据库托管碎片副本。\n要更新群集元数据，请使用特殊的`/_dbs`数据库，该数据库是内部CouchDB数据库，它将数据库映射到分片和节点。该数据库在节点之间复制。它只能通过节点本地端口（通常是端口5986）进行访问。默认情况下，出于安全目的，此端口仅在localhost接口上可用。\n首先，检索数据库的当前元数据：\n```\ncurl http://localhost:5986/_dbs/{name}\n{\n  \"_id\": \"{name}\",\n  \"_rev\": \"1-e13fb7e79af3b3107ed62925058bfa3a\",\n  \"shard_suffix\": [46, 49, 53, 51, 48, 50, 51, 50, 53, 50, 54],\n  \"changelog\": [\n    [\"add\", \"00000000-1fffffff\", \"node1@xxx.xxx.xxx.xxx\"],\n    [\"add\", \"00000000-1fffffff\", \"node2@xxx.xxx.xxx.xxx\"],\n    [\"add\", \"00000000-1fffffff\", \"node3@xxx.xxx.xxx.xxx\"],\n    ...\n  ],\n  \"by_node\": {\n    \"node1@xxx.xxx.xxx.xxx\": [\n      \"00000000-1fffffff\",\n      ...\n    ],\n    ... \n  },\n  \"by_range\": {\n    \"00000000-1fffffff\": [\n      \"node1@xxx.xxx.xxx.xxx\",\n      \"node2@xxx.xxx.xxx.xxx\",\n      \"node3@xxx.xxx.xxx.xxx\"\n    ],\n    ...\n   } \n}\n```\n这是该文档的简要剖析：\n\n* `_id`:数据库的名字\n* `_rev`:元数据的当前版本\n* `shard_suffix`:数据库创建时的时间戳，在Unix时期映射到ASCII数字的代码点后的秒。\n* `changelog`:数据库分片的历史\n* `by_node`:每个节点的分片列表\n* `by_range`:每个分片由哪些节点持有。\n\n要反映元数据中的分片移动，请执行以下三个步骤：\n\n1. 添加合适的`changelog`实体。\n2. 更新`by_node`实体。\n3. 更新`by_range`实体。\n\n在修改时，此过程必须手动完成。\n要将分片添加到节点，请将以下条目添加到数据库元数据的`changelog`属性中：\n```\n[\"add\", \"<range>\", \"<node-name>\"]\n```\n`<range>`是特定的硬范围设置。`<node-name>`应该与集群中GET`/_membership`中显示的节点的名称和地址匹配。\n如果从节点移除一个分片，简单地将`add`替换为`remove`。\n找到新的变更日志条目后，将需要更新`by_node`和`by_range`以反映谁在存储哪些分片。 更改日志条目中的数据和这些属性必须匹配。 否则，数据库可能会损坏。\n继续我们的示例，这是上面的元数据的更新版本，该版本将分片添加到名为node4的其他节点中：\n```\n{\n\n  \"_id\": \"{name}\",\n  \"_rev\": \"1-e13fb7e79af3b3107ed62925058bfa3a\",\n  \"shard_suffix\": [46, 49, 53, 51, 48, 50, 51, 50, 53, 50, 54],\n  \"changelog\": [\n    [\"add\", \"00000000-1fffffff\", \"node1@xxx.xxx.xxx.xxx\"],\n    [\"add\", \"00000000-1fffffff\", \"node2@xxx.xxx.xxx.xxx\"],\n    [\"add\", \"00000000-1fffffff\", \"node3@xxx.xxx.xxx.xxx\"],\n    ...\n    [\"add\", \"00000000-1fffffff\", \"node4@xxx.xxx.xxx.xxx\"]\n  ],\n  \"by_node\": {\n    \"node1@xxx.xxx.xxx.xxx\": [\n      \"00000000-1fffffff\",\n    ... \n    ],\n    ...\n    \"node4@xxx.xxx.xxx.xxx\": [\n      \"00000000-1fffffff\"\n    ]\n  },\n  \"by_range\": {\n    \"00000000-1fffffff\": [\n      \"node1@xxx.xxx.xxx.xxx\",\n      \"node2@xxx.xxx.xxx.xxx\",\n      \"node3@xxx.xxx.xxx.xxx\",\n      \"node4@xxx.xxx.xxx.xxx\"\n    ],\n    ...\n  }\n}\n```\n现在可以`PUT`新元数据：\n```\ncurl -X PUT http://localhost:5986/_dbs/{name} -d '{...}'\n```\n#### 强制同步分片\n无论您是否将分片预先复制到新节点，都可以强制CouchDB同步所有分片的所有副本。\n具有`/db/_sync_shards`端点的数据库中的分片：\n```\ncurl -X POST $COUCH_URL:5984/{dbname}/_sync_shards\n{\"ok\":true}\n```\n这将启动同步过程。 请注意，这将给群集增加额外的负载，这可能会影响性能。\n通过写入存储在该分片中的文档，也可以在每个分片的基础上强制进行同步。\n#### 监视内部复制以确保最新的分片\n完成上一步后，CouchDB将开始同步分片。可以通过监视`/_node/<nodename>/_system`端点(包括`internal_replication_jobs`指标)来观察这种情况。\n一旦此指标从开始分片同步之前返回到基线，或者为0，分片副本就可以提供数据了，我们可以使节点退出维护模式。\n#### 清除目标节点的维护模式\n现在，可以像在步骤2中一样，通过在维护模式配置端点上放置`false`,使节点开始为数据请求提供服务。\n通过在该节点的单个端点上执行GET`/_up`来验证该节点是否不在维护模式下。最后，检查负载均衡器是否已将该节点返回到可用后端节点池中。\n#### 再次更新集群元数据移除原分片\n现在，以与在步骤2中将新目标分片添加到分片图中相同的方式，从分片图中删除源分片。确保将`[“ remove”，<range>，<source-shard>]`条目添加到 更改日志的末尾，以及修改数据库元数据文档的`by_node`和`by_range`部分。\n#### 移除原节点的分片和任何辅助索引分片\n最后，可以通过从源主机上的命令行中删除源碎片副本的文件以及任何视图碎片副本来删除源碎片副本：\n```\nrm <couch-dir>/data/shards/<range>/<dbname>.<datecode>.couch\nrm -r <couch-dir>/data/.shards/<range>/<dbname>.<datecode>*\n```\n恭喜你！ 您已经移动了数据库分片副本。通过以这种方式添加和删除数据库分片副本，您可以更改集群的分片布局，也称为分片映射。\n### 指定数据库放置位置\n您可以配置CouchDB，以使用放置规则在数据库创建时将碎片副本放置在某些节点上。\n首先，每个节点必须标记有zone属性。 这定义了每个节点所在的区域。您可以通过编辑`/_nodes`数据库中的节点文档来实现，该文档可通过本地节点端口进行访问。 添加以下形式的键值对：\n```\n\"zone\": \"{zone-name}\"\n```\n在集群中的每一个节点都这样做：\n```\ncurl -X PUT http://localhost:5986/_nodes/<node-name> \\ -d '{ \\\n        \"_id\": \"<node-name>\",\n        \"_rev\": \"<rev>\",\n        \"zone\": \"<zone-name>\"\n        }'\n```\n在每个节点的本地配置文件（local.ini）中，定义一个一致的群集范围设置，例如：\n```\n[cluster]\nplacement = <zone-name-1>:2,<zone-name-2>:1\n```\n在此示例中，CouchDB将确保将分区的两个副本托管在区域属性设置为`<zone-name-1>`的节点上，并将一个副本托管在新的区域属性设置为`<zone-name-2>`的节点上。\n这种方法非常灵活，因为您还可以在创建数据库时使用与ini文件相同的语法，通过将放置设置指定为查询参数来基于每个数据库指定区域：\n```\ncurl -X PUT $COUCH_URL:5984/<dbname>?zone=<zone>\n```\n也可以指定放置参数。 请注意，这将覆盖确定已创建副本的数量！\n请注意，您还可以使用此系统来确保群集中的某些节点不为新主机托管任何副本。\n通过为它们提供一个不会出现在`[cluster]`放置字符串中的`zone`属性，来创建数据库。\n### 修改数据库到一个新的`q`值\n数据库的q值只能在创建数据库时设置，不能进行实时重新分片。 相反，要重新分片数据库，必须重新生成它。 步骤如下：\n\n1. 通过在PUT操作期间将`q`值指定为查询参数来创建具有所需分片设置的临时数据库。\n2. 停止客户端访问数据库\n3. 将主数据库复制到临时数据库。 如果主数据库正在使用中，则可能需要多次复制。\n4. 删除主数据库，**确保没有人在使用!**\n5. 使用所需的分片设置重新创建主数据库。\n6. 客户端现在可以再次访问数据库。\n7. 将临时数据库复制回主数据库。\n8. 删除临时数据库.\n\n一旦完成所有步骤，即可再次使用该数据库。 集群将根据放置规则自动创建并分发其碎片。\n如果可以指示客户端应用程序使用新数据库而不是旧数据库，并且可以在非常短暂的中断窗口内进行切换，则可以避免生产中的停机时间。\n## 集群清除\n群集清除的主要目的是清除具有多个删除的逻辑删除或包含大量冲突的单个文档的数据库。 但是，它也可以用于清除具有任何修订版本的任何文档（已删除或未删除）。\n群集清除旨在维护最终的一致性并防止不必要的二级索引无效。 为此，每个数据库都会跟踪数据库中请求的一定数量的历史清除以及其当前的`purge_seq`。 内部复制和二级索引处理数据库的清除，并定期更新其相应的清除检查点文档以报告由其处理的`purge_seq`。 为了确保最终的一致性，数据库将仅在内部复制作业和二级索引处理了存储的历史清除请求之后，才删除它们。\n### 内部结构\n为了在节点和二级索引之间实现内部清除信息的复制，将两个内部清除树添加到数据库文件中以跟踪历史清除。\n```\npurge_tree: UUID -> {PurgeSeq, DocId, Revs}\npurge_seq_tree: PurgeSeq -> {UUID, DocId, Revs}\n```\n每次对`_purge API`的交互式请求，都会在增加`purge_seq`和`purge_request`时创建成对的有序集合，其中`purge_request`是一个包含`docid`和修订列表的元组。 对于每个`purge_request`都会生成`uuid`。清除请求将添加到内部清除树：将元组`{UUID-> {PurgeSeq，DocId，Revs}}`添加到`purge_tree`，元组 `{PurgeSeq-> {UUID，DocId，Revs}}`添加到`purge_seq_tree`。\n### 压缩清除\n在数据库压缩期间，最旧的清除请求将被删除，以仅在数据库中存储`purged_infos_limit`个清除数目。 但是，为了使数据库与索引和其他副本保持一致，我们只能删除索引和内部复制作业已处理的清除请求。因此，有时清除树可能存储的数据超过`purged_infos_limit`清除数目。 如果数据库中存储的清除数量超出`purged_infos_limit`某个阈值，则日志中会产生警告，表明数据库的清除与索引和其他副本的同步问题。\n### 本地清除检查点文档\n具有清除的数据库索引和内部复制会创建并定期更新本地检查点清除文档：`_local/purge-$type-$hash`。 这些文档报告了它们最后处理的`purge_seq`以及最后处理的时间戳。 本地检查点清除文档的示例：\n```\n{\n\"_id\": \"_local/purge-mrview-86cacdfbaf6968d4ebbc324dd3723fe7\", \"type\": \"mrview\",\n\"purge_seq\": 10,\n\"updated_on\": 1540541874,\n\"ddoc_id\": \"_design/foo\",\n\"signature\": \"5d10247925f826ae3e00966ec24b7bf6\"\n}\n```\n### 内部复制\n清除请求将以最终一致的方式在所有节点上重播。 清除的内部复制包括两个步骤：\n1.拉取复制。内部复制首先要从目标中清除并将其应用于源，以确保我们不会重新引入目标中已清除的源文档/修订版。 在这一步中，我们使用存储在目标上的清除检查点文档来跟踪源处理的最后一个目标的`purge_seq`。 我们发现清除请求在此`purge_seq`之后发生，并在源上重播它们。 通过使用最新进程`purge_seq`和时间戳更新目标的检查点清除文档来完成此步骤。\n2.推送复制。 然后，内部复制将照常进行，并插入一个额外的步骤以将源的清除请求推送到目标。 在此步骤中，我们使用本地内部复制检查点文档，这些文档在目标和源上均已更新。\n在正常情况下，交互式清除请求已发送到包含数据库碎片副本的每个节点，并应用于每个副本。节点之间清除的内部复制只是确保副本之间一致性的一个额外步骤，在此副本上，一个节点上的所有清除请求都会在另一个节点上重播。为了不在副本上重播相同的清除请求，每个交互式清除请求都用唯一的`uuid`标记。内部复制会过滤出副本的`purge_tree`中已存在的`UUID`的清除请求，并仅应用`purge_tree`中不存在的`UUID`的清除请求。 这就是为什么我们需要有两个内部清除树的原因：1）`purge_tree：{UUID-> {PurgeSeq，DocId，Revs}}`可以快速找到带有已存在的`UUID`的`purge requests`存在的副本； 2）`purge_seq_tree：{PurgeSeq-> {UUID，DocId，Revs }}`允许从给定的`purge_seq`进行迭代，以收集在此`purge_seq`之后发生的所有清除请求。\n### 索引\n每个清除请求将增加数据库的`update_seq`，以便还更新每个辅助索引，以便应用清除请求以维护主数据库内的一致性。\n### 配置设置\n这些设置可以在`default.ini`或`local.ini`中进行更新：\n\n|字段|描述|默认值|\n|---|---|---|\n|max_document_id_number|一个清除请求中允许的最大文档数|100|\n|max_revisions_number|一项清除请求中允许的最大累积修订版本数|1000|\n|allowed_purge_seq_lag|除了`purged_infos_limit`外，还允许其他缓冲区存储清除请求|100|\n|index_lag_warn_seconds|本地清除检查点文档的索引未更新时的允许持续时间|86400|\n\n在数据库压缩期间，我们检查所有检查点清除文档。 允许客户端（索引或内部复制作业）的上一次报告的`purge_seq`小于当前数据库碎片的`purge_seq`的值(`purged_infos_limit + allowed_purge_seq_lag`)。如果客户端的`purge_seq`甚至更小，并且客户端未在`index_lag_warn_seconds`内设置检查点，则它会阻止清除清除树，因此我们必须对此客户端发出以下日志警告：\n```\nPurge checkpoint '_local/purge-mrview-9152d15c12011288629bcffba7693fd4’\nnot updated in 86400 seconds in\n<<\"shards/00000000-1fffffff/testdb12.1491979089\">>\n```\n如果发生这种类型的日志警告，请检查客户端以查看为什么清除请求的处理停滞在其中。\n索引的设计文档和本地检查点文档之间存在映射关系。 如果更新或删除了索引的设计文档，则也应自动删除相应的本地检查点文档。 但是在意外情况下，当设计文档被更新/删除但其检查点文档仍然存在于数据库中时，将发出以下警告：\n```\n\"Invalid purge doc '<<\"_design/bar\">>' on database\n<<\"shards/00000000-1fffffff/testdb12.1491979089\">>\nwith purge_seq '50'\"\n```\n如果发生这种类型的日志警告，请从数据库中删除本地清除文档。","slug":"blog/couchDB/CouchDB学习-集群管理","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqygv0010k0vqbjii90p4","content":"<p><a href=\"https://docs.couchdb.org/en/stable/cluster/index.html\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1 id=\"集群管理\"><a href=\"#集群管理\" class=\"headerlink\" title=\"集群管理\"></a>集群管理</h1><h2 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h2><p>在<code>etc/fefault.ini</code>文件中有以下部分：</p>\n<pre><code>[cluster]\nq=8\nn=3</code></pre><ul>\n<li>q - 分片的数量</li>\n<li>n - 每一份文档的拷贝数量(加上原文档一共几份副本)</li>\n</ul>\n<p>创建数据库时可以通过覆盖该值修改为自己的值。<br>在集群操作中，获取操作中CouchDB返回状态码200或者是写操作返回状态码201即为大多数成员达成一致。大多数成员定义为相关拷贝的数量的一半。对于“读写”操作，“相关副本”的定义稍有不同。<br>对于读操作，相关副本的数量是保存请求数据的当前可访问分片的数量，这意味着在发生故障或网络分区的情况下，相关副本的数量可能少于集群中的副本数量。 可以使用r参数设置读取份数。<br>对于写操作，相关副本的数量始终为n，即集群中的副本数量。 对于写操作，可以使用w参数设置份数。 如果少于此数量的可用节点，则返回202。</p>\n<h2 id=\"节点管理\"><a href=\"#节点管理\" class=\"headerlink\" title=\"节点管理\"></a>节点管理</h2><h3 id=\"查看所有节点\"><a href=\"#查看所有节点\" class=\"headerlink\" title=\"查看所有节点\"></a>查看所有节点</h3><pre><code>curl -u admin:adminpw -X GET http://localhost:5984/_membership\n{\n    &quot;all_nodes&quot;:[   # 当前节点所知道的节点\n        &quot;node1@xxx.xxx.xxx.xxx&quot;],\n    &quot;cluster_nodes&quot;:[ #当前节点所连接的节点\n        &quot;node1@xxx.xxx.xxx.xxx&quot;],\n}</code></pre><h3 id=\"添加一个节点\"><a href=\"#添加一个节点\" class=\"headerlink\" title=\"添加一个节点\"></a>添加一个节点</h3><pre><code>curl -u admin:adminpw -X PUT http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy -d {}</code></pre><h3 id=\"删除一个节点\"><a href=\"#删除一个节点\" class=\"headerlink\" title=\"删除一个节点\"></a>删除一个节点</h3><pre><code>#首先获取关于文档的revision\ncurl -u admin:adminpw -X GET &quot;http://xxx.xxx.xxx.xxx:5986/_nodes/node2@yyy.yyy.yyy.yyy&quot;\n{&quot;_id&quot;:&quot;node2@yyy.yyy.yyy.yyy&quot;,&quot;_rev&quot;:&quot;1-967a00dff5e02add41820138abb3284d&quot;}    \n#删除节点\ncurl -u admin:adminpw -X DELETE http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy?rev=1-967a00dff5e02add41820138abb3284d</code></pre><h2 id=\"数据库管理\"><a href=\"#数据库管理\" class=\"headerlink\" title=\"数据库管理\"></a>数据库管理</h2><h3 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h3><p>数据库名字不支持大写字符，只支持[a-z],[0-9],以及特殊字符:<code>_ $ ( ) + - /</code></p>\n<pre><code>#创建一个数据库名字为db_name \ncurl -u admin:adminpw -X PUT http://localhost:5984/db_name?q=4&amp;n=2</code></pre><h3 id=\"删除数据库\"><a href=\"#删除数据库\" class=\"headerlink\" title=\"删除数据库\"></a>删除数据库</h3><pre><code>curl -u admin:adminpw -X DELETE http://localhost:5984/db_name</code></pre><h3 id=\"在一个具体的节点放置数据库\"><a href=\"#在一个具体的节点放置数据库\" class=\"headerlink\" title=\"在一个具体的节点放置数据库\"></a>在一个具体的节点放置数据库</h3><p>在CouchDB 2.0群集功能的前身BigCouch中，存在区域的概念。 CouchDB 2.0通过集群放置规则来实现这一目标。<br>使用<code>placement</code>参数将覆盖分片副本基数的标准逻辑（由[cluster] <code>n</code>指定）。<br>首先，每个节点必须标记有zone属性。 这定义了每个节点所在的区域。您可以通过编辑<code>/nodes</code>数据库中的节点文档来实现，该文档可通过“后门”（5986）端口进行访问。 添加以下形式的键值对：</p>\n<pre><code>&quot;zone&quot;:&quot;metro-dc-a&quot;</code></pre><p>在集群上所有节点上操作。<br>在每一个节点的配置文件<code>local.ini</code>或者<code>default.ini</code>中，定义相同的集群设置：</p>\n<pre><code>[cluster]\nplacement = metro-dc-a:2,metro-dc-b:1</code></pre><p>在此示例中，它将确保将一个分区的两个副本托管在将<code>zone</code>属性设置为<code>metro-dc-a</code>的节点上，并将一个副本副本托管在一个将<code>zone</code>属性设置为<code>metro-dc-b</code>的新副本上。<br>请注意，您还可以使用该系统，通过为群集中的某些节点提供不出现在[cluster]放置字符串中的zone属性，来确保它们不承载新创建的数据库的任何副本。</p>\n<h2 id=\"分片管理\"><a href=\"#分片管理\" class=\"headerlink\" title=\"分片管理\"></a>分片管理</h2><h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><p>本文档讨论了分片在CouchDB中的工作方式，以及如何安全地添加，移动，删除和创建分片和分片副本的放置规则。<br>分片是数据库中数据的水平分区。将数据划分为多个碎片，并将每个碎片的副本（称为“碎片副本”或简称为“副本”）分布到群集中的不同节点，可以提高数据的持久性，防止节点丢失。CouchDB群集自动分片数据库，并在节点之间分配组成每个分片的文档子集。 修改群集成员身份和分片行为必须手动完成。</p>\n<h3 id=\"分片和复制\"><a href=\"#分片和复制\" class=\"headerlink\" title=\"分片和复制\"></a>分片和复制</h3><p>可以在全局级别或每个数据库的基础上设置每个数据库有多少个分片和副本。 相关参数是<code>q</code>和<code>n</code>。<br><code>q</code>是要维护的数据库分片数。<code>n</code>是要分发的每个文档的副本数。<code>n</code>的默认值为3,<code>q</code>的默认值为8。当<code>q</code>= 8时，数据库分为8个分片。在<code>n</code>=3的情况下，群集分发每个分片的三个副本。总共，一个数据库有24个分片副本。在默认的3节点群集中，每个节点将接收8个分片。在4节点群集中，每个节点将接收6个分片。 在一般情况下，我们建议群集中的节点数应为n的倍数，以使碎片均匀分布。<br>CouchDB节点的<code>etc/local.ini</code>文件中的<code>cluster</code>部分：</p>\n<pre><code>[cluster]\nq=8\nn=3</code></pre><p>可以修改这些设置以设置所有数据库的分片默认值，或者可以在创建数据库时通过指定q和n查询参数来针对每个数据库进行设置。 例如：</p>\n<pre><code>curl -X PUT &quot;http://localhost:5984/database-name?q=4&amp;n=2&quot;</code></pre><p>这将创建一个数据库，该数据库分为4个分片和2个副本，从而产生8个分片副本分布在整个数据库中<br>的集群上。</p>\n<h3 id=\"Quorum\"><a href=\"#Quorum\" class=\"headerlink\" title=\"Quorum\"></a>Quorum</h3><p>取决于集群的大小，每个数据库的分片数量以及分片副本的数量，并非每个节点都可以访问每个分片，但是每个节点都知道可以通过CouchDB的内部分片在哪里找到每个分片的所有副本。<br>进入CouchDB集群的每个请求均由任意一个随机协调节点处理。该协调节点将请求代理给其他具有相关数据的节点，这些数据可能包含也可能不包含自身。一旦达到法定数量的数据库节点响应，协调节点就会向客户端发送响应。2 默认情况下,默认的法定仲裁大小等于<code>r=w=((n+1)/2)</code>，其中<code>r</code>表示读取仲裁的大小，<code>w</code>表示写入仲裁的大小，<code>n</code>表示数字每个分片的副本。在n为3的默认群集中，<code>((n+1)/2)</code>将为2。<br>集群中的每个节点都可以作为任何请求的协调节点。集群内部没有专门的节点角色。<br>可以在请求时通过设置文档和视图读取的<code>r</code>参数以及文档写入的<code>w</code>参数来配置所需仲裁的大小。例如，这是一个请求，一旦至少两个节点已响应，该请求便指示协调节点发送响应：</p>\n<pre><code>curl &quot;$COUCH_URL:5984/&lt;db&gt;/&lt;doc&gt;?r=2&quot;</code></pre><p>这是写文档的类似示例：</p>\n<pre><code>curl -X PUT &quot;$COUCH_URL:5984/&lt;db&gt;/&lt;doc&gt;?w=2&quot; -d &#39;{...}&#39;</code></pre><p>将<code>r</code>或<code>w</code>设置为等于n（副本数）意味着只有在所有具有相关分片的节点都响应或超时后，您才会收到响应，因此这种方法不能保证<code>ACID</code>的一致性。 将<code>r</code>或<code>w</code>设置为1意味着仅一个相关节点响应后，您将收到响应。</p>\n<h3 id=\"数据库分片测试\"><a href=\"#数据库分片测试\" class=\"headerlink\" title=\"数据库分片测试\"></a>数据库分片测试</h3><p>有一些API端点可以帮助您了解如何分片数据库。 首先，在集群上创建一个新数据库，然后将几个文档放入其中：</p>\n<pre><code>$ curl -X PUT $COUCH_URL:5984/mydb\n{&quot;ok&quot;:true}\n$ curl -X PUT $COUCH_URL:5984/mydb/joan -d &#39;{&quot;loves&quot;:&quot;cats&quot;}&#39;\n{&quot;ok&quot;:true,&quot;id&quot;:&quot;joan&quot;,&quot;rev&quot;:&quot;1-cc240d66a894a7ee7ad3160e69f9051f&quot;}\n$ curl -X PUT $COUCH_URL:5984/mydb/robert -d &#39;{&quot;loves&quot;:&quot;dogs&quot;}&#39;\n{&quot;ok&quot;:true,&quot;id&quot;:&quot;robert&quot;,&quot;rev&quot;:&quot;1-4032b428c7574a85bc04f1f271be446e&quot;}</code></pre><p>首先，<code>/db</code>将告诉您数据库的分片参数：</p>\n<pre><code>curl -s $COUCH_URL:5984/db | jq .\n{\n  &quot;db_name&quot;: &quot;mydb&quot;,\n...\n  &quot;cluster&quot;: {\n    &quot;q&quot;: 8,\n    &quot;n&quot;: 3,\n    &quot;w&quot;: 2,\n    &quot;r&quot;: 2\n}, ...\n}</code></pre><p>因此，我们知道此数据库是由8个分片(<code>q</code>=8)建的，每个分片具有3个副本(<code>n</code>=3),集群中节点之间总共有24个分片副本。<br>现在，让我们看一下这些分片副本如何通过<code>/db/_shards</code>端点放置在集群上：</p>\n<pre><code>curl -s $COUCH_URL:5984/mydb/_shards | jq .\n{\n&quot;shards&quot;: {\n    &quot;00000000-1fffffff&quot;: [\n      &quot;node1@127.0.0.1&quot;,\n      &quot;node2@127.0.0.1&quot;,\n      &quot;node4@127.0.0.1&quot;\n    ],\n    &quot;20000000-3fffffff&quot;: [\n      &quot;node1@127.0.0.1&quot;,\n      &quot;node2@127.0.0.1&quot;,\n      &quot;node3@127.0.0.1&quot;\n    ],\n    ...\n  }\n}</code></pre><p>现在我们看到该集群中实际上有4个节点，并且CouchDB已将这24个分片副本均匀地分布在所有4个节点上。<br>我们还可以确切地看到哪个分片包含具有<code>/db/_shards/doc</code>端点的给定文档：</p>\n<pre><code>curl -s $COUCH_URL:5984/mydb/_shards/joan | jq .\n{\n  &quot;range&quot;: &quot;e0000000-ffffffff&quot;,\n  &quot;nodes&quot;: [\n    &quot;node1@127.0.0.1&quot;,\n    &quot;node3@127.0.0.1&quot;,\n    &quot;node4@127.0.0.1&quot;\n] }\n$ curl -s $COUCH_URL:5984/mydb/_shards/robert | jq .\n{\n  &quot;range&quot;: &quot;60000000-7fffffff&quot;,\n  &quot;nodes&quot;: [\n   &quot;node1@127.0.0.1&quot;,\n    &quot;node3@127.0.0.1&quot;,\n    &quot;node4@127.0.0.1&quot;\n   ] \n}</code></pre><p>CouchDB向我们展示了两个示例文档中每个映射到的特定分片。</p>\n<h3 id=\"移动一个分片\"><a href=\"#移动一个分片\" class=\"headerlink\" title=\"移动一个分片\"></a>移动一个分片</h3><p>本节介绍如何手动放置和更换碎片。 当您确定群集太大或太小，并且想要成功调整其大小，或者从服务器指标中注意到数据库/碎片布局不是最佳的，并且您需要一些“热点”时，这些活动是至关重要的步骤 解决。<br>考虑一个<code>q</code>=8和<code>n</code>=3的三节点群集。每个数据库有24个分片，分布在三个节点上。如果将第四个节点添加到集群，则CouchDB不会将现有数据库分片重新分配给该集群。 这将导致负载不平衡，因为新节点将仅托管其加入集群后创建的数据库的分片。 为了平衡现有数据库中的分片分布，必须手动移动它们。<br>在集群中的节点上移动分片涉及以下几个步骤：</p>\n<ol>\n<li>确保目标节点已经加入集群</li>\n<li>将分片和任何辅助索引分片复制到目标节点上。</li>\n<li>设置目标节点为维护模式。</li>\n<li>更新集群元数据反映新的目标分片。</li>\n<li>监视内部复制以确保最新的分片。</li>\n<li>清除目标节点的维护模式。</li>\n<li>再次更新集群元数据移除原分片。</li>\n<li>移除原节点的分片和任何辅助索引分片.</li>\n</ol>\n<h4 id=\"拷贝分片文件\"><a href=\"#拷贝分片文件\" class=\"headerlink\" title=\"拷贝分片文件\"></a>拷贝分片文件</h4><p>从技术上讲，复制数据库和辅助索引碎片是可选的。 如果在不执行此数据副本的情况下继续进行下一步，则CouchDB将使用内部复制来填充新添加的分片副本。 但是，复制文件的速度比内部复制快，尤其是在繁忙的群集上，这就是为什么我们建议首先执行此手动数据复制的原因。<br>碎片文件位于CouchDB安装目录的<code>data/shards</code>目录中。这些子目录中包含分片文件本身。例如，对于一个名为<code>abc</code>的<code>q</code>=8数据库，这是其数据库分片文件：</p>\n<pre><code>data/shards/00000000-1fffffff/abc.1529362187.couch\ndata/shards/20000000-3fffffff/abc.1529362187.couch\ndata/shards/40000000-5fffffff/abc.1529362187.couch\n...</code></pre><p>辅助索引(包括<code>JavaScript</code>视图，<code>Erlang</code>视图和<code>Mango</code>索引)也被分片，并且应移动它们的分片以节省新节点重建视图的工作量。查看<code>data/.</code>中的分片。例如：</p>\n<pre><code>data/.shards\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design/mrview\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design/mrview/3e823c2a4383ac0c18d4e574135a5b08.view\n...</code></pre><p>由于它们是文件，因此可以使用<code>cp</code>，<code>rsync</code>，<code>scp</code>或其他文件复制命令将它们从一个节点复制到另一个节点。 例如：</p>\n<pre><code># 1主机\n$ mkdir -p data/.shards/&lt;range&gt;\n$ mkdir -p data/shards/&lt;range&gt;\n# 2主机\n$ scp &lt;couch-dir&gt;/data/.shards/&lt;range&gt;/&lt;database&gt;.&lt;datecode&gt;* \\\n&lt;node&gt;:&lt;couch-dir&gt;/data/.shards/&lt;range&gt;/\n$ scp &lt;couch-dir&gt;/data/shards/&lt;range&gt;/&lt;database&gt;.&lt;datecode&gt;.couch \\\n  &lt;node&gt;:&lt;couch-dir&gt;/data/shards/&lt;range&gt;/</code></pre><p>先移动视图文件再移动数据库文件！ 如果视图索引在其数据库之前，则数据库将从头开始重建它。</p>\n<h4 id=\"设置目标节点为维护模式\"><a href=\"#设置目标节点为维护模式\" class=\"headerlink\" title=\"设置目标节点为维护模式\"></a>设置目标节点为维护模式</h4><p>在告诉CouchDB节点上的这些新分片之前，必须将节点置于维护模式。维护模式指示CouchDB返回<code>404 Not Found</code>响应在<code>/_up</code>端点，并确保其不参与其分片的常规交互式集群请求。使用GET<code>/_up</code>检查节点的运行状况的正确配置的负载均衡器将检测到此404并将该节点从循环中删除，从而阻止将请求发送到该节点。 例如，要将HAProxy配置为使用<code>/_up</code>端点，请使用：</p>\n<pre><code>http-check disable-on-404\noption httpchk GET /_up</code></pre><p>如果未设置维护模式，或者负载平衡器忽略了此维护模式状态，则在执行下一步之后，群集在咨询相关节点时可能会返回错误的响应。不要这样做！在接下来的步骤中，我们将确保此分片是最新的，然后再允许其参与最终用户的请求。<br>启用维护模式：</p>\n<pre><code> curl -X PUT -H &quot;Content-type: application/json&quot; \\ $COUCH_URL:5984/_node/&lt;nodename&gt;/_config/couchdb/maintenance_mode \\ -d &quot;\\&quot;true\\&quot;&quot;</code></pre><p>然后，通过在该节点的单个端点上执行GET<code>/_up</code>来验证该节点是否处于维护模式：</p>\n<pre><code>curl -v $COUCH_URL/_up\n...\n&lt; HTTP/1.1 404 Object Not Found\n...\n{&quot;status&quot;:&quot;maintenance_mode&quot;}</code></pre><p>最后，检查负载均衡器是否已从可用后端节点池中删除了该节点。</p>\n<h4 id=\"更新集群元数据反映新的目标分片。\"><a href=\"#更新集群元数据反映新的目标分片。\" class=\"headerlink\" title=\"更新集群元数据反映新的目标分片。\"></a>更新集群元数据反映新的目标分片。</h4><p>现在我们需要告诉CouchDB，目标节点（必须已经加入集群）应该为给定数据库托管碎片副本。<br>要更新群集元数据，请使用特殊的<code>/_dbs</code>数据库，该数据库是内部CouchDB数据库，它将数据库映射到分片和节点。该数据库在节点之间复制。它只能通过节点本地端口（通常是端口5986）进行访问。默认情况下，出于安全目的，此端口仅在localhost接口上可用。<br>首先，检索数据库的当前元数据：</p>\n<pre><code>curl http://localhost:5986/_dbs/{name}\n{\n  &quot;_id&quot;: &quot;{name}&quot;,\n  &quot;_rev&quot;: &quot;1-e13fb7e79af3b3107ed62925058bfa3a&quot;,\n  &quot;shard_suffix&quot;: [46, 49, 53, 51, 48, 50, 51, 50, 53, 50, 54],\n  &quot;changelog&quot;: [\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node1@xxx.xxx.xxx.xxx&quot;],\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node2@xxx.xxx.xxx.xxx&quot;],\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node3@xxx.xxx.xxx.xxx&quot;],\n    ...\n  ],\n  &quot;by_node&quot;: {\n    &quot;node1@xxx.xxx.xxx.xxx&quot;: [\n      &quot;00000000-1fffffff&quot;,\n      ...\n    ],\n    ... \n  },\n  &quot;by_range&quot;: {\n    &quot;00000000-1fffffff&quot;: [\n      &quot;node1@xxx.xxx.xxx.xxx&quot;,\n      &quot;node2@xxx.xxx.xxx.xxx&quot;,\n      &quot;node3@xxx.xxx.xxx.xxx&quot;\n    ],\n    ...\n   } \n}</code></pre><p>这是该文档的简要剖析：</p>\n<ul>\n<li><code>_id</code>:数据库的名字</li>\n<li><code>_rev</code>:元数据的当前版本</li>\n<li><code>shard_suffix</code>:数据库创建时的时间戳，在Unix时期映射到ASCII数字的代码点后的秒。</li>\n<li><code>changelog</code>:数据库分片的历史</li>\n<li><code>by_node</code>:每个节点的分片列表</li>\n<li><code>by_range</code>:每个分片由哪些节点持有。</li>\n</ul>\n<p>要反映元数据中的分片移动，请执行以下三个步骤：</p>\n<ol>\n<li>添加合适的<code>changelog</code>实体。</li>\n<li>更新<code>by_node</code>实体。</li>\n<li>更新<code>by_range</code>实体。</li>\n</ol>\n<p>在修改时，此过程必须手动完成。<br>要将分片添加到节点，请将以下条目添加到数据库元数据的<code>changelog</code>属性中：</p>\n<pre><code>[&quot;add&quot;, &quot;&lt;range&gt;&quot;, &quot;&lt;node-name&gt;&quot;]</code></pre><p><code>&lt;range&gt;</code>是特定的硬范围设置。<code>&lt;node-name&gt;</code>应该与集群中GET<code>/_membership</code>中显示的节点的名称和地址匹配。<br>如果从节点移除一个分片，简单地将<code>add</code>替换为<code>remove</code>。<br>找到新的变更日志条目后，将需要更新<code>by_node</code>和<code>by_range</code>以反映谁在存储哪些分片。 更改日志条目中的数据和这些属性必须匹配。 否则，数据库可能会损坏。<br>继续我们的示例，这是上面的元数据的更新版本，该版本将分片添加到名为node4的其他节点中：</p>\n<pre><code>{\n\n  &quot;_id&quot;: &quot;{name}&quot;,\n  &quot;_rev&quot;: &quot;1-e13fb7e79af3b3107ed62925058bfa3a&quot;,\n  &quot;shard_suffix&quot;: [46, 49, 53, 51, 48, 50, 51, 50, 53, 50, 54],\n  &quot;changelog&quot;: [\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node1@xxx.xxx.xxx.xxx&quot;],\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node2@xxx.xxx.xxx.xxx&quot;],\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node3@xxx.xxx.xxx.xxx&quot;],\n    ...\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node4@xxx.xxx.xxx.xxx&quot;]\n  ],\n  &quot;by_node&quot;: {\n    &quot;node1@xxx.xxx.xxx.xxx&quot;: [\n      &quot;00000000-1fffffff&quot;,\n    ... \n    ],\n    ...\n    &quot;node4@xxx.xxx.xxx.xxx&quot;: [\n      &quot;00000000-1fffffff&quot;\n    ]\n  },\n  &quot;by_range&quot;: {\n    &quot;00000000-1fffffff&quot;: [\n      &quot;node1@xxx.xxx.xxx.xxx&quot;,\n      &quot;node2@xxx.xxx.xxx.xxx&quot;,\n      &quot;node3@xxx.xxx.xxx.xxx&quot;,\n      &quot;node4@xxx.xxx.xxx.xxx&quot;\n    ],\n    ...\n  }\n}</code></pre><p>现在可以<code>PUT</code>新元数据：</p>\n<pre><code>curl -X PUT http://localhost:5986/_dbs/{name} -d &#39;{...}&#39;</code></pre><h4 id=\"强制同步分片\"><a href=\"#强制同步分片\" class=\"headerlink\" title=\"强制同步分片\"></a>强制同步分片</h4><p>无论您是否将分片预先复制到新节点，都可以强制CouchDB同步所有分片的所有副本。<br>具有<code>/db/_sync_shards</code>端点的数据库中的分片：</p>\n<pre><code>curl -X POST $COUCH_URL:5984/{dbname}/_sync_shards\n{&quot;ok&quot;:true}</code></pre><p>这将启动同步过程。 请注意，这将给群集增加额外的负载，这可能会影响性能。<br>通过写入存储在该分片中的文档，也可以在每个分片的基础上强制进行同步。</p>\n<h4 id=\"监视内部复制以确保最新的分片\"><a href=\"#监视内部复制以确保最新的分片\" class=\"headerlink\" title=\"监视内部复制以确保最新的分片\"></a>监视内部复制以确保最新的分片</h4><p>完成上一步后，CouchDB将开始同步分片。可以通过监视<code>/_node/&lt;nodename&gt;/_system</code>端点(包括<code>internal_replication_jobs</code>指标)来观察这种情况。<br>一旦此指标从开始分片同步之前返回到基线，或者为0，分片副本就可以提供数据了，我们可以使节点退出维护模式。</p>\n<h4 id=\"清除目标节点的维护模式\"><a href=\"#清除目标节点的维护模式\" class=\"headerlink\" title=\"清除目标节点的维护模式\"></a>清除目标节点的维护模式</h4><p>现在，可以像在步骤2中一样，通过在维护模式配置端点上放置<code>false</code>,使节点开始为数据请求提供服务。<br>通过在该节点的单个端点上执行GET<code>/_up</code>来验证该节点是否不在维护模式下。最后，检查负载均衡器是否已将该节点返回到可用后端节点池中。</p>\n<h4 id=\"再次更新集群元数据移除原分片\"><a href=\"#再次更新集群元数据移除原分片\" class=\"headerlink\" title=\"再次更新集群元数据移除原分片\"></a>再次更新集群元数据移除原分片</h4><p>现在，以与在步骤2中将新目标分片添加到分片图中相同的方式，从分片图中删除源分片。确保将<code>[“ remove”，&lt;range&gt;，&lt;source-shard&gt;]</code>条目添加到 更改日志的末尾，以及修改数据库元数据文档的<code>by_node</code>和<code>by_range</code>部分。</p>\n<h4 id=\"移除原节点的分片和任何辅助索引分片\"><a href=\"#移除原节点的分片和任何辅助索引分片\" class=\"headerlink\" title=\"移除原节点的分片和任何辅助索引分片\"></a>移除原节点的分片和任何辅助索引分片</h4><p>最后，可以通过从源主机上的命令行中删除源碎片副本的文件以及任何视图碎片副本来删除源碎片副本：</p>\n<pre><code>rm &lt;couch-dir&gt;/data/shards/&lt;range&gt;/&lt;dbname&gt;.&lt;datecode&gt;.couch\nrm -r &lt;couch-dir&gt;/data/.shards/&lt;range&gt;/&lt;dbname&gt;.&lt;datecode&gt;*</code></pre><p>恭喜你！ 您已经移动了数据库分片副本。通过以这种方式添加和删除数据库分片副本，您可以更改集群的分片布局，也称为分片映射。</p>\n<h3 id=\"指定数据库放置位置\"><a href=\"#指定数据库放置位置\" class=\"headerlink\" title=\"指定数据库放置位置\"></a>指定数据库放置位置</h3><p>您可以配置CouchDB，以使用放置规则在数据库创建时将碎片副本放置在某些节点上。<br>首先，每个节点必须标记有zone属性。 这定义了每个节点所在的区域。您可以通过编辑<code>/_nodes</code>数据库中的节点文档来实现，该文档可通过本地节点端口进行访问。 添加以下形式的键值对：</p>\n<pre><code>&quot;zone&quot;: &quot;{zone-name}&quot;</code></pre><p>在集群中的每一个节点都这样做：</p>\n<pre><code>curl -X PUT http://localhost:5986/_nodes/&lt;node-name&gt; \\ -d &#39;{ \\\n        &quot;_id&quot;: &quot;&lt;node-name&gt;&quot;,\n        &quot;_rev&quot;: &quot;&lt;rev&gt;&quot;,\n        &quot;zone&quot;: &quot;&lt;zone-name&gt;&quot;\n        }&#39;</code></pre><p>在每个节点的本地配置文件（local.ini）中，定义一个一致的群集范围设置，例如：</p>\n<pre><code>[cluster]\nplacement = &lt;zone-name-1&gt;:2,&lt;zone-name-2&gt;:1</code></pre><p>在此示例中，CouchDB将确保将分区的两个副本托管在区域属性设置为<code>&lt;zone-name-1&gt;</code>的节点上，并将一个副本托管在新的区域属性设置为<code>&lt;zone-name-2&gt;</code>的节点上。<br>这种方法非常灵活，因为您还可以在创建数据库时使用与ini文件相同的语法，通过将放置设置指定为查询参数来基于每个数据库指定区域：</p>\n<pre><code>curl -X PUT $COUCH_URL:5984/&lt;dbname&gt;?zone=&lt;zone&gt;</code></pre><p>也可以指定放置参数。 请注意，这将覆盖确定已创建副本的数量！<br>请注意，您还可以使用此系统来确保群集中的某些节点不为新主机托管任何副本。<br>通过为它们提供一个不会出现在<code>[cluster]</code>放置字符串中的<code>zone</code>属性，来创建数据库。</p>\n<h3 id=\"修改数据库到一个新的q值\"><a href=\"#修改数据库到一个新的q值\" class=\"headerlink\" title=\"修改数据库到一个新的q值\"></a>修改数据库到一个新的<code>q</code>值</h3><p>数据库的q值只能在创建数据库时设置，不能进行实时重新分片。 相反，要重新分片数据库，必须重新生成它。 步骤如下：</p>\n<ol>\n<li>通过在PUT操作期间将<code>q</code>值指定为查询参数来创建具有所需分片设置的临时数据库。</li>\n<li>停止客户端访问数据库</li>\n<li>将主数据库复制到临时数据库。 如果主数据库正在使用中，则可能需要多次复制。</li>\n<li>删除主数据库，<strong>确保没有人在使用!</strong></li>\n<li>使用所需的分片设置重新创建主数据库。</li>\n<li>客户端现在可以再次访问数据库。</li>\n<li>将临时数据库复制回主数据库。</li>\n<li>删除临时数据库.</li>\n</ol>\n<p>一旦完成所有步骤，即可再次使用该数据库。 集群将根据放置规则自动创建并分发其碎片。<br>如果可以指示客户端应用程序使用新数据库而不是旧数据库，并且可以在非常短暂的中断窗口内进行切换，则可以避免生产中的停机时间。</p>\n<h2 id=\"集群清除\"><a href=\"#集群清除\" class=\"headerlink\" title=\"集群清除\"></a>集群清除</h2><p>群集清除的主要目的是清除具有多个删除的逻辑删除或包含大量冲突的单个文档的数据库。 但是，它也可以用于清除具有任何修订版本的任何文档（已删除或未删除）。<br>群集清除旨在维护最终的一致性并防止不必要的二级索引无效。 为此，每个数据库都会跟踪数据库中请求的一定数量的历史清除以及其当前的<code>purge_seq</code>。 内部复制和二级索引处理数据库的清除，并定期更新其相应的清除检查点文档以报告由其处理的<code>purge_seq</code>。 为了确保最终的一致性，数据库将仅在内部复制作业和二级索引处理了存储的历史清除请求之后，才删除它们。</p>\n<h3 id=\"内部结构\"><a href=\"#内部结构\" class=\"headerlink\" title=\"内部结构\"></a>内部结构</h3><p>为了在节点和二级索引之间实现内部清除信息的复制，将两个内部清除树添加到数据库文件中以跟踪历史清除。</p>\n<pre><code>purge_tree: UUID -&gt; {PurgeSeq, DocId, Revs}\npurge_seq_tree: PurgeSeq -&gt; {UUID, DocId, Revs}</code></pre><p>每次对<code>_purge API</code>的交互式请求，都会在增加<code>purge_seq</code>和<code>purge_request</code>时创建成对的有序集合，其中<code>purge_request</code>是一个包含<code>docid</code>和修订列表的元组。 对于每个<code>purge_request</code>都会生成<code>uuid</code>。清除请求将添加到内部清除树：将元组<code>{UUID-&gt; {PurgeSeq，DocId，Revs}}</code>添加到<code>purge_tree</code>，元组 <code>{PurgeSeq-&gt; {UUID，DocId，Revs}}</code>添加到<code>purge_seq_tree</code>。</p>\n<h3 id=\"压缩清除\"><a href=\"#压缩清除\" class=\"headerlink\" title=\"压缩清除\"></a>压缩清除</h3><p>在数据库压缩期间，最旧的清除请求将被删除，以仅在数据库中存储<code>purged_infos_limit</code>个清除数目。 但是，为了使数据库与索引和其他副本保持一致，我们只能删除索引和内部复制作业已处理的清除请求。因此，有时清除树可能存储的数据超过<code>purged_infos_limit</code>清除数目。 如果数据库中存储的清除数量超出<code>purged_infos_limit</code>某个阈值，则日志中会产生警告，表明数据库的清除与索引和其他副本的同步问题。</p>\n<h3 id=\"本地清除检查点文档\"><a href=\"#本地清除检查点文档\" class=\"headerlink\" title=\"本地清除检查点文档\"></a>本地清除检查点文档</h3><p>具有清除的数据库索引和内部复制会创建并定期更新本地检查点清除文档：<code>_local/purge-$type-$hash</code>。 这些文档报告了它们最后处理的<code>purge_seq</code>以及最后处理的时间戳。 本地检查点清除文档的示例：</p>\n<pre><code>{\n&quot;_id&quot;: &quot;_local/purge-mrview-86cacdfbaf6968d4ebbc324dd3723fe7&quot;, &quot;type&quot;: &quot;mrview&quot;,\n&quot;purge_seq&quot;: 10,\n&quot;updated_on&quot;: 1540541874,\n&quot;ddoc_id&quot;: &quot;_design/foo&quot;,\n&quot;signature&quot;: &quot;5d10247925f826ae3e00966ec24b7bf6&quot;\n}</code></pre><h3 id=\"内部复制\"><a href=\"#内部复制\" class=\"headerlink\" title=\"内部复制\"></a>内部复制</h3><p>清除请求将以最终一致的方式在所有节点上重播。 清除的内部复制包括两个步骤：<br>1.拉取复制。内部复制首先要从目标中清除并将其应用于源，以确保我们不会重新引入目标中已清除的源文档/修订版。 在这一步中，我们使用存储在目标上的清除检查点文档来跟踪源处理的最后一个目标的<code>purge_seq</code>。 我们发现清除请求在此<code>purge_seq</code>之后发生，并在源上重播它们。 通过使用最新进程<code>purge_seq</code>和时间戳更新目标的检查点清除文档来完成此步骤。<br>2.推送复制。 然后，内部复制将照常进行，并插入一个额外的步骤以将源的清除请求推送到目标。 在此步骤中，我们使用本地内部复制检查点文档，这些文档在目标和源上均已更新。<br>在正常情况下，交互式清除请求已发送到包含数据库碎片副本的每个节点，并应用于每个副本。节点之间清除的内部复制只是确保副本之间一致性的一个额外步骤，在此副本上，一个节点上的所有清除请求都会在另一个节点上重播。为了不在副本上重播相同的清除请求，每个交互式清除请求都用唯一的<code>uuid</code>标记。内部复制会过滤出副本的<code>purge_tree</code>中已存在的<code>UUID</code>的清除请求，并仅应用<code>purge_tree</code>中不存在的<code>UUID</code>的清除请求。 这就是为什么我们需要有两个内部清除树的原因：1）<code>purge_tree：{UUID-&gt; {PurgeSeq，DocId，Revs}}</code>可以快速找到带有已存在的<code>UUID</code>的<code>purge requests</code>存在的副本； 2）<code>purge_seq_tree：{PurgeSeq-&gt; {UUID，DocId，Revs }}</code>允许从给定的<code>purge_seq</code>进行迭代，以收集在此<code>purge_seq</code>之后发生的所有清除请求。</p>\n<h3 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h3><p>每个清除请求将增加数据库的<code>update_seq</code>，以便还更新每个辅助索引，以便应用清除请求以维护主数据库内的一致性。</p>\n<h3 id=\"配置设置\"><a href=\"#配置设置\" class=\"headerlink\" title=\"配置设置\"></a>配置设置</h3><p>这些设置可以在<code>default.ini</code>或<code>local.ini</code>中进行更新：</p>\n<table>\n<thead>\n<tr>\n<th>字段</th>\n<th>描述</th>\n<th>默认值</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>max_document_id_number</td>\n<td>一个清除请求中允许的最大文档数</td>\n<td>100</td>\n</tr>\n<tr>\n<td>max_revisions_number</td>\n<td>一项清除请求中允许的最大累积修订版本数</td>\n<td>1000</td>\n</tr>\n<tr>\n<td>allowed_purge_seq_lag</td>\n<td>除了<code>purged_infos_limit</code>外，还允许其他缓冲区存储清除请求</td>\n<td>100</td>\n</tr>\n<tr>\n<td>index_lag_warn_seconds</td>\n<td>本地清除检查点文档的索引未更新时的允许持续时间</td>\n<td>86400</td>\n</tr>\n</tbody></table>\n<p>在数据库压缩期间，我们检查所有检查点清除文档。 允许客户端（索引或内部复制作业）的上一次报告的<code>purge_seq</code>小于当前数据库碎片的<code>purge_seq</code>的值(<code>purged_infos_limit + allowed_purge_seq_lag</code>)。如果客户端的<code>purge_seq</code>甚至更小，并且客户端未在<code>index_lag_warn_seconds</code>内设置检查点，则它会阻止清除清除树，因此我们必须对此客户端发出以下日志警告：</p>\n<pre><code>Purge checkpoint &#39;_local/purge-mrview-9152d15c12011288629bcffba7693fd4’\nnot updated in 86400 seconds in\n&lt;&lt;&quot;shards/00000000-1fffffff/testdb12.1491979089&quot;&gt;&gt;</code></pre><p>如果发生这种类型的日志警告，请检查客户端以查看为什么清除请求的处理停滞在其中。<br>索引的设计文档和本地检查点文档之间存在映射关系。 如果更新或删除了索引的设计文档，则也应自动删除相应的本地检查点文档。 但是在意外情况下，当设计文档被更新/删除但其检查点文档仍然存在于数据库中时，将发出以下警告：</p>\n<pre><code>&quot;Invalid purge doc &#39;&lt;&lt;&quot;_design/bar&quot;&gt;&gt;&#39; on database\n&lt;&lt;&quot;shards/00000000-1fffffff/testdb12.1491979089&quot;&gt;&gt;\nwith purge_seq &#39;50&#39;&quot;</code></pre><p>如果发生这种类型的日志警告，请从数据库中删除本地清除文档。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://docs.couchdb.org/en/stable/cluster/index.html\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1 id=\"集群管理\"><a href=\"#集群管理\" class=\"headerlink\" title=\"集群管理\"></a>集群管理</h1><h2 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h2><p>在<code>etc/fefault.ini</code>文件中有以下部分：</p>\n<pre><code>[cluster]\nq=8\nn=3</code></pre><ul>\n<li>q - 分片的数量</li>\n<li>n - 每一份文档的拷贝数量(加上原文档一共几份副本)</li>\n</ul>\n<p>创建数据库时可以通过覆盖该值修改为自己的值。<br>在集群操作中，获取操作中CouchDB返回状态码200或者是写操作返回状态码201即为大多数成员达成一致。大多数成员定义为相关拷贝的数量的一半。对于“读写”操作，“相关副本”的定义稍有不同。<br>对于读操作，相关副本的数量是保存请求数据的当前可访问分片的数量，这意味着在发生故障或网络分区的情况下，相关副本的数量可能少于集群中的副本数量。 可以使用r参数设置读取份数。<br>对于写操作，相关副本的数量始终为n，即集群中的副本数量。 对于写操作，可以使用w参数设置份数。 如果少于此数量的可用节点，则返回202。</p>\n<h2 id=\"节点管理\"><a href=\"#节点管理\" class=\"headerlink\" title=\"节点管理\"></a>节点管理</h2><h3 id=\"查看所有节点\"><a href=\"#查看所有节点\" class=\"headerlink\" title=\"查看所有节点\"></a>查看所有节点</h3><pre><code>curl -u admin:adminpw -X GET http://localhost:5984/_membership\n{\n    &quot;all_nodes&quot;:[   # 当前节点所知道的节点\n        &quot;node1@xxx.xxx.xxx.xxx&quot;],\n    &quot;cluster_nodes&quot;:[ #当前节点所连接的节点\n        &quot;node1@xxx.xxx.xxx.xxx&quot;],\n}</code></pre><h3 id=\"添加一个节点\"><a href=\"#添加一个节点\" class=\"headerlink\" title=\"添加一个节点\"></a>添加一个节点</h3><pre><code>curl -u admin:adminpw -X PUT http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy -d {}</code></pre><h3 id=\"删除一个节点\"><a href=\"#删除一个节点\" class=\"headerlink\" title=\"删除一个节点\"></a>删除一个节点</h3><pre><code>#首先获取关于文档的revision\ncurl -u admin:adminpw -X GET &quot;http://xxx.xxx.xxx.xxx:5986/_nodes/node2@yyy.yyy.yyy.yyy&quot;\n{&quot;_id&quot;:&quot;node2@yyy.yyy.yyy.yyy&quot;,&quot;_rev&quot;:&quot;1-967a00dff5e02add41820138abb3284d&quot;}    \n#删除节点\ncurl -u admin:adminpw -X DELETE http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy?rev=1-967a00dff5e02add41820138abb3284d</code></pre><h2 id=\"数据库管理\"><a href=\"#数据库管理\" class=\"headerlink\" title=\"数据库管理\"></a>数据库管理</h2><h3 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h3><p>数据库名字不支持大写字符，只支持[a-z],[0-9],以及特殊字符:<code>_ $ ( ) + - /</code></p>\n<pre><code>#创建一个数据库名字为db_name \ncurl -u admin:adminpw -X PUT http://localhost:5984/db_name?q=4&amp;n=2</code></pre><h3 id=\"删除数据库\"><a href=\"#删除数据库\" class=\"headerlink\" title=\"删除数据库\"></a>删除数据库</h3><pre><code>curl -u admin:adminpw -X DELETE http://localhost:5984/db_name</code></pre><h3 id=\"在一个具体的节点放置数据库\"><a href=\"#在一个具体的节点放置数据库\" class=\"headerlink\" title=\"在一个具体的节点放置数据库\"></a>在一个具体的节点放置数据库</h3><p>在CouchDB 2.0群集功能的前身BigCouch中，存在区域的概念。 CouchDB 2.0通过集群放置规则来实现这一目标。<br>使用<code>placement</code>参数将覆盖分片副本基数的标准逻辑（由[cluster] <code>n</code>指定）。<br>首先，每个节点必须标记有zone属性。 这定义了每个节点所在的区域。您可以通过编辑<code>/nodes</code>数据库中的节点文档来实现，该文档可通过“后门”（5986）端口进行访问。 添加以下形式的键值对：</p>\n<pre><code>&quot;zone&quot;:&quot;metro-dc-a&quot;</code></pre><p>在集群上所有节点上操作。<br>在每一个节点的配置文件<code>local.ini</code>或者<code>default.ini</code>中，定义相同的集群设置：</p>\n<pre><code>[cluster]\nplacement = metro-dc-a:2,metro-dc-b:1</code></pre><p>在此示例中，它将确保将一个分区的两个副本托管在将<code>zone</code>属性设置为<code>metro-dc-a</code>的节点上，并将一个副本副本托管在一个将<code>zone</code>属性设置为<code>metro-dc-b</code>的新副本上。<br>请注意，您还可以使用该系统，通过为群集中的某些节点提供不出现在[cluster]放置字符串中的zone属性，来确保它们不承载新创建的数据库的任何副本。</p>\n<h2 id=\"分片管理\"><a href=\"#分片管理\" class=\"headerlink\" title=\"分片管理\"></a>分片管理</h2><h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><p>本文档讨论了分片在CouchDB中的工作方式，以及如何安全地添加，移动，删除和创建分片和分片副本的放置规则。<br>分片是数据库中数据的水平分区。将数据划分为多个碎片，并将每个碎片的副本（称为“碎片副本”或简称为“副本”）分布到群集中的不同节点，可以提高数据的持久性，防止节点丢失。CouchDB群集自动分片数据库，并在节点之间分配组成每个分片的文档子集。 修改群集成员身份和分片行为必须手动完成。</p>\n<h3 id=\"分片和复制\"><a href=\"#分片和复制\" class=\"headerlink\" title=\"分片和复制\"></a>分片和复制</h3><p>可以在全局级别或每个数据库的基础上设置每个数据库有多少个分片和副本。 相关参数是<code>q</code>和<code>n</code>。<br><code>q</code>是要维护的数据库分片数。<code>n</code>是要分发的每个文档的副本数。<code>n</code>的默认值为3,<code>q</code>的默认值为8。当<code>q</code>= 8时，数据库分为8个分片。在<code>n</code>=3的情况下，群集分发每个分片的三个副本。总共，一个数据库有24个分片副本。在默认的3节点群集中，每个节点将接收8个分片。在4节点群集中，每个节点将接收6个分片。 在一般情况下，我们建议群集中的节点数应为n的倍数，以使碎片均匀分布。<br>CouchDB节点的<code>etc/local.ini</code>文件中的<code>cluster</code>部分：</p>\n<pre><code>[cluster]\nq=8\nn=3</code></pre><p>可以修改这些设置以设置所有数据库的分片默认值，或者可以在创建数据库时通过指定q和n查询参数来针对每个数据库进行设置。 例如：</p>\n<pre><code>curl -X PUT &quot;http://localhost:5984/database-name?q=4&amp;n=2&quot;</code></pre><p>这将创建一个数据库，该数据库分为4个分片和2个副本，从而产生8个分片副本分布在整个数据库中<br>的集群上。</p>\n<h3 id=\"Quorum\"><a href=\"#Quorum\" class=\"headerlink\" title=\"Quorum\"></a>Quorum</h3><p>取决于集群的大小，每个数据库的分片数量以及分片副本的数量，并非每个节点都可以访问每个分片，但是每个节点都知道可以通过CouchDB的内部分片在哪里找到每个分片的所有副本。<br>进入CouchDB集群的每个请求均由任意一个随机协调节点处理。该协调节点将请求代理给其他具有相关数据的节点，这些数据可能包含也可能不包含自身。一旦达到法定数量的数据库节点响应，协调节点就会向客户端发送响应。2 默认情况下,默认的法定仲裁大小等于<code>r=w=((n+1)/2)</code>，其中<code>r</code>表示读取仲裁的大小，<code>w</code>表示写入仲裁的大小，<code>n</code>表示数字每个分片的副本。在n为3的默认群集中，<code>((n+1)/2)</code>将为2。<br>集群中的每个节点都可以作为任何请求的协调节点。集群内部没有专门的节点角色。<br>可以在请求时通过设置文档和视图读取的<code>r</code>参数以及文档写入的<code>w</code>参数来配置所需仲裁的大小。例如，这是一个请求，一旦至少两个节点已响应，该请求便指示协调节点发送响应：</p>\n<pre><code>curl &quot;$COUCH_URL:5984/&lt;db&gt;/&lt;doc&gt;?r=2&quot;</code></pre><p>这是写文档的类似示例：</p>\n<pre><code>curl -X PUT &quot;$COUCH_URL:5984/&lt;db&gt;/&lt;doc&gt;?w=2&quot; -d &#39;{...}&#39;</code></pre><p>将<code>r</code>或<code>w</code>设置为等于n（副本数）意味着只有在所有具有相关分片的节点都响应或超时后，您才会收到响应，因此这种方法不能保证<code>ACID</code>的一致性。 将<code>r</code>或<code>w</code>设置为1意味着仅一个相关节点响应后，您将收到响应。</p>\n<h3 id=\"数据库分片测试\"><a href=\"#数据库分片测试\" class=\"headerlink\" title=\"数据库分片测试\"></a>数据库分片测试</h3><p>有一些API端点可以帮助您了解如何分片数据库。 首先，在集群上创建一个新数据库，然后将几个文档放入其中：</p>\n<pre><code>$ curl -X PUT $COUCH_URL:5984/mydb\n{&quot;ok&quot;:true}\n$ curl -X PUT $COUCH_URL:5984/mydb/joan -d &#39;{&quot;loves&quot;:&quot;cats&quot;}&#39;\n{&quot;ok&quot;:true,&quot;id&quot;:&quot;joan&quot;,&quot;rev&quot;:&quot;1-cc240d66a894a7ee7ad3160e69f9051f&quot;}\n$ curl -X PUT $COUCH_URL:5984/mydb/robert -d &#39;{&quot;loves&quot;:&quot;dogs&quot;}&#39;\n{&quot;ok&quot;:true,&quot;id&quot;:&quot;robert&quot;,&quot;rev&quot;:&quot;1-4032b428c7574a85bc04f1f271be446e&quot;}</code></pre><p>首先，<code>/db</code>将告诉您数据库的分片参数：</p>\n<pre><code>curl -s $COUCH_URL:5984/db | jq .\n{\n  &quot;db_name&quot;: &quot;mydb&quot;,\n...\n  &quot;cluster&quot;: {\n    &quot;q&quot;: 8,\n    &quot;n&quot;: 3,\n    &quot;w&quot;: 2,\n    &quot;r&quot;: 2\n}, ...\n}</code></pre><p>因此，我们知道此数据库是由8个分片(<code>q</code>=8)建的，每个分片具有3个副本(<code>n</code>=3),集群中节点之间总共有24个分片副本。<br>现在，让我们看一下这些分片副本如何通过<code>/db/_shards</code>端点放置在集群上：</p>\n<pre><code>curl -s $COUCH_URL:5984/mydb/_shards | jq .\n{\n&quot;shards&quot;: {\n    &quot;00000000-1fffffff&quot;: [\n      &quot;node1@127.0.0.1&quot;,\n      &quot;node2@127.0.0.1&quot;,\n      &quot;node4@127.0.0.1&quot;\n    ],\n    &quot;20000000-3fffffff&quot;: [\n      &quot;node1@127.0.0.1&quot;,\n      &quot;node2@127.0.0.1&quot;,\n      &quot;node3@127.0.0.1&quot;\n    ],\n    ...\n  }\n}</code></pre><p>现在我们看到该集群中实际上有4个节点，并且CouchDB已将这24个分片副本均匀地分布在所有4个节点上。<br>我们还可以确切地看到哪个分片包含具有<code>/db/_shards/doc</code>端点的给定文档：</p>\n<pre><code>curl -s $COUCH_URL:5984/mydb/_shards/joan | jq .\n{\n  &quot;range&quot;: &quot;e0000000-ffffffff&quot;,\n  &quot;nodes&quot;: [\n    &quot;node1@127.0.0.1&quot;,\n    &quot;node3@127.0.0.1&quot;,\n    &quot;node4@127.0.0.1&quot;\n] }\n$ curl -s $COUCH_URL:5984/mydb/_shards/robert | jq .\n{\n  &quot;range&quot;: &quot;60000000-7fffffff&quot;,\n  &quot;nodes&quot;: [\n   &quot;node1@127.0.0.1&quot;,\n    &quot;node3@127.0.0.1&quot;,\n    &quot;node4@127.0.0.1&quot;\n   ] \n}</code></pre><p>CouchDB向我们展示了两个示例文档中每个映射到的特定分片。</p>\n<h3 id=\"移动一个分片\"><a href=\"#移动一个分片\" class=\"headerlink\" title=\"移动一个分片\"></a>移动一个分片</h3><p>本节介绍如何手动放置和更换碎片。 当您确定群集太大或太小，并且想要成功调整其大小，或者从服务器指标中注意到数据库/碎片布局不是最佳的，并且您需要一些“热点”时，这些活动是至关重要的步骤 解决。<br>考虑一个<code>q</code>=8和<code>n</code>=3的三节点群集。每个数据库有24个分片，分布在三个节点上。如果将第四个节点添加到集群，则CouchDB不会将现有数据库分片重新分配给该集群。 这将导致负载不平衡，因为新节点将仅托管其加入集群后创建的数据库的分片。 为了平衡现有数据库中的分片分布，必须手动移动它们。<br>在集群中的节点上移动分片涉及以下几个步骤：</p>\n<ol>\n<li>确保目标节点已经加入集群</li>\n<li>将分片和任何辅助索引分片复制到目标节点上。</li>\n<li>设置目标节点为维护模式。</li>\n<li>更新集群元数据反映新的目标分片。</li>\n<li>监视内部复制以确保最新的分片。</li>\n<li>清除目标节点的维护模式。</li>\n<li>再次更新集群元数据移除原分片。</li>\n<li>移除原节点的分片和任何辅助索引分片.</li>\n</ol>\n<h4 id=\"拷贝分片文件\"><a href=\"#拷贝分片文件\" class=\"headerlink\" title=\"拷贝分片文件\"></a>拷贝分片文件</h4><p>从技术上讲，复制数据库和辅助索引碎片是可选的。 如果在不执行此数据副本的情况下继续进行下一步，则CouchDB将使用内部复制来填充新添加的分片副本。 但是，复制文件的速度比内部复制快，尤其是在繁忙的群集上，这就是为什么我们建议首先执行此手动数据复制的原因。<br>碎片文件位于CouchDB安装目录的<code>data/shards</code>目录中。这些子目录中包含分片文件本身。例如，对于一个名为<code>abc</code>的<code>q</code>=8数据库，这是其数据库分片文件：</p>\n<pre><code>data/shards/00000000-1fffffff/abc.1529362187.couch\ndata/shards/20000000-3fffffff/abc.1529362187.couch\ndata/shards/40000000-5fffffff/abc.1529362187.couch\n...</code></pre><p>辅助索引(包括<code>JavaScript</code>视图，<code>Erlang</code>视图和<code>Mango</code>索引)也被分片，并且应移动它们的分片以节省新节点重建视图的工作量。查看<code>data/.</code>中的分片。例如：</p>\n<pre><code>data/.shards\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design/mrview\ndata/.shards/e0000000-ffffffff/_replicator.1518451591_design/mrview/3e823c2a4383ac0c18d4e574135a5b08.view\n...</code></pre><p>由于它们是文件，因此可以使用<code>cp</code>，<code>rsync</code>，<code>scp</code>或其他文件复制命令将它们从一个节点复制到另一个节点。 例如：</p>\n<pre><code># 1主机\n$ mkdir -p data/.shards/&lt;range&gt;\n$ mkdir -p data/shards/&lt;range&gt;\n# 2主机\n$ scp &lt;couch-dir&gt;/data/.shards/&lt;range&gt;/&lt;database&gt;.&lt;datecode&gt;* \\\n&lt;node&gt;:&lt;couch-dir&gt;/data/.shards/&lt;range&gt;/\n$ scp &lt;couch-dir&gt;/data/shards/&lt;range&gt;/&lt;database&gt;.&lt;datecode&gt;.couch \\\n  &lt;node&gt;:&lt;couch-dir&gt;/data/shards/&lt;range&gt;/</code></pre><p>先移动视图文件再移动数据库文件！ 如果视图索引在其数据库之前，则数据库将从头开始重建它。</p>\n<h4 id=\"设置目标节点为维护模式\"><a href=\"#设置目标节点为维护模式\" class=\"headerlink\" title=\"设置目标节点为维护模式\"></a>设置目标节点为维护模式</h4><p>在告诉CouchDB节点上的这些新分片之前，必须将节点置于维护模式。维护模式指示CouchDB返回<code>404 Not Found</code>响应在<code>/_up</code>端点，并确保其不参与其分片的常规交互式集群请求。使用GET<code>/_up</code>检查节点的运行状况的正确配置的负载均衡器将检测到此404并将该节点从循环中删除，从而阻止将请求发送到该节点。 例如，要将HAProxy配置为使用<code>/_up</code>端点，请使用：</p>\n<pre><code>http-check disable-on-404\noption httpchk GET /_up</code></pre><p>如果未设置维护模式，或者负载平衡器忽略了此维护模式状态，则在执行下一步之后，群集在咨询相关节点时可能会返回错误的响应。不要这样做！在接下来的步骤中，我们将确保此分片是最新的，然后再允许其参与最终用户的请求。<br>启用维护模式：</p>\n<pre><code> curl -X PUT -H &quot;Content-type: application/json&quot; \\ $COUCH_URL:5984/_node/&lt;nodename&gt;/_config/couchdb/maintenance_mode \\ -d &quot;\\&quot;true\\&quot;&quot;</code></pre><p>然后，通过在该节点的单个端点上执行GET<code>/_up</code>来验证该节点是否处于维护模式：</p>\n<pre><code>curl -v $COUCH_URL/_up\n...\n&lt; HTTP/1.1 404 Object Not Found\n...\n{&quot;status&quot;:&quot;maintenance_mode&quot;}</code></pre><p>最后，检查负载均衡器是否已从可用后端节点池中删除了该节点。</p>\n<h4 id=\"更新集群元数据反映新的目标分片。\"><a href=\"#更新集群元数据反映新的目标分片。\" class=\"headerlink\" title=\"更新集群元数据反映新的目标分片。\"></a>更新集群元数据反映新的目标分片。</h4><p>现在我们需要告诉CouchDB，目标节点（必须已经加入集群）应该为给定数据库托管碎片副本。<br>要更新群集元数据，请使用特殊的<code>/_dbs</code>数据库，该数据库是内部CouchDB数据库，它将数据库映射到分片和节点。该数据库在节点之间复制。它只能通过节点本地端口（通常是端口5986）进行访问。默认情况下，出于安全目的，此端口仅在localhost接口上可用。<br>首先，检索数据库的当前元数据：</p>\n<pre><code>curl http://localhost:5986/_dbs/{name}\n{\n  &quot;_id&quot;: &quot;{name}&quot;,\n  &quot;_rev&quot;: &quot;1-e13fb7e79af3b3107ed62925058bfa3a&quot;,\n  &quot;shard_suffix&quot;: [46, 49, 53, 51, 48, 50, 51, 50, 53, 50, 54],\n  &quot;changelog&quot;: [\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node1@xxx.xxx.xxx.xxx&quot;],\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node2@xxx.xxx.xxx.xxx&quot;],\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node3@xxx.xxx.xxx.xxx&quot;],\n    ...\n  ],\n  &quot;by_node&quot;: {\n    &quot;node1@xxx.xxx.xxx.xxx&quot;: [\n      &quot;00000000-1fffffff&quot;,\n      ...\n    ],\n    ... \n  },\n  &quot;by_range&quot;: {\n    &quot;00000000-1fffffff&quot;: [\n      &quot;node1@xxx.xxx.xxx.xxx&quot;,\n      &quot;node2@xxx.xxx.xxx.xxx&quot;,\n      &quot;node3@xxx.xxx.xxx.xxx&quot;\n    ],\n    ...\n   } \n}</code></pre><p>这是该文档的简要剖析：</p>\n<ul>\n<li><code>_id</code>:数据库的名字</li>\n<li><code>_rev</code>:元数据的当前版本</li>\n<li><code>shard_suffix</code>:数据库创建时的时间戳，在Unix时期映射到ASCII数字的代码点后的秒。</li>\n<li><code>changelog</code>:数据库分片的历史</li>\n<li><code>by_node</code>:每个节点的分片列表</li>\n<li><code>by_range</code>:每个分片由哪些节点持有。</li>\n</ul>\n<p>要反映元数据中的分片移动，请执行以下三个步骤：</p>\n<ol>\n<li>添加合适的<code>changelog</code>实体。</li>\n<li>更新<code>by_node</code>实体。</li>\n<li>更新<code>by_range</code>实体。</li>\n</ol>\n<p>在修改时，此过程必须手动完成。<br>要将分片添加到节点，请将以下条目添加到数据库元数据的<code>changelog</code>属性中：</p>\n<pre><code>[&quot;add&quot;, &quot;&lt;range&gt;&quot;, &quot;&lt;node-name&gt;&quot;]</code></pre><p><code>&lt;range&gt;</code>是特定的硬范围设置。<code>&lt;node-name&gt;</code>应该与集群中GET<code>/_membership</code>中显示的节点的名称和地址匹配。<br>如果从节点移除一个分片，简单地将<code>add</code>替换为<code>remove</code>。<br>找到新的变更日志条目后，将需要更新<code>by_node</code>和<code>by_range</code>以反映谁在存储哪些分片。 更改日志条目中的数据和这些属性必须匹配。 否则，数据库可能会损坏。<br>继续我们的示例，这是上面的元数据的更新版本，该版本将分片添加到名为node4的其他节点中：</p>\n<pre><code>{\n\n  &quot;_id&quot;: &quot;{name}&quot;,\n  &quot;_rev&quot;: &quot;1-e13fb7e79af3b3107ed62925058bfa3a&quot;,\n  &quot;shard_suffix&quot;: [46, 49, 53, 51, 48, 50, 51, 50, 53, 50, 54],\n  &quot;changelog&quot;: [\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node1@xxx.xxx.xxx.xxx&quot;],\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node2@xxx.xxx.xxx.xxx&quot;],\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node3@xxx.xxx.xxx.xxx&quot;],\n    ...\n    [&quot;add&quot;, &quot;00000000-1fffffff&quot;, &quot;node4@xxx.xxx.xxx.xxx&quot;]\n  ],\n  &quot;by_node&quot;: {\n    &quot;node1@xxx.xxx.xxx.xxx&quot;: [\n      &quot;00000000-1fffffff&quot;,\n    ... \n    ],\n    ...\n    &quot;node4@xxx.xxx.xxx.xxx&quot;: [\n      &quot;00000000-1fffffff&quot;\n    ]\n  },\n  &quot;by_range&quot;: {\n    &quot;00000000-1fffffff&quot;: [\n      &quot;node1@xxx.xxx.xxx.xxx&quot;,\n      &quot;node2@xxx.xxx.xxx.xxx&quot;,\n      &quot;node3@xxx.xxx.xxx.xxx&quot;,\n      &quot;node4@xxx.xxx.xxx.xxx&quot;\n    ],\n    ...\n  }\n}</code></pre><p>现在可以<code>PUT</code>新元数据：</p>\n<pre><code>curl -X PUT http://localhost:5986/_dbs/{name} -d &#39;{...}&#39;</code></pre><h4 id=\"强制同步分片\"><a href=\"#强制同步分片\" class=\"headerlink\" title=\"强制同步分片\"></a>强制同步分片</h4><p>无论您是否将分片预先复制到新节点，都可以强制CouchDB同步所有分片的所有副本。<br>具有<code>/db/_sync_shards</code>端点的数据库中的分片：</p>\n<pre><code>curl -X POST $COUCH_URL:5984/{dbname}/_sync_shards\n{&quot;ok&quot;:true}</code></pre><p>这将启动同步过程。 请注意，这将给群集增加额外的负载，这可能会影响性能。<br>通过写入存储在该分片中的文档，也可以在每个分片的基础上强制进行同步。</p>\n<h4 id=\"监视内部复制以确保最新的分片\"><a href=\"#监视内部复制以确保最新的分片\" class=\"headerlink\" title=\"监视内部复制以确保最新的分片\"></a>监视内部复制以确保最新的分片</h4><p>完成上一步后，CouchDB将开始同步分片。可以通过监视<code>/_node/&lt;nodename&gt;/_system</code>端点(包括<code>internal_replication_jobs</code>指标)来观察这种情况。<br>一旦此指标从开始分片同步之前返回到基线，或者为0，分片副本就可以提供数据了，我们可以使节点退出维护模式。</p>\n<h4 id=\"清除目标节点的维护模式\"><a href=\"#清除目标节点的维护模式\" class=\"headerlink\" title=\"清除目标节点的维护模式\"></a>清除目标节点的维护模式</h4><p>现在，可以像在步骤2中一样，通过在维护模式配置端点上放置<code>false</code>,使节点开始为数据请求提供服务。<br>通过在该节点的单个端点上执行GET<code>/_up</code>来验证该节点是否不在维护模式下。最后，检查负载均衡器是否已将该节点返回到可用后端节点池中。</p>\n<h4 id=\"再次更新集群元数据移除原分片\"><a href=\"#再次更新集群元数据移除原分片\" class=\"headerlink\" title=\"再次更新集群元数据移除原分片\"></a>再次更新集群元数据移除原分片</h4><p>现在，以与在步骤2中将新目标分片添加到分片图中相同的方式，从分片图中删除源分片。确保将<code>[“ remove”，&lt;range&gt;，&lt;source-shard&gt;]</code>条目添加到 更改日志的末尾，以及修改数据库元数据文档的<code>by_node</code>和<code>by_range</code>部分。</p>\n<h4 id=\"移除原节点的分片和任何辅助索引分片\"><a href=\"#移除原节点的分片和任何辅助索引分片\" class=\"headerlink\" title=\"移除原节点的分片和任何辅助索引分片\"></a>移除原节点的分片和任何辅助索引分片</h4><p>最后，可以通过从源主机上的命令行中删除源碎片副本的文件以及任何视图碎片副本来删除源碎片副本：</p>\n<pre><code>rm &lt;couch-dir&gt;/data/shards/&lt;range&gt;/&lt;dbname&gt;.&lt;datecode&gt;.couch\nrm -r &lt;couch-dir&gt;/data/.shards/&lt;range&gt;/&lt;dbname&gt;.&lt;datecode&gt;*</code></pre><p>恭喜你！ 您已经移动了数据库分片副本。通过以这种方式添加和删除数据库分片副本，您可以更改集群的分片布局，也称为分片映射。</p>\n<h3 id=\"指定数据库放置位置\"><a href=\"#指定数据库放置位置\" class=\"headerlink\" title=\"指定数据库放置位置\"></a>指定数据库放置位置</h3><p>您可以配置CouchDB，以使用放置规则在数据库创建时将碎片副本放置在某些节点上。<br>首先，每个节点必须标记有zone属性。 这定义了每个节点所在的区域。您可以通过编辑<code>/_nodes</code>数据库中的节点文档来实现，该文档可通过本地节点端口进行访问。 添加以下形式的键值对：</p>\n<pre><code>&quot;zone&quot;: &quot;{zone-name}&quot;</code></pre><p>在集群中的每一个节点都这样做：</p>\n<pre><code>curl -X PUT http://localhost:5986/_nodes/&lt;node-name&gt; \\ -d &#39;{ \\\n        &quot;_id&quot;: &quot;&lt;node-name&gt;&quot;,\n        &quot;_rev&quot;: &quot;&lt;rev&gt;&quot;,\n        &quot;zone&quot;: &quot;&lt;zone-name&gt;&quot;\n        }&#39;</code></pre><p>在每个节点的本地配置文件（local.ini）中，定义一个一致的群集范围设置，例如：</p>\n<pre><code>[cluster]\nplacement = &lt;zone-name-1&gt;:2,&lt;zone-name-2&gt;:1</code></pre><p>在此示例中，CouchDB将确保将分区的两个副本托管在区域属性设置为<code>&lt;zone-name-1&gt;</code>的节点上，并将一个副本托管在新的区域属性设置为<code>&lt;zone-name-2&gt;</code>的节点上。<br>这种方法非常灵活，因为您还可以在创建数据库时使用与ini文件相同的语法，通过将放置设置指定为查询参数来基于每个数据库指定区域：</p>\n<pre><code>curl -X PUT $COUCH_URL:5984/&lt;dbname&gt;?zone=&lt;zone&gt;</code></pre><p>也可以指定放置参数。 请注意，这将覆盖确定已创建副本的数量！<br>请注意，您还可以使用此系统来确保群集中的某些节点不为新主机托管任何副本。<br>通过为它们提供一个不会出现在<code>[cluster]</code>放置字符串中的<code>zone</code>属性，来创建数据库。</p>\n<h3 id=\"修改数据库到一个新的q值\"><a href=\"#修改数据库到一个新的q值\" class=\"headerlink\" title=\"修改数据库到一个新的q值\"></a>修改数据库到一个新的<code>q</code>值</h3><p>数据库的q值只能在创建数据库时设置，不能进行实时重新分片。 相反，要重新分片数据库，必须重新生成它。 步骤如下：</p>\n<ol>\n<li>通过在PUT操作期间将<code>q</code>值指定为查询参数来创建具有所需分片设置的临时数据库。</li>\n<li>停止客户端访问数据库</li>\n<li>将主数据库复制到临时数据库。 如果主数据库正在使用中，则可能需要多次复制。</li>\n<li>删除主数据库，<strong>确保没有人在使用!</strong></li>\n<li>使用所需的分片设置重新创建主数据库。</li>\n<li>客户端现在可以再次访问数据库。</li>\n<li>将临时数据库复制回主数据库。</li>\n<li>删除临时数据库.</li>\n</ol>\n<p>一旦完成所有步骤，即可再次使用该数据库。 集群将根据放置规则自动创建并分发其碎片。<br>如果可以指示客户端应用程序使用新数据库而不是旧数据库，并且可以在非常短暂的中断窗口内进行切换，则可以避免生产中的停机时间。</p>\n<h2 id=\"集群清除\"><a href=\"#集群清除\" class=\"headerlink\" title=\"集群清除\"></a>集群清除</h2><p>群集清除的主要目的是清除具有多个删除的逻辑删除或包含大量冲突的单个文档的数据库。 但是，它也可以用于清除具有任何修订版本的任何文档（已删除或未删除）。<br>群集清除旨在维护最终的一致性并防止不必要的二级索引无效。 为此，每个数据库都会跟踪数据库中请求的一定数量的历史清除以及其当前的<code>purge_seq</code>。 内部复制和二级索引处理数据库的清除，并定期更新其相应的清除检查点文档以报告由其处理的<code>purge_seq</code>。 为了确保最终的一致性，数据库将仅在内部复制作业和二级索引处理了存储的历史清除请求之后，才删除它们。</p>\n<h3 id=\"内部结构\"><a href=\"#内部结构\" class=\"headerlink\" title=\"内部结构\"></a>内部结构</h3><p>为了在节点和二级索引之间实现内部清除信息的复制，将两个内部清除树添加到数据库文件中以跟踪历史清除。</p>\n<pre><code>purge_tree: UUID -&gt; {PurgeSeq, DocId, Revs}\npurge_seq_tree: PurgeSeq -&gt; {UUID, DocId, Revs}</code></pre><p>每次对<code>_purge API</code>的交互式请求，都会在增加<code>purge_seq</code>和<code>purge_request</code>时创建成对的有序集合，其中<code>purge_request</code>是一个包含<code>docid</code>和修订列表的元组。 对于每个<code>purge_request</code>都会生成<code>uuid</code>。清除请求将添加到内部清除树：将元组<code>{UUID-&gt; {PurgeSeq，DocId，Revs}}</code>添加到<code>purge_tree</code>，元组 <code>{PurgeSeq-&gt; {UUID，DocId，Revs}}</code>添加到<code>purge_seq_tree</code>。</p>\n<h3 id=\"压缩清除\"><a href=\"#压缩清除\" class=\"headerlink\" title=\"压缩清除\"></a>压缩清除</h3><p>在数据库压缩期间，最旧的清除请求将被删除，以仅在数据库中存储<code>purged_infos_limit</code>个清除数目。 但是，为了使数据库与索引和其他副本保持一致，我们只能删除索引和内部复制作业已处理的清除请求。因此，有时清除树可能存储的数据超过<code>purged_infos_limit</code>清除数目。 如果数据库中存储的清除数量超出<code>purged_infos_limit</code>某个阈值，则日志中会产生警告，表明数据库的清除与索引和其他副本的同步问题。</p>\n<h3 id=\"本地清除检查点文档\"><a href=\"#本地清除检查点文档\" class=\"headerlink\" title=\"本地清除检查点文档\"></a>本地清除检查点文档</h3><p>具有清除的数据库索引和内部复制会创建并定期更新本地检查点清除文档：<code>_local/purge-$type-$hash</code>。 这些文档报告了它们最后处理的<code>purge_seq</code>以及最后处理的时间戳。 本地检查点清除文档的示例：</p>\n<pre><code>{\n&quot;_id&quot;: &quot;_local/purge-mrview-86cacdfbaf6968d4ebbc324dd3723fe7&quot;, &quot;type&quot;: &quot;mrview&quot;,\n&quot;purge_seq&quot;: 10,\n&quot;updated_on&quot;: 1540541874,\n&quot;ddoc_id&quot;: &quot;_design/foo&quot;,\n&quot;signature&quot;: &quot;5d10247925f826ae3e00966ec24b7bf6&quot;\n}</code></pre><h3 id=\"内部复制\"><a href=\"#内部复制\" class=\"headerlink\" title=\"内部复制\"></a>内部复制</h3><p>清除请求将以最终一致的方式在所有节点上重播。 清除的内部复制包括两个步骤：<br>1.拉取复制。内部复制首先要从目标中清除并将其应用于源，以确保我们不会重新引入目标中已清除的源文档/修订版。 在这一步中，我们使用存储在目标上的清除检查点文档来跟踪源处理的最后一个目标的<code>purge_seq</code>。 我们发现清除请求在此<code>purge_seq</code>之后发生，并在源上重播它们。 通过使用最新进程<code>purge_seq</code>和时间戳更新目标的检查点清除文档来完成此步骤。<br>2.推送复制。 然后，内部复制将照常进行，并插入一个额外的步骤以将源的清除请求推送到目标。 在此步骤中，我们使用本地内部复制检查点文档，这些文档在目标和源上均已更新。<br>在正常情况下，交互式清除请求已发送到包含数据库碎片副本的每个节点，并应用于每个副本。节点之间清除的内部复制只是确保副本之间一致性的一个额外步骤，在此副本上，一个节点上的所有清除请求都会在另一个节点上重播。为了不在副本上重播相同的清除请求，每个交互式清除请求都用唯一的<code>uuid</code>标记。内部复制会过滤出副本的<code>purge_tree</code>中已存在的<code>UUID</code>的清除请求，并仅应用<code>purge_tree</code>中不存在的<code>UUID</code>的清除请求。 这就是为什么我们需要有两个内部清除树的原因：1）<code>purge_tree：{UUID-&gt; {PurgeSeq，DocId，Revs}}</code>可以快速找到带有已存在的<code>UUID</code>的<code>purge requests</code>存在的副本； 2）<code>purge_seq_tree：{PurgeSeq-&gt; {UUID，DocId，Revs }}</code>允许从给定的<code>purge_seq</code>进行迭代，以收集在此<code>purge_seq</code>之后发生的所有清除请求。</p>\n<h3 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h3><p>每个清除请求将增加数据库的<code>update_seq</code>，以便还更新每个辅助索引，以便应用清除请求以维护主数据库内的一致性。</p>\n<h3 id=\"配置设置\"><a href=\"#配置设置\" class=\"headerlink\" title=\"配置设置\"></a>配置设置</h3><p>这些设置可以在<code>default.ini</code>或<code>local.ini</code>中进行更新：</p>\n<table>\n<thead>\n<tr>\n<th>字段</th>\n<th>描述</th>\n<th>默认值</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>max_document_id_number</td>\n<td>一个清除请求中允许的最大文档数</td>\n<td>100</td>\n</tr>\n<tr>\n<td>max_revisions_number</td>\n<td>一项清除请求中允许的最大累积修订版本数</td>\n<td>1000</td>\n</tr>\n<tr>\n<td>allowed_purge_seq_lag</td>\n<td>除了<code>purged_infos_limit</code>外，还允许其他缓冲区存储清除请求</td>\n<td>100</td>\n</tr>\n<tr>\n<td>index_lag_warn_seconds</td>\n<td>本地清除检查点文档的索引未更新时的允许持续时间</td>\n<td>86400</td>\n</tr>\n</tbody></table>\n<p>在数据库压缩期间，我们检查所有检查点清除文档。 允许客户端（索引或内部复制作业）的上一次报告的<code>purge_seq</code>小于当前数据库碎片的<code>purge_seq</code>的值(<code>purged_infos_limit + allowed_purge_seq_lag</code>)。如果客户端的<code>purge_seq</code>甚至更小，并且客户端未在<code>index_lag_warn_seconds</code>内设置检查点，则它会阻止清除清除树，因此我们必须对此客户端发出以下日志警告：</p>\n<pre><code>Purge checkpoint &#39;_local/purge-mrview-9152d15c12011288629bcffba7693fd4’\nnot updated in 86400 seconds in\n&lt;&lt;&quot;shards/00000000-1fffffff/testdb12.1491979089&quot;&gt;&gt;</code></pre><p>如果发生这种类型的日志警告，请检查客户端以查看为什么清除请求的处理停滞在其中。<br>索引的设计文档和本地检查点文档之间存在映射关系。 如果更新或删除了索引的设计文档，则也应自动删除相应的本地检查点文档。 但是在意外情况下，当设计文档被更新/删除但其检查点文档仍然存在于数据库中时，将发出以下警告：</p>\n<pre><code>&quot;Invalid purge doc &#39;&lt;&lt;&quot;_design/bar&quot;&gt;&gt;&#39; on database\n&lt;&lt;&quot;shards/00000000-1fffffff/testdb12.1491979089&quot;&gt;&gt;\nwith purge_seq &#39;50&#39;&quot;</code></pre><p>如果发生这种类型的日志警告，请从数据库中删除本地清除文档。</p>\n"},{"title":"CouchDB学习-介绍","date":"2019-12-21T04:24:56.000Z","_content":"[官方文档](https://docs.couchdb.org/en/stable/intro/index.html)\n# CouchDB\n## 1文档存储\nCouchDB服务器主机是一个存储**文档**的数据库。每一个文档在数据库中都有唯一的名字。CouchDB提供RESTful HTTP API用来读取和更新(添加，编辑，删除)数据库文档。\n文档是CouchDB数据库中的主要单元数据由任意字段和附件组成。文档也包括由数据库系统维护的元数据信息。文档字段具有唯一的名字并且包含多种类型(文本，数字，布尔值，列表等)的值。并且文本大小或元素数量没有限制。\nCouchDB文档更新模式是无锁的并且优化的。由客户端应用加载文档进行文档编辑，使用更新，最后存储回数据库。如果另一个客户端编辑了相同的文档并且首先保存了更新。那么客户端将会在存储时得到一个编辑冲突的错误。为了解决更新冲突，最新的文档版本将会打开，编辑将重新应用并再次尝试更新。\n单个文档更新(添加，编辑，删除)是全有或者全没有的，要么完全成功，要么完全失败。数据库不支持并行存储或更新文档。\n## 2ACID属性\nCouchDB文件层和系统提交特性具有全部的原子性，一致性，隔离性和持久性(ACID)属性。在硬盘中，CouchDB从不覆盖提交的数据或者被连接的结构。确保数据库文件总是保持一致性，这是一种“仅崩溃”设计，其中CouchDB服务器不执行关闭过程，而只是终止。\n文档更新(添加，编辑，删除)是串行的，除了二进制字节的写操作是并行的。数据库读取者从来不会被锁住也不会写操作完成或者其他读者在读时等待。任何数量的客户端可以读取文档不需要加锁或者是被并行的更新打断即使是同一个文档。CouchDB读操作使用多版本并发控制(MVCC)模式。每一个客户端从开始读操作到结束看到的都是一致的数据库快照。这意味着CouchDB可以保证每个文档的事务语义。\n文档通过他们的名字和序列号通过B-trees进行索引。每一次更新到数据库实例都会生成一个新的序列号。序列ID稍后用于在数据库中增量查找更改。文档存储或删除时B-tree索引也会同时进行更新。索引更新总是发生在文件(更新时只添加)最后。\n文档数据的优势在于总是方便打包存储而不是和众多数据库系统一样分离成多个表和行。当文档提交到硬盘中，文档字段和元数据将序列化地一个文档接着一个文档(有助于稍后高效地构建视图)打包进缓冲区。\n当CouchDB文档被更新后，所有的数据和相关的索引将被刷新到硬盘中总是以事务提交的方式使得数据库保持一致状态。提交通过两个步骤进行：\n\n1. 所有的文档数据和相关索引同步更新和刷新到硬盘。\n2. 更新后的数据库头以两个连续的相同块写入，以构成文件的前4k，然后同步刷新到磁盘。\n\n当在步骤一处系统崩溃或者电源被关闭，部分更新只是简单的抛弃并重新启动。如果崩溃发生在步骤二(头部信息提交时)，仍保留以前相同标头的副本，以确保所有先前提交的数据的一致性。 除标题区域外，在崩溃或断电后无需进行一致性检查或修复。\n## 3压缩\n通过偶尔压缩来回收浪费的空间。 按计划，或者当数据库文件超过一定数量的浪费空间时，压缩过程会将所有活动数据克隆到新文件中，然后丢弃旧文件。数据库在整个过程中始终保持完全在线，并且所有更新和读取都可以成功完成。仅当所有数据都已复制并且所有用户都已转移到新文件时，才删除旧数据库文件。\n## 4视图\nACID属性仅处理存储和更新，但是我们还需要以有趣且有用的方式显示数据的能力，不像SQL数据库的数据必须小心地分解表，CouchDB中的数据以细小的机构存储在文档中。CouchDB文档变得更灵活并且他们拥有自己的隐含的结构。这减轻了双向复制表模式及其所包含数据的最困难的问题和陷阱。\n但是，除了充当精美的文件服务器之外，用于数据存储和共享的简单文档模型太简单了，无法在其上构建真实的应用程序–它根本无法满足我们的期望和期望。 我们想要切片和切块，并以多种不同方式查看我们的数据。 现在需要一种过滤，组织和报告尚未分解为表格的数据的方法。\n### 4.1视图模型\n为了解决将结构添加回非结构化和半结构化数据的问题，CouchDB集成了视图模型。 视图是聚合和报告数据库中文档的方法，并且按需构建以聚合，联接和报告数据库文档。 由于视图是动态构建的，并且不会影响基础文档，因此您可以根据需要使用相同数据的不同视图表示。\n视图定义严格是虚拟的，并且仅显示当前数据库实例中的文档，从而使其与显示的数据分离并与复制兼容。 CouchDB视图是在特殊**设计文档**中定义的，并且可以跨数据库实例（如常规文档）进行复制，因此不仅数据可以在CouchDB中复制，而且整个应用程序设计也可以复制。\n### 4.2JavaScript视图功能\n视图是使用JavaScript功能定义的，该功能在map-reduce系统中充当map的一部分。 视图函数将CouchDB文档作为参数，然后进行所需的任何计算以确定通过视图提供的数据（如果有）。 它可以基于单个文档向视图添加多行，也可以根本不添加任何行。\n### 4.3视图索引\n视图是数据库实际文档内容的动态表示，而CouchDB可以轻松创建有用的数据视图。 但是，生成包含数十万或数百万个文档的数据库视图既浪费时间和资源，又不是系统每次都要从头做的事情。\n为了保持视图的快速查询，视图引擎维护其视图的索引，并对其进行增量更新以反映数据库中的更改。 CouchDB的核心设计在很大程度上围绕着对视图及其索引的高效，增量创建的需求进行了优化。\n视图及其功能在特殊的“设计”文档中定义，并且设计文档可以包含任意数量的唯一命名的视图功能。 当用户打开一个视图并自动更新其索引时，同一设计文档中的所有视图都被索引为一个组。\n视图构建器使用数据库序列ID来确定视图组是否与数据库完全同步。 如果不是，则视图引擎将检查自上次刷新以来更改的所有数据库文档（以打包的顺序排列）。 按照在磁盘文件中出现的顺序读取文档，从而减少了磁盘头搜索的频率和成本。\n可以同时读取和查询视图，同时也可以刷新视图。 如果客户端正在缓慢地流出大视图的内容，则可以同时为另一个客户端打开和刷新同一视图，而不会阻塞第一个客户端。 这适用于任何数量的同时进行的客户端阅读器，它们可以在同时为其他客户端刷新索引的同时读取和查询视图，而不会给阅读器造成问题。\n当视图引擎通过您的“地图”和“缩小”功能处理文档时，它们的前一行值将从视图索引中删除（如果存在）。 如果通过视图功能选择了文档，则功能结果将作为新行插入到视图中。\n将视图索引更改写入磁盘后，更新总是附加在文件末尾，以减少磁盘提交期间的磁盘头查找时间，并确保崩溃和电源故障不会导致索引损坏。 如果在更新视图索引时发生崩溃，则不完整的索引更新将丢失并从其先前提交的状态逐步重建。\n## 5安全与验证\n为了保护可以读取和更新文档的人员，CouchDB具有简单的读取器访问和更新验证模型，该模型可以扩展为实现自定义安全模型。\n### 5.1 管理员访问\nCouchDB数据库实例具有管理员帐户。 管理员帐户可以创建其他管理员帐户并更新设计文档。 设计文档是包含视图定义和其他特殊公式以及常规字段和Blob的特殊文档。\n### 5.2 更新验证\n将文档写入磁盘后，可以使用JavaScript函数动态地对其进行验证，以实现安全性和数据验证。 当文档通过所有公式验证标准时，将允许更新继续。 如果验证失败，更新将中止，用户客户端将收到错误响应。\n用户凭证和更新的文档都作为验证公式的输入，可以通过验证用户的文档更新权限来实现自定义安全模型。\n基本的“仅作者”更新文档模型的实现很简单，其中验证文档更新以检查用户是否在现有文档的“作者”字段中列出。 还可以使用更多的动态模型，例如检查单独的用户帐户配置文件的权限设置。\n对于实时使用情况和复制的更新都执行更新验证，以确保共享的分布式系统中的安全性和数据验证。\n## 6分布式更新和复制\nCouchDB是基于节点的分布式数据库系统。它允许用户和服务器在断开连接时访问和更新相同的共享数据。这些更改随后可以双向复制。\nCouchDB文档的存储，视图和安全模型旨在协同工作，以使真正的双向复制高效且可靠。文档和设计都可以复制，从而允许将完整的数据库应用程序（包括应用程序设计，逻辑和数据）复制到便携式计算机上以供脱机使用，或复制到远程办公室中的连接缓慢或不可靠，难以共享数据的服务器。\n复制过程是增量的。在数据库级别，复制仅检查自上次复制以来已更新的文档。如果由于网络问题或崩溃等原因导致复制在任何步骤上失败，则下一个复制将在最后一个检查点重新启动。\n可以创建和维护部分副本。可以通过JavaScript函数过滤复制，以便仅复制特定文档或满足特定条件的文档。这可以允许用户脱机使用大型共享数据库应用程序的子集供自己使用，同时保持与应用程序和该数据子集的正常交互。\n### 6.1冲突\n冲突检测和管理是任何分布式编辑系统的关键问题。 CouchDB存储系统将编辑冲突视为一种常见状态，而不是例外状态。 冲突处理模型简单且“无损”，同时保留了单个文档的语义并允许分散式冲突解决。\nCouchDB允许数据库中同时存在任何数量冲突的文档，每个数据库实例都确定性地确定哪个文档是“赢家”，哪些是冲突。 只有赢家文档可以显示在视图中，而“丢失”冲突仍然可以访问并保留在数据库中，直到在数据库压缩期间将其删除或清除为止。 因为冲突文档仍然是常规文档，所以它们像常规文档一样进行复制，并且要遵循相同的安全性和验证规则。\n当发生分布式编辑冲突时，每个数据库副本都会看到相同的胜出版本，并且每个都有解决冲突的机会。 解决冲突可以手动完成，也可以根据数据的性质和冲突由自动代理完成。 该系统使分散式冲突解决成为可能，同时保持了单文档数据库的语义。\n即使多个断开连接的用户或代理尝试解决相同的冲突，冲突管理也继续起作用。 如果解决的冲突导致更多的冲突，则系统将以相同的方式处理它们，在每台机器上确定相同的获胜者，并维护单个文档的语义。\n### 6.2应用\n仅使用基本复制模型，几乎无需额外的工作就可以使许多传统的单服务器数据库应用程序分布式。 CouchDB复制旨在立即用于基本数据库应用程序，同时还可以扩展以用于更详尽和功能齐全的用途。\n只需很少的数据库工作，就可以构建具有精细安全性和完整修订历史记录的分布式文档管理应用程序。 可以实施文档更新以利用增量字段和Blob复制，其中复制的更新几乎与实际编辑差异（“差异”）一样高效和增量。","source":"_posts/blog/couchDB/CouchDB学习-介绍.md","raw":"---\ntitle: CouchDB学习-介绍\ndate: 2019-12-21 12:24:56\ntags: CouchDb\ncategories: CouchDb学习\n---\n[官方文档](https://docs.couchdb.org/en/stable/intro/index.html)\n# CouchDB\n## 1文档存储\nCouchDB服务器主机是一个存储**文档**的数据库。每一个文档在数据库中都有唯一的名字。CouchDB提供RESTful HTTP API用来读取和更新(添加，编辑，删除)数据库文档。\n文档是CouchDB数据库中的主要单元数据由任意字段和附件组成。文档也包括由数据库系统维护的元数据信息。文档字段具有唯一的名字并且包含多种类型(文本，数字，布尔值，列表等)的值。并且文本大小或元素数量没有限制。\nCouchDB文档更新模式是无锁的并且优化的。由客户端应用加载文档进行文档编辑，使用更新，最后存储回数据库。如果另一个客户端编辑了相同的文档并且首先保存了更新。那么客户端将会在存储时得到一个编辑冲突的错误。为了解决更新冲突，最新的文档版本将会打开，编辑将重新应用并再次尝试更新。\n单个文档更新(添加，编辑，删除)是全有或者全没有的，要么完全成功，要么完全失败。数据库不支持并行存储或更新文档。\n## 2ACID属性\nCouchDB文件层和系统提交特性具有全部的原子性，一致性，隔离性和持久性(ACID)属性。在硬盘中，CouchDB从不覆盖提交的数据或者被连接的结构。确保数据库文件总是保持一致性，这是一种“仅崩溃”设计，其中CouchDB服务器不执行关闭过程，而只是终止。\n文档更新(添加，编辑，删除)是串行的，除了二进制字节的写操作是并行的。数据库读取者从来不会被锁住也不会写操作完成或者其他读者在读时等待。任何数量的客户端可以读取文档不需要加锁或者是被并行的更新打断即使是同一个文档。CouchDB读操作使用多版本并发控制(MVCC)模式。每一个客户端从开始读操作到结束看到的都是一致的数据库快照。这意味着CouchDB可以保证每个文档的事务语义。\n文档通过他们的名字和序列号通过B-trees进行索引。每一次更新到数据库实例都会生成一个新的序列号。序列ID稍后用于在数据库中增量查找更改。文档存储或删除时B-tree索引也会同时进行更新。索引更新总是发生在文件(更新时只添加)最后。\n文档数据的优势在于总是方便打包存储而不是和众多数据库系统一样分离成多个表和行。当文档提交到硬盘中，文档字段和元数据将序列化地一个文档接着一个文档(有助于稍后高效地构建视图)打包进缓冲区。\n当CouchDB文档被更新后，所有的数据和相关的索引将被刷新到硬盘中总是以事务提交的方式使得数据库保持一致状态。提交通过两个步骤进行：\n\n1. 所有的文档数据和相关索引同步更新和刷新到硬盘。\n2. 更新后的数据库头以两个连续的相同块写入，以构成文件的前4k，然后同步刷新到磁盘。\n\n当在步骤一处系统崩溃或者电源被关闭，部分更新只是简单的抛弃并重新启动。如果崩溃发生在步骤二(头部信息提交时)，仍保留以前相同标头的副本，以确保所有先前提交的数据的一致性。 除标题区域外，在崩溃或断电后无需进行一致性检查或修复。\n## 3压缩\n通过偶尔压缩来回收浪费的空间。 按计划，或者当数据库文件超过一定数量的浪费空间时，压缩过程会将所有活动数据克隆到新文件中，然后丢弃旧文件。数据库在整个过程中始终保持完全在线，并且所有更新和读取都可以成功完成。仅当所有数据都已复制并且所有用户都已转移到新文件时，才删除旧数据库文件。\n## 4视图\nACID属性仅处理存储和更新，但是我们还需要以有趣且有用的方式显示数据的能力，不像SQL数据库的数据必须小心地分解表，CouchDB中的数据以细小的机构存储在文档中。CouchDB文档变得更灵活并且他们拥有自己的隐含的结构。这减轻了双向复制表模式及其所包含数据的最困难的问题和陷阱。\n但是，除了充当精美的文件服务器之外，用于数据存储和共享的简单文档模型太简单了，无法在其上构建真实的应用程序–它根本无法满足我们的期望和期望。 我们想要切片和切块，并以多种不同方式查看我们的数据。 现在需要一种过滤，组织和报告尚未分解为表格的数据的方法。\n### 4.1视图模型\n为了解决将结构添加回非结构化和半结构化数据的问题，CouchDB集成了视图模型。 视图是聚合和报告数据库中文档的方法，并且按需构建以聚合，联接和报告数据库文档。 由于视图是动态构建的，并且不会影响基础文档，因此您可以根据需要使用相同数据的不同视图表示。\n视图定义严格是虚拟的，并且仅显示当前数据库实例中的文档，从而使其与显示的数据分离并与复制兼容。 CouchDB视图是在特殊**设计文档**中定义的，并且可以跨数据库实例（如常规文档）进行复制，因此不仅数据可以在CouchDB中复制，而且整个应用程序设计也可以复制。\n### 4.2JavaScript视图功能\n视图是使用JavaScript功能定义的，该功能在map-reduce系统中充当map的一部分。 视图函数将CouchDB文档作为参数，然后进行所需的任何计算以确定通过视图提供的数据（如果有）。 它可以基于单个文档向视图添加多行，也可以根本不添加任何行。\n### 4.3视图索引\n视图是数据库实际文档内容的动态表示，而CouchDB可以轻松创建有用的数据视图。 但是，生成包含数十万或数百万个文档的数据库视图既浪费时间和资源，又不是系统每次都要从头做的事情。\n为了保持视图的快速查询，视图引擎维护其视图的索引，并对其进行增量更新以反映数据库中的更改。 CouchDB的核心设计在很大程度上围绕着对视图及其索引的高效，增量创建的需求进行了优化。\n视图及其功能在特殊的“设计”文档中定义，并且设计文档可以包含任意数量的唯一命名的视图功能。 当用户打开一个视图并自动更新其索引时，同一设计文档中的所有视图都被索引为一个组。\n视图构建器使用数据库序列ID来确定视图组是否与数据库完全同步。 如果不是，则视图引擎将检查自上次刷新以来更改的所有数据库文档（以打包的顺序排列）。 按照在磁盘文件中出现的顺序读取文档，从而减少了磁盘头搜索的频率和成本。\n可以同时读取和查询视图，同时也可以刷新视图。 如果客户端正在缓慢地流出大视图的内容，则可以同时为另一个客户端打开和刷新同一视图，而不会阻塞第一个客户端。 这适用于任何数量的同时进行的客户端阅读器，它们可以在同时为其他客户端刷新索引的同时读取和查询视图，而不会给阅读器造成问题。\n当视图引擎通过您的“地图”和“缩小”功能处理文档时，它们的前一行值将从视图索引中删除（如果存在）。 如果通过视图功能选择了文档，则功能结果将作为新行插入到视图中。\n将视图索引更改写入磁盘后，更新总是附加在文件末尾，以减少磁盘提交期间的磁盘头查找时间，并确保崩溃和电源故障不会导致索引损坏。 如果在更新视图索引时发生崩溃，则不完整的索引更新将丢失并从其先前提交的状态逐步重建。\n## 5安全与验证\n为了保护可以读取和更新文档的人员，CouchDB具有简单的读取器访问和更新验证模型，该模型可以扩展为实现自定义安全模型。\n### 5.1 管理员访问\nCouchDB数据库实例具有管理员帐户。 管理员帐户可以创建其他管理员帐户并更新设计文档。 设计文档是包含视图定义和其他特殊公式以及常规字段和Blob的特殊文档。\n### 5.2 更新验证\n将文档写入磁盘后，可以使用JavaScript函数动态地对其进行验证，以实现安全性和数据验证。 当文档通过所有公式验证标准时，将允许更新继续。 如果验证失败，更新将中止，用户客户端将收到错误响应。\n用户凭证和更新的文档都作为验证公式的输入，可以通过验证用户的文档更新权限来实现自定义安全模型。\n基本的“仅作者”更新文档模型的实现很简单，其中验证文档更新以检查用户是否在现有文档的“作者”字段中列出。 还可以使用更多的动态模型，例如检查单独的用户帐户配置文件的权限设置。\n对于实时使用情况和复制的更新都执行更新验证，以确保共享的分布式系统中的安全性和数据验证。\n## 6分布式更新和复制\nCouchDB是基于节点的分布式数据库系统。它允许用户和服务器在断开连接时访问和更新相同的共享数据。这些更改随后可以双向复制。\nCouchDB文档的存储，视图和安全模型旨在协同工作，以使真正的双向复制高效且可靠。文档和设计都可以复制，从而允许将完整的数据库应用程序（包括应用程序设计，逻辑和数据）复制到便携式计算机上以供脱机使用，或复制到远程办公室中的连接缓慢或不可靠，难以共享数据的服务器。\n复制过程是增量的。在数据库级别，复制仅检查自上次复制以来已更新的文档。如果由于网络问题或崩溃等原因导致复制在任何步骤上失败，则下一个复制将在最后一个检查点重新启动。\n可以创建和维护部分副本。可以通过JavaScript函数过滤复制，以便仅复制特定文档或满足特定条件的文档。这可以允许用户脱机使用大型共享数据库应用程序的子集供自己使用，同时保持与应用程序和该数据子集的正常交互。\n### 6.1冲突\n冲突检测和管理是任何分布式编辑系统的关键问题。 CouchDB存储系统将编辑冲突视为一种常见状态，而不是例外状态。 冲突处理模型简单且“无损”，同时保留了单个文档的语义并允许分散式冲突解决。\nCouchDB允许数据库中同时存在任何数量冲突的文档，每个数据库实例都确定性地确定哪个文档是“赢家”，哪些是冲突。 只有赢家文档可以显示在视图中，而“丢失”冲突仍然可以访问并保留在数据库中，直到在数据库压缩期间将其删除或清除为止。 因为冲突文档仍然是常规文档，所以它们像常规文档一样进行复制，并且要遵循相同的安全性和验证规则。\n当发生分布式编辑冲突时，每个数据库副本都会看到相同的胜出版本，并且每个都有解决冲突的机会。 解决冲突可以手动完成，也可以根据数据的性质和冲突由自动代理完成。 该系统使分散式冲突解决成为可能，同时保持了单文档数据库的语义。\n即使多个断开连接的用户或代理尝试解决相同的冲突，冲突管理也继续起作用。 如果解决的冲突导致更多的冲突，则系统将以相同的方式处理它们，在每台机器上确定相同的获胜者，并维护单个文档的语义。\n### 6.2应用\n仅使用基本复制模型，几乎无需额外的工作就可以使许多传统的单服务器数据库应用程序分布式。 CouchDB复制旨在立即用于基本数据库应用程序，同时还可以扩展以用于更详尽和功能齐全的用途。\n只需很少的数据库工作，就可以构建具有精细安全性和完整修订历史记录的分布式文档管理应用程序。 可以实施文档更新以利用增量字段和Blob复制，其中复制的更新几乎与实际编辑差异（“差异”）一样高效和增量。","slug":"blog/couchDB/CouchDB学习-介绍","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqygy0014k0vqb6x85zp3","content":"<p><a href=\"https://docs.couchdb.org/en/stable/intro/index.html\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1 id=\"CouchDB\"><a href=\"#CouchDB\" class=\"headerlink\" title=\"CouchDB\"></a>CouchDB</h1><h2 id=\"1文档存储\"><a href=\"#1文档存储\" class=\"headerlink\" title=\"1文档存储\"></a>1文档存储</h2><p>CouchDB服务器主机是一个存储<strong>文档</strong>的数据库。每一个文档在数据库中都有唯一的名字。CouchDB提供RESTful HTTP API用来读取和更新(添加，编辑，删除)数据库文档。<br>文档是CouchDB数据库中的主要单元数据由任意字段和附件组成。文档也包括由数据库系统维护的元数据信息。文档字段具有唯一的名字并且包含多种类型(文本，数字，布尔值，列表等)的值。并且文本大小或元素数量没有限制。<br>CouchDB文档更新模式是无锁的并且优化的。由客户端应用加载文档进行文档编辑，使用更新，最后存储回数据库。如果另一个客户端编辑了相同的文档并且首先保存了更新。那么客户端将会在存储时得到一个编辑冲突的错误。为了解决更新冲突，最新的文档版本将会打开，编辑将重新应用并再次尝试更新。<br>单个文档更新(添加，编辑，删除)是全有或者全没有的，要么完全成功，要么完全失败。数据库不支持并行存储或更新文档。</p>\n<h2 id=\"2ACID属性\"><a href=\"#2ACID属性\" class=\"headerlink\" title=\"2ACID属性\"></a>2ACID属性</h2><p>CouchDB文件层和系统提交特性具有全部的原子性，一致性，隔离性和持久性(ACID)属性。在硬盘中，CouchDB从不覆盖提交的数据或者被连接的结构。确保数据库文件总是保持一致性，这是一种“仅崩溃”设计，其中CouchDB服务器不执行关闭过程，而只是终止。<br>文档更新(添加，编辑，删除)是串行的，除了二进制字节的写操作是并行的。数据库读取者从来不会被锁住也不会写操作完成或者其他读者在读时等待。任何数量的客户端可以读取文档不需要加锁或者是被并行的更新打断即使是同一个文档。CouchDB读操作使用多版本并发控制(MVCC)模式。每一个客户端从开始读操作到结束看到的都是一致的数据库快照。这意味着CouchDB可以保证每个文档的事务语义。<br>文档通过他们的名字和序列号通过B-trees进行索引。每一次更新到数据库实例都会生成一个新的序列号。序列ID稍后用于在数据库中增量查找更改。文档存储或删除时B-tree索引也会同时进行更新。索引更新总是发生在文件(更新时只添加)最后。<br>文档数据的优势在于总是方便打包存储而不是和众多数据库系统一样分离成多个表和行。当文档提交到硬盘中，文档字段和元数据将序列化地一个文档接着一个文档(有助于稍后高效地构建视图)打包进缓冲区。<br>当CouchDB文档被更新后，所有的数据和相关的索引将被刷新到硬盘中总是以事务提交的方式使得数据库保持一致状态。提交通过两个步骤进行：</p>\n<ol>\n<li>所有的文档数据和相关索引同步更新和刷新到硬盘。</li>\n<li>更新后的数据库头以两个连续的相同块写入，以构成文件的前4k，然后同步刷新到磁盘。</li>\n</ol>\n<p>当在步骤一处系统崩溃或者电源被关闭，部分更新只是简单的抛弃并重新启动。如果崩溃发生在步骤二(头部信息提交时)，仍保留以前相同标头的副本，以确保所有先前提交的数据的一致性。 除标题区域外，在崩溃或断电后无需进行一致性检查或修复。</p>\n<h2 id=\"3压缩\"><a href=\"#3压缩\" class=\"headerlink\" title=\"3压缩\"></a>3压缩</h2><p>通过偶尔压缩来回收浪费的空间。 按计划，或者当数据库文件超过一定数量的浪费空间时，压缩过程会将所有活动数据克隆到新文件中，然后丢弃旧文件。数据库在整个过程中始终保持完全在线，并且所有更新和读取都可以成功完成。仅当所有数据都已复制并且所有用户都已转移到新文件时，才删除旧数据库文件。</p>\n<h2 id=\"4视图\"><a href=\"#4视图\" class=\"headerlink\" title=\"4视图\"></a>4视图</h2><p>ACID属性仅处理存储和更新，但是我们还需要以有趣且有用的方式显示数据的能力，不像SQL数据库的数据必须小心地分解表，CouchDB中的数据以细小的机构存储在文档中。CouchDB文档变得更灵活并且他们拥有自己的隐含的结构。这减轻了双向复制表模式及其所包含数据的最困难的问题和陷阱。<br>但是，除了充当精美的文件服务器之外，用于数据存储和共享的简单文档模型太简单了，无法在其上构建真实的应用程序–它根本无法满足我们的期望和期望。 我们想要切片和切块，并以多种不同方式查看我们的数据。 现在需要一种过滤，组织和报告尚未分解为表格的数据的方法。</p>\n<h3 id=\"4-1视图模型\"><a href=\"#4-1视图模型\" class=\"headerlink\" title=\"4.1视图模型\"></a>4.1视图模型</h3><p>为了解决将结构添加回非结构化和半结构化数据的问题，CouchDB集成了视图模型。 视图是聚合和报告数据库中文档的方法，并且按需构建以聚合，联接和报告数据库文档。 由于视图是动态构建的，并且不会影响基础文档，因此您可以根据需要使用相同数据的不同视图表示。<br>视图定义严格是虚拟的，并且仅显示当前数据库实例中的文档，从而使其与显示的数据分离并与复制兼容。 CouchDB视图是在特殊<strong>设计文档</strong>中定义的，并且可以跨数据库实例（如常规文档）进行复制，因此不仅数据可以在CouchDB中复制，而且整个应用程序设计也可以复制。</p>\n<h3 id=\"4-2JavaScript视图功能\"><a href=\"#4-2JavaScript视图功能\" class=\"headerlink\" title=\"4.2JavaScript视图功能\"></a>4.2JavaScript视图功能</h3><p>视图是使用JavaScript功能定义的，该功能在map-reduce系统中充当map的一部分。 视图函数将CouchDB文档作为参数，然后进行所需的任何计算以确定通过视图提供的数据（如果有）。 它可以基于单个文档向视图添加多行，也可以根本不添加任何行。</p>\n<h3 id=\"4-3视图索引\"><a href=\"#4-3视图索引\" class=\"headerlink\" title=\"4.3视图索引\"></a>4.3视图索引</h3><p>视图是数据库实际文档内容的动态表示，而CouchDB可以轻松创建有用的数据视图。 但是，生成包含数十万或数百万个文档的数据库视图既浪费时间和资源，又不是系统每次都要从头做的事情。<br>为了保持视图的快速查询，视图引擎维护其视图的索引，并对其进行增量更新以反映数据库中的更改。 CouchDB的核心设计在很大程度上围绕着对视图及其索引的高效，增量创建的需求进行了优化。<br>视图及其功能在特殊的“设计”文档中定义，并且设计文档可以包含任意数量的唯一命名的视图功能。 当用户打开一个视图并自动更新其索引时，同一设计文档中的所有视图都被索引为一个组。<br>视图构建器使用数据库序列ID来确定视图组是否与数据库完全同步。 如果不是，则视图引擎将检查自上次刷新以来更改的所有数据库文档（以打包的顺序排列）。 按照在磁盘文件中出现的顺序读取文档，从而减少了磁盘头搜索的频率和成本。<br>可以同时读取和查询视图，同时也可以刷新视图。 如果客户端正在缓慢地流出大视图的内容，则可以同时为另一个客户端打开和刷新同一视图，而不会阻塞第一个客户端。 这适用于任何数量的同时进行的客户端阅读器，它们可以在同时为其他客户端刷新索引的同时读取和查询视图，而不会给阅读器造成问题。<br>当视图引擎通过您的“地图”和“缩小”功能处理文档时，它们的前一行值将从视图索引中删除（如果存在）。 如果通过视图功能选择了文档，则功能结果将作为新行插入到视图中。<br>将视图索引更改写入磁盘后，更新总是附加在文件末尾，以减少磁盘提交期间的磁盘头查找时间，并确保崩溃和电源故障不会导致索引损坏。 如果在更新视图索引时发生崩溃，则不完整的索引更新将丢失并从其先前提交的状态逐步重建。</p>\n<h2 id=\"5安全与验证\"><a href=\"#5安全与验证\" class=\"headerlink\" title=\"5安全与验证\"></a>5安全与验证</h2><p>为了保护可以读取和更新文档的人员，CouchDB具有简单的读取器访问和更新验证模型，该模型可以扩展为实现自定义安全模型。</p>\n<h3 id=\"5-1-管理员访问\"><a href=\"#5-1-管理员访问\" class=\"headerlink\" title=\"5.1 管理员访问\"></a>5.1 管理员访问</h3><p>CouchDB数据库实例具有管理员帐户。 管理员帐户可以创建其他管理员帐户并更新设计文档。 设计文档是包含视图定义和其他特殊公式以及常规字段和Blob的特殊文档。</p>\n<h3 id=\"5-2-更新验证\"><a href=\"#5-2-更新验证\" class=\"headerlink\" title=\"5.2 更新验证\"></a>5.2 更新验证</h3><p>将文档写入磁盘后，可以使用JavaScript函数动态地对其进行验证，以实现安全性和数据验证。 当文档通过所有公式验证标准时，将允许更新继续。 如果验证失败，更新将中止，用户客户端将收到错误响应。<br>用户凭证和更新的文档都作为验证公式的输入，可以通过验证用户的文档更新权限来实现自定义安全模型。<br>基本的“仅作者”更新文档模型的实现很简单，其中验证文档更新以检查用户是否在现有文档的“作者”字段中列出。 还可以使用更多的动态模型，例如检查单独的用户帐户配置文件的权限设置。<br>对于实时使用情况和复制的更新都执行更新验证，以确保共享的分布式系统中的安全性和数据验证。</p>\n<h2 id=\"6分布式更新和复制\"><a href=\"#6分布式更新和复制\" class=\"headerlink\" title=\"6分布式更新和复制\"></a>6分布式更新和复制</h2><p>CouchDB是基于节点的分布式数据库系统。它允许用户和服务器在断开连接时访问和更新相同的共享数据。这些更改随后可以双向复制。<br>CouchDB文档的存储，视图和安全模型旨在协同工作，以使真正的双向复制高效且可靠。文档和设计都可以复制，从而允许将完整的数据库应用程序（包括应用程序设计，逻辑和数据）复制到便携式计算机上以供脱机使用，或复制到远程办公室中的连接缓慢或不可靠，难以共享数据的服务器。<br>复制过程是增量的。在数据库级别，复制仅检查自上次复制以来已更新的文档。如果由于网络问题或崩溃等原因导致复制在任何步骤上失败，则下一个复制将在最后一个检查点重新启动。<br>可以创建和维护部分副本。可以通过JavaScript函数过滤复制，以便仅复制特定文档或满足特定条件的文档。这可以允许用户脱机使用大型共享数据库应用程序的子集供自己使用，同时保持与应用程序和该数据子集的正常交互。</p>\n<h3 id=\"6-1冲突\"><a href=\"#6-1冲突\" class=\"headerlink\" title=\"6.1冲突\"></a>6.1冲突</h3><p>冲突检测和管理是任何分布式编辑系统的关键问题。 CouchDB存储系统将编辑冲突视为一种常见状态，而不是例外状态。 冲突处理模型简单且“无损”，同时保留了单个文档的语义并允许分散式冲突解决。<br>CouchDB允许数据库中同时存在任何数量冲突的文档，每个数据库实例都确定性地确定哪个文档是“赢家”，哪些是冲突。 只有赢家文档可以显示在视图中，而“丢失”冲突仍然可以访问并保留在数据库中，直到在数据库压缩期间将其删除或清除为止。 因为冲突文档仍然是常规文档，所以它们像常规文档一样进行复制，并且要遵循相同的安全性和验证规则。<br>当发生分布式编辑冲突时，每个数据库副本都会看到相同的胜出版本，并且每个都有解决冲突的机会。 解决冲突可以手动完成，也可以根据数据的性质和冲突由自动代理完成。 该系统使分散式冲突解决成为可能，同时保持了单文档数据库的语义。<br>即使多个断开连接的用户或代理尝试解决相同的冲突，冲突管理也继续起作用。 如果解决的冲突导致更多的冲突，则系统将以相同的方式处理它们，在每台机器上确定相同的获胜者，并维护单个文档的语义。</p>\n<h3 id=\"6-2应用\"><a href=\"#6-2应用\" class=\"headerlink\" title=\"6.2应用\"></a>6.2应用</h3><p>仅使用基本复制模型，几乎无需额外的工作就可以使许多传统的单服务器数据库应用程序分布式。 CouchDB复制旨在立即用于基本数据库应用程序，同时还可以扩展以用于更详尽和功能齐全的用途。<br>只需很少的数据库工作，就可以构建具有精细安全性和完整修订历史记录的分布式文档管理应用程序。 可以实施文档更新以利用增量字段和Blob复制，其中复制的更新几乎与实际编辑差异（“差异”）一样高效和增量。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://docs.couchdb.org/en/stable/intro/index.html\" target=\"_blank\" rel=\"noopener\">官方文档</a></p>\n<h1 id=\"CouchDB\"><a href=\"#CouchDB\" class=\"headerlink\" title=\"CouchDB\"></a>CouchDB</h1><h2 id=\"1文档存储\"><a href=\"#1文档存储\" class=\"headerlink\" title=\"1文档存储\"></a>1文档存储</h2><p>CouchDB服务器主机是一个存储<strong>文档</strong>的数据库。每一个文档在数据库中都有唯一的名字。CouchDB提供RESTful HTTP API用来读取和更新(添加，编辑，删除)数据库文档。<br>文档是CouchDB数据库中的主要单元数据由任意字段和附件组成。文档也包括由数据库系统维护的元数据信息。文档字段具有唯一的名字并且包含多种类型(文本，数字，布尔值，列表等)的值。并且文本大小或元素数量没有限制。<br>CouchDB文档更新模式是无锁的并且优化的。由客户端应用加载文档进行文档编辑，使用更新，最后存储回数据库。如果另一个客户端编辑了相同的文档并且首先保存了更新。那么客户端将会在存储时得到一个编辑冲突的错误。为了解决更新冲突，最新的文档版本将会打开，编辑将重新应用并再次尝试更新。<br>单个文档更新(添加，编辑，删除)是全有或者全没有的，要么完全成功，要么完全失败。数据库不支持并行存储或更新文档。</p>\n<h2 id=\"2ACID属性\"><a href=\"#2ACID属性\" class=\"headerlink\" title=\"2ACID属性\"></a>2ACID属性</h2><p>CouchDB文件层和系统提交特性具有全部的原子性，一致性，隔离性和持久性(ACID)属性。在硬盘中，CouchDB从不覆盖提交的数据或者被连接的结构。确保数据库文件总是保持一致性，这是一种“仅崩溃”设计，其中CouchDB服务器不执行关闭过程，而只是终止。<br>文档更新(添加，编辑，删除)是串行的，除了二进制字节的写操作是并行的。数据库读取者从来不会被锁住也不会写操作完成或者其他读者在读时等待。任何数量的客户端可以读取文档不需要加锁或者是被并行的更新打断即使是同一个文档。CouchDB读操作使用多版本并发控制(MVCC)模式。每一个客户端从开始读操作到结束看到的都是一致的数据库快照。这意味着CouchDB可以保证每个文档的事务语义。<br>文档通过他们的名字和序列号通过B-trees进行索引。每一次更新到数据库实例都会生成一个新的序列号。序列ID稍后用于在数据库中增量查找更改。文档存储或删除时B-tree索引也会同时进行更新。索引更新总是发生在文件(更新时只添加)最后。<br>文档数据的优势在于总是方便打包存储而不是和众多数据库系统一样分离成多个表和行。当文档提交到硬盘中，文档字段和元数据将序列化地一个文档接着一个文档(有助于稍后高效地构建视图)打包进缓冲区。<br>当CouchDB文档被更新后，所有的数据和相关的索引将被刷新到硬盘中总是以事务提交的方式使得数据库保持一致状态。提交通过两个步骤进行：</p>\n<ol>\n<li>所有的文档数据和相关索引同步更新和刷新到硬盘。</li>\n<li>更新后的数据库头以两个连续的相同块写入，以构成文件的前4k，然后同步刷新到磁盘。</li>\n</ol>\n<p>当在步骤一处系统崩溃或者电源被关闭，部分更新只是简单的抛弃并重新启动。如果崩溃发生在步骤二(头部信息提交时)，仍保留以前相同标头的副本，以确保所有先前提交的数据的一致性。 除标题区域外，在崩溃或断电后无需进行一致性检查或修复。</p>\n<h2 id=\"3压缩\"><a href=\"#3压缩\" class=\"headerlink\" title=\"3压缩\"></a>3压缩</h2><p>通过偶尔压缩来回收浪费的空间。 按计划，或者当数据库文件超过一定数量的浪费空间时，压缩过程会将所有活动数据克隆到新文件中，然后丢弃旧文件。数据库在整个过程中始终保持完全在线，并且所有更新和读取都可以成功完成。仅当所有数据都已复制并且所有用户都已转移到新文件时，才删除旧数据库文件。</p>\n<h2 id=\"4视图\"><a href=\"#4视图\" class=\"headerlink\" title=\"4视图\"></a>4视图</h2><p>ACID属性仅处理存储和更新，但是我们还需要以有趣且有用的方式显示数据的能力，不像SQL数据库的数据必须小心地分解表，CouchDB中的数据以细小的机构存储在文档中。CouchDB文档变得更灵活并且他们拥有自己的隐含的结构。这减轻了双向复制表模式及其所包含数据的最困难的问题和陷阱。<br>但是，除了充当精美的文件服务器之外，用于数据存储和共享的简单文档模型太简单了，无法在其上构建真实的应用程序–它根本无法满足我们的期望和期望。 我们想要切片和切块，并以多种不同方式查看我们的数据。 现在需要一种过滤，组织和报告尚未分解为表格的数据的方法。</p>\n<h3 id=\"4-1视图模型\"><a href=\"#4-1视图模型\" class=\"headerlink\" title=\"4.1视图模型\"></a>4.1视图模型</h3><p>为了解决将结构添加回非结构化和半结构化数据的问题，CouchDB集成了视图模型。 视图是聚合和报告数据库中文档的方法，并且按需构建以聚合，联接和报告数据库文档。 由于视图是动态构建的，并且不会影响基础文档，因此您可以根据需要使用相同数据的不同视图表示。<br>视图定义严格是虚拟的，并且仅显示当前数据库实例中的文档，从而使其与显示的数据分离并与复制兼容。 CouchDB视图是在特殊<strong>设计文档</strong>中定义的，并且可以跨数据库实例（如常规文档）进行复制，因此不仅数据可以在CouchDB中复制，而且整个应用程序设计也可以复制。</p>\n<h3 id=\"4-2JavaScript视图功能\"><a href=\"#4-2JavaScript视图功能\" class=\"headerlink\" title=\"4.2JavaScript视图功能\"></a>4.2JavaScript视图功能</h3><p>视图是使用JavaScript功能定义的，该功能在map-reduce系统中充当map的一部分。 视图函数将CouchDB文档作为参数，然后进行所需的任何计算以确定通过视图提供的数据（如果有）。 它可以基于单个文档向视图添加多行，也可以根本不添加任何行。</p>\n<h3 id=\"4-3视图索引\"><a href=\"#4-3视图索引\" class=\"headerlink\" title=\"4.3视图索引\"></a>4.3视图索引</h3><p>视图是数据库实际文档内容的动态表示，而CouchDB可以轻松创建有用的数据视图。 但是，生成包含数十万或数百万个文档的数据库视图既浪费时间和资源，又不是系统每次都要从头做的事情。<br>为了保持视图的快速查询，视图引擎维护其视图的索引，并对其进行增量更新以反映数据库中的更改。 CouchDB的核心设计在很大程度上围绕着对视图及其索引的高效，增量创建的需求进行了优化。<br>视图及其功能在特殊的“设计”文档中定义，并且设计文档可以包含任意数量的唯一命名的视图功能。 当用户打开一个视图并自动更新其索引时，同一设计文档中的所有视图都被索引为一个组。<br>视图构建器使用数据库序列ID来确定视图组是否与数据库完全同步。 如果不是，则视图引擎将检查自上次刷新以来更改的所有数据库文档（以打包的顺序排列）。 按照在磁盘文件中出现的顺序读取文档，从而减少了磁盘头搜索的频率和成本。<br>可以同时读取和查询视图，同时也可以刷新视图。 如果客户端正在缓慢地流出大视图的内容，则可以同时为另一个客户端打开和刷新同一视图，而不会阻塞第一个客户端。 这适用于任何数量的同时进行的客户端阅读器，它们可以在同时为其他客户端刷新索引的同时读取和查询视图，而不会给阅读器造成问题。<br>当视图引擎通过您的“地图”和“缩小”功能处理文档时，它们的前一行值将从视图索引中删除（如果存在）。 如果通过视图功能选择了文档，则功能结果将作为新行插入到视图中。<br>将视图索引更改写入磁盘后，更新总是附加在文件末尾，以减少磁盘提交期间的磁盘头查找时间，并确保崩溃和电源故障不会导致索引损坏。 如果在更新视图索引时发生崩溃，则不完整的索引更新将丢失并从其先前提交的状态逐步重建。</p>\n<h2 id=\"5安全与验证\"><a href=\"#5安全与验证\" class=\"headerlink\" title=\"5安全与验证\"></a>5安全与验证</h2><p>为了保护可以读取和更新文档的人员，CouchDB具有简单的读取器访问和更新验证模型，该模型可以扩展为实现自定义安全模型。</p>\n<h3 id=\"5-1-管理员访问\"><a href=\"#5-1-管理员访问\" class=\"headerlink\" title=\"5.1 管理员访问\"></a>5.1 管理员访问</h3><p>CouchDB数据库实例具有管理员帐户。 管理员帐户可以创建其他管理员帐户并更新设计文档。 设计文档是包含视图定义和其他特殊公式以及常规字段和Blob的特殊文档。</p>\n<h3 id=\"5-2-更新验证\"><a href=\"#5-2-更新验证\" class=\"headerlink\" title=\"5.2 更新验证\"></a>5.2 更新验证</h3><p>将文档写入磁盘后，可以使用JavaScript函数动态地对其进行验证，以实现安全性和数据验证。 当文档通过所有公式验证标准时，将允许更新继续。 如果验证失败，更新将中止，用户客户端将收到错误响应。<br>用户凭证和更新的文档都作为验证公式的输入，可以通过验证用户的文档更新权限来实现自定义安全模型。<br>基本的“仅作者”更新文档模型的实现很简单，其中验证文档更新以检查用户是否在现有文档的“作者”字段中列出。 还可以使用更多的动态模型，例如检查单独的用户帐户配置文件的权限设置。<br>对于实时使用情况和复制的更新都执行更新验证，以确保共享的分布式系统中的安全性和数据验证。</p>\n<h2 id=\"6分布式更新和复制\"><a href=\"#6分布式更新和复制\" class=\"headerlink\" title=\"6分布式更新和复制\"></a>6分布式更新和复制</h2><p>CouchDB是基于节点的分布式数据库系统。它允许用户和服务器在断开连接时访问和更新相同的共享数据。这些更改随后可以双向复制。<br>CouchDB文档的存储，视图和安全模型旨在协同工作，以使真正的双向复制高效且可靠。文档和设计都可以复制，从而允许将完整的数据库应用程序（包括应用程序设计，逻辑和数据）复制到便携式计算机上以供脱机使用，或复制到远程办公室中的连接缓慢或不可靠，难以共享数据的服务器。<br>复制过程是增量的。在数据库级别，复制仅检查自上次复制以来已更新的文档。如果由于网络问题或崩溃等原因导致复制在任何步骤上失败，则下一个复制将在最后一个检查点重新启动。<br>可以创建和维护部分副本。可以通过JavaScript函数过滤复制，以便仅复制特定文档或满足特定条件的文档。这可以允许用户脱机使用大型共享数据库应用程序的子集供自己使用，同时保持与应用程序和该数据子集的正常交互。</p>\n<h3 id=\"6-1冲突\"><a href=\"#6-1冲突\" class=\"headerlink\" title=\"6.1冲突\"></a>6.1冲突</h3><p>冲突检测和管理是任何分布式编辑系统的关键问题。 CouchDB存储系统将编辑冲突视为一种常见状态，而不是例外状态。 冲突处理模型简单且“无损”，同时保留了单个文档的语义并允许分散式冲突解决。<br>CouchDB允许数据库中同时存在任何数量冲突的文档，每个数据库实例都确定性地确定哪个文档是“赢家”，哪些是冲突。 只有赢家文档可以显示在视图中，而“丢失”冲突仍然可以访问并保留在数据库中，直到在数据库压缩期间将其删除或清除为止。 因为冲突文档仍然是常规文档，所以它们像常规文档一样进行复制，并且要遵循相同的安全性和验证规则。<br>当发生分布式编辑冲突时，每个数据库副本都会看到相同的胜出版本，并且每个都有解决冲突的机会。 解决冲突可以手动完成，也可以根据数据的性质和冲突由自动代理完成。 该系统使分散式冲突解决成为可能，同时保持了单文档数据库的语义。<br>即使多个断开连接的用户或代理尝试解决相同的冲突，冲突管理也继续起作用。 如果解决的冲突导致更多的冲突，则系统将以相同的方式处理它们，在每台机器上确定相同的获胜者，并维护单个文档的语义。</p>\n<h3 id=\"6-2应用\"><a href=\"#6-2应用\" class=\"headerlink\" title=\"6.2应用\"></a>6.2应用</h3><p>仅使用基本复制模型，几乎无需额外的工作就可以使许多传统的单服务器数据库应用程序分布式。 CouchDB复制旨在立即用于基本数据库应用程序，同时还可以扩展以用于更详尽和功能齐全的用途。<br>只需很少的数据库工作，就可以构建具有精细安全性和完整修订历史记录的分布式文档管理应用程序。 可以实施文档更新以利用增量字段和Blob复制，其中复制的更新几乎与实际编辑差异（“差异”）一样高效和增量。</p>\n"},{"title":"CouchDB学习一","date":"2019-11-24T07:24:56.000Z","_content":"## 端口\n\n|端口号|协议|作用|\n|---|---|---|\n|5984|tcp|标椎集群端口用于所有的HTTP API请求|\n|5986|tcp|用于管理员对节点与分片的管理|\n|4369|tcp|Erlang端口到daemon的映射|\n\n### 配置介绍\n#### 配置文件\nCouchDb从以下位置按顺序读取配置文件\n* etc/fefault.ini\n* etc/default.d/*.ini\n* etc/local.ini\n* etc/local.d/*.ini\n\n类UNIX系统：`/opt/couchdb/`\nWindows系统:`C:\\CouchDB`\nmaxOS:`Applications/Apache CouchDB.app/Contents/Resources/couchdbx-core/etc`下的`default.ini`和`default.d`文件夹。`/Users/youruser/Library/Application Support/CouchDB2/etc/couchdb`下的`default.ini`和`default.d`文件夹.\n\n#### 通过HTTP API修改参数\n**集群**：\n```\ncurl -X PUT http://localhost:5984/_node/name@host/_config/uuids/algorithm -d '\"random\"'\n```\n**单节点**\n```\ncurl -X PUT http://localhost:5984/_node/_local/_config/uuids/algorithm -d '\"random\"'\n```\n### 基本配置\n#### couchdb配置\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/couchdb/`后接参数.\n\n* attachment_stream-buffer_size\n    缓冲池大小，越大读性能越好，但增加写操作的响应时间.\n* database_dir\n    数据库文件地址\n* default_security\n    默认的安全级别`admin_local`. `everyone`:任何人都可以读和写。`admin_only`：只有admin可以读和写.`admin_local`:被分片的数据库可以被任何人读和写，但分片只可以被admin读和写.\n* delayed_commits\n    延迟提交 `false`保证同步.`true`可以提高性能。\n* file_compression\n    文件压缩默认`snappy`。`none`：不进行压缩。`snappy`：使用谷歌的snappy.`deflate_N`：使用`zlib`,N为压缩等级从1(速度最快，压缩率最低)到9(速度最慢，压缩率最高).\n* fsync_options\n    缓冲区内的文件是否与操作系统同步刷新到硬盘中.一般不需要修改.`fsync_options=[before_header,after_header,on_file_open]`\n* max_dbs_open\n    同时打开数据库的最大数量默认为100.\n* os_process_timeout\n    处理超时时间默认5000ms\n* uri_file\n    该参数指定的文件包含完整的用来访问CouchDB数据库实例的`URI`.默认值：`/var/run/couchdb/couchdb.uri`\n* users_db_suffix\n    用户数据库后缀，默认`_users`.\n* util_driver_dir\n    二进制驱动的位置。\n* uuid\n    CouchDb服务器实例的唯一标识符\n* view_index_dir\n    CouchDB视图索引文件的位置。默认值:`/var/lib/couchdb`\n* maintenance_mode\n    CouchDb节点可以使用的两种维护模式 `true`:该节点不会响应集群中其他节点的请求并且`/_up`端点将返回404响应.`nolb`:`/_up`端点将返回404响应.`false`:该节点正常响应200。\n* max_document_size\n    文档的最大的大小默认为4GB\n\n### cluster 配置\n#### 集群选项\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/cluster/`后接参数.\n* q\n    新创建的数据库的分片数量，默认为8\n* n\n    集群中每一个数据库文档的副本数。单节点为1，每个节点最多仅持有一个副本。\n* placement\n* seedlist\n    以逗号分隔的节点名称列表，当前节点应与之通信加入集群。\n\n### couch_peruser\n#### couch_peruser配置\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/couch_peruser/`后接参数.\n* enable\n    如果设置为true，则_users为私有的数据库。且只允许当前节点操作。\n* delete_dbs\n    如果设置为true，当用户被删除，则相关的数据库也一同删除。\n### CouchDB HTTP 服务器\n#### HTTP Server配置\n**[chttpd]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/chttpd/`后接参数.\n\n* bind_address :集群端口绑定的IP地址\n    * 127.0.0.1\n    * 0.0.0.0：任何IP地址\n    * ::1:IPV6\n    * :: 任何IPV6的地址\n* port:集群端口号\n    * 5984:默认\n    * 0：可使用任何端口号\n* prefer_minimal:如果一个请求含有请求头:Prefer,则只返回`prefer_minimal`配置列表的头部信息\n* authentication_handlers:CouchDB使用的认证头部信息。可以使用第三方插件进行扩展。\n    * {chttpd_auth, cookie_authentication_handler}: Cookie认证;\n    * {couch_httpd_auth, proxy_authentication_handler}:代理认证;\n    * {chttpd_auth, default_authentication_handler}: 基本认证：\n    * {couch_httpd_auth, null_authentication_handler}:取消认证\n\n**[httpd]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/httpd/`后接参数.\n在CouchDB2.X版本，这一部分默认使用5986为默认端口。用于管理员任务与系统维护。应该总是绑定私有的LAN 127.0.0.1地址。\n\n* allow_jsonp\n    * 是否支持JSONP，默认false    \n* bind_address\n    * 本地节点可获得的IP地址。建议总是使用127.0.0.1或IPV6 ::1\n* changes_timeout\n    * 默认超时时间默认为6000ms\n* config_whitelist\n    * 配置信息修改白名单。只有白名单内的值可以通过CONFIG API修改配置。为了允许管理员通过HTTP修改该值，需要包括{httpd,config——whitelist}自己。\n    * config_whitelist = [{httpd,config_whitelist}, {log,level}, {etc,etc}]\n* default_handler\n    * 具体的默认HTTP请求Handler\n    * default_handler = {couch_httpd_db, handle_request}\n* enable_cors\n    * 控制CORS特性,默认false\n* port\n    * 定义监听的端口号 默认5986\n* redirect_vhost_handler\n    * Handler请求到`virtual hosts`的定制的默认功能\n    * redirect_vhost_handler = {Module, Fun}\n* server_options\n    * 可以被添加到配置文件的`MochiWeb`组件的服务器选项。\n    * server_options = [{backlog, 128}, {acceptor_pool_size, 16}]\n* secure_rewrites\n    * 是否允许通过子域隔离数据库。\n* socket_options\n    * CouchDB监听Socket选项。可以定义为元组列表.\n    * socket_options = [{sndbuf, 262144}, {nodelay, true}]\n* server_options\n    * CouchDB中mochiweb acceptor池中任何socket服务器选项，可以定义为元组列表.\n    * server_options = [{recbuf, undefined}]\n* vhost_global_handlers\n    * 对于`virtual hosts`全局的Handlers列表\n    * vhost_global_handlers = _utils, _uuids, _session, _users\n\n* x_forwarded_host\n    * 用于转发`HOST`头部字段的原始值。例如一个转发代理在请求Couchd前重写`Host`头部信息到内部主机。\n    * x_forwarded_host = X-Forwarded-Host\n* x_forwarded_proto\n    * 认证原始的HTTP协议\n* x_forwarded_ssl\n    * 用于告诉CouchDB使用https代替http\n* enable_xframe_options\n    * 控制是否开启特性\n* WWW-Authenticate\n    * 设置在基本认证下不具备权限的请求头部信息\n* max_http_request_size\n    * 限制HTTP请求体最大值大小默认4GB\n\n#### HTTPS (SSL/TLS)选项\nCouchDb支持本地TLS/SSL，不需要使用代理服务器.HTTPS设置比较容器，只需要两个文件：一个证书个一个私钥。可以通过`OpenSSL`命令生成自签名证书。\n```\nshell> mkdir /etc/couchdb/cert\nshell> cd /etc/couchdb/cert\nshell> openssl genrsa > privkey.pem\nshell> openssl req -new -x509 -key privkey.pem -out couchdb.pem -days 1095\nshell> chmod 600 privkey.pem couchdb.pem\nshell> chown couchdb privkey.pem couchdb.pem\n```\n编辑CouchDB配置文件`local.ini`：\n```\nenable = true\ncert_file = /etc/couchdb/cert/couchdb.pem\nkey_file = /etc/couchdb/cert/privkey.pem\n```\n使用自签名证书可以通过参数`-k`忽略警告信息\n```\ncurl -k https://127.0.0.1:6984\n```\n**[ssl]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/ssl/`后接参数.\n\n* cacert_file\n    * 包含PEM编码的CA证书路径,CA证书用于构建服务器证书链。用于客户端权限认证。\n* cert_file\n    * 包含用户证书文件的路径\n* key_file\n    * 包含用户PEM编码的私钥文件路径\n* password\n    * 包含用户密码的字符串,当用户私钥通过密码保护时使用\n* ssl_certifacate_max_depth\n    * 最大的节点证书深度\n* verify_fun\n    * 如果不指定具体的验证功能，则使用默认的验证功能\n* verify_ssl_certificates\n    * 如果为true则验证节点证书\n* fail_if_no_peer_cert\n    * true：如果客户端没有发送证书，则终止TLS/SSL握手\n    * false：只当客户端发送无效证书时，终止TLS/SSL握手\n* secure_renegotiate\n* ciphers\n    * 设置erlang格式的被支持的加密套件\n    * ciphers = [\"ECDHE-ECDSA-AES128-SHA256\", \"ECDHE-ECDSA-AES128-SHA\"]\n* tls_versions\n    * 设置允许的SSL/TLS协议版本列表\n    * tls_versions = [tlsv1 | 'tlsv1.1' | 'tlsv1.2']\n\n#### 跨域资源分享\n跨域资源分享.比如浏览器中运行js的网页通过AJAX请求到不同的域。不需要破坏任何一方的安全。\n一个典型的用例是通过静态网页通过CDN请求另一资源。比如CouchDB实例。这避免了使用JSONP或类似的变通方法来检索和托管内容的中间代理。\nCouchDB实例可以接受直接的连接保护数据库和实例。不会造成浏览器功能由于相同的域的限制被阻塞。CORS支持当前90%的浏览器。\n**[cors]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/httpd/`后接参数.\n* enable_cors\n需要将`httpd/enable_cors`选项设置为`true`。\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/cors/`后接参数.\n\n* credentials\n    * 默认情况下，请求和响应中均不包含身份验证标头或cookie。\n* origins\n    * 通过，分隔接受的原始的URL列表。不能同时设置origins=*和credentials=true\n\n* headers\n    * 通过，分隔的可接受的请求头列表。\n* methods\n    * 可接受的请求方法\n* max_age\n    * Access-Control-Max-Age\n\n#### 虚拟主机\n虚拟主机\nCouchDB可以基于`Host`请求头映射请求到绑定同一个IP地址的不同的位置。\n允许同一机器上不同虚拟机映射到不同的数据库或者是设计文档。\n通过为域名添加一个`CNAME`指针到DNS。在测试或者开发环境下，添加一个实体到hosts文件,如类UNIX系统：\n```\n127.0.0.1       couchdb.local\n```\n最后添加一个实体到配置文件的`[vhosts]`部分：\n```\ncouchdb.local:5984 = /example\n*.couchdb.local:5984 = /example\n```\n如果CouchDB监听在默认的HTTP端口80，或者之前设置了代理，则不需要在`vhosts`中指定端口号.\n第一行将重写请求以显示示例数据库的内容。 仅当Host标头为couchdb.local且不适用于CNAME时，此规则才有效，另一方面，第二条规则将所有CNAME与示例db匹配，这样www.couchdb.local和db.couchdb.local都可以使用。\n**[vhosts]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/vhosts/`后接参数.\n* couchdb.local\n### 认证与权限\n#### 服务器管理员\n默认的CouchDB提供了管理员级别的可以访问所有连接的用户。配置在`Admin Party`部分。不应该在生产环境中使用。可以在创建第一个管理员账户后删除这一部分。CouchDB服务管理员和密码没有存储在`_users`数据库。但是CouchDB加载ini文件时可以在最后发现`admin`部分。这个文件(可能为`etc/local.ini`或者`etc/local.d/10-admins.ini`在Debian/Ubuntu系统从包中安装时发现。)应该安全并且只能由系统管理员可读.\n管理员可以直接添加到`admin`部分，当CouchDB重新启动时，密码将会自动加密。也可以通过HTTP接口创建管理员账户不需要重启CouchDB。HTTP`/_node/{node-name}/_config/admins`地址支持查询，删除或者是创建新管理员账户。\n**[admins]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/admins/`后接参数.\n#### 认证配置\n**[chttpd]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/chttpd/`后接参数.\n\n* require_valid_user\n    * true:不允许来自匿名用户的任何请求\n\n**[couch_httpd_auth]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/couch_httpd_auth/`后接参数.\n\n* allow_persistent_cookies\n    * 使得Cookie持久性。\n* cookie_domain\n    * 配置`AuthSession`Cookie的域属性。默认为空。\n    * cookie_domain=example.com\n* auth_cache_size\n    * 内容中用户对象缓存数量，减少硬盘读写，默认50\n* authentication_redirect\n    * 权限成功验证后，客户端接受`text/html`响应情况下具体的重定向的位置。\n    * authentication_redirect = /_utils/session.html\n\n\n* iterations\n    * 由PBKDF2算法哈希的密码迭代的数量。\n* min_iterations\n    * 最小迭代的数量\n* max_iterations\n    * 最大迭代的数量\n* proxy_use_secret\n    * true:`couch_httpd_auth/secret`选项要求代理身份认证\n* public_fields\n    * 用户文档中可以被任何用户读的由逗号分隔的字段名称。\n    * public_fields = first_name, last_name, contacts, url\n* require_valid_user\n    * true:不允许来自匿名用户的任何请求\n* secret\n    * 用于代理身份认证与Cookie身份认证的secret\n* timeout\n    * 最后一次请求之后session超时过期时间默认600\n* users_db_public\n    * 允许用户查看所有用户文档，默认情况下，只有管理员可以查看所有用户的文档,用户只能查看自己的文档。\n* x_auth_roles\n    * HTTP请求头包含用户的角色，用逗号分隔。用于代理身份认证\n* x_auth_token\n    * HTTP请求头包含用户的token，用逗号分隔。用于代理身份认证\n* x_auth_username\n    * HTTP请求头包含用户的用户名，用逗号分隔。用于代理身份认证\n\n### 压缩配置\n#### 数据库压缩配置\n**[database_compaction]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/database_compaction/`后接参数.\n\n* doc_buffer_size\n    * 具体的拷贝缓冲区大小\n* checkpoint_after\n    * 在具体数量的比特后激活检查点拷贝到压缩数据库。\n\n#### 压缩程序规则\n**[compactions]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/compactions/`后接参数.\n列表中的规则确定什么时候运行自动压缩。配置可以是指定数据库或者是全局的。格式如下:\n```\ndatabase_name = [ {ParamName, ParamValue}, {ParamName, ParamValue}, ... ]\n_default = [ {ParamName, ParamValue}, {ParamName, ParamValue}, ... ]\n_default = [{db_fragmentation, \"70%\"}, {view_fragmentation, \"60%\"}, {from,\"23:00\"}, {to, \"04:00\"}]\n```\n\n1. db_fragmentation:数据库中数据压缩率，包括元数据。计算方法：(file_size-data_size)/file_size*100\n2. view_fragmentation：数据库中索引文件.....\n3. form,to:允许进行压缩的时间段,格式：`HH:MM - HH:MM  (HH in [0..23], MM in [0..59])`\n4. strict_window:如果为true，并且在超时时间后还没有压缩完则终止压缩。\n5. parallel_view_compaction：是否数据和视图同时进行压缩。\n\n#### 压缩程序配置\n**[compaction_daemon]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/compaction_daemon/`后接参数.\n\n* check_interval\n    * 两次检查数据库和视图索引是否需要被压缩的时间间隔，默认为3600\n* min_file_size\n    * 如果数据库或者视图索引文件大小小于该值，则不进行压缩。\n* snooze_period_ms\n\n#### 视图压缩选项\n**[view_compaction]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/view_compaction/`后接参数.\n\n* keyvalue_buffer_size\n    * 压缩时具体的最大拷贝缓冲区大小.\n\n### 日志\n#### 日志选项\n**[log]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/log/`后接参数.\n\n* writer\n    * stderr：日志信息发送到stderr(默认)\n    * file:日志信息存储到文件\n    * syslog：日志信息发送到syslog daemon\n* file\n    * 日志保存到文件的具体的位置(默认`/var/log/couchdb/couch.log`)\n* write_buffer\n    * 写日志缓冲区大小默认0\n* write_delay\n    * 写日志到硬盘延迟默认为0\n* level\n    * 日志级别\n    * debug\n    * info：包括HTTP请求，启动外部程序\n    * notice\n    * warning,warn：警告信息，例如硬盘空间不足\n    * error,err：只输出错误信息\n    * critical crit\n    * alert\n    * emergency emerg\n    * none:不输入任何日志\n* include_sasl\n    * 是否在日志中包含SASL信息\n* syslog_host\n    * 具体的syslog 主机将日志发送到的位置默认localhost\n* syslog_port\n    * 当发送日志信息时连接的syslog端口\n* syslog_appid\n    * 具体的应用名称默认couchdb\n* syslog_facility\n\n### 复制者\n#### 数据库复制配置\n**[replicator]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/replicator/`后接参数.\n\n* max_jobs\n    * 活跃的运行的复制任务数量。\n* interval\n* max_churn\n* update_docs\n* worker_batch_size\n* worker_processes\n* http_connections\n* connection_timeout\n* retries_per_request\n* socket_options\n* checkpoint_interval\n* use_checkpoints\n* cert_file\n* key_file\n* password\n* verify_ssl_certificates\n* ssl_trusted_certificates_file\n* ssl_certificate_max_depth\n* auth_plugins\n\n### Query Servers\n#### Query Servers Definition\n**[query_servers]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/query_servers/`后接参数.\n#### Query Servers Configuration\n**[query_sercver_config]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/query_sercver_config/`后接参数.\n\n* commit_freq\n* os_process_limit\n* os_process_soft_limit\n* reduce_limit\n\n#### Native Erlang Query Server\n**[native_query_servers]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/native_query_servers/`后接参数.\n\n### CouchDB Internal Services\n#### CouchDB Daemonized Mini Apps\n**[daemons]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/daemons/`后接参数.\n\n* auth_cache\n* compaction_daemon\n* external_manager\n* httpd\n* index_server\n* query_servers\n* replicator_manager\n* stats_aggregator\n* stats_collector\n* uuids\n* vhosts\n\n### Miscellaneous Parameters\n#### Configuration of Attachment Storage\n**[attachments]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/attachments/`后接参数.\n\n* compression_level\n* compressible_types\n\n#### Statistic Calculation\n**[stats]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/stats/`后接参数.\n\n* rate\n* samples\n\n#### UUIDs Configuration\n**[uuids]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/uuids/`后接参数.\n\n* algorithm\n* utc_id_suffix\n* max_count\n\n#### Vendor information\n**[vendor]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/vendor/`后接参数.\n\n#### Content-Security_Policy\n**[csp]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/csp/`后接参数.\n\n* enable\n* header_value\n\n## 操作\n### 节点操作\n#### 查看所有节点\n```\ncurl -u admin:adminpw -X GET http://localhost:5984/_membership\n{\n    \"all_nodes\":[   # 当前节点所知道的节点\n        \"node1@xxx.xxx.xxx.xxx\"],\n    \"cluster_nodes\":[ #当前节点所连接的节点\n        \"node1@xxx.xxx.xxx.xxx\"],\n}\n```\n#### 添加一个节点\n```\ncurl -u admin:adminpw -X PUT http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy -d {}\n```\n#### 删除一个节点\n```\n#首先获取关于文档的revision\ncurl -u admin:adminpw -X GET \"http://xxx.xxx.xxx.xxx:5986/_nodes/node2@yyy.yyy.yyy.yyy\"\n{\"_id\":\"node2@yyy.yyy.yyy.yyy\",\"_rev\":\"1-967a00dff5e02add41820138abb3284d\"}\t\n#删除节点\ncurl -u admin:adminpw -X DELETE http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy?rev=1-967a00dff5e02add41820138abb3284d\n```\n\n### 数据库操作\n#### 创建数据库\n数据库名字不支持大写字符，只支持[a-z],[0-9],以及特殊字符:`_ $ ( ) + - /`\n```\n#创建一个数据库名字为db_name \ncurl -u admin:adminpw -X PUT http://localhost:5984/db_name?q=4&n=2\n```\n#### 删除数据库\n```\ncurl -u admin:adminpw -X DELETE http://localhost:5984/db_name\n```","source":"_posts/blog/couchDB/CouchDB学习一.md","raw":"---\ntitle: CouchDB学习一\ndate: 2019-11-24 15:24:56\ntags: CouchDb\ncategories: CouchDb学习\n---\n## 端口\n\n|端口号|协议|作用|\n|---|---|---|\n|5984|tcp|标椎集群端口用于所有的HTTP API请求|\n|5986|tcp|用于管理员对节点与分片的管理|\n|4369|tcp|Erlang端口到daemon的映射|\n\n### 配置介绍\n#### 配置文件\nCouchDb从以下位置按顺序读取配置文件\n* etc/fefault.ini\n* etc/default.d/*.ini\n* etc/local.ini\n* etc/local.d/*.ini\n\n类UNIX系统：`/opt/couchdb/`\nWindows系统:`C:\\CouchDB`\nmaxOS:`Applications/Apache CouchDB.app/Contents/Resources/couchdbx-core/etc`下的`default.ini`和`default.d`文件夹。`/Users/youruser/Library/Application Support/CouchDB2/etc/couchdb`下的`default.ini`和`default.d`文件夹.\n\n#### 通过HTTP API修改参数\n**集群**：\n```\ncurl -X PUT http://localhost:5984/_node/name@host/_config/uuids/algorithm -d '\"random\"'\n```\n**单节点**\n```\ncurl -X PUT http://localhost:5984/_node/_local/_config/uuids/algorithm -d '\"random\"'\n```\n### 基本配置\n#### couchdb配置\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/couchdb/`后接参数.\n\n* attachment_stream-buffer_size\n    缓冲池大小，越大读性能越好，但增加写操作的响应时间.\n* database_dir\n    数据库文件地址\n* default_security\n    默认的安全级别`admin_local`. `everyone`:任何人都可以读和写。`admin_only`：只有admin可以读和写.`admin_local`:被分片的数据库可以被任何人读和写，但分片只可以被admin读和写.\n* delayed_commits\n    延迟提交 `false`保证同步.`true`可以提高性能。\n* file_compression\n    文件压缩默认`snappy`。`none`：不进行压缩。`snappy`：使用谷歌的snappy.`deflate_N`：使用`zlib`,N为压缩等级从1(速度最快，压缩率最低)到9(速度最慢，压缩率最高).\n* fsync_options\n    缓冲区内的文件是否与操作系统同步刷新到硬盘中.一般不需要修改.`fsync_options=[before_header,after_header,on_file_open]`\n* max_dbs_open\n    同时打开数据库的最大数量默认为100.\n* os_process_timeout\n    处理超时时间默认5000ms\n* uri_file\n    该参数指定的文件包含完整的用来访问CouchDB数据库实例的`URI`.默认值：`/var/run/couchdb/couchdb.uri`\n* users_db_suffix\n    用户数据库后缀，默认`_users`.\n* util_driver_dir\n    二进制驱动的位置。\n* uuid\n    CouchDb服务器实例的唯一标识符\n* view_index_dir\n    CouchDB视图索引文件的位置。默认值:`/var/lib/couchdb`\n* maintenance_mode\n    CouchDb节点可以使用的两种维护模式 `true`:该节点不会响应集群中其他节点的请求并且`/_up`端点将返回404响应.`nolb`:`/_up`端点将返回404响应.`false`:该节点正常响应200。\n* max_document_size\n    文档的最大的大小默认为4GB\n\n### cluster 配置\n#### 集群选项\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/cluster/`后接参数.\n* q\n    新创建的数据库的分片数量，默认为8\n* n\n    集群中每一个数据库文档的副本数。单节点为1，每个节点最多仅持有一个副本。\n* placement\n* seedlist\n    以逗号分隔的节点名称列表，当前节点应与之通信加入集群。\n\n### couch_peruser\n#### couch_peruser配置\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/couch_peruser/`后接参数.\n* enable\n    如果设置为true，则_users为私有的数据库。且只允许当前节点操作。\n* delete_dbs\n    如果设置为true，当用户被删除，则相关的数据库也一同删除。\n### CouchDB HTTP 服务器\n#### HTTP Server配置\n**[chttpd]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/chttpd/`后接参数.\n\n* bind_address :集群端口绑定的IP地址\n    * 127.0.0.1\n    * 0.0.0.0：任何IP地址\n    * ::1:IPV6\n    * :: 任何IPV6的地址\n* port:集群端口号\n    * 5984:默认\n    * 0：可使用任何端口号\n* prefer_minimal:如果一个请求含有请求头:Prefer,则只返回`prefer_minimal`配置列表的头部信息\n* authentication_handlers:CouchDB使用的认证头部信息。可以使用第三方插件进行扩展。\n    * {chttpd_auth, cookie_authentication_handler}: Cookie认证;\n    * {couch_httpd_auth, proxy_authentication_handler}:代理认证;\n    * {chttpd_auth, default_authentication_handler}: 基本认证：\n    * {couch_httpd_auth, null_authentication_handler}:取消认证\n\n**[httpd]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/httpd/`后接参数.\n在CouchDB2.X版本，这一部分默认使用5986为默认端口。用于管理员任务与系统维护。应该总是绑定私有的LAN 127.0.0.1地址。\n\n* allow_jsonp\n    * 是否支持JSONP，默认false    \n* bind_address\n    * 本地节点可获得的IP地址。建议总是使用127.0.0.1或IPV6 ::1\n* changes_timeout\n    * 默认超时时间默认为6000ms\n* config_whitelist\n    * 配置信息修改白名单。只有白名单内的值可以通过CONFIG API修改配置。为了允许管理员通过HTTP修改该值，需要包括{httpd,config——whitelist}自己。\n    * config_whitelist = [{httpd,config_whitelist}, {log,level}, {etc,etc}]\n* default_handler\n    * 具体的默认HTTP请求Handler\n    * default_handler = {couch_httpd_db, handle_request}\n* enable_cors\n    * 控制CORS特性,默认false\n* port\n    * 定义监听的端口号 默认5986\n* redirect_vhost_handler\n    * Handler请求到`virtual hosts`的定制的默认功能\n    * redirect_vhost_handler = {Module, Fun}\n* server_options\n    * 可以被添加到配置文件的`MochiWeb`组件的服务器选项。\n    * server_options = [{backlog, 128}, {acceptor_pool_size, 16}]\n* secure_rewrites\n    * 是否允许通过子域隔离数据库。\n* socket_options\n    * CouchDB监听Socket选项。可以定义为元组列表.\n    * socket_options = [{sndbuf, 262144}, {nodelay, true}]\n* server_options\n    * CouchDB中mochiweb acceptor池中任何socket服务器选项，可以定义为元组列表.\n    * server_options = [{recbuf, undefined}]\n* vhost_global_handlers\n    * 对于`virtual hosts`全局的Handlers列表\n    * vhost_global_handlers = _utils, _uuids, _session, _users\n\n* x_forwarded_host\n    * 用于转发`HOST`头部字段的原始值。例如一个转发代理在请求Couchd前重写`Host`头部信息到内部主机。\n    * x_forwarded_host = X-Forwarded-Host\n* x_forwarded_proto\n    * 认证原始的HTTP协议\n* x_forwarded_ssl\n    * 用于告诉CouchDB使用https代替http\n* enable_xframe_options\n    * 控制是否开启特性\n* WWW-Authenticate\n    * 设置在基本认证下不具备权限的请求头部信息\n* max_http_request_size\n    * 限制HTTP请求体最大值大小默认4GB\n\n#### HTTPS (SSL/TLS)选项\nCouchDb支持本地TLS/SSL，不需要使用代理服务器.HTTPS设置比较容器，只需要两个文件：一个证书个一个私钥。可以通过`OpenSSL`命令生成自签名证书。\n```\nshell> mkdir /etc/couchdb/cert\nshell> cd /etc/couchdb/cert\nshell> openssl genrsa > privkey.pem\nshell> openssl req -new -x509 -key privkey.pem -out couchdb.pem -days 1095\nshell> chmod 600 privkey.pem couchdb.pem\nshell> chown couchdb privkey.pem couchdb.pem\n```\n编辑CouchDB配置文件`local.ini`：\n```\nenable = true\ncert_file = /etc/couchdb/cert/couchdb.pem\nkey_file = /etc/couchdb/cert/privkey.pem\n```\n使用自签名证书可以通过参数`-k`忽略警告信息\n```\ncurl -k https://127.0.0.1:6984\n```\n**[ssl]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/ssl/`后接参数.\n\n* cacert_file\n    * 包含PEM编码的CA证书路径,CA证书用于构建服务器证书链。用于客户端权限认证。\n* cert_file\n    * 包含用户证书文件的路径\n* key_file\n    * 包含用户PEM编码的私钥文件路径\n* password\n    * 包含用户密码的字符串,当用户私钥通过密码保护时使用\n* ssl_certifacate_max_depth\n    * 最大的节点证书深度\n* verify_fun\n    * 如果不指定具体的验证功能，则使用默认的验证功能\n* verify_ssl_certificates\n    * 如果为true则验证节点证书\n* fail_if_no_peer_cert\n    * true：如果客户端没有发送证书，则终止TLS/SSL握手\n    * false：只当客户端发送无效证书时，终止TLS/SSL握手\n* secure_renegotiate\n* ciphers\n    * 设置erlang格式的被支持的加密套件\n    * ciphers = [\"ECDHE-ECDSA-AES128-SHA256\", \"ECDHE-ECDSA-AES128-SHA\"]\n* tls_versions\n    * 设置允许的SSL/TLS协议版本列表\n    * tls_versions = [tlsv1 | 'tlsv1.1' | 'tlsv1.2']\n\n#### 跨域资源分享\n跨域资源分享.比如浏览器中运行js的网页通过AJAX请求到不同的域。不需要破坏任何一方的安全。\n一个典型的用例是通过静态网页通过CDN请求另一资源。比如CouchDB实例。这避免了使用JSONP或类似的变通方法来检索和托管内容的中间代理。\nCouchDB实例可以接受直接的连接保护数据库和实例。不会造成浏览器功能由于相同的域的限制被阻塞。CORS支持当前90%的浏览器。\n**[cors]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/httpd/`后接参数.\n* enable_cors\n需要将`httpd/enable_cors`选项设置为`true`。\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/cors/`后接参数.\n\n* credentials\n    * 默认情况下，请求和响应中均不包含身份验证标头或cookie。\n* origins\n    * 通过，分隔接受的原始的URL列表。不能同时设置origins=*和credentials=true\n\n* headers\n    * 通过，分隔的可接受的请求头列表。\n* methods\n    * 可接受的请求方法\n* max_age\n    * Access-Control-Max-Age\n\n#### 虚拟主机\n虚拟主机\nCouchDB可以基于`Host`请求头映射请求到绑定同一个IP地址的不同的位置。\n允许同一机器上不同虚拟机映射到不同的数据库或者是设计文档。\n通过为域名添加一个`CNAME`指针到DNS。在测试或者开发环境下，添加一个实体到hosts文件,如类UNIX系统：\n```\n127.0.0.1       couchdb.local\n```\n最后添加一个实体到配置文件的`[vhosts]`部分：\n```\ncouchdb.local:5984 = /example\n*.couchdb.local:5984 = /example\n```\n如果CouchDB监听在默认的HTTP端口80，或者之前设置了代理，则不需要在`vhosts`中指定端口号.\n第一行将重写请求以显示示例数据库的内容。 仅当Host标头为couchdb.local且不适用于CNAME时，此规则才有效，另一方面，第二条规则将所有CNAME与示例db匹配，这样www.couchdb.local和db.couchdb.local都可以使用。\n**[vhosts]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/vhosts/`后接参数.\n* couchdb.local\n### 认证与权限\n#### 服务器管理员\n默认的CouchDB提供了管理员级别的可以访问所有连接的用户。配置在`Admin Party`部分。不应该在生产环境中使用。可以在创建第一个管理员账户后删除这一部分。CouchDB服务管理员和密码没有存储在`_users`数据库。但是CouchDB加载ini文件时可以在最后发现`admin`部分。这个文件(可能为`etc/local.ini`或者`etc/local.d/10-admins.ini`在Debian/Ubuntu系统从包中安装时发现。)应该安全并且只能由系统管理员可读.\n管理员可以直接添加到`admin`部分，当CouchDB重新启动时，密码将会自动加密。也可以通过HTTP接口创建管理员账户不需要重启CouchDB。HTTP`/_node/{node-name}/_config/admins`地址支持查询，删除或者是创建新管理员账户。\n**[admins]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/admins/`后接参数.\n#### 认证配置\n**[chttpd]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/chttpd/`后接参数.\n\n* require_valid_user\n    * true:不允许来自匿名用户的任何请求\n\n**[couch_httpd_auth]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/couch_httpd_auth/`后接参数.\n\n* allow_persistent_cookies\n    * 使得Cookie持久性。\n* cookie_domain\n    * 配置`AuthSession`Cookie的域属性。默认为空。\n    * cookie_domain=example.com\n* auth_cache_size\n    * 内容中用户对象缓存数量，减少硬盘读写，默认50\n* authentication_redirect\n    * 权限成功验证后，客户端接受`text/html`响应情况下具体的重定向的位置。\n    * authentication_redirect = /_utils/session.html\n\n\n* iterations\n    * 由PBKDF2算法哈希的密码迭代的数量。\n* min_iterations\n    * 最小迭代的数量\n* max_iterations\n    * 最大迭代的数量\n* proxy_use_secret\n    * true:`couch_httpd_auth/secret`选项要求代理身份认证\n* public_fields\n    * 用户文档中可以被任何用户读的由逗号分隔的字段名称。\n    * public_fields = first_name, last_name, contacts, url\n* require_valid_user\n    * true:不允许来自匿名用户的任何请求\n* secret\n    * 用于代理身份认证与Cookie身份认证的secret\n* timeout\n    * 最后一次请求之后session超时过期时间默认600\n* users_db_public\n    * 允许用户查看所有用户文档，默认情况下，只有管理员可以查看所有用户的文档,用户只能查看自己的文档。\n* x_auth_roles\n    * HTTP请求头包含用户的角色，用逗号分隔。用于代理身份认证\n* x_auth_token\n    * HTTP请求头包含用户的token，用逗号分隔。用于代理身份认证\n* x_auth_username\n    * HTTP请求头包含用户的用户名，用逗号分隔。用于代理身份认证\n\n### 压缩配置\n#### 数据库压缩配置\n**[database_compaction]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/database_compaction/`后接参数.\n\n* doc_buffer_size\n    * 具体的拷贝缓冲区大小\n* checkpoint_after\n    * 在具体数量的比特后激活检查点拷贝到压缩数据库。\n\n#### 压缩程序规则\n**[compactions]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/compactions/`后接参数.\n列表中的规则确定什么时候运行自动压缩。配置可以是指定数据库或者是全局的。格式如下:\n```\ndatabase_name = [ {ParamName, ParamValue}, {ParamName, ParamValue}, ... ]\n_default = [ {ParamName, ParamValue}, {ParamName, ParamValue}, ... ]\n_default = [{db_fragmentation, \"70%\"}, {view_fragmentation, \"60%\"}, {from,\"23:00\"}, {to, \"04:00\"}]\n```\n\n1. db_fragmentation:数据库中数据压缩率，包括元数据。计算方法：(file_size-data_size)/file_size*100\n2. view_fragmentation：数据库中索引文件.....\n3. form,to:允许进行压缩的时间段,格式：`HH:MM - HH:MM  (HH in [0..23], MM in [0..59])`\n4. strict_window:如果为true，并且在超时时间后还没有压缩完则终止压缩。\n5. parallel_view_compaction：是否数据和视图同时进行压缩。\n\n#### 压缩程序配置\n**[compaction_daemon]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/compaction_daemon/`后接参数.\n\n* check_interval\n    * 两次检查数据库和视图索引是否需要被压缩的时间间隔，默认为3600\n* min_file_size\n    * 如果数据库或者视图索引文件大小小于该值，则不进行压缩。\n* snooze_period_ms\n\n#### 视图压缩选项\n**[view_compaction]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/view_compaction/`后接参数.\n\n* keyvalue_buffer_size\n    * 压缩时具体的最大拷贝缓冲区大小.\n\n### 日志\n#### 日志选项\n**[log]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/log/`后接参数.\n\n* writer\n    * stderr：日志信息发送到stderr(默认)\n    * file:日志信息存储到文件\n    * syslog：日志信息发送到syslog daemon\n* file\n    * 日志保存到文件的具体的位置(默认`/var/log/couchdb/couch.log`)\n* write_buffer\n    * 写日志缓冲区大小默认0\n* write_delay\n    * 写日志到硬盘延迟默认为0\n* level\n    * 日志级别\n    * debug\n    * info：包括HTTP请求，启动外部程序\n    * notice\n    * warning,warn：警告信息，例如硬盘空间不足\n    * error,err：只输出错误信息\n    * critical crit\n    * alert\n    * emergency emerg\n    * none:不输入任何日志\n* include_sasl\n    * 是否在日志中包含SASL信息\n* syslog_host\n    * 具体的syslog 主机将日志发送到的位置默认localhost\n* syslog_port\n    * 当发送日志信息时连接的syslog端口\n* syslog_appid\n    * 具体的应用名称默认couchdb\n* syslog_facility\n\n### 复制者\n#### 数据库复制配置\n**[replicator]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/replicator/`后接参数.\n\n* max_jobs\n    * 活跃的运行的复制任务数量。\n* interval\n* max_churn\n* update_docs\n* worker_batch_size\n* worker_processes\n* http_connections\n* connection_timeout\n* retries_per_request\n* socket_options\n* checkpoint_interval\n* use_checkpoints\n* cert_file\n* key_file\n* password\n* verify_ssl_certificates\n* ssl_trusted_certificates_file\n* ssl_certificate_max_depth\n* auth_plugins\n\n### Query Servers\n#### Query Servers Definition\n**[query_servers]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/query_servers/`后接参数.\n#### Query Servers Configuration\n**[query_sercver_config]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/query_sercver_config/`后接参数.\n\n* commit_freq\n* os_process_limit\n* os_process_soft_limit\n* reduce_limit\n\n#### Native Erlang Query Server\n**[native_query_servers]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/native_query_servers/`后接参数.\n\n### CouchDB Internal Services\n#### CouchDB Daemonized Mini Apps\n**[daemons]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/daemons/`后接参数.\n\n* auth_cache\n* compaction_daemon\n* external_manager\n* httpd\n* index_server\n* query_servers\n* replicator_manager\n* stats_aggregator\n* stats_collector\n* uuids\n* vhosts\n\n### Miscellaneous Parameters\n#### Configuration of Attachment Storage\n**[attachments]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/attachments/`后接参数.\n\n* compression_level\n* compressible_types\n\n#### Statistic Calculation\n**[stats]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/stats/`后接参数.\n\n* rate\n* samples\n\n#### UUIDs Configuration\n**[uuids]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/uuids/`后接参数.\n\n* algorithm\n* utc_id_suffix\n* max_count\n\n#### Vendor information\n**[vendor]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/vendor/`后接参数.\n\n#### Content-Security_Policy\n**[csp]**\n`curl`地址：`http://localhost:5984/_node/<name@host>/_config/csp/`后接参数.\n\n* enable\n* header_value\n\n## 操作\n### 节点操作\n#### 查看所有节点\n```\ncurl -u admin:adminpw -X GET http://localhost:5984/_membership\n{\n    \"all_nodes\":[   # 当前节点所知道的节点\n        \"node1@xxx.xxx.xxx.xxx\"],\n    \"cluster_nodes\":[ #当前节点所连接的节点\n        \"node1@xxx.xxx.xxx.xxx\"],\n}\n```\n#### 添加一个节点\n```\ncurl -u admin:adminpw -X PUT http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy -d {}\n```\n#### 删除一个节点\n```\n#首先获取关于文档的revision\ncurl -u admin:adminpw -X GET \"http://xxx.xxx.xxx.xxx:5986/_nodes/node2@yyy.yyy.yyy.yyy\"\n{\"_id\":\"node2@yyy.yyy.yyy.yyy\",\"_rev\":\"1-967a00dff5e02add41820138abb3284d\"}\t\n#删除节点\ncurl -u admin:adminpw -X DELETE http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy?rev=1-967a00dff5e02add41820138abb3284d\n```\n\n### 数据库操作\n#### 创建数据库\n数据库名字不支持大写字符，只支持[a-z],[0-9],以及特殊字符:`_ $ ( ) + - /`\n```\n#创建一个数据库名字为db_name \ncurl -u admin:adminpw -X PUT http://localhost:5984/db_name?q=4&n=2\n```\n#### 删除数据库\n```\ncurl -u admin:adminpw -X DELETE http://localhost:5984/db_name\n```","slug":"blog/couchDB/CouchDB学习一","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqygz0018k0vq2prmfo1h","content":"<h2 id=\"端口\"><a href=\"#端口\" class=\"headerlink\" title=\"端口\"></a>端口</h2><table>\n<thead>\n<tr>\n<th>端口号</th>\n<th>协议</th>\n<th>作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>5984</td>\n<td>tcp</td>\n<td>标椎集群端口用于所有的HTTP API请求</td>\n</tr>\n<tr>\n<td>5986</td>\n<td>tcp</td>\n<td>用于管理员对节点与分片的管理</td>\n</tr>\n<tr>\n<td>4369</td>\n<td>tcp</td>\n<td>Erlang端口到daemon的映射</td>\n</tr>\n</tbody></table>\n<h3 id=\"配置介绍\"><a href=\"#配置介绍\" class=\"headerlink\" title=\"配置介绍\"></a>配置介绍</h3><h4 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h4><p>CouchDb从以下位置按顺序读取配置文件</p>\n<ul>\n<li>etc/fefault.ini</li>\n<li>etc/default.d/*.ini</li>\n<li>etc/local.ini</li>\n<li>etc/local.d/*.ini</li>\n</ul>\n<p>类UNIX系统：<code>/opt/couchdb/</code><br>Windows系统:<code>C:\\CouchDB</code><br>maxOS:<code>Applications/Apache CouchDB.app/Contents/Resources/couchdbx-core/etc</code>下的<code>default.ini</code>和<code>default.d</code>文件夹。<code>/Users/youruser/Library/Application Support/CouchDB2/etc/couchdb</code>下的<code>default.ini</code>和<code>default.d</code>文件夹.</p>\n<h4 id=\"通过HTTP-API修改参数\"><a href=\"#通过HTTP-API修改参数\" class=\"headerlink\" title=\"通过HTTP API修改参数\"></a>通过HTTP API修改参数</h4><p><strong>集群</strong>：</p>\n<pre><code>curl -X PUT http://localhost:5984/_node/name@host/_config/uuids/algorithm -d &#39;&quot;random&quot;&#39;</code></pre><p><strong>单节点</strong></p>\n<pre><code>curl -X PUT http://localhost:5984/_node/_local/_config/uuids/algorithm -d &#39;&quot;random&quot;&#39;</code></pre><h3 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h3><h4 id=\"couchdb配置\"><a href=\"#couchdb配置\" class=\"headerlink\" title=\"couchdb配置\"></a>couchdb配置</h4><p><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/couchdb/</code>后接参数.</p>\n<ul>\n<li>attachment_stream-buffer_size<br>  缓冲池大小，越大读性能越好，但增加写操作的响应时间.</li>\n<li>database_dir<br>  数据库文件地址</li>\n<li>default_security<br>  默认的安全级别<code>admin_local</code>. <code>everyone</code>:任何人都可以读和写。<code>admin_only</code>：只有admin可以读和写.<code>admin_local</code>:被分片的数据库可以被任何人读和写，但分片只可以被admin读和写.</li>\n<li>delayed_commits<br>  延迟提交 <code>false</code>保证同步.<code>true</code>可以提高性能。</li>\n<li>file_compression<br>  文件压缩默认<code>snappy</code>。<code>none</code>：不进行压缩。<code>snappy</code>：使用谷歌的snappy.<code>deflate_N</code>：使用<code>zlib</code>,N为压缩等级从1(速度最快，压缩率最低)到9(速度最慢，压缩率最高).</li>\n<li>fsync_options<br>  缓冲区内的文件是否与操作系统同步刷新到硬盘中.一般不需要修改.<code>fsync_options=[before_header,after_header,on_file_open]</code></li>\n<li>max_dbs_open<br>  同时打开数据库的最大数量默认为100.</li>\n<li>os_process_timeout<br>  处理超时时间默认5000ms</li>\n<li>uri_file<br>  该参数指定的文件包含完整的用来访问CouchDB数据库实例的<code>URI</code>.默认值：<code>/var/run/couchdb/couchdb.uri</code></li>\n<li>users_db_suffix<br>  用户数据库后缀，默认<code>_users</code>.</li>\n<li>util_driver_dir<br>  二进制驱动的位置。</li>\n<li>uuid<br>  CouchDb服务器实例的唯一标识符</li>\n<li>view_index_dir<br>  CouchDB视图索引文件的位置。默认值:<code>/var/lib/couchdb</code></li>\n<li>maintenance_mode<br>  CouchDb节点可以使用的两种维护模式 <code>true</code>:该节点不会响应集群中其他节点的请求并且<code>/_up</code>端点将返回404响应.<code>nolb</code>:<code>/_up</code>端点将返回404响应.<code>false</code>:该节点正常响应200。</li>\n<li>max_document_size<br>  文档的最大的大小默认为4GB</li>\n</ul>\n<h3 id=\"cluster-配置\"><a href=\"#cluster-配置\" class=\"headerlink\" title=\"cluster 配置\"></a>cluster 配置</h3><h4 id=\"集群选项\"><a href=\"#集群选项\" class=\"headerlink\" title=\"集群选项\"></a>集群选项</h4><p><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/cluster/</code>后接参数.</p>\n<ul>\n<li>q<br>  新创建的数据库的分片数量，默认为8</li>\n<li>n<br>  集群中每一个数据库文档的副本数。单节点为1，每个节点最多仅持有一个副本。</li>\n<li>placement</li>\n<li>seedlist<br>  以逗号分隔的节点名称列表，当前节点应与之通信加入集群。</li>\n</ul>\n<h3 id=\"couch-peruser\"><a href=\"#couch-peruser\" class=\"headerlink\" title=\"couch_peruser\"></a>couch_peruser</h3><h4 id=\"couch-peruser配置\"><a href=\"#couch-peruser配置\" class=\"headerlink\" title=\"couch_peruser配置\"></a>couch_peruser配置</h4><p><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/couch_peruser/</code>后接参数.</p>\n<ul>\n<li><p>enable<br>  如果设置为true，则_users为私有的数据库。且只允许当前节点操作。</p>\n</li>\n<li><p>delete_dbs<br>  如果设置为true，当用户被删除，则相关的数据库也一同删除。</p>\n<h3 id=\"CouchDB-HTTP-服务器\"><a href=\"#CouchDB-HTTP-服务器\" class=\"headerlink\" title=\"CouchDB HTTP 服务器\"></a>CouchDB HTTP 服务器</h3><h4 id=\"HTTP-Server配置\"><a href=\"#HTTP-Server配置\" class=\"headerlink\" title=\"HTTP Server配置\"></a>HTTP Server配置</h4></li>\n<li><p>*[chttpd]**<br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/chttpd/</code>后接参数.</p>\n</li>\n<li><p>bind_address :集群端口绑定的IP地址</p>\n<ul>\n<li>127.0.0.1</li>\n<li>0.0.0.0：任何IP地址</li>\n<li>::1:IPV6</li>\n<li>:: 任何IPV6的地址</li>\n</ul>\n</li>\n<li><p>port:集群端口号</p>\n<ul>\n<li>5984:默认</li>\n<li>0：可使用任何端口号</li>\n</ul>\n</li>\n<li><p>prefer_minimal:如果一个请求含有请求头:Prefer,则只返回<code>prefer_minimal</code>配置列表的头部信息</p>\n</li>\n<li><p>authentication_handlers:CouchDB使用的认证头部信息。可以使用第三方插件进行扩展。</p>\n<ul>\n<li>{chttpd_auth, cookie_authentication_handler}: Cookie认证;</li>\n<li>{couch_httpd_auth, proxy_authentication_handler}:代理认证;</li>\n<li>{chttpd_auth, default_authentication_handler}: 基本认证：</li>\n<li>{couch_httpd_auth, null_authentication_handler}:取消认证</li>\n</ul>\n</li>\n</ul>\n<p><strong>[httpd]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/httpd/</code>后接参数.<br>在CouchDB2.X版本，这一部分默认使用5986为默认端口。用于管理员任务与系统维护。应该总是绑定私有的LAN 127.0.0.1地址。</p>\n<ul>\n<li><p>allow_jsonp</p>\n<ul>\n<li>是否支持JSONP，默认false    </li>\n</ul>\n</li>\n<li><p>bind_address</p>\n<ul>\n<li>本地节点可获得的IP地址。建议总是使用127.0.0.1或IPV6 ::1</li>\n</ul>\n</li>\n<li><p>changes_timeout</p>\n<ul>\n<li>默认超时时间默认为6000ms</li>\n</ul>\n</li>\n<li><p>config_whitelist</p>\n<ul>\n<li>配置信息修改白名单。只有白名单内的值可以通过CONFIG API修改配置。为了允许管理员通过HTTP修改该值，需要包括{httpd,config——whitelist}自己。</li>\n<li>config_whitelist = [{httpd,config_whitelist}, {log,level}, {etc,etc}]</li>\n</ul>\n</li>\n<li><p>default_handler</p>\n<ul>\n<li>具体的默认HTTP请求Handler</li>\n<li>default_handler = {couch_httpd_db, handle_request}</li>\n</ul>\n</li>\n<li><p>enable_cors</p>\n<ul>\n<li>控制CORS特性,默认false</li>\n</ul>\n</li>\n<li><p>port</p>\n<ul>\n<li>定义监听的端口号 默认5986</li>\n</ul>\n</li>\n<li><p>redirect_vhost_handler</p>\n<ul>\n<li>Handler请求到<code>virtual hosts</code>的定制的默认功能</li>\n<li>redirect_vhost_handler = {Module, Fun}</li>\n</ul>\n</li>\n<li><p>server_options</p>\n<ul>\n<li>可以被添加到配置文件的<code>MochiWeb</code>组件的服务器选项。</li>\n<li>server_options = [{backlog, 128}, {acceptor_pool_size, 16}]</li>\n</ul>\n</li>\n<li><p>secure_rewrites</p>\n<ul>\n<li>是否允许通过子域隔离数据库。</li>\n</ul>\n</li>\n<li><p>socket_options</p>\n<ul>\n<li>CouchDB监听Socket选项。可以定义为元组列表.</li>\n<li>socket_options = [{sndbuf, 262144}, {nodelay, true}]</li>\n</ul>\n</li>\n<li><p>server_options</p>\n<ul>\n<li>CouchDB中mochiweb acceptor池中任何socket服务器选项，可以定义为元组列表.</li>\n<li>server_options = [{recbuf, undefined}]</li>\n</ul>\n</li>\n<li><p>vhost_global_handlers</p>\n<ul>\n<li>对于<code>virtual hosts</code>全局的Handlers列表</li>\n<li>vhost_global_handlers = _utils, _uuids, _session, _users</li>\n</ul>\n</li>\n<li><p>x_forwarded_host</p>\n<ul>\n<li>用于转发<code>HOST</code>头部字段的原始值。例如一个转发代理在请求Couchd前重写<code>Host</code>头部信息到内部主机。</li>\n<li>x_forwarded_host = X-Forwarded-Host</li>\n</ul>\n</li>\n<li><p>x_forwarded_proto</p>\n<ul>\n<li>认证原始的HTTP协议</li>\n</ul>\n</li>\n<li><p>x_forwarded_ssl</p>\n<ul>\n<li>用于告诉CouchDB使用https代替http</li>\n</ul>\n</li>\n<li><p>enable_xframe_options</p>\n<ul>\n<li>控制是否开启特性</li>\n</ul>\n</li>\n<li><p>WWW-Authenticate</p>\n<ul>\n<li>设置在基本认证下不具备权限的请求头部信息</li>\n</ul>\n</li>\n<li><p>max_http_request_size</p>\n<ul>\n<li>限制HTTP请求体最大值大小默认4GB</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"HTTPS-SSL-TLS-选项\"><a href=\"#HTTPS-SSL-TLS-选项\" class=\"headerlink\" title=\"HTTPS (SSL/TLS)选项\"></a>HTTPS (SSL/TLS)选项</h4><p>CouchDb支持本地TLS/SSL，不需要使用代理服务器.HTTPS设置比较容器，只需要两个文件：一个证书个一个私钥。可以通过<code>OpenSSL</code>命令生成自签名证书。</p>\n<pre><code>shell&gt; mkdir /etc/couchdb/cert\nshell&gt; cd /etc/couchdb/cert\nshell&gt; openssl genrsa &gt; privkey.pem\nshell&gt; openssl req -new -x509 -key privkey.pem -out couchdb.pem -days 1095\nshell&gt; chmod 600 privkey.pem couchdb.pem\nshell&gt; chown couchdb privkey.pem couchdb.pem</code></pre><p>编辑CouchDB配置文件<code>local.ini</code>：</p>\n<pre><code>enable = true\ncert_file = /etc/couchdb/cert/couchdb.pem\nkey_file = /etc/couchdb/cert/privkey.pem</code></pre><p>使用自签名证书可以通过参数<code>-k</code>忽略警告信息</p>\n<pre><code>curl -k https://127.0.0.1:6984</code></pre><p><strong>[ssl]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/ssl/</code>后接参数.</p>\n<ul>\n<li>cacert_file<ul>\n<li>包含PEM编码的CA证书路径,CA证书用于构建服务器证书链。用于客户端权限认证。</li>\n</ul>\n</li>\n<li>cert_file<ul>\n<li>包含用户证书文件的路径</li>\n</ul>\n</li>\n<li>key_file<ul>\n<li>包含用户PEM编码的私钥文件路径</li>\n</ul>\n</li>\n<li>password<ul>\n<li>包含用户密码的字符串,当用户私钥通过密码保护时使用</li>\n</ul>\n</li>\n<li>ssl_certifacate_max_depth<ul>\n<li>最大的节点证书深度</li>\n</ul>\n</li>\n<li>verify_fun<ul>\n<li>如果不指定具体的验证功能，则使用默认的验证功能</li>\n</ul>\n</li>\n<li>verify_ssl_certificates<ul>\n<li>如果为true则验证节点证书</li>\n</ul>\n</li>\n<li>fail_if_no_peer_cert<ul>\n<li>true：如果客户端没有发送证书，则终止TLS/SSL握手</li>\n<li>false：只当客户端发送无效证书时，终止TLS/SSL握手</li>\n</ul>\n</li>\n<li>secure_renegotiate</li>\n<li>ciphers<ul>\n<li>设置erlang格式的被支持的加密套件</li>\n<li>ciphers = [“ECDHE-ECDSA-AES128-SHA256”, “ECDHE-ECDSA-AES128-SHA”]</li>\n</ul>\n</li>\n<li>tls_versions<ul>\n<li>设置允许的SSL/TLS协议版本列表</li>\n<li>tls_versions = [tlsv1 | ‘tlsv1.1’ | ‘tlsv1.2’]</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"跨域资源分享\"><a href=\"#跨域资源分享\" class=\"headerlink\" title=\"跨域资源分享\"></a>跨域资源分享</h4><p>跨域资源分享.比如浏览器中运行js的网页通过AJAX请求到不同的域。不需要破坏任何一方的安全。<br>一个典型的用例是通过静态网页通过CDN请求另一资源。比如CouchDB实例。这避免了使用JSONP或类似的变通方法来检索和托管内容的中间代理。<br>CouchDB实例可以接受直接的连接保护数据库和实例。不会造成浏览器功能由于相同的域的限制被阻塞。CORS支持当前90%的浏览器。<br><strong>[cors]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/httpd/</code>后接参数.</p>\n<ul>\n<li><p>enable_cors<br>需要将<code>httpd/enable_cors</code>选项设置为<code>true</code>。<br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/cors/</code>后接参数.</p>\n</li>\n<li><p>credentials</p>\n<ul>\n<li>默认情况下，请求和响应中均不包含身份验证标头或cookie。</li>\n</ul>\n</li>\n<li><p>origins</p>\n<ul>\n<li>通过，分隔接受的原始的URL列表。不能同时设置origins=*和credentials=true</li>\n</ul>\n</li>\n<li><p>headers</p>\n<ul>\n<li>通过，分隔的可接受的请求头列表。</li>\n</ul>\n</li>\n<li><p>methods</p>\n<ul>\n<li>可接受的请求方法</li>\n</ul>\n</li>\n<li><p>max_age</p>\n<ul>\n<li>Access-Control-Max-Age</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"虚拟主机\"><a href=\"#虚拟主机\" class=\"headerlink\" title=\"虚拟主机\"></a>虚拟主机</h4><p>虚拟主机<br>CouchDB可以基于<code>Host</code>请求头映射请求到绑定同一个IP地址的不同的位置。<br>允许同一机器上不同虚拟机映射到不同的数据库或者是设计文档。<br>通过为域名添加一个<code>CNAME</code>指针到DNS。在测试或者开发环境下，添加一个实体到hosts文件,如类UNIX系统：</p>\n<pre><code>127.0.0.1       couchdb.local</code></pre><p>最后添加一个实体到配置文件的<code>[vhosts]</code>部分：</p>\n<pre><code>couchdb.local:5984 = /example\n*.couchdb.local:5984 = /example</code></pre><p>如果CouchDB监听在默认的HTTP端口80，或者之前设置了代理，则不需要在<code>vhosts</code>中指定端口号.<br>第一行将重写请求以显示示例数据库的内容。 仅当Host标头为couchdb.local且不适用于CNAME时，此规则才有效，另一方面，第二条规则将所有CNAME与示例db匹配，这样<a href=\"http://www.couchdb.local和db.couchdb.local都可以使用。\" target=\"_blank\" rel=\"noopener\">www.couchdb.local和db.couchdb.local都可以使用。</a><br><strong>[vhosts]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/vhosts/</code>后接参数.</p>\n<ul>\n<li><p>couchdb.local</p>\n<h3 id=\"认证与权限\"><a href=\"#认证与权限\" class=\"headerlink\" title=\"认证与权限\"></a>认证与权限</h3><h4 id=\"服务器管理员\"><a href=\"#服务器管理员\" class=\"headerlink\" title=\"服务器管理员\"></a>服务器管理员</h4><p>默认的CouchDB提供了管理员级别的可以访问所有连接的用户。配置在<code>Admin Party</code>部分。不应该在生产环境中使用。可以在创建第一个管理员账户后删除这一部分。CouchDB服务管理员和密码没有存储在<code>_users</code>数据库。但是CouchDB加载ini文件时可以在最后发现<code>admin</code>部分。这个文件(可能为<code>etc/local.ini</code>或者<code>etc/local.d/10-admins.ini</code>在Debian/Ubuntu系统从包中安装时发现。)应该安全并且只能由系统管理员可读.<br>管理员可以直接添加到<code>admin</code>部分，当CouchDB重新启动时，密码将会自动加密。也可以通过HTTP接口创建管理员账户不需要重启CouchDB。HTTP<code>/_node/{node-name}/_config/admins</code>地址支持查询，删除或者是创建新管理员账户。</p>\n</li>\n<li><p>*[admins]**<br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/admins/</code>后接参数.</p>\n<h4 id=\"认证配置\"><a href=\"#认证配置\" class=\"headerlink\" title=\"认证配置\"></a>认证配置</h4></li>\n<li><p>*[chttpd]**<br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/chttpd/</code>后接参数.</p>\n</li>\n<li><p>require_valid_user</p>\n<ul>\n<li>true:不允许来自匿名用户的任何请求</li>\n</ul>\n</li>\n</ul>\n<p><strong>[couch_httpd_auth]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/couch_httpd_auth/</code>后接参数.</p>\n<ul>\n<li>allow_persistent_cookies<ul>\n<li>使得Cookie持久性。</li>\n</ul>\n</li>\n<li>cookie_domain<ul>\n<li>配置<code>AuthSession</code>Cookie的域属性。默认为空。</li>\n<li>cookie_domain=example.com</li>\n</ul>\n</li>\n<li>auth_cache_size<ul>\n<li>内容中用户对象缓存数量，减少硬盘读写，默认50</li>\n</ul>\n</li>\n<li>authentication_redirect<ul>\n<li>权限成功验证后，客户端接受<code>text/html</code>响应情况下具体的重定向的位置。</li>\n<li>authentication_redirect = /_utils/session.html</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>iterations<ul>\n<li>由PBKDF2算法哈希的密码迭代的数量。</li>\n</ul>\n</li>\n<li>min_iterations<ul>\n<li>最小迭代的数量</li>\n</ul>\n</li>\n<li>max_iterations<ul>\n<li>最大迭代的数量</li>\n</ul>\n</li>\n<li>proxy_use_secret<ul>\n<li>true:<code>couch_httpd_auth/secret</code>选项要求代理身份认证</li>\n</ul>\n</li>\n<li>public_fields<ul>\n<li>用户文档中可以被任何用户读的由逗号分隔的字段名称。</li>\n<li>public_fields = first_name, last_name, contacts, url</li>\n</ul>\n</li>\n<li>require_valid_user<ul>\n<li>true:不允许来自匿名用户的任何请求</li>\n</ul>\n</li>\n<li>secret<ul>\n<li>用于代理身份认证与Cookie身份认证的secret</li>\n</ul>\n</li>\n<li>timeout<ul>\n<li>最后一次请求之后session超时过期时间默认600</li>\n</ul>\n</li>\n<li>users_db_public<ul>\n<li>允许用户查看所有用户文档，默认情况下，只有管理员可以查看所有用户的文档,用户只能查看自己的文档。</li>\n</ul>\n</li>\n<li>x_auth_roles<ul>\n<li>HTTP请求头包含用户的角色，用逗号分隔。用于代理身份认证</li>\n</ul>\n</li>\n<li>x_auth_token<ul>\n<li>HTTP请求头包含用户的token，用逗号分隔。用于代理身份认证</li>\n</ul>\n</li>\n<li>x_auth_username<ul>\n<li>HTTP请求头包含用户的用户名，用逗号分隔。用于代理身份认证</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"压缩配置\"><a href=\"#压缩配置\" class=\"headerlink\" title=\"压缩配置\"></a>压缩配置</h3><h4 id=\"数据库压缩配置\"><a href=\"#数据库压缩配置\" class=\"headerlink\" title=\"数据库压缩配置\"></a>数据库压缩配置</h4><p><strong>[database_compaction]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/database_compaction/</code>后接参数.</p>\n<ul>\n<li>doc_buffer_size<ul>\n<li>具体的拷贝缓冲区大小</li>\n</ul>\n</li>\n<li>checkpoint_after<ul>\n<li>在具体数量的比特后激活检查点拷贝到压缩数据库。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"压缩程序规则\"><a href=\"#压缩程序规则\" class=\"headerlink\" title=\"压缩程序规则\"></a>压缩程序规则</h4><p><strong>[compactions]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/compactions/</code>后接参数.<br>列表中的规则确定什么时候运行自动压缩。配置可以是指定数据库或者是全局的。格式如下:</p>\n<pre><code>database_name = [ {ParamName, ParamValue}, {ParamName, ParamValue}, ... ]\n_default = [ {ParamName, ParamValue}, {ParamName, ParamValue}, ... ]\n_default = [{db_fragmentation, &quot;70%&quot;}, {view_fragmentation, &quot;60%&quot;}, {from,&quot;23:00&quot;}, {to, &quot;04:00&quot;}]</code></pre><ol>\n<li>db_fragmentation:数据库中数据压缩率，包括元数据。计算方法：(file_size-data_size)/file_size*100</li>\n<li>view_fragmentation：数据库中索引文件…..</li>\n<li>form,to:允许进行压缩的时间段,格式：<code>HH:MM - HH:MM  (HH in [0..23], MM in [0..59])</code></li>\n<li>strict_window:如果为true，并且在超时时间后还没有压缩完则终止压缩。</li>\n<li>parallel_view_compaction：是否数据和视图同时进行压缩。</li>\n</ol>\n<h4 id=\"压缩程序配置\"><a href=\"#压缩程序配置\" class=\"headerlink\" title=\"压缩程序配置\"></a>压缩程序配置</h4><p><strong>[compaction_daemon]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/compaction_daemon/</code>后接参数.</p>\n<ul>\n<li>check_interval<ul>\n<li>两次检查数据库和视图索引是否需要被压缩的时间间隔，默认为3600</li>\n</ul>\n</li>\n<li>min_file_size<ul>\n<li>如果数据库或者视图索引文件大小小于该值，则不进行压缩。</li>\n</ul>\n</li>\n<li>snooze_period_ms</li>\n</ul>\n<h4 id=\"视图压缩选项\"><a href=\"#视图压缩选项\" class=\"headerlink\" title=\"视图压缩选项\"></a>视图压缩选项</h4><p><strong>[view_compaction]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/view_compaction/</code>后接参数.</p>\n<ul>\n<li>keyvalue_buffer_size<ul>\n<li>压缩时具体的最大拷贝缓冲区大小.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"日志\"><a href=\"#日志\" class=\"headerlink\" title=\"日志\"></a>日志</h3><h4 id=\"日志选项\"><a href=\"#日志选项\" class=\"headerlink\" title=\"日志选项\"></a>日志选项</h4><p><strong>[log]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/log/</code>后接参数.</p>\n<ul>\n<li>writer<ul>\n<li>stderr：日志信息发送到stderr(默认)</li>\n<li>file:日志信息存储到文件</li>\n<li>syslog：日志信息发送到syslog daemon</li>\n</ul>\n</li>\n<li>file<ul>\n<li>日志保存到文件的具体的位置(默认<code>/var/log/couchdb/couch.log</code>)</li>\n</ul>\n</li>\n<li>write_buffer<ul>\n<li>写日志缓冲区大小默认0</li>\n</ul>\n</li>\n<li>write_delay<ul>\n<li>写日志到硬盘延迟默认为0</li>\n</ul>\n</li>\n<li>level<ul>\n<li>日志级别</li>\n<li>debug</li>\n<li>info：包括HTTP请求，启动外部程序</li>\n<li>notice</li>\n<li>warning,warn：警告信息，例如硬盘空间不足</li>\n<li>error,err：只输出错误信息</li>\n<li>critical crit</li>\n<li>alert</li>\n<li>emergency emerg</li>\n<li>none:不输入任何日志</li>\n</ul>\n</li>\n<li>include_sasl<ul>\n<li>是否在日志中包含SASL信息</li>\n</ul>\n</li>\n<li>syslog_host<ul>\n<li>具体的syslog 主机将日志发送到的位置默认localhost</li>\n</ul>\n</li>\n<li>syslog_port<ul>\n<li>当发送日志信息时连接的syslog端口</li>\n</ul>\n</li>\n<li>syslog_appid<ul>\n<li>具体的应用名称默认couchdb</li>\n</ul>\n</li>\n<li>syslog_facility</li>\n</ul>\n<h3 id=\"复制者\"><a href=\"#复制者\" class=\"headerlink\" title=\"复制者\"></a>复制者</h3><h4 id=\"数据库复制配置\"><a href=\"#数据库复制配置\" class=\"headerlink\" title=\"数据库复制配置\"></a>数据库复制配置</h4><p><strong>[replicator]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/replicator/</code>后接参数.</p>\n<ul>\n<li>max_jobs<ul>\n<li>活跃的运行的复制任务数量。</li>\n</ul>\n</li>\n<li>interval</li>\n<li>max_churn</li>\n<li>update_docs</li>\n<li>worker_batch_size</li>\n<li>worker_processes</li>\n<li>http_connections</li>\n<li>connection_timeout</li>\n<li>retries_per_request</li>\n<li>socket_options</li>\n<li>checkpoint_interval</li>\n<li>use_checkpoints</li>\n<li>cert_file</li>\n<li>key_file</li>\n<li>password</li>\n<li>verify_ssl_certificates</li>\n<li>ssl_trusted_certificates_file</li>\n<li>ssl_certificate_max_depth</li>\n<li>auth_plugins</li>\n</ul>\n<h3 id=\"Query-Servers\"><a href=\"#Query-Servers\" class=\"headerlink\" title=\"Query Servers\"></a>Query Servers</h3><h4 id=\"Query-Servers-Definition\"><a href=\"#Query-Servers-Definition\" class=\"headerlink\" title=\"Query Servers Definition\"></a>Query Servers Definition</h4><p><strong>[query_servers]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/query_servers/</code>后接参数.</p>\n<h4 id=\"Query-Servers-Configuration\"><a href=\"#Query-Servers-Configuration\" class=\"headerlink\" title=\"Query Servers Configuration\"></a>Query Servers Configuration</h4><p><strong>[query_sercver_config]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/query_sercver_config/</code>后接参数.</p>\n<ul>\n<li>commit_freq</li>\n<li>os_process_limit</li>\n<li>os_process_soft_limit</li>\n<li>reduce_limit</li>\n</ul>\n<h4 id=\"Native-Erlang-Query-Server\"><a href=\"#Native-Erlang-Query-Server\" class=\"headerlink\" title=\"Native Erlang Query Server\"></a>Native Erlang Query Server</h4><p><strong>[native_query_servers]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/native_query_servers/</code>后接参数.</p>\n<h3 id=\"CouchDB-Internal-Services\"><a href=\"#CouchDB-Internal-Services\" class=\"headerlink\" title=\"CouchDB Internal Services\"></a>CouchDB Internal Services</h3><h4 id=\"CouchDB-Daemonized-Mini-Apps\"><a href=\"#CouchDB-Daemonized-Mini-Apps\" class=\"headerlink\" title=\"CouchDB Daemonized Mini Apps\"></a>CouchDB Daemonized Mini Apps</h4><p><strong>[daemons]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/daemons/</code>后接参数.</p>\n<ul>\n<li>auth_cache</li>\n<li>compaction_daemon</li>\n<li>external_manager</li>\n<li>httpd</li>\n<li>index_server</li>\n<li>query_servers</li>\n<li>replicator_manager</li>\n<li>stats_aggregator</li>\n<li>stats_collector</li>\n<li>uuids</li>\n<li>vhosts</li>\n</ul>\n<h3 id=\"Miscellaneous-Parameters\"><a href=\"#Miscellaneous-Parameters\" class=\"headerlink\" title=\"Miscellaneous Parameters\"></a>Miscellaneous Parameters</h3><h4 id=\"Configuration-of-Attachment-Storage\"><a href=\"#Configuration-of-Attachment-Storage\" class=\"headerlink\" title=\"Configuration of Attachment Storage\"></a>Configuration of Attachment Storage</h4><p><strong>[attachments]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/attachments/</code>后接参数.</p>\n<ul>\n<li>compression_level</li>\n<li>compressible_types</li>\n</ul>\n<h4 id=\"Statistic-Calculation\"><a href=\"#Statistic-Calculation\" class=\"headerlink\" title=\"Statistic Calculation\"></a>Statistic Calculation</h4><p><strong>[stats]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/stats/</code>后接参数.</p>\n<ul>\n<li>rate</li>\n<li>samples</li>\n</ul>\n<h4 id=\"UUIDs-Configuration\"><a href=\"#UUIDs-Configuration\" class=\"headerlink\" title=\"UUIDs Configuration\"></a>UUIDs Configuration</h4><p><strong>[uuids]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/uuids/</code>后接参数.</p>\n<ul>\n<li>algorithm</li>\n<li>utc_id_suffix</li>\n<li>max_count</li>\n</ul>\n<h4 id=\"Vendor-information\"><a href=\"#Vendor-information\" class=\"headerlink\" title=\"Vendor information\"></a>Vendor information</h4><p><strong>[vendor]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/vendor/</code>后接参数.</p>\n<h4 id=\"Content-Security-Policy\"><a href=\"#Content-Security-Policy\" class=\"headerlink\" title=\"Content-Security_Policy\"></a>Content-Security_Policy</h4><p><strong>[csp]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/csp/</code>后接参数.</p>\n<ul>\n<li>enable</li>\n<li>header_value</li>\n</ul>\n<h2 id=\"操作\"><a href=\"#操作\" class=\"headerlink\" title=\"操作\"></a>操作</h2><h3 id=\"节点操作\"><a href=\"#节点操作\" class=\"headerlink\" title=\"节点操作\"></a>节点操作</h3><h4 id=\"查看所有节点\"><a href=\"#查看所有节点\" class=\"headerlink\" title=\"查看所有节点\"></a>查看所有节点</h4><pre><code>curl -u admin:adminpw -X GET http://localhost:5984/_membership\n{\n    &quot;all_nodes&quot;:[   # 当前节点所知道的节点\n        &quot;node1@xxx.xxx.xxx.xxx&quot;],\n    &quot;cluster_nodes&quot;:[ #当前节点所连接的节点\n        &quot;node1@xxx.xxx.xxx.xxx&quot;],\n}</code></pre><h4 id=\"添加一个节点\"><a href=\"#添加一个节点\" class=\"headerlink\" title=\"添加一个节点\"></a>添加一个节点</h4><pre><code>curl -u admin:adminpw -X PUT http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy -d {}</code></pre><h4 id=\"删除一个节点\"><a href=\"#删除一个节点\" class=\"headerlink\" title=\"删除一个节点\"></a>删除一个节点</h4><pre><code>#首先获取关于文档的revision\ncurl -u admin:adminpw -X GET &quot;http://xxx.xxx.xxx.xxx:5986/_nodes/node2@yyy.yyy.yyy.yyy&quot;\n{&quot;_id&quot;:&quot;node2@yyy.yyy.yyy.yyy&quot;,&quot;_rev&quot;:&quot;1-967a00dff5e02add41820138abb3284d&quot;}    \n#删除节点\ncurl -u admin:adminpw -X DELETE http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy?rev=1-967a00dff5e02add41820138abb3284d</code></pre><h3 id=\"数据库操作\"><a href=\"#数据库操作\" class=\"headerlink\" title=\"数据库操作\"></a>数据库操作</h3><h4 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h4><p>数据库名字不支持大写字符，只支持[a-z],[0-9],以及特殊字符:<code>_ $ ( ) + - /</code></p>\n<pre><code>#创建一个数据库名字为db_name \ncurl -u admin:adminpw -X PUT http://localhost:5984/db_name?q=4&amp;n=2</code></pre><h4 id=\"删除数据库\"><a href=\"#删除数据库\" class=\"headerlink\" title=\"删除数据库\"></a>删除数据库</h4><pre><code>curl -u admin:adminpw -X DELETE http://localhost:5984/db_name</code></pre>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"端口\"><a href=\"#端口\" class=\"headerlink\" title=\"端口\"></a>端口</h2><table>\n<thead>\n<tr>\n<th>端口号</th>\n<th>协议</th>\n<th>作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>5984</td>\n<td>tcp</td>\n<td>标椎集群端口用于所有的HTTP API请求</td>\n</tr>\n<tr>\n<td>5986</td>\n<td>tcp</td>\n<td>用于管理员对节点与分片的管理</td>\n</tr>\n<tr>\n<td>4369</td>\n<td>tcp</td>\n<td>Erlang端口到daemon的映射</td>\n</tr>\n</tbody></table>\n<h3 id=\"配置介绍\"><a href=\"#配置介绍\" class=\"headerlink\" title=\"配置介绍\"></a>配置介绍</h3><h4 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h4><p>CouchDb从以下位置按顺序读取配置文件</p>\n<ul>\n<li>etc/fefault.ini</li>\n<li>etc/default.d/*.ini</li>\n<li>etc/local.ini</li>\n<li>etc/local.d/*.ini</li>\n</ul>\n<p>类UNIX系统：<code>/opt/couchdb/</code><br>Windows系统:<code>C:\\CouchDB</code><br>maxOS:<code>Applications/Apache CouchDB.app/Contents/Resources/couchdbx-core/etc</code>下的<code>default.ini</code>和<code>default.d</code>文件夹。<code>/Users/youruser/Library/Application Support/CouchDB2/etc/couchdb</code>下的<code>default.ini</code>和<code>default.d</code>文件夹.</p>\n<h4 id=\"通过HTTP-API修改参数\"><a href=\"#通过HTTP-API修改参数\" class=\"headerlink\" title=\"通过HTTP API修改参数\"></a>通过HTTP API修改参数</h4><p><strong>集群</strong>：</p>\n<pre><code>curl -X PUT http://localhost:5984/_node/name@host/_config/uuids/algorithm -d &#39;&quot;random&quot;&#39;</code></pre><p><strong>单节点</strong></p>\n<pre><code>curl -X PUT http://localhost:5984/_node/_local/_config/uuids/algorithm -d &#39;&quot;random&quot;&#39;</code></pre><h3 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h3><h4 id=\"couchdb配置\"><a href=\"#couchdb配置\" class=\"headerlink\" title=\"couchdb配置\"></a>couchdb配置</h4><p><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/couchdb/</code>后接参数.</p>\n<ul>\n<li>attachment_stream-buffer_size<br>  缓冲池大小，越大读性能越好，但增加写操作的响应时间.</li>\n<li>database_dir<br>  数据库文件地址</li>\n<li>default_security<br>  默认的安全级别<code>admin_local</code>. <code>everyone</code>:任何人都可以读和写。<code>admin_only</code>：只有admin可以读和写.<code>admin_local</code>:被分片的数据库可以被任何人读和写，但分片只可以被admin读和写.</li>\n<li>delayed_commits<br>  延迟提交 <code>false</code>保证同步.<code>true</code>可以提高性能。</li>\n<li>file_compression<br>  文件压缩默认<code>snappy</code>。<code>none</code>：不进行压缩。<code>snappy</code>：使用谷歌的snappy.<code>deflate_N</code>：使用<code>zlib</code>,N为压缩等级从1(速度最快，压缩率最低)到9(速度最慢，压缩率最高).</li>\n<li>fsync_options<br>  缓冲区内的文件是否与操作系统同步刷新到硬盘中.一般不需要修改.<code>fsync_options=[before_header,after_header,on_file_open]</code></li>\n<li>max_dbs_open<br>  同时打开数据库的最大数量默认为100.</li>\n<li>os_process_timeout<br>  处理超时时间默认5000ms</li>\n<li>uri_file<br>  该参数指定的文件包含完整的用来访问CouchDB数据库实例的<code>URI</code>.默认值：<code>/var/run/couchdb/couchdb.uri</code></li>\n<li>users_db_suffix<br>  用户数据库后缀，默认<code>_users</code>.</li>\n<li>util_driver_dir<br>  二进制驱动的位置。</li>\n<li>uuid<br>  CouchDb服务器实例的唯一标识符</li>\n<li>view_index_dir<br>  CouchDB视图索引文件的位置。默认值:<code>/var/lib/couchdb</code></li>\n<li>maintenance_mode<br>  CouchDb节点可以使用的两种维护模式 <code>true</code>:该节点不会响应集群中其他节点的请求并且<code>/_up</code>端点将返回404响应.<code>nolb</code>:<code>/_up</code>端点将返回404响应.<code>false</code>:该节点正常响应200。</li>\n<li>max_document_size<br>  文档的最大的大小默认为4GB</li>\n</ul>\n<h3 id=\"cluster-配置\"><a href=\"#cluster-配置\" class=\"headerlink\" title=\"cluster 配置\"></a>cluster 配置</h3><h4 id=\"集群选项\"><a href=\"#集群选项\" class=\"headerlink\" title=\"集群选项\"></a>集群选项</h4><p><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/cluster/</code>后接参数.</p>\n<ul>\n<li>q<br>  新创建的数据库的分片数量，默认为8</li>\n<li>n<br>  集群中每一个数据库文档的副本数。单节点为1，每个节点最多仅持有一个副本。</li>\n<li>placement</li>\n<li>seedlist<br>  以逗号分隔的节点名称列表，当前节点应与之通信加入集群。</li>\n</ul>\n<h3 id=\"couch-peruser\"><a href=\"#couch-peruser\" class=\"headerlink\" title=\"couch_peruser\"></a>couch_peruser</h3><h4 id=\"couch-peruser配置\"><a href=\"#couch-peruser配置\" class=\"headerlink\" title=\"couch_peruser配置\"></a>couch_peruser配置</h4><p><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/couch_peruser/</code>后接参数.</p>\n<ul>\n<li><p>enable<br>  如果设置为true，则_users为私有的数据库。且只允许当前节点操作。</p>\n</li>\n<li><p>delete_dbs<br>  如果设置为true，当用户被删除，则相关的数据库也一同删除。</p>\n<h3 id=\"CouchDB-HTTP-服务器\"><a href=\"#CouchDB-HTTP-服务器\" class=\"headerlink\" title=\"CouchDB HTTP 服务器\"></a>CouchDB HTTP 服务器</h3><h4 id=\"HTTP-Server配置\"><a href=\"#HTTP-Server配置\" class=\"headerlink\" title=\"HTTP Server配置\"></a>HTTP Server配置</h4></li>\n<li><p>*[chttpd]**<br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/chttpd/</code>后接参数.</p>\n</li>\n<li><p>bind_address :集群端口绑定的IP地址</p>\n<ul>\n<li>127.0.0.1</li>\n<li>0.0.0.0：任何IP地址</li>\n<li>::1:IPV6</li>\n<li>:: 任何IPV6的地址</li>\n</ul>\n</li>\n<li><p>port:集群端口号</p>\n<ul>\n<li>5984:默认</li>\n<li>0：可使用任何端口号</li>\n</ul>\n</li>\n<li><p>prefer_minimal:如果一个请求含有请求头:Prefer,则只返回<code>prefer_minimal</code>配置列表的头部信息</p>\n</li>\n<li><p>authentication_handlers:CouchDB使用的认证头部信息。可以使用第三方插件进行扩展。</p>\n<ul>\n<li>{chttpd_auth, cookie_authentication_handler}: Cookie认证;</li>\n<li>{couch_httpd_auth, proxy_authentication_handler}:代理认证;</li>\n<li>{chttpd_auth, default_authentication_handler}: 基本认证：</li>\n<li>{couch_httpd_auth, null_authentication_handler}:取消认证</li>\n</ul>\n</li>\n</ul>\n<p><strong>[httpd]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/httpd/</code>后接参数.<br>在CouchDB2.X版本，这一部分默认使用5986为默认端口。用于管理员任务与系统维护。应该总是绑定私有的LAN 127.0.0.1地址。</p>\n<ul>\n<li><p>allow_jsonp</p>\n<ul>\n<li>是否支持JSONP，默认false    </li>\n</ul>\n</li>\n<li><p>bind_address</p>\n<ul>\n<li>本地节点可获得的IP地址。建议总是使用127.0.0.1或IPV6 ::1</li>\n</ul>\n</li>\n<li><p>changes_timeout</p>\n<ul>\n<li>默认超时时间默认为6000ms</li>\n</ul>\n</li>\n<li><p>config_whitelist</p>\n<ul>\n<li>配置信息修改白名单。只有白名单内的值可以通过CONFIG API修改配置。为了允许管理员通过HTTP修改该值，需要包括{httpd,config——whitelist}自己。</li>\n<li>config_whitelist = [{httpd,config_whitelist}, {log,level}, {etc,etc}]</li>\n</ul>\n</li>\n<li><p>default_handler</p>\n<ul>\n<li>具体的默认HTTP请求Handler</li>\n<li>default_handler = {couch_httpd_db, handle_request}</li>\n</ul>\n</li>\n<li><p>enable_cors</p>\n<ul>\n<li>控制CORS特性,默认false</li>\n</ul>\n</li>\n<li><p>port</p>\n<ul>\n<li>定义监听的端口号 默认5986</li>\n</ul>\n</li>\n<li><p>redirect_vhost_handler</p>\n<ul>\n<li>Handler请求到<code>virtual hosts</code>的定制的默认功能</li>\n<li>redirect_vhost_handler = {Module, Fun}</li>\n</ul>\n</li>\n<li><p>server_options</p>\n<ul>\n<li>可以被添加到配置文件的<code>MochiWeb</code>组件的服务器选项。</li>\n<li>server_options = [{backlog, 128}, {acceptor_pool_size, 16}]</li>\n</ul>\n</li>\n<li><p>secure_rewrites</p>\n<ul>\n<li>是否允许通过子域隔离数据库。</li>\n</ul>\n</li>\n<li><p>socket_options</p>\n<ul>\n<li>CouchDB监听Socket选项。可以定义为元组列表.</li>\n<li>socket_options = [{sndbuf, 262144}, {nodelay, true}]</li>\n</ul>\n</li>\n<li><p>server_options</p>\n<ul>\n<li>CouchDB中mochiweb acceptor池中任何socket服务器选项，可以定义为元组列表.</li>\n<li>server_options = [{recbuf, undefined}]</li>\n</ul>\n</li>\n<li><p>vhost_global_handlers</p>\n<ul>\n<li>对于<code>virtual hosts</code>全局的Handlers列表</li>\n<li>vhost_global_handlers = _utils, _uuids, _session, _users</li>\n</ul>\n</li>\n<li><p>x_forwarded_host</p>\n<ul>\n<li>用于转发<code>HOST</code>头部字段的原始值。例如一个转发代理在请求Couchd前重写<code>Host</code>头部信息到内部主机。</li>\n<li>x_forwarded_host = X-Forwarded-Host</li>\n</ul>\n</li>\n<li><p>x_forwarded_proto</p>\n<ul>\n<li>认证原始的HTTP协议</li>\n</ul>\n</li>\n<li><p>x_forwarded_ssl</p>\n<ul>\n<li>用于告诉CouchDB使用https代替http</li>\n</ul>\n</li>\n<li><p>enable_xframe_options</p>\n<ul>\n<li>控制是否开启特性</li>\n</ul>\n</li>\n<li><p>WWW-Authenticate</p>\n<ul>\n<li>设置在基本认证下不具备权限的请求头部信息</li>\n</ul>\n</li>\n<li><p>max_http_request_size</p>\n<ul>\n<li>限制HTTP请求体最大值大小默认4GB</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"HTTPS-SSL-TLS-选项\"><a href=\"#HTTPS-SSL-TLS-选项\" class=\"headerlink\" title=\"HTTPS (SSL/TLS)选项\"></a>HTTPS (SSL/TLS)选项</h4><p>CouchDb支持本地TLS/SSL，不需要使用代理服务器.HTTPS设置比较容器，只需要两个文件：一个证书个一个私钥。可以通过<code>OpenSSL</code>命令生成自签名证书。</p>\n<pre><code>shell&gt; mkdir /etc/couchdb/cert\nshell&gt; cd /etc/couchdb/cert\nshell&gt; openssl genrsa &gt; privkey.pem\nshell&gt; openssl req -new -x509 -key privkey.pem -out couchdb.pem -days 1095\nshell&gt; chmod 600 privkey.pem couchdb.pem\nshell&gt; chown couchdb privkey.pem couchdb.pem</code></pre><p>编辑CouchDB配置文件<code>local.ini</code>：</p>\n<pre><code>enable = true\ncert_file = /etc/couchdb/cert/couchdb.pem\nkey_file = /etc/couchdb/cert/privkey.pem</code></pre><p>使用自签名证书可以通过参数<code>-k</code>忽略警告信息</p>\n<pre><code>curl -k https://127.0.0.1:6984</code></pre><p><strong>[ssl]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/ssl/</code>后接参数.</p>\n<ul>\n<li>cacert_file<ul>\n<li>包含PEM编码的CA证书路径,CA证书用于构建服务器证书链。用于客户端权限认证。</li>\n</ul>\n</li>\n<li>cert_file<ul>\n<li>包含用户证书文件的路径</li>\n</ul>\n</li>\n<li>key_file<ul>\n<li>包含用户PEM编码的私钥文件路径</li>\n</ul>\n</li>\n<li>password<ul>\n<li>包含用户密码的字符串,当用户私钥通过密码保护时使用</li>\n</ul>\n</li>\n<li>ssl_certifacate_max_depth<ul>\n<li>最大的节点证书深度</li>\n</ul>\n</li>\n<li>verify_fun<ul>\n<li>如果不指定具体的验证功能，则使用默认的验证功能</li>\n</ul>\n</li>\n<li>verify_ssl_certificates<ul>\n<li>如果为true则验证节点证书</li>\n</ul>\n</li>\n<li>fail_if_no_peer_cert<ul>\n<li>true：如果客户端没有发送证书，则终止TLS/SSL握手</li>\n<li>false：只当客户端发送无效证书时，终止TLS/SSL握手</li>\n</ul>\n</li>\n<li>secure_renegotiate</li>\n<li>ciphers<ul>\n<li>设置erlang格式的被支持的加密套件</li>\n<li>ciphers = [“ECDHE-ECDSA-AES128-SHA256”, “ECDHE-ECDSA-AES128-SHA”]</li>\n</ul>\n</li>\n<li>tls_versions<ul>\n<li>设置允许的SSL/TLS协议版本列表</li>\n<li>tls_versions = [tlsv1 | ‘tlsv1.1’ | ‘tlsv1.2’]</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"跨域资源分享\"><a href=\"#跨域资源分享\" class=\"headerlink\" title=\"跨域资源分享\"></a>跨域资源分享</h4><p>跨域资源分享.比如浏览器中运行js的网页通过AJAX请求到不同的域。不需要破坏任何一方的安全。<br>一个典型的用例是通过静态网页通过CDN请求另一资源。比如CouchDB实例。这避免了使用JSONP或类似的变通方法来检索和托管内容的中间代理。<br>CouchDB实例可以接受直接的连接保护数据库和实例。不会造成浏览器功能由于相同的域的限制被阻塞。CORS支持当前90%的浏览器。<br><strong>[cors]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/httpd/</code>后接参数.</p>\n<ul>\n<li><p>enable_cors<br>需要将<code>httpd/enable_cors</code>选项设置为<code>true</code>。<br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/cors/</code>后接参数.</p>\n</li>\n<li><p>credentials</p>\n<ul>\n<li>默认情况下，请求和响应中均不包含身份验证标头或cookie。</li>\n</ul>\n</li>\n<li><p>origins</p>\n<ul>\n<li>通过，分隔接受的原始的URL列表。不能同时设置origins=*和credentials=true</li>\n</ul>\n</li>\n<li><p>headers</p>\n<ul>\n<li>通过，分隔的可接受的请求头列表。</li>\n</ul>\n</li>\n<li><p>methods</p>\n<ul>\n<li>可接受的请求方法</li>\n</ul>\n</li>\n<li><p>max_age</p>\n<ul>\n<li>Access-Control-Max-Age</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"虚拟主机\"><a href=\"#虚拟主机\" class=\"headerlink\" title=\"虚拟主机\"></a>虚拟主机</h4><p>虚拟主机<br>CouchDB可以基于<code>Host</code>请求头映射请求到绑定同一个IP地址的不同的位置。<br>允许同一机器上不同虚拟机映射到不同的数据库或者是设计文档。<br>通过为域名添加一个<code>CNAME</code>指针到DNS。在测试或者开发环境下，添加一个实体到hosts文件,如类UNIX系统：</p>\n<pre><code>127.0.0.1       couchdb.local</code></pre><p>最后添加一个实体到配置文件的<code>[vhosts]</code>部分：</p>\n<pre><code>couchdb.local:5984 = /example\n*.couchdb.local:5984 = /example</code></pre><p>如果CouchDB监听在默认的HTTP端口80，或者之前设置了代理，则不需要在<code>vhosts</code>中指定端口号.<br>第一行将重写请求以显示示例数据库的内容。 仅当Host标头为couchdb.local且不适用于CNAME时，此规则才有效，另一方面，第二条规则将所有CNAME与示例db匹配，这样<a href=\"http://www.couchdb.local和db.couchdb.local都可以使用。\" target=\"_blank\" rel=\"noopener\">www.couchdb.local和db.couchdb.local都可以使用。</a><br><strong>[vhosts]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/vhosts/</code>后接参数.</p>\n<ul>\n<li><p>couchdb.local</p>\n<h3 id=\"认证与权限\"><a href=\"#认证与权限\" class=\"headerlink\" title=\"认证与权限\"></a>认证与权限</h3><h4 id=\"服务器管理员\"><a href=\"#服务器管理员\" class=\"headerlink\" title=\"服务器管理员\"></a>服务器管理员</h4><p>默认的CouchDB提供了管理员级别的可以访问所有连接的用户。配置在<code>Admin Party</code>部分。不应该在生产环境中使用。可以在创建第一个管理员账户后删除这一部分。CouchDB服务管理员和密码没有存储在<code>_users</code>数据库。但是CouchDB加载ini文件时可以在最后发现<code>admin</code>部分。这个文件(可能为<code>etc/local.ini</code>或者<code>etc/local.d/10-admins.ini</code>在Debian/Ubuntu系统从包中安装时发现。)应该安全并且只能由系统管理员可读.<br>管理员可以直接添加到<code>admin</code>部分，当CouchDB重新启动时，密码将会自动加密。也可以通过HTTP接口创建管理员账户不需要重启CouchDB。HTTP<code>/_node/{node-name}/_config/admins</code>地址支持查询，删除或者是创建新管理员账户。</p>\n</li>\n<li><p>*[admins]**<br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/admins/</code>后接参数.</p>\n<h4 id=\"认证配置\"><a href=\"#认证配置\" class=\"headerlink\" title=\"认证配置\"></a>认证配置</h4></li>\n<li><p>*[chttpd]**<br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/chttpd/</code>后接参数.</p>\n</li>\n<li><p>require_valid_user</p>\n<ul>\n<li>true:不允许来自匿名用户的任何请求</li>\n</ul>\n</li>\n</ul>\n<p><strong>[couch_httpd_auth]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/couch_httpd_auth/</code>后接参数.</p>\n<ul>\n<li>allow_persistent_cookies<ul>\n<li>使得Cookie持久性。</li>\n</ul>\n</li>\n<li>cookie_domain<ul>\n<li>配置<code>AuthSession</code>Cookie的域属性。默认为空。</li>\n<li>cookie_domain=example.com</li>\n</ul>\n</li>\n<li>auth_cache_size<ul>\n<li>内容中用户对象缓存数量，减少硬盘读写，默认50</li>\n</ul>\n</li>\n<li>authentication_redirect<ul>\n<li>权限成功验证后，客户端接受<code>text/html</code>响应情况下具体的重定向的位置。</li>\n<li>authentication_redirect = /_utils/session.html</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>iterations<ul>\n<li>由PBKDF2算法哈希的密码迭代的数量。</li>\n</ul>\n</li>\n<li>min_iterations<ul>\n<li>最小迭代的数量</li>\n</ul>\n</li>\n<li>max_iterations<ul>\n<li>最大迭代的数量</li>\n</ul>\n</li>\n<li>proxy_use_secret<ul>\n<li>true:<code>couch_httpd_auth/secret</code>选项要求代理身份认证</li>\n</ul>\n</li>\n<li>public_fields<ul>\n<li>用户文档中可以被任何用户读的由逗号分隔的字段名称。</li>\n<li>public_fields = first_name, last_name, contacts, url</li>\n</ul>\n</li>\n<li>require_valid_user<ul>\n<li>true:不允许来自匿名用户的任何请求</li>\n</ul>\n</li>\n<li>secret<ul>\n<li>用于代理身份认证与Cookie身份认证的secret</li>\n</ul>\n</li>\n<li>timeout<ul>\n<li>最后一次请求之后session超时过期时间默认600</li>\n</ul>\n</li>\n<li>users_db_public<ul>\n<li>允许用户查看所有用户文档，默认情况下，只有管理员可以查看所有用户的文档,用户只能查看自己的文档。</li>\n</ul>\n</li>\n<li>x_auth_roles<ul>\n<li>HTTP请求头包含用户的角色，用逗号分隔。用于代理身份认证</li>\n</ul>\n</li>\n<li>x_auth_token<ul>\n<li>HTTP请求头包含用户的token，用逗号分隔。用于代理身份认证</li>\n</ul>\n</li>\n<li>x_auth_username<ul>\n<li>HTTP请求头包含用户的用户名，用逗号分隔。用于代理身份认证</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"压缩配置\"><a href=\"#压缩配置\" class=\"headerlink\" title=\"压缩配置\"></a>压缩配置</h3><h4 id=\"数据库压缩配置\"><a href=\"#数据库压缩配置\" class=\"headerlink\" title=\"数据库压缩配置\"></a>数据库压缩配置</h4><p><strong>[database_compaction]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/database_compaction/</code>后接参数.</p>\n<ul>\n<li>doc_buffer_size<ul>\n<li>具体的拷贝缓冲区大小</li>\n</ul>\n</li>\n<li>checkpoint_after<ul>\n<li>在具体数量的比特后激活检查点拷贝到压缩数据库。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"压缩程序规则\"><a href=\"#压缩程序规则\" class=\"headerlink\" title=\"压缩程序规则\"></a>压缩程序规则</h4><p><strong>[compactions]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/compactions/</code>后接参数.<br>列表中的规则确定什么时候运行自动压缩。配置可以是指定数据库或者是全局的。格式如下:</p>\n<pre><code>database_name = [ {ParamName, ParamValue}, {ParamName, ParamValue}, ... ]\n_default = [ {ParamName, ParamValue}, {ParamName, ParamValue}, ... ]\n_default = [{db_fragmentation, &quot;70%&quot;}, {view_fragmentation, &quot;60%&quot;}, {from,&quot;23:00&quot;}, {to, &quot;04:00&quot;}]</code></pre><ol>\n<li>db_fragmentation:数据库中数据压缩率，包括元数据。计算方法：(file_size-data_size)/file_size*100</li>\n<li>view_fragmentation：数据库中索引文件…..</li>\n<li>form,to:允许进行压缩的时间段,格式：<code>HH:MM - HH:MM  (HH in [0..23], MM in [0..59])</code></li>\n<li>strict_window:如果为true，并且在超时时间后还没有压缩完则终止压缩。</li>\n<li>parallel_view_compaction：是否数据和视图同时进行压缩。</li>\n</ol>\n<h4 id=\"压缩程序配置\"><a href=\"#压缩程序配置\" class=\"headerlink\" title=\"压缩程序配置\"></a>压缩程序配置</h4><p><strong>[compaction_daemon]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/compaction_daemon/</code>后接参数.</p>\n<ul>\n<li>check_interval<ul>\n<li>两次检查数据库和视图索引是否需要被压缩的时间间隔，默认为3600</li>\n</ul>\n</li>\n<li>min_file_size<ul>\n<li>如果数据库或者视图索引文件大小小于该值，则不进行压缩。</li>\n</ul>\n</li>\n<li>snooze_period_ms</li>\n</ul>\n<h4 id=\"视图压缩选项\"><a href=\"#视图压缩选项\" class=\"headerlink\" title=\"视图压缩选项\"></a>视图压缩选项</h4><p><strong>[view_compaction]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/view_compaction/</code>后接参数.</p>\n<ul>\n<li>keyvalue_buffer_size<ul>\n<li>压缩时具体的最大拷贝缓冲区大小.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"日志\"><a href=\"#日志\" class=\"headerlink\" title=\"日志\"></a>日志</h3><h4 id=\"日志选项\"><a href=\"#日志选项\" class=\"headerlink\" title=\"日志选项\"></a>日志选项</h4><p><strong>[log]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/log/</code>后接参数.</p>\n<ul>\n<li>writer<ul>\n<li>stderr：日志信息发送到stderr(默认)</li>\n<li>file:日志信息存储到文件</li>\n<li>syslog：日志信息发送到syslog daemon</li>\n</ul>\n</li>\n<li>file<ul>\n<li>日志保存到文件的具体的位置(默认<code>/var/log/couchdb/couch.log</code>)</li>\n</ul>\n</li>\n<li>write_buffer<ul>\n<li>写日志缓冲区大小默认0</li>\n</ul>\n</li>\n<li>write_delay<ul>\n<li>写日志到硬盘延迟默认为0</li>\n</ul>\n</li>\n<li>level<ul>\n<li>日志级别</li>\n<li>debug</li>\n<li>info：包括HTTP请求，启动外部程序</li>\n<li>notice</li>\n<li>warning,warn：警告信息，例如硬盘空间不足</li>\n<li>error,err：只输出错误信息</li>\n<li>critical crit</li>\n<li>alert</li>\n<li>emergency emerg</li>\n<li>none:不输入任何日志</li>\n</ul>\n</li>\n<li>include_sasl<ul>\n<li>是否在日志中包含SASL信息</li>\n</ul>\n</li>\n<li>syslog_host<ul>\n<li>具体的syslog 主机将日志发送到的位置默认localhost</li>\n</ul>\n</li>\n<li>syslog_port<ul>\n<li>当发送日志信息时连接的syslog端口</li>\n</ul>\n</li>\n<li>syslog_appid<ul>\n<li>具体的应用名称默认couchdb</li>\n</ul>\n</li>\n<li>syslog_facility</li>\n</ul>\n<h3 id=\"复制者\"><a href=\"#复制者\" class=\"headerlink\" title=\"复制者\"></a>复制者</h3><h4 id=\"数据库复制配置\"><a href=\"#数据库复制配置\" class=\"headerlink\" title=\"数据库复制配置\"></a>数据库复制配置</h4><p><strong>[replicator]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/replicator/</code>后接参数.</p>\n<ul>\n<li>max_jobs<ul>\n<li>活跃的运行的复制任务数量。</li>\n</ul>\n</li>\n<li>interval</li>\n<li>max_churn</li>\n<li>update_docs</li>\n<li>worker_batch_size</li>\n<li>worker_processes</li>\n<li>http_connections</li>\n<li>connection_timeout</li>\n<li>retries_per_request</li>\n<li>socket_options</li>\n<li>checkpoint_interval</li>\n<li>use_checkpoints</li>\n<li>cert_file</li>\n<li>key_file</li>\n<li>password</li>\n<li>verify_ssl_certificates</li>\n<li>ssl_trusted_certificates_file</li>\n<li>ssl_certificate_max_depth</li>\n<li>auth_plugins</li>\n</ul>\n<h3 id=\"Query-Servers\"><a href=\"#Query-Servers\" class=\"headerlink\" title=\"Query Servers\"></a>Query Servers</h3><h4 id=\"Query-Servers-Definition\"><a href=\"#Query-Servers-Definition\" class=\"headerlink\" title=\"Query Servers Definition\"></a>Query Servers Definition</h4><p><strong>[query_servers]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/query_servers/</code>后接参数.</p>\n<h4 id=\"Query-Servers-Configuration\"><a href=\"#Query-Servers-Configuration\" class=\"headerlink\" title=\"Query Servers Configuration\"></a>Query Servers Configuration</h4><p><strong>[query_sercver_config]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/query_sercver_config/</code>后接参数.</p>\n<ul>\n<li>commit_freq</li>\n<li>os_process_limit</li>\n<li>os_process_soft_limit</li>\n<li>reduce_limit</li>\n</ul>\n<h4 id=\"Native-Erlang-Query-Server\"><a href=\"#Native-Erlang-Query-Server\" class=\"headerlink\" title=\"Native Erlang Query Server\"></a>Native Erlang Query Server</h4><p><strong>[native_query_servers]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/native_query_servers/</code>后接参数.</p>\n<h3 id=\"CouchDB-Internal-Services\"><a href=\"#CouchDB-Internal-Services\" class=\"headerlink\" title=\"CouchDB Internal Services\"></a>CouchDB Internal Services</h3><h4 id=\"CouchDB-Daemonized-Mini-Apps\"><a href=\"#CouchDB-Daemonized-Mini-Apps\" class=\"headerlink\" title=\"CouchDB Daemonized Mini Apps\"></a>CouchDB Daemonized Mini Apps</h4><p><strong>[daemons]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/daemons/</code>后接参数.</p>\n<ul>\n<li>auth_cache</li>\n<li>compaction_daemon</li>\n<li>external_manager</li>\n<li>httpd</li>\n<li>index_server</li>\n<li>query_servers</li>\n<li>replicator_manager</li>\n<li>stats_aggregator</li>\n<li>stats_collector</li>\n<li>uuids</li>\n<li>vhosts</li>\n</ul>\n<h3 id=\"Miscellaneous-Parameters\"><a href=\"#Miscellaneous-Parameters\" class=\"headerlink\" title=\"Miscellaneous Parameters\"></a>Miscellaneous Parameters</h3><h4 id=\"Configuration-of-Attachment-Storage\"><a href=\"#Configuration-of-Attachment-Storage\" class=\"headerlink\" title=\"Configuration of Attachment Storage\"></a>Configuration of Attachment Storage</h4><p><strong>[attachments]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/attachments/</code>后接参数.</p>\n<ul>\n<li>compression_level</li>\n<li>compressible_types</li>\n</ul>\n<h4 id=\"Statistic-Calculation\"><a href=\"#Statistic-Calculation\" class=\"headerlink\" title=\"Statistic Calculation\"></a>Statistic Calculation</h4><p><strong>[stats]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/stats/</code>后接参数.</p>\n<ul>\n<li>rate</li>\n<li>samples</li>\n</ul>\n<h4 id=\"UUIDs-Configuration\"><a href=\"#UUIDs-Configuration\" class=\"headerlink\" title=\"UUIDs Configuration\"></a>UUIDs Configuration</h4><p><strong>[uuids]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/uuids/</code>后接参数.</p>\n<ul>\n<li>algorithm</li>\n<li>utc_id_suffix</li>\n<li>max_count</li>\n</ul>\n<h4 id=\"Vendor-information\"><a href=\"#Vendor-information\" class=\"headerlink\" title=\"Vendor information\"></a>Vendor information</h4><p><strong>[vendor]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/vendor/</code>后接参数.</p>\n<h4 id=\"Content-Security-Policy\"><a href=\"#Content-Security-Policy\" class=\"headerlink\" title=\"Content-Security_Policy\"></a>Content-Security_Policy</h4><p><strong>[csp]</strong><br><code>curl</code>地址：<code>http://localhost:5984/_node/&lt;name@host&gt;/_config/csp/</code>后接参数.</p>\n<ul>\n<li>enable</li>\n<li>header_value</li>\n</ul>\n<h2 id=\"操作\"><a href=\"#操作\" class=\"headerlink\" title=\"操作\"></a>操作</h2><h3 id=\"节点操作\"><a href=\"#节点操作\" class=\"headerlink\" title=\"节点操作\"></a>节点操作</h3><h4 id=\"查看所有节点\"><a href=\"#查看所有节点\" class=\"headerlink\" title=\"查看所有节点\"></a>查看所有节点</h4><pre><code>curl -u admin:adminpw -X GET http://localhost:5984/_membership\n{\n    &quot;all_nodes&quot;:[   # 当前节点所知道的节点\n        &quot;node1@xxx.xxx.xxx.xxx&quot;],\n    &quot;cluster_nodes&quot;:[ #当前节点所连接的节点\n        &quot;node1@xxx.xxx.xxx.xxx&quot;],\n}</code></pre><h4 id=\"添加一个节点\"><a href=\"#添加一个节点\" class=\"headerlink\" title=\"添加一个节点\"></a>添加一个节点</h4><pre><code>curl -u admin:adminpw -X PUT http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy -d {}</code></pre><h4 id=\"删除一个节点\"><a href=\"#删除一个节点\" class=\"headerlink\" title=\"删除一个节点\"></a>删除一个节点</h4><pre><code>#首先获取关于文档的revision\ncurl -u admin:adminpw -X GET &quot;http://xxx.xxx.xxx.xxx:5986/_nodes/node2@yyy.yyy.yyy.yyy&quot;\n{&quot;_id&quot;:&quot;node2@yyy.yyy.yyy.yyy&quot;,&quot;_rev&quot;:&quot;1-967a00dff5e02add41820138abb3284d&quot;}    \n#删除节点\ncurl -u admin:adminpw -X DELETE http://localhost:5986/_nodes/node2@yyy.yyy.yyy.yyy?rev=1-967a00dff5e02add41820138abb3284d</code></pre><h3 id=\"数据库操作\"><a href=\"#数据库操作\" class=\"headerlink\" title=\"数据库操作\"></a>数据库操作</h3><h4 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h4><p>数据库名字不支持大写字符，只支持[a-z],[0-9],以及特殊字符:<code>_ $ ( ) + - /</code></p>\n<pre><code>#创建一个数据库名字为db_name \ncurl -u admin:adminpw -X PUT http://localhost:5984/db_name?q=4&amp;n=2</code></pre><h4 id=\"删除数据库\"><a href=\"#删除数据库\" class=\"headerlink\" title=\"删除数据库\"></a>删除数据库</h4><pre><code>curl -u admin:adminpw -X DELETE http://localhost:5984/db_name</code></pre>"},{"title":"配置参数","date":"2019-11-24T03:29:53.000Z","_content":"原文地址：[Configuration flags](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md)\netcd通过配置文件，多命令行参数和环境变量进行配置，\n\n可重用的配置文件是YAML文件，其名称和值由一个或多个下面描述的命令行标志组成。为了使用此文件，请将文件路径指定为`--config-file`标志或`ETCD_CONFIG_FILE`环境变量的值。如果需要的话[配置文件示例](https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample)可以作为入口点创建新的配置文件。\n\n在命令行上设置的选项优先于环境中的选项。 如果提供了配置文件，则其他命令行标志和环境变量将被忽略。例如，`etcd --config-file etcd.conf.yml.sample --data-dir /tmp`将会忽略`--data-dir`参数。\n\n参数`--my-flag`的环境变量的格式为`ETCD_MY_FLAG`.它适用于所有参数。\n\n客户端请求[官方的etcd端口](http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.txt)为2379,2380是节点通信端口。可以将etcd端口设置为接受TLS流量，非TLS流量，或同时接受TLS和非TLS流量。\n\n要在Linux启动时使用自定义设置自动启动etcd，强烈建议使用[systemd](freedesktop.org/wiki/Software/systemd/)单元。\n\n## 成员标记\n\n* * *\n\n\n**--name**\n\n* 人类可读的该成员的名字\n* 默认值：\"default\"\n* 环境变量：ETCD_NAME\n* 该值被该节点吃的`--initial-cluster`参数引用(例如 `default=http://localhost:2380`).如果使用[静态引导程序]()，则需要与标志中使用的键匹配。当使用发现服务时，每一个成员需要有唯一的名字。`Hostname`或者`machine-id`是好的选择。\n    \n**--data-dir**\n\n* 数据目录的路径\n* 默认值：\"${name}.etcd\"\n* 环境变量：ETCD_DATA_DIR\n\n**--wal-dir**\n\n* 专用的wal目录的路径。如果这个参数被设置，etcd将会写WAL文件到walDir而不是dataDir，允许使用专用磁盘，并有助于避免日志记录和其他IO操作之间的io竞争。\n* 默认值：\"\"\n* 环境变量：ETCD_WAL_DIR\n\n**--snapshot-count**\n\n* 触发一个快照到磁盘的已提交交易的数量\n* 默认值：\"100000\"\n* 环境变量：ETCD_SNAPSHOP_COUNT\n\n**--heartbeat-interval**\n\n* 心跳间隔(毫秒为单位)\n* 默认值:\"100\"\n* 环境变量：ETCD_HEARTBEAT_INTERVAL\n\n**--election-timeout**\n\n* 选举超时时间(毫秒为单位)，从[文档/tuning.md](https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md)发现更多细节\n* 默认值：\"1000\"\n* 环境变量：ETCD_ELECTION_TIMEOUT\n\n**--listen-peer-urls**\n\n* 监听在对等节点流量上的URL列表，该参数告诉etcd在指定的协议://IP:port组合上接受来自其对等方的传入请求。协议可以是http或者https。或者，使用`unix://<file-path>`或者`unixs://<file-path>`到unix sockets。如果将0.0.0.0作为IP，etcd将监听在所有的接口上的给定端口。如果给定了Ip和端口，etcd将监听指定的接口和端口。可以使用多个URL指定要监听的地址和端口的数量。 etcd将响应来自任何列出的地址和端口的请求。\n* 默认值：\"[http://localhost:2380](http://localhost:2380)\"\n* 环境变量:ETCD_LISTEN_PEER_URLS\n* 示例：\"[http://10.0.0.1:2380](http://10.0.0.1:2380)\"\n* 无效的示例：\"[http://example.com:2380](http://example.com:2380)\"(绑定的域名是无效的)\n\n**--listen-client-urls**\n\n* 监听在客户端流量上的URL列表，该参数告诉etcd在指定的协议://IP:port组合上接受来自客户端的传入请求。协议可以是http或者https。或者，使用`unix://<file-path>`或者`unixs://<file-path>`到unix sockets。如果将0.0.0.0作为IP，etcd将监听在所有的接口上的给定端口。如果给定了Ip和端口，etcd将监听指定的接口和端口。可以使用多个URL指定要监听的地址和端口的数量。 etcd将响应来自任何列出的地址和端口的请求。\n* 默认值：\"[http://localhost:2379](http://localhost:2379)\"\n* 环境变量:ETCD_LISTEN_CLIENT_URLS\n* 示例：\"[http://10.0.0.1:2379](http://10.0.0.1:2379)\"\n* 无效的示例：\"[http://example.com:2379](http://example.com:2379)\"(绑定的域名是无效的)\n\n**--max-snapshots**\n\n* 保留的快照文件最大数量（0为无限）\n* 默认值：5\n* 环境变量：ETCD_MAX_SNAPSHOTS\n* Windows用户的默认设置是无限制的，建议手动设置到5（或出于安全性的考虑）。\n\n**--max-wals**\n\n* 保留的wal文件最大数量（0为无限）\n* 默认值：5\n* 环境变量：ETCD_MAX_WALS\n* Windows用户的默认设置是无限制的，建议手动设置到5（或出于安全性的考虑）。\n\n**--cors**\n\n* 以逗号分隔的CORS来源白名单（跨来源资源共享）。\n* 默认值：\"\"\n* 环境变量：ETCD_CORS\n\n\n**--quota-backent-bytes**\n\n* 后端大小超过给定配额时引发警报（0默认为低空间配额）。\n* 默认值：0\n* 环境变量：ETCD_QUOTA_BACKEND_BYTES\n\n\n**--backend-batch-limit**\n\n* BackendBatchLimit是提交后端事务之前的最大数量的操作。\n* 默认值：0\n* 环境变量：ETCD_BACKEND_BATCH_LIMIT\n\n\n**--backend-bbolt-freelist-type**\n\n* etcd后端（bboltdb）使用的自由列表类型（支持数组和映射的类型）。\n* 默认值：map\n* 环境变量：ETCD_BACKEND_BBOLT_FREELIST_TYPE\n\n\n**--backend-batch-interval**\n\n* BackendBatchInterval是提交后端事务之前的最长时间。\n* 默认值：0\n* 环境变量：ETCD_BACKEND_BATCH_INTERVAL\n\n\n**--max-txn-ops**\n\n* 交易中允许的最大操作数。\n* 默认值：128\n* 环境变量：ETCD_MAX_TXN_OPS\n\n\n**--max-request-bytes**\n\n* 服务器将接受的最大客户端请求大小（以字节为单位）。\n* 默认值：1572864\n* 环境变量：ETCD_MAX_REQUEST_BYTES\n\n\n**--grpc-keepalive-min-time**\n\n* 客户端在ping服务器之前应等待的最小持续时间间隔。\n* 默认值：5s\n* 环境变量：ETCD_GRPC_KEEPALIVE_MIN_TIME\n\n\n**--grpc-keepalive-interval**\n\n* 服务器到客户端ping的频率持续时间，以检查连接是否有效（0禁用）。\n* 默认值：2h\n* 环境变量：ETCD_GRPC_KEEPALIVE_INTERVAL\n\n\n**--grpc-keepalive-timeout**\n\n* 关闭无响应的连接之前的额外等待时间（0禁用）。\n* 默认值：20s\n* 环境变量：ETCD_GRPC_KEEPALIVE_TIMEOUT\n\n## 集群参数\n\n* * *\n`--initial-advertise-peer-urls`,`--initial-cluster`,`--initial-cluster-state`,和`--initial-cluster-token`参数用于启动([静态启动](),[发现服务启动]()或者[运行时重新配置](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/))一个新成员，当重启已经存在的成员时将忽略。\n前缀为`--discovery`的参数在使用[发现服务](https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/)时需要被设置。\n\n**--initial-advertise-peer-urls**\n\n* 此成员的对等URL的列表，以通告到集群的其余部分。 这些地址用于在集群周围传送etcd数据。 所有集群成员必须至少有一个路由。 这些URL可以包含域名。\n* 默认值：\"[http://localhost:2380](http://localhost:2380)\"\n* 环境变量：ETCD_INITIAL_ADVERTISE_PEER_URLS\n* 示例：\"[http://example.com:2380](http://example.com:2380), [http://10.0.0.1:2380](http://10.0.0.1:2380)\"\n\n\n**--initial-cluster**\n\n* 启动集群的初始化配置\n* 默认值：\"default=[http://localhost:2380](http://localhost:2380)\"\n* 环境变量：ETCD_INITIAL_CLUSTER\n* 关键是所提供的每个节点的`--name`参数的值。 默认值使用`default`作为密钥，因为这是`--name`参数的默认值。\n\n\n**--initial-cluster-state**\n\n* 初始群集状态（“新”或“现有”）。 对于在初始静态或DNS引导过程中存在的所有成员，将其设置为`new`。 如果此选项设置为`existing`，则etcd将尝试加入现存集群。 如果设置了错误的值，etcd将尝试启动，但会安全地失败。\n* 默认值：\"new:\n* 环境变量：ETCD_INITIAL_CLUSTER_STATE\n\n\n**--initial-cluster-token**\n\n* 引导期间etcd群集的初始集群令牌。\n* 默认值：\"etcd-cluster\"\n* 环境变量：ETCD_INITIAL_CLUSTER_TOKEN\n\n\n**--advertise-client-urls**\n\n* 此成员的客户端URL的列表，这些URL广播给集群的其余部分。 这些URL可以包含域名。\n* 默认值：[http://localhost:2379](http://localhost:2379)\n* 环境变量：ETCD_ADVERTISE_CLIENT_URLS\n* 示例：\"[http://example.com:2379](http://example.com:2379), [http://10.0.0.1:2379](http://10.0.0.1:2379)\"\n* 如果从集群成员中发布诸如``http://localhost:2379``之类的URL并使用etcd的代理功能，请小心。这将导致循环，因为代理将向其自身转发请求，直到其资源（内存，文件描述符）最终耗尽为止。\n\n\n**--discovery**\n\n* 发现URL用于引导启动集群\n* 默认值：\"\"\n* 环境变量：ETCD_DISCOVERY\n\n\n**--discovery-srv**\n\n* 用于引导集群的DNS srv域。\n* 默认值：\"\"\n* 环境变量：ETCD_DISCOVERY_SRV\n\n\n**--discovery-srv-name**\n\n* 使用DNS引导时查询的DNS srv名称的后缀。\n* 默认值：\"\"\n* 环境变量：ETCD_DISCOVERY_SRV_NAME\n\n\n**--discovery-fallback**\n\n* 发现服务失败时的预期行为(“退出”或“代理”)。“代理”仅支持v2 API。\n* 默认值： \"proxy\"\n* 环境变量：ETCD_DISCOVERY_FALLBACK\n\n\n**--discovery-proxy**\n\n* HTTP代理，用于发现服务的流量。\n* 默认值：\"\"\n* 环境变量：ETCD_DISCOVERY_PROXY\n\n\n**--strict-reconfig-check**\n\n* 拒绝可能导致quorum丢失的重新配置请求。\n* 默认值：true\n* 环境变量：ETCD_STRICT_RECONFIG_CHECK\n\n\n**--auto-compaction-retention**\n\n* mvcc密钥值存储的自动压缩保留时间（小时）。 0表示禁用自动压缩。\n* 默认值：0\n* 环境变量：ETCD_AUTO_COMPACTION_RETENTION\n\n\n**--auto-compaction-mode**\n\n* 解释“自动压缩保留”之一：“定期”，“修订”。 基于期限的保留的“定期”，如果未提供时间单位（例如“ 5m”），则默认为小时。 “修订”用于基于修订号的保留。\n* 默认值：periodic\n* 环境变量：ETCD_AUTO_COMPACTION_MODE\n\n\n**--enable-v2**\n\n* 接受etcd V2客户端请求\n* 默认值：false\n* 环境变量：ETCD_ENABLE_V2\n\n## 代理参数\n\n* * *\n--proxy前缀标志将etcd配置为以代理模式运行。 “代理”仅支持v2 API。\n\n**--proxy**\n\n* 代理模式设置(”off\",\"readonly\"或者\"on\")\n* 默认值：\"off\"\n* 环境变量：ETCD_PROXY\n\n\n**--proxy-failure-wait**\n\n* 在重新考虑端点请求之前，端点将保持故障状态的时间（以毫秒为单位）。\n* 默认值：5000\n* 环境变量：ETCD_PROXY_FAILURE_WAIT\n\n\n**--proxy-refresh-interval**\n\n* 节点刷新间隔的时间（以毫秒为单位）。\n* 默认值：30000\n* 环境变量：ETCD_PROXY_REFRESH_INTERVAL\n\n\n**--proxy-dial-timeout**\n\n* 拨号超时的时间（以毫秒为单位），或0以禁用超时\n* 默认值：1000\n* 环境变量：ETCD_PROXY_DIAL_TIMEOUT\n\n\n**--proxy-write-timeout**\n\n* 写入超时的时间（以毫秒为单位）或禁用超时的时间为0。\n* 默认值：5000\n* 环境变量：ETCD_PROXY_WRITE_TIMEOUT\n\n\n**--proxy-read-timeout**\n\n* 读取超时的时间（以毫秒为单位），或者为0以禁用超时。\n* 如果使用Watch，请勿更改此值，因为会使用较长的轮询请求。\n* 默认值：0\n* 环境变量：ETCD_PROXY_READ_TIMEOUT\n\n## 安全参数\n\n* * *\n安全参数有助于[构建一个安全的etcd集群]()\n**--ca-file**\n**DEPRECATED**\n\n* 客户端服务器TLS CA文件的路径。 `--ca-file ca.crt`可以替换为`--trusted-ca-file ca.crt --client-cert-auth`，而etcd将执行相同的操作。\n* 默认值：\"\"\n* 环境变量：ETCD_CA_FILE\n\n\n**--cert-file**\n\n* 客户端服务器TLS证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_CERT_FILE\n\n\n**--key-file**\n\n* 客户端服务器TLS秘钥文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_KEY_FILE\n\n\n**--client-cert-auth**\n\n* 开启客户端证书认证\n* 默认值：false\n* 环境变量：ETCD_CLIENT_CERT_AUTH\n* CN 权限认证不支持gRPC-网关\n\n**--client-crl-file**\n\n* 客户端被撤销的TLS证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_CLIENT_CERT_ALLOWED_HOSTNAME\n\n\n**--client-cert-allowed-hostname**\n\n* 允许客户端证书身份验证的TLS名称。\n* 默认值：\"\"\n* 环境变量：ETCD_CLIENT_CERT_ALLOWED_HOSTNAME\n\n\n**--trusted-ca-file**\n\n* 客户端服务器受信任的TLS CA证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_TRUSTED_CA_FILE\n\n\n**--auto-tls**\n* 客户端TLS使用自动生成的证书\n* 默认值：false\n* 环境变量：ETCD_AUTO_TLS\n\n\n**--peer-ca-file**\n**已淘汰**\n\n* 节点TLS CA文件的路径.`--peer-ca-file`可以替换为`--peer-trusted-ca-file ca.crt --peer-client-cert-auth`，而etcd将执行相同的操作。\n* 默认值：”“\n* 环境变量：ETCD_PEER_CA_FILE\n\n\n**--peer-cert-file**\n\n* 对等服务器TLS证书文件的路径。 这是对等节点通信证书，在服务器和客户端都可以使用。\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_CERT_FILE\n\n\n**--peer-key-file**\n\n* 对等服务器TLS秘钥文件的路径。 这是对等节点通信秘钥，在服务器和客户端都可以使用。\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_KEY_FILE\n\n\n**--peer-client-cert-auth**\n\n* 启动节点客户端证书认证\n* 默认值：false\n* 环境变量：ETCD_PEER_CLIENT_CERT_AUTH\n\n\n**--peer-crl-file**\n\n* 节点被撤销的TLS证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_CRL_FILE\n\n\n**--peer-trusted-ca-file**\n\n* 节点受信任的TLS CA证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_TRUSTED_CA_FILE\n\n\n**--peer-auto-tls**\n\n* 节点使用自动生成的证书\n* 默认值：false\n* 环境变量：ETCD_PEER_AUTO_TLS\n\n\n**--peer-cert-allowed-cn**\n\n* 允许使用CommonName进行对等身份验证。\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_CERT_ALLOWED_CN\n\n\n**--peer-cert-allowed-hostname**\n\n* 允许的TLS证书名称用于对等身份验证。\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_CERT_ALLOWED_HOSTNAME\n\n\n**--cipher-suites**\n\n* 以逗号分隔的服务器/客户端和对等方之间受支持的TLS密码套件列表。\n* 默认值：\"\"\n* 环境变量：ETCD_CIPHER_SUITES\n\n## 日志参数\n\n* * *\n\n**--logger**\n\n**v3.4可以使用，警告：`--logger=capnslog`在v3.5被抛弃使用**\n\n* 指定“ zap”用于结构化日志记录或“ capnslog”。 \n* 默认值：capnslog\n* 环境变量：ETCD_LOGGER\n\n\n**--log-outputs**\n\n* 指定“ stdout”或“ stderr”以跳过日志记录，即使在systemd或逗号分隔的输出目标列表下运行时也是如此。\n* 默认值：defalut\n* 环境变量：ETCD_LOG_OUTPUTS\n* `default`在zap logger迁移期间对v3.4使用`stderr`配置\n\n\n**--log-level**\n**v3.4可以使用**\n\n* 配置日志等级，仅支持`debug,info,warn,error,panic,fatal`\n* 默认值：info\n* 环境变量：ETCD_LOG_LEVEL\n* `default`使用`info`.\n\n\n**--debug**\n**警告：在v3.5被抛弃使用**\n\n* 将所有子程序包的默认日志级别降为DEBUG。\n* 默认值：false(所有的包使用INFO)\n* 环境变量：ETCD_DEBUG\n\n\n**--log-package-levels**\n**警告：在v3.5被抛弃使用**\n\n* 将各个etcd子软件包设置为特定的日志级别。 一个例子是`etcdserver = WARNING，security = DEBUG`\n* 默认值：\"\"(所有的包使用INFO)\n* 环境变量：ETCD_LOG_PACKAGE_LEVELS\n\n## 风险参数\n\n* * *\n使用不安全标志时请小心，因为它将破坏共识协议提供的保证。 例如，如果群集中的其他成员仍然存在，可能会`panic`。 使用这些标志时，请遵循说明。\n**--force-new-cluster**\n\n* 强制创建一个新的单成员群集。 它提交配置更改，以强制删除群集中的所有现有成员并添加自身，但是强烈建议不要这样做。 请查看[灾难恢复文档]()以了解首选的v3恢复过程。\n* 默认值：false\n* 环境变量：ETCD_FORCE_NEW_CLUSTER\n\n## 杂项参数\n\n* * *\n\n**--version**\n\n* 打印版本并退出\n* 默认值：false\n\n\n**--config-file**\n\n* 从文件加载服务器配置。 请注意，如果提供了配置文件，则其他命令行标志和环境变量将被忽略。\n* 默认值：\"\"\n* 示例：[配置文件示例](https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample)\n* 环境变量：ETCD_CONFIG_FILE\n\n## 分析参数\n\n* * *\n\n**--enable-pprof**\n\n* 通过HTTP服务器启用运行时分析数据。地址位于客户端`URL+“/debug/pprof/”`\n* 默认值：false\n* 环境变量：ETCD_ENABLE_PPROF\n\n\n**--metrics**\n\n* 设置导出指标的详细程度，specify 'extensive' to include server side grpc histogram metrics.\n* 默认值：basic\n* 环境变量：ETCD_METRICS\n\n\n**--listen-metrics-urls**\n\n* 可以响应`/metrics`和`/health`端点的其他URL列表\n* 默认值：\"\"\n* 环境变量：ETCD_LISTEN_METRICS_URLS\n\n## 权限参数\n\n* * *\n\n**--auth-token**\n\n* 指定令牌类型和特定于令牌的选项，特别是对于JWT,格式为`type,var1=val1,var2=val2,...`,可能的类型是`simple`或者`jwt`.对于具体的签名方法jwt可能的变量为`sign-method`（可能的值为`'ES256', 'ES384', 'ES512', 'HS256', 'HS384', 'HS512', 'RS256', 'RS384', 'RS512', 'PS256', 'PS384','PS512'`）\n* 对于非对称算法（“ RS”，“ PS”，“ ES”），公钥是可选的，因为私钥包含足够的信息来签名和验证令牌。`pub-key`用于指定用于验证jwt的公钥的路径,`priv-key`用于指定用于对jwt进行签名的私钥的路径，`ttl`用于指定jwt令牌的TTL。\n* JWT的示例选项：`-auth-token jwt，pub-key=app.rsa.pub，privkey=app.rsasign-method = RS512，ttl = 10m`\n* 默认值：\"simple\"\n* 环境变量：ETCD_AUTH_TOKEN\n\n\n**--bcrypt-cost**\n\n* 指定用于哈希认证密码的bcrypt算法的成本/强度。 有效值在4到31之间。\n* 默认值：10\n* 环境变量：(不支持)\n\n## 实验参数\n\n* * *\n\n**--experimental-corrupt-check-time**\n\n* 群集损坏检查通过之间的时间间隔\n* 默认值：0s\n* 环境变量：ETCD_EXPERIMENTAL_CORRUPT_CHECK_TIME\n\n\n**--experimental-compaction-batch-limit**\n\n* 设置每个压缩批处理中删除的最大修订。\n* 默认值：1000\n* 环境变量：ETCD_EXPERIMENTAL_COMPACTION_BATCH_LIMIT\n\n\n**--experimental-peer-skip-client-san-verification**\n\n* 跳过客户端证书中对等连接的SAN字段验证。 这可能是有帮助的，例如 如果群集成员在NAT后面的不同网络中运行。在这种情况下，请确保使用基于私有证书颁发机构的对等证书.`--peer-cert-file, --peer-key-file, --peer-trusted-ca-file`\n* 默认值：false\n* 环境变量：ETCD_EXPERIMENTAL_PEER_SKIP_CLIENT_SAN_VERIFICATION","source":"_posts/blog/etcd/ETCD配置参数.md","raw":"---\ntitle: 配置参数\ndate: 2019-11-24 11:29:53\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址：[Configuration flags](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md)\netcd通过配置文件，多命令行参数和环境变量进行配置，\n\n可重用的配置文件是YAML文件，其名称和值由一个或多个下面描述的命令行标志组成。为了使用此文件，请将文件路径指定为`--config-file`标志或`ETCD_CONFIG_FILE`环境变量的值。如果需要的话[配置文件示例](https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample)可以作为入口点创建新的配置文件。\n\n在命令行上设置的选项优先于环境中的选项。 如果提供了配置文件，则其他命令行标志和环境变量将被忽略。例如，`etcd --config-file etcd.conf.yml.sample --data-dir /tmp`将会忽略`--data-dir`参数。\n\n参数`--my-flag`的环境变量的格式为`ETCD_MY_FLAG`.它适用于所有参数。\n\n客户端请求[官方的etcd端口](http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.txt)为2379,2380是节点通信端口。可以将etcd端口设置为接受TLS流量，非TLS流量，或同时接受TLS和非TLS流量。\n\n要在Linux启动时使用自定义设置自动启动etcd，强烈建议使用[systemd](freedesktop.org/wiki/Software/systemd/)单元。\n\n## 成员标记\n\n* * *\n\n\n**--name**\n\n* 人类可读的该成员的名字\n* 默认值：\"default\"\n* 环境变量：ETCD_NAME\n* 该值被该节点吃的`--initial-cluster`参数引用(例如 `default=http://localhost:2380`).如果使用[静态引导程序]()，则需要与标志中使用的键匹配。当使用发现服务时，每一个成员需要有唯一的名字。`Hostname`或者`machine-id`是好的选择。\n    \n**--data-dir**\n\n* 数据目录的路径\n* 默认值：\"${name}.etcd\"\n* 环境变量：ETCD_DATA_DIR\n\n**--wal-dir**\n\n* 专用的wal目录的路径。如果这个参数被设置，etcd将会写WAL文件到walDir而不是dataDir，允许使用专用磁盘，并有助于避免日志记录和其他IO操作之间的io竞争。\n* 默认值：\"\"\n* 环境变量：ETCD_WAL_DIR\n\n**--snapshot-count**\n\n* 触发一个快照到磁盘的已提交交易的数量\n* 默认值：\"100000\"\n* 环境变量：ETCD_SNAPSHOP_COUNT\n\n**--heartbeat-interval**\n\n* 心跳间隔(毫秒为单位)\n* 默认值:\"100\"\n* 环境变量：ETCD_HEARTBEAT_INTERVAL\n\n**--election-timeout**\n\n* 选举超时时间(毫秒为单位)，从[文档/tuning.md](https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md)发现更多细节\n* 默认值：\"1000\"\n* 环境变量：ETCD_ELECTION_TIMEOUT\n\n**--listen-peer-urls**\n\n* 监听在对等节点流量上的URL列表，该参数告诉etcd在指定的协议://IP:port组合上接受来自其对等方的传入请求。协议可以是http或者https。或者，使用`unix://<file-path>`或者`unixs://<file-path>`到unix sockets。如果将0.0.0.0作为IP，etcd将监听在所有的接口上的给定端口。如果给定了Ip和端口，etcd将监听指定的接口和端口。可以使用多个URL指定要监听的地址和端口的数量。 etcd将响应来自任何列出的地址和端口的请求。\n* 默认值：\"[http://localhost:2380](http://localhost:2380)\"\n* 环境变量:ETCD_LISTEN_PEER_URLS\n* 示例：\"[http://10.0.0.1:2380](http://10.0.0.1:2380)\"\n* 无效的示例：\"[http://example.com:2380](http://example.com:2380)\"(绑定的域名是无效的)\n\n**--listen-client-urls**\n\n* 监听在客户端流量上的URL列表，该参数告诉etcd在指定的协议://IP:port组合上接受来自客户端的传入请求。协议可以是http或者https。或者，使用`unix://<file-path>`或者`unixs://<file-path>`到unix sockets。如果将0.0.0.0作为IP，etcd将监听在所有的接口上的给定端口。如果给定了Ip和端口，etcd将监听指定的接口和端口。可以使用多个URL指定要监听的地址和端口的数量。 etcd将响应来自任何列出的地址和端口的请求。\n* 默认值：\"[http://localhost:2379](http://localhost:2379)\"\n* 环境变量:ETCD_LISTEN_CLIENT_URLS\n* 示例：\"[http://10.0.0.1:2379](http://10.0.0.1:2379)\"\n* 无效的示例：\"[http://example.com:2379](http://example.com:2379)\"(绑定的域名是无效的)\n\n**--max-snapshots**\n\n* 保留的快照文件最大数量（0为无限）\n* 默认值：5\n* 环境变量：ETCD_MAX_SNAPSHOTS\n* Windows用户的默认设置是无限制的，建议手动设置到5（或出于安全性的考虑）。\n\n**--max-wals**\n\n* 保留的wal文件最大数量（0为无限）\n* 默认值：5\n* 环境变量：ETCD_MAX_WALS\n* Windows用户的默认设置是无限制的，建议手动设置到5（或出于安全性的考虑）。\n\n**--cors**\n\n* 以逗号分隔的CORS来源白名单（跨来源资源共享）。\n* 默认值：\"\"\n* 环境变量：ETCD_CORS\n\n\n**--quota-backent-bytes**\n\n* 后端大小超过给定配额时引发警报（0默认为低空间配额）。\n* 默认值：0\n* 环境变量：ETCD_QUOTA_BACKEND_BYTES\n\n\n**--backend-batch-limit**\n\n* BackendBatchLimit是提交后端事务之前的最大数量的操作。\n* 默认值：0\n* 环境变量：ETCD_BACKEND_BATCH_LIMIT\n\n\n**--backend-bbolt-freelist-type**\n\n* etcd后端（bboltdb）使用的自由列表类型（支持数组和映射的类型）。\n* 默认值：map\n* 环境变量：ETCD_BACKEND_BBOLT_FREELIST_TYPE\n\n\n**--backend-batch-interval**\n\n* BackendBatchInterval是提交后端事务之前的最长时间。\n* 默认值：0\n* 环境变量：ETCD_BACKEND_BATCH_INTERVAL\n\n\n**--max-txn-ops**\n\n* 交易中允许的最大操作数。\n* 默认值：128\n* 环境变量：ETCD_MAX_TXN_OPS\n\n\n**--max-request-bytes**\n\n* 服务器将接受的最大客户端请求大小（以字节为单位）。\n* 默认值：1572864\n* 环境变量：ETCD_MAX_REQUEST_BYTES\n\n\n**--grpc-keepalive-min-time**\n\n* 客户端在ping服务器之前应等待的最小持续时间间隔。\n* 默认值：5s\n* 环境变量：ETCD_GRPC_KEEPALIVE_MIN_TIME\n\n\n**--grpc-keepalive-interval**\n\n* 服务器到客户端ping的频率持续时间，以检查连接是否有效（0禁用）。\n* 默认值：2h\n* 环境变量：ETCD_GRPC_KEEPALIVE_INTERVAL\n\n\n**--grpc-keepalive-timeout**\n\n* 关闭无响应的连接之前的额外等待时间（0禁用）。\n* 默认值：20s\n* 环境变量：ETCD_GRPC_KEEPALIVE_TIMEOUT\n\n## 集群参数\n\n* * *\n`--initial-advertise-peer-urls`,`--initial-cluster`,`--initial-cluster-state`,和`--initial-cluster-token`参数用于启动([静态启动](),[发现服务启动]()或者[运行时重新配置](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/))一个新成员，当重启已经存在的成员时将忽略。\n前缀为`--discovery`的参数在使用[发现服务](https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/)时需要被设置。\n\n**--initial-advertise-peer-urls**\n\n* 此成员的对等URL的列表，以通告到集群的其余部分。 这些地址用于在集群周围传送etcd数据。 所有集群成员必须至少有一个路由。 这些URL可以包含域名。\n* 默认值：\"[http://localhost:2380](http://localhost:2380)\"\n* 环境变量：ETCD_INITIAL_ADVERTISE_PEER_URLS\n* 示例：\"[http://example.com:2380](http://example.com:2380), [http://10.0.0.1:2380](http://10.0.0.1:2380)\"\n\n\n**--initial-cluster**\n\n* 启动集群的初始化配置\n* 默认值：\"default=[http://localhost:2380](http://localhost:2380)\"\n* 环境变量：ETCD_INITIAL_CLUSTER\n* 关键是所提供的每个节点的`--name`参数的值。 默认值使用`default`作为密钥，因为这是`--name`参数的默认值。\n\n\n**--initial-cluster-state**\n\n* 初始群集状态（“新”或“现有”）。 对于在初始静态或DNS引导过程中存在的所有成员，将其设置为`new`。 如果此选项设置为`existing`，则etcd将尝试加入现存集群。 如果设置了错误的值，etcd将尝试启动，但会安全地失败。\n* 默认值：\"new:\n* 环境变量：ETCD_INITIAL_CLUSTER_STATE\n\n\n**--initial-cluster-token**\n\n* 引导期间etcd群集的初始集群令牌。\n* 默认值：\"etcd-cluster\"\n* 环境变量：ETCD_INITIAL_CLUSTER_TOKEN\n\n\n**--advertise-client-urls**\n\n* 此成员的客户端URL的列表，这些URL广播给集群的其余部分。 这些URL可以包含域名。\n* 默认值：[http://localhost:2379](http://localhost:2379)\n* 环境变量：ETCD_ADVERTISE_CLIENT_URLS\n* 示例：\"[http://example.com:2379](http://example.com:2379), [http://10.0.0.1:2379](http://10.0.0.1:2379)\"\n* 如果从集群成员中发布诸如``http://localhost:2379``之类的URL并使用etcd的代理功能，请小心。这将导致循环，因为代理将向其自身转发请求，直到其资源（内存，文件描述符）最终耗尽为止。\n\n\n**--discovery**\n\n* 发现URL用于引导启动集群\n* 默认值：\"\"\n* 环境变量：ETCD_DISCOVERY\n\n\n**--discovery-srv**\n\n* 用于引导集群的DNS srv域。\n* 默认值：\"\"\n* 环境变量：ETCD_DISCOVERY_SRV\n\n\n**--discovery-srv-name**\n\n* 使用DNS引导时查询的DNS srv名称的后缀。\n* 默认值：\"\"\n* 环境变量：ETCD_DISCOVERY_SRV_NAME\n\n\n**--discovery-fallback**\n\n* 发现服务失败时的预期行为(“退出”或“代理”)。“代理”仅支持v2 API。\n* 默认值： \"proxy\"\n* 环境变量：ETCD_DISCOVERY_FALLBACK\n\n\n**--discovery-proxy**\n\n* HTTP代理，用于发现服务的流量。\n* 默认值：\"\"\n* 环境变量：ETCD_DISCOVERY_PROXY\n\n\n**--strict-reconfig-check**\n\n* 拒绝可能导致quorum丢失的重新配置请求。\n* 默认值：true\n* 环境变量：ETCD_STRICT_RECONFIG_CHECK\n\n\n**--auto-compaction-retention**\n\n* mvcc密钥值存储的自动压缩保留时间（小时）。 0表示禁用自动压缩。\n* 默认值：0\n* 环境变量：ETCD_AUTO_COMPACTION_RETENTION\n\n\n**--auto-compaction-mode**\n\n* 解释“自动压缩保留”之一：“定期”，“修订”。 基于期限的保留的“定期”，如果未提供时间单位（例如“ 5m”），则默认为小时。 “修订”用于基于修订号的保留。\n* 默认值：periodic\n* 环境变量：ETCD_AUTO_COMPACTION_MODE\n\n\n**--enable-v2**\n\n* 接受etcd V2客户端请求\n* 默认值：false\n* 环境变量：ETCD_ENABLE_V2\n\n## 代理参数\n\n* * *\n--proxy前缀标志将etcd配置为以代理模式运行。 “代理”仅支持v2 API。\n\n**--proxy**\n\n* 代理模式设置(”off\",\"readonly\"或者\"on\")\n* 默认值：\"off\"\n* 环境变量：ETCD_PROXY\n\n\n**--proxy-failure-wait**\n\n* 在重新考虑端点请求之前，端点将保持故障状态的时间（以毫秒为单位）。\n* 默认值：5000\n* 环境变量：ETCD_PROXY_FAILURE_WAIT\n\n\n**--proxy-refresh-interval**\n\n* 节点刷新间隔的时间（以毫秒为单位）。\n* 默认值：30000\n* 环境变量：ETCD_PROXY_REFRESH_INTERVAL\n\n\n**--proxy-dial-timeout**\n\n* 拨号超时的时间（以毫秒为单位），或0以禁用超时\n* 默认值：1000\n* 环境变量：ETCD_PROXY_DIAL_TIMEOUT\n\n\n**--proxy-write-timeout**\n\n* 写入超时的时间（以毫秒为单位）或禁用超时的时间为0。\n* 默认值：5000\n* 环境变量：ETCD_PROXY_WRITE_TIMEOUT\n\n\n**--proxy-read-timeout**\n\n* 读取超时的时间（以毫秒为单位），或者为0以禁用超时。\n* 如果使用Watch，请勿更改此值，因为会使用较长的轮询请求。\n* 默认值：0\n* 环境变量：ETCD_PROXY_READ_TIMEOUT\n\n## 安全参数\n\n* * *\n安全参数有助于[构建一个安全的etcd集群]()\n**--ca-file**\n**DEPRECATED**\n\n* 客户端服务器TLS CA文件的路径。 `--ca-file ca.crt`可以替换为`--trusted-ca-file ca.crt --client-cert-auth`，而etcd将执行相同的操作。\n* 默认值：\"\"\n* 环境变量：ETCD_CA_FILE\n\n\n**--cert-file**\n\n* 客户端服务器TLS证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_CERT_FILE\n\n\n**--key-file**\n\n* 客户端服务器TLS秘钥文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_KEY_FILE\n\n\n**--client-cert-auth**\n\n* 开启客户端证书认证\n* 默认值：false\n* 环境变量：ETCD_CLIENT_CERT_AUTH\n* CN 权限认证不支持gRPC-网关\n\n**--client-crl-file**\n\n* 客户端被撤销的TLS证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_CLIENT_CERT_ALLOWED_HOSTNAME\n\n\n**--client-cert-allowed-hostname**\n\n* 允许客户端证书身份验证的TLS名称。\n* 默认值：\"\"\n* 环境变量：ETCD_CLIENT_CERT_ALLOWED_HOSTNAME\n\n\n**--trusted-ca-file**\n\n* 客户端服务器受信任的TLS CA证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_TRUSTED_CA_FILE\n\n\n**--auto-tls**\n* 客户端TLS使用自动生成的证书\n* 默认值：false\n* 环境变量：ETCD_AUTO_TLS\n\n\n**--peer-ca-file**\n**已淘汰**\n\n* 节点TLS CA文件的路径.`--peer-ca-file`可以替换为`--peer-trusted-ca-file ca.crt --peer-client-cert-auth`，而etcd将执行相同的操作。\n* 默认值：”“\n* 环境变量：ETCD_PEER_CA_FILE\n\n\n**--peer-cert-file**\n\n* 对等服务器TLS证书文件的路径。 这是对等节点通信证书，在服务器和客户端都可以使用。\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_CERT_FILE\n\n\n**--peer-key-file**\n\n* 对等服务器TLS秘钥文件的路径。 这是对等节点通信秘钥，在服务器和客户端都可以使用。\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_KEY_FILE\n\n\n**--peer-client-cert-auth**\n\n* 启动节点客户端证书认证\n* 默认值：false\n* 环境变量：ETCD_PEER_CLIENT_CERT_AUTH\n\n\n**--peer-crl-file**\n\n* 节点被撤销的TLS证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_CRL_FILE\n\n\n**--peer-trusted-ca-file**\n\n* 节点受信任的TLS CA证书文件的路径\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_TRUSTED_CA_FILE\n\n\n**--peer-auto-tls**\n\n* 节点使用自动生成的证书\n* 默认值：false\n* 环境变量：ETCD_PEER_AUTO_TLS\n\n\n**--peer-cert-allowed-cn**\n\n* 允许使用CommonName进行对等身份验证。\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_CERT_ALLOWED_CN\n\n\n**--peer-cert-allowed-hostname**\n\n* 允许的TLS证书名称用于对等身份验证。\n* 默认值：\"\"\n* 环境变量：ETCD_PEER_CERT_ALLOWED_HOSTNAME\n\n\n**--cipher-suites**\n\n* 以逗号分隔的服务器/客户端和对等方之间受支持的TLS密码套件列表。\n* 默认值：\"\"\n* 环境变量：ETCD_CIPHER_SUITES\n\n## 日志参数\n\n* * *\n\n**--logger**\n\n**v3.4可以使用，警告：`--logger=capnslog`在v3.5被抛弃使用**\n\n* 指定“ zap”用于结构化日志记录或“ capnslog”。 \n* 默认值：capnslog\n* 环境变量：ETCD_LOGGER\n\n\n**--log-outputs**\n\n* 指定“ stdout”或“ stderr”以跳过日志记录，即使在systemd或逗号分隔的输出目标列表下运行时也是如此。\n* 默认值：defalut\n* 环境变量：ETCD_LOG_OUTPUTS\n* `default`在zap logger迁移期间对v3.4使用`stderr`配置\n\n\n**--log-level**\n**v3.4可以使用**\n\n* 配置日志等级，仅支持`debug,info,warn,error,panic,fatal`\n* 默认值：info\n* 环境变量：ETCD_LOG_LEVEL\n* `default`使用`info`.\n\n\n**--debug**\n**警告：在v3.5被抛弃使用**\n\n* 将所有子程序包的默认日志级别降为DEBUG。\n* 默认值：false(所有的包使用INFO)\n* 环境变量：ETCD_DEBUG\n\n\n**--log-package-levels**\n**警告：在v3.5被抛弃使用**\n\n* 将各个etcd子软件包设置为特定的日志级别。 一个例子是`etcdserver = WARNING，security = DEBUG`\n* 默认值：\"\"(所有的包使用INFO)\n* 环境变量：ETCD_LOG_PACKAGE_LEVELS\n\n## 风险参数\n\n* * *\n使用不安全标志时请小心，因为它将破坏共识协议提供的保证。 例如，如果群集中的其他成员仍然存在，可能会`panic`。 使用这些标志时，请遵循说明。\n**--force-new-cluster**\n\n* 强制创建一个新的单成员群集。 它提交配置更改，以强制删除群集中的所有现有成员并添加自身，但是强烈建议不要这样做。 请查看[灾难恢复文档]()以了解首选的v3恢复过程。\n* 默认值：false\n* 环境变量：ETCD_FORCE_NEW_CLUSTER\n\n## 杂项参数\n\n* * *\n\n**--version**\n\n* 打印版本并退出\n* 默认值：false\n\n\n**--config-file**\n\n* 从文件加载服务器配置。 请注意，如果提供了配置文件，则其他命令行标志和环境变量将被忽略。\n* 默认值：\"\"\n* 示例：[配置文件示例](https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample)\n* 环境变量：ETCD_CONFIG_FILE\n\n## 分析参数\n\n* * *\n\n**--enable-pprof**\n\n* 通过HTTP服务器启用运行时分析数据。地址位于客户端`URL+“/debug/pprof/”`\n* 默认值：false\n* 环境变量：ETCD_ENABLE_PPROF\n\n\n**--metrics**\n\n* 设置导出指标的详细程度，specify 'extensive' to include server side grpc histogram metrics.\n* 默认值：basic\n* 环境变量：ETCD_METRICS\n\n\n**--listen-metrics-urls**\n\n* 可以响应`/metrics`和`/health`端点的其他URL列表\n* 默认值：\"\"\n* 环境变量：ETCD_LISTEN_METRICS_URLS\n\n## 权限参数\n\n* * *\n\n**--auth-token**\n\n* 指定令牌类型和特定于令牌的选项，特别是对于JWT,格式为`type,var1=val1,var2=val2,...`,可能的类型是`simple`或者`jwt`.对于具体的签名方法jwt可能的变量为`sign-method`（可能的值为`'ES256', 'ES384', 'ES512', 'HS256', 'HS384', 'HS512', 'RS256', 'RS384', 'RS512', 'PS256', 'PS384','PS512'`）\n* 对于非对称算法（“ RS”，“ PS”，“ ES”），公钥是可选的，因为私钥包含足够的信息来签名和验证令牌。`pub-key`用于指定用于验证jwt的公钥的路径,`priv-key`用于指定用于对jwt进行签名的私钥的路径，`ttl`用于指定jwt令牌的TTL。\n* JWT的示例选项：`-auth-token jwt，pub-key=app.rsa.pub，privkey=app.rsasign-method = RS512，ttl = 10m`\n* 默认值：\"simple\"\n* 环境变量：ETCD_AUTH_TOKEN\n\n\n**--bcrypt-cost**\n\n* 指定用于哈希认证密码的bcrypt算法的成本/强度。 有效值在4到31之间。\n* 默认值：10\n* 环境变量：(不支持)\n\n## 实验参数\n\n* * *\n\n**--experimental-corrupt-check-time**\n\n* 群集损坏检查通过之间的时间间隔\n* 默认值：0s\n* 环境变量：ETCD_EXPERIMENTAL_CORRUPT_CHECK_TIME\n\n\n**--experimental-compaction-batch-limit**\n\n* 设置每个压缩批处理中删除的最大修订。\n* 默认值：1000\n* 环境变量：ETCD_EXPERIMENTAL_COMPACTION_BATCH_LIMIT\n\n\n**--experimental-peer-skip-client-san-verification**\n\n* 跳过客户端证书中对等连接的SAN字段验证。 这可能是有帮助的，例如 如果群集成员在NAT后面的不同网络中运行。在这种情况下，请确保使用基于私有证书颁发机构的对等证书.`--peer-cert-file, --peer-key-file, --peer-trusted-ca-file`\n* 默认值：false\n* 环境变量：ETCD_EXPERIMENTAL_PEER_SKIP_CLIENT_SAN_VERIFICATION","slug":"blog/etcd/ETCD配置参数","published":1,"updated":"2020-05-11T03:54:47.958Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyh1001bk0vq0b9x6ag2","content":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md\" target=\"_blank\" rel=\"noopener\">Configuration flags</a><br>etcd通过配置文件，多命令行参数和环境变量进行配置，</p>\n<p>可重用的配置文件是YAML文件，其名称和值由一个或多个下面描述的命令行标志组成。为了使用此文件，请将文件路径指定为<code>--config-file</code>标志或<code>ETCD_CONFIG_FILE</code>环境变量的值。如果需要的话<a href=\"https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample\" target=\"_blank\" rel=\"noopener\">配置文件示例</a>可以作为入口点创建新的配置文件。</p>\n<p>在命令行上设置的选项优先于环境中的选项。 如果提供了配置文件，则其他命令行标志和环境变量将被忽略。例如，<code>etcd --config-file etcd.conf.yml.sample --data-dir /tmp</code>将会忽略<code>--data-dir</code>参数。</p>\n<p>参数<code>--my-flag</code>的环境变量的格式为<code>ETCD_MY_FLAG</code>.它适用于所有参数。</p>\n<p>客户端请求<a href=\"http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.txt\" target=\"_blank\" rel=\"noopener\">官方的etcd端口</a>为2379,2380是节点通信端口。可以将etcd端口设置为接受TLS流量，非TLS流量，或同时接受TLS和非TLS流量。</p>\n<p>要在Linux启动时使用自定义设置自动启动etcd，强烈建议使用<a href=\"freedesktop.org/wiki/Software/systemd/\">systemd</a>单元。</p>\n<h2 id=\"成员标记\"><a href=\"#成员标记\" class=\"headerlink\" title=\"成员标记\"></a>成员标记</h2><hr>\n<p><strong>–name</strong></p>\n<ul>\n<li>人类可读的该成员的名字</li>\n<li>默认值：”default”</li>\n<li>环境变量：ETCD_NAME</li>\n<li>该值被该节点吃的<code>--initial-cluster</code>参数引用(例如 <code>default=http://localhost:2380</code>).如果使用<a href=\"\">静态引导程序</a>，则需要与标志中使用的键匹配。当使用发现服务时，每一个成员需要有唯一的名字。<code>Hostname</code>或者<code>machine-id</code>是好的选择。</li>\n</ul>\n<p><strong>–data-dir</strong></p>\n<ul>\n<li>数据目录的路径</li>\n<li>默认值：”${name}.etcd”</li>\n<li>环境变量：ETCD_DATA_DIR</li>\n</ul>\n<p><strong>–wal-dir</strong></p>\n<ul>\n<li>专用的wal目录的路径。如果这个参数被设置，etcd将会写WAL文件到walDir而不是dataDir，允许使用专用磁盘，并有助于避免日志记录和其他IO操作之间的io竞争。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_WAL_DIR</li>\n</ul>\n<p><strong>–snapshot-count</strong></p>\n<ul>\n<li>触发一个快照到磁盘的已提交交易的数量</li>\n<li>默认值：”100000”</li>\n<li>环境变量：ETCD_SNAPSHOP_COUNT</li>\n</ul>\n<p><strong>–heartbeat-interval</strong></p>\n<ul>\n<li>心跳间隔(毫秒为单位)</li>\n<li>默认值:”100”</li>\n<li>环境变量：ETCD_HEARTBEAT_INTERVAL</li>\n</ul>\n<p><strong>–election-timeout</strong></p>\n<ul>\n<li>选举超时时间(毫秒为单位)，从<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md\" target=\"_blank\" rel=\"noopener\">文档/tuning.md</a>发现更多细节</li>\n<li>默认值：”1000”</li>\n<li>环境变量：ETCD_ELECTION_TIMEOUT</li>\n</ul>\n<p><strong>–listen-peer-urls</strong></p>\n<ul>\n<li>监听在对等节点流量上的URL列表，该参数告诉etcd在指定的协议://IP:port组合上接受来自其对等方的传入请求。协议可以是http或者https。或者，使用<code>unix://&lt;file-path&gt;</code>或者<code>unixs://&lt;file-path&gt;</code>到unix sockets。如果将0.0.0.0作为IP，etcd将监听在所有的接口上的给定端口。如果给定了Ip和端口，etcd将监听指定的接口和端口。可以使用多个URL指定要监听的地址和端口的数量。 etcd将响应来自任何列出的地址和端口的请求。</li>\n<li>默认值：”<a href=\"http://localhost:2380\" target=\"_blank\" rel=\"noopener\">http://localhost:2380</a>“</li>\n<li>环境变量:ETCD_LISTEN_PEER_URLS</li>\n<li>示例：”<a href=\"http://10.0.0.1:2380\" target=\"_blank\" rel=\"noopener\">http://10.0.0.1:2380</a>“</li>\n<li>无效的示例：”<a href=\"http://example.com:2380\" target=\"_blank\" rel=\"noopener\">http://example.com:2380</a>“(绑定的域名是无效的)</li>\n</ul>\n<p><strong>–listen-client-urls</strong></p>\n<ul>\n<li>监听在客户端流量上的URL列表，该参数告诉etcd在指定的协议://IP:port组合上接受来自客户端的传入请求。协议可以是http或者https。或者，使用<code>unix://&lt;file-path&gt;</code>或者<code>unixs://&lt;file-path&gt;</code>到unix sockets。如果将0.0.0.0作为IP，etcd将监听在所有的接口上的给定端口。如果给定了Ip和端口，etcd将监听指定的接口和端口。可以使用多个URL指定要监听的地址和端口的数量。 etcd将响应来自任何列出的地址和端口的请求。</li>\n<li>默认值：”<a href=\"http://localhost:2379\" target=\"_blank\" rel=\"noopener\">http://localhost:2379</a>“</li>\n<li>环境变量:ETCD_LISTEN_CLIENT_URLS</li>\n<li>示例：”<a href=\"http://10.0.0.1:2379\" target=\"_blank\" rel=\"noopener\">http://10.0.0.1:2379</a>“</li>\n<li>无效的示例：”<a href=\"http://example.com:2379\" target=\"_blank\" rel=\"noopener\">http://example.com:2379</a>“(绑定的域名是无效的)</li>\n</ul>\n<p><strong>–max-snapshots</strong></p>\n<ul>\n<li>保留的快照文件最大数量（0为无限）</li>\n<li>默认值：5</li>\n<li>环境变量：ETCD_MAX_SNAPSHOTS</li>\n<li>Windows用户的默认设置是无限制的，建议手动设置到5（或出于安全性的考虑）。</li>\n</ul>\n<p><strong>–max-wals</strong></p>\n<ul>\n<li>保留的wal文件最大数量（0为无限）</li>\n<li>默认值：5</li>\n<li>环境变量：ETCD_MAX_WALS</li>\n<li>Windows用户的默认设置是无限制的，建议手动设置到5（或出于安全性的考虑）。</li>\n</ul>\n<p><strong>–cors</strong></p>\n<ul>\n<li>以逗号分隔的CORS来源白名单（跨来源资源共享）。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CORS</li>\n</ul>\n<p><strong>–quota-backent-bytes</strong></p>\n<ul>\n<li>后端大小超过给定配额时引发警报（0默认为低空间配额）。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_QUOTA_BACKEND_BYTES</li>\n</ul>\n<p><strong>–backend-batch-limit</strong></p>\n<ul>\n<li>BackendBatchLimit是提交后端事务之前的最大数量的操作。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_BACKEND_BATCH_LIMIT</li>\n</ul>\n<p><strong>–backend-bbolt-freelist-type</strong></p>\n<ul>\n<li>etcd后端（bboltdb）使用的自由列表类型（支持数组和映射的类型）。</li>\n<li>默认值：map</li>\n<li>环境变量：ETCD_BACKEND_BBOLT_FREELIST_TYPE</li>\n</ul>\n<p><strong>–backend-batch-interval</strong></p>\n<ul>\n<li>BackendBatchInterval是提交后端事务之前的最长时间。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_BACKEND_BATCH_INTERVAL</li>\n</ul>\n<p><strong>–max-txn-ops</strong></p>\n<ul>\n<li>交易中允许的最大操作数。</li>\n<li>默认值：128</li>\n<li>环境变量：ETCD_MAX_TXN_OPS</li>\n</ul>\n<p><strong>–max-request-bytes</strong></p>\n<ul>\n<li>服务器将接受的最大客户端请求大小（以字节为单位）。</li>\n<li>默认值：1572864</li>\n<li>环境变量：ETCD_MAX_REQUEST_BYTES</li>\n</ul>\n<p><strong>–grpc-keepalive-min-time</strong></p>\n<ul>\n<li>客户端在ping服务器之前应等待的最小持续时间间隔。</li>\n<li>默认值：5s</li>\n<li>环境变量：ETCD_GRPC_KEEPALIVE_MIN_TIME</li>\n</ul>\n<p><strong>–grpc-keepalive-interval</strong></p>\n<ul>\n<li>服务器到客户端ping的频率持续时间，以检查连接是否有效（0禁用）。</li>\n<li>默认值：2h</li>\n<li>环境变量：ETCD_GRPC_KEEPALIVE_INTERVAL</li>\n</ul>\n<p><strong>–grpc-keepalive-timeout</strong></p>\n<ul>\n<li>关闭无响应的连接之前的额外等待时间（0禁用）。</li>\n<li>默认值：20s</li>\n<li>环境变量：ETCD_GRPC_KEEPALIVE_TIMEOUT</li>\n</ul>\n<h2 id=\"集群参数\"><a href=\"#集群参数\" class=\"headerlink\" title=\"集群参数\"></a>集群参数</h2><hr>\n<p><code>--initial-advertise-peer-urls</code>,<code>--initial-cluster</code>,<code>--initial-cluster-state</code>,和<code>--initial-cluster-token</code>参数用于启动(<a href=\"\">静态启动</a>,<a href=\"\">发现服务启动</a>或者<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">运行时重新配置</a>)一个新成员，当重启已经存在的成员时将忽略。<br>前缀为<code>--discovery</code>的参数在使用<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/\">发现服务</a>时需要被设置。</p>\n<p><strong>–initial-advertise-peer-urls</strong></p>\n<ul>\n<li>此成员的对等URL的列表，以通告到集群的其余部分。 这些地址用于在集群周围传送etcd数据。 所有集群成员必须至少有一个路由。 这些URL可以包含域名。</li>\n<li>默认值：”<a href=\"http://localhost:2380\" target=\"_blank\" rel=\"noopener\">http://localhost:2380</a>“</li>\n<li>环境变量：ETCD_INITIAL_ADVERTISE_PEER_URLS</li>\n<li>示例：”<a href=\"http://example.com:2380\" target=\"_blank\" rel=\"noopener\">http://example.com:2380</a>, <a href=\"http://10.0.0.1:2380\" target=\"_blank\" rel=\"noopener\">http://10.0.0.1:2380</a>“</li>\n</ul>\n<p><strong>–initial-cluster</strong></p>\n<ul>\n<li>启动集群的初始化配置</li>\n<li>默认值：”default=<a href=\"http://localhost:2380\" target=\"_blank\" rel=\"noopener\">http://localhost:2380</a>“</li>\n<li>环境变量：ETCD_INITIAL_CLUSTER</li>\n<li>关键是所提供的每个节点的<code>--name</code>参数的值。 默认值使用<code>default</code>作为密钥，因为这是<code>--name</code>参数的默认值。</li>\n</ul>\n<p><strong>–initial-cluster-state</strong></p>\n<ul>\n<li>初始群集状态（“新”或“现有”）。 对于在初始静态或DNS引导过程中存在的所有成员，将其设置为<code>new</code>。 如果此选项设置为<code>existing</code>，则etcd将尝试加入现存集群。 如果设置了错误的值，etcd将尝试启动，但会安全地失败。</li>\n<li>默认值：”new:</li>\n<li>环境变量：ETCD_INITIAL_CLUSTER_STATE</li>\n</ul>\n<p><strong>–initial-cluster-token</strong></p>\n<ul>\n<li>引导期间etcd群集的初始集群令牌。</li>\n<li>默认值：”etcd-cluster”</li>\n<li>环境变量：ETCD_INITIAL_CLUSTER_TOKEN</li>\n</ul>\n<p><strong>–advertise-client-urls</strong></p>\n<ul>\n<li>此成员的客户端URL的列表，这些URL广播给集群的其余部分。 这些URL可以包含域名。</li>\n<li>默认值：<a href=\"http://localhost:2379\" target=\"_blank\" rel=\"noopener\">http://localhost:2379</a></li>\n<li>环境变量：ETCD_ADVERTISE_CLIENT_URLS</li>\n<li>示例：”<a href=\"http://example.com:2379\" target=\"_blank\" rel=\"noopener\">http://example.com:2379</a>, <a href=\"http://10.0.0.1:2379\" target=\"_blank\" rel=\"noopener\">http://10.0.0.1:2379</a>“</li>\n<li>如果从集群成员中发布诸如<code>http://localhost:2379</code>之类的URL并使用etcd的代理功能，请小心。这将导致循环，因为代理将向其自身转发请求，直到其资源（内存，文件描述符）最终耗尽为止。</li>\n</ul>\n<p><strong>–discovery</strong></p>\n<ul>\n<li>发现URL用于引导启动集群</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_DISCOVERY</li>\n</ul>\n<p><strong>–discovery-srv</strong></p>\n<ul>\n<li>用于引导集群的DNS srv域。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_DISCOVERY_SRV</li>\n</ul>\n<p><strong>–discovery-srv-name</strong></p>\n<ul>\n<li>使用DNS引导时查询的DNS srv名称的后缀。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_DISCOVERY_SRV_NAME</li>\n</ul>\n<p><strong>–discovery-fallback</strong></p>\n<ul>\n<li>发现服务失败时的预期行为(“退出”或“代理”)。“代理”仅支持v2 API。</li>\n<li>默认值： “proxy”</li>\n<li>环境变量：ETCD_DISCOVERY_FALLBACK</li>\n</ul>\n<p><strong>–discovery-proxy</strong></p>\n<ul>\n<li>HTTP代理，用于发现服务的流量。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_DISCOVERY_PROXY</li>\n</ul>\n<p><strong>–strict-reconfig-check</strong></p>\n<ul>\n<li>拒绝可能导致quorum丢失的重新配置请求。</li>\n<li>默认值：true</li>\n<li>环境变量：ETCD_STRICT_RECONFIG_CHECK</li>\n</ul>\n<p><strong>–auto-compaction-retention</strong></p>\n<ul>\n<li>mvcc密钥值存储的自动压缩保留时间（小时）。 0表示禁用自动压缩。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_AUTO_COMPACTION_RETENTION</li>\n</ul>\n<p><strong>–auto-compaction-mode</strong></p>\n<ul>\n<li>解释“自动压缩保留”之一：“定期”，“修订”。 基于期限的保留的“定期”，如果未提供时间单位（例如“ 5m”），则默认为小时。 “修订”用于基于修订号的保留。</li>\n<li>默认值：periodic</li>\n<li>环境变量：ETCD_AUTO_COMPACTION_MODE</li>\n</ul>\n<p><strong>–enable-v2</strong></p>\n<ul>\n<li>接受etcd V2客户端请求</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_ENABLE_V2</li>\n</ul>\n<h2 id=\"代理参数\"><a href=\"#代理参数\" class=\"headerlink\" title=\"代理参数\"></a>代理参数</h2><hr>\n<p>–proxy前缀标志将etcd配置为以代理模式运行。 “代理”仅支持v2 API。</p>\n<p><strong>–proxy</strong></p>\n<ul>\n<li>代理模式设置(”off”,”readonly”或者”on”)</li>\n<li>默认值：”off”</li>\n<li>环境变量：ETCD_PROXY</li>\n</ul>\n<p><strong>–proxy-failure-wait</strong></p>\n<ul>\n<li>在重新考虑端点请求之前，端点将保持故障状态的时间（以毫秒为单位）。</li>\n<li>默认值：5000</li>\n<li>环境变量：ETCD_PROXY_FAILURE_WAIT</li>\n</ul>\n<p><strong>–proxy-refresh-interval</strong></p>\n<ul>\n<li>节点刷新间隔的时间（以毫秒为单位）。</li>\n<li>默认值：30000</li>\n<li>环境变量：ETCD_PROXY_REFRESH_INTERVAL</li>\n</ul>\n<p><strong>–proxy-dial-timeout</strong></p>\n<ul>\n<li>拨号超时的时间（以毫秒为单位），或0以禁用超时</li>\n<li>默认值：1000</li>\n<li>环境变量：ETCD_PROXY_DIAL_TIMEOUT</li>\n</ul>\n<p><strong>–proxy-write-timeout</strong></p>\n<ul>\n<li>写入超时的时间（以毫秒为单位）或禁用超时的时间为0。</li>\n<li>默认值：5000</li>\n<li>环境变量：ETCD_PROXY_WRITE_TIMEOUT</li>\n</ul>\n<p><strong>–proxy-read-timeout</strong></p>\n<ul>\n<li>读取超时的时间（以毫秒为单位），或者为0以禁用超时。</li>\n<li>如果使用Watch，请勿更改此值，因为会使用较长的轮询请求。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_PROXY_READ_TIMEOUT</li>\n</ul>\n<h2 id=\"安全参数\"><a href=\"#安全参数\" class=\"headerlink\" title=\"安全参数\"></a>安全参数</h2><hr>\n<p>安全参数有助于<a href=\"\">构建一个安全的etcd集群</a><br><strong>–ca-file</strong><br><strong>DEPRECATED</strong></p>\n<ul>\n<li>客户端服务器TLS CA文件的路径。 <code>--ca-file ca.crt</code>可以替换为<code>--trusted-ca-file ca.crt --client-cert-auth</code>，而etcd将执行相同的操作。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CA_FILE</li>\n</ul>\n<p><strong>–cert-file</strong></p>\n<ul>\n<li>客户端服务器TLS证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CERT_FILE</li>\n</ul>\n<p><strong>–key-file</strong></p>\n<ul>\n<li>客户端服务器TLS秘钥文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_KEY_FILE</li>\n</ul>\n<p><strong>–client-cert-auth</strong></p>\n<ul>\n<li>开启客户端证书认证</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_CLIENT_CERT_AUTH</li>\n<li>CN 权限认证不支持gRPC-网关</li>\n</ul>\n<p><strong>–client-crl-file</strong></p>\n<ul>\n<li>客户端被撤销的TLS证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CLIENT_CERT_ALLOWED_HOSTNAME</li>\n</ul>\n<p><strong>–client-cert-allowed-hostname</strong></p>\n<ul>\n<li>允许客户端证书身份验证的TLS名称。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CLIENT_CERT_ALLOWED_HOSTNAME</li>\n</ul>\n<p><strong>–trusted-ca-file</strong></p>\n<ul>\n<li>客户端服务器受信任的TLS CA证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_TRUSTED_CA_FILE</li>\n</ul>\n<p><strong>–auto-tls</strong></p>\n<ul>\n<li>客户端TLS使用自动生成的证书</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_AUTO_TLS</li>\n</ul>\n<p><strong>–peer-ca-file</strong><br><strong>已淘汰</strong></p>\n<ul>\n<li>节点TLS CA文件的路径.<code>--peer-ca-file</code>可以替换为<code>--peer-trusted-ca-file ca.crt --peer-client-cert-auth</code>，而etcd将执行相同的操作。</li>\n<li>默认值：”“</li>\n<li>环境变量：ETCD_PEER_CA_FILE</li>\n</ul>\n<p><strong>–peer-cert-file</strong></p>\n<ul>\n<li>对等服务器TLS证书文件的路径。 这是对等节点通信证书，在服务器和客户端都可以使用。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_CERT_FILE</li>\n</ul>\n<p><strong>–peer-key-file</strong></p>\n<ul>\n<li>对等服务器TLS秘钥文件的路径。 这是对等节点通信秘钥，在服务器和客户端都可以使用。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_KEY_FILE</li>\n</ul>\n<p><strong>–peer-client-cert-auth</strong></p>\n<ul>\n<li>启动节点客户端证书认证</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_PEER_CLIENT_CERT_AUTH</li>\n</ul>\n<p><strong>–peer-crl-file</strong></p>\n<ul>\n<li>节点被撤销的TLS证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_CRL_FILE</li>\n</ul>\n<p><strong>–peer-trusted-ca-file</strong></p>\n<ul>\n<li>节点受信任的TLS CA证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_TRUSTED_CA_FILE</li>\n</ul>\n<p><strong>–peer-auto-tls</strong></p>\n<ul>\n<li>节点使用自动生成的证书</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_PEER_AUTO_TLS</li>\n</ul>\n<p><strong>–peer-cert-allowed-cn</strong></p>\n<ul>\n<li>允许使用CommonName进行对等身份验证。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_CERT_ALLOWED_CN</li>\n</ul>\n<p><strong>–peer-cert-allowed-hostname</strong></p>\n<ul>\n<li>允许的TLS证书名称用于对等身份验证。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_CERT_ALLOWED_HOSTNAME</li>\n</ul>\n<p><strong>–cipher-suites</strong></p>\n<ul>\n<li>以逗号分隔的服务器/客户端和对等方之间受支持的TLS密码套件列表。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CIPHER_SUITES</li>\n</ul>\n<h2 id=\"日志参数\"><a href=\"#日志参数\" class=\"headerlink\" title=\"日志参数\"></a>日志参数</h2><hr>\n<p><strong>–logger</strong></p>\n<p><strong>v3.4可以使用，警告：<code>--logger=capnslog</code>在v3.5被抛弃使用</strong></p>\n<ul>\n<li>指定“ zap”用于结构化日志记录或“ capnslog”。 </li>\n<li>默认值：capnslog</li>\n<li>环境变量：ETCD_LOGGER</li>\n</ul>\n<p><strong>–log-outputs</strong></p>\n<ul>\n<li>指定“ stdout”或“ stderr”以跳过日志记录，即使在systemd或逗号分隔的输出目标列表下运行时也是如此。</li>\n<li>默认值：defalut</li>\n<li>环境变量：ETCD_LOG_OUTPUTS</li>\n<li><code>default</code>在zap logger迁移期间对v3.4使用<code>stderr</code>配置</li>\n</ul>\n<p><strong>–log-level</strong><br><strong>v3.4可以使用</strong></p>\n<ul>\n<li>配置日志等级，仅支持<code>debug,info,warn,error,panic,fatal</code></li>\n<li>默认值：info</li>\n<li>环境变量：ETCD_LOG_LEVEL</li>\n<li><code>default</code>使用<code>info</code>.</li>\n</ul>\n<p><strong>–debug</strong><br><strong>警告：在v3.5被抛弃使用</strong></p>\n<ul>\n<li>将所有子程序包的默认日志级别降为DEBUG。</li>\n<li>默认值：false(所有的包使用INFO)</li>\n<li>环境变量：ETCD_DEBUG</li>\n</ul>\n<p><strong>–log-package-levels</strong><br><strong>警告：在v3.5被抛弃使用</strong></p>\n<ul>\n<li>将各个etcd子软件包设置为特定的日志级别。 一个例子是<code>etcdserver = WARNING，security = DEBUG</code></li>\n<li>默认值：””(所有的包使用INFO)</li>\n<li>环境变量：ETCD_LOG_PACKAGE_LEVELS</li>\n</ul>\n<h2 id=\"风险参数\"><a href=\"#风险参数\" class=\"headerlink\" title=\"风险参数\"></a>风险参数</h2><hr>\n<p>使用不安全标志时请小心，因为它将破坏共识协议提供的保证。 例如，如果群集中的其他成员仍然存在，可能会<code>panic</code>。 使用这些标志时，请遵循说明。<br><strong>–force-new-cluster</strong></p>\n<ul>\n<li>强制创建一个新的单成员群集。 它提交配置更改，以强制删除群集中的所有现有成员并添加自身，但是强烈建议不要这样做。 请查看<a href=\"\">灾难恢复文档</a>以了解首选的v3恢复过程。</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_FORCE_NEW_CLUSTER</li>\n</ul>\n<h2 id=\"杂项参数\"><a href=\"#杂项参数\" class=\"headerlink\" title=\"杂项参数\"></a>杂项参数</h2><hr>\n<p><strong>–version</strong></p>\n<ul>\n<li>打印版本并退出</li>\n<li>默认值：false</li>\n</ul>\n<p><strong>–config-file</strong></p>\n<ul>\n<li>从文件加载服务器配置。 请注意，如果提供了配置文件，则其他命令行标志和环境变量将被忽略。</li>\n<li>默认值：””</li>\n<li>示例：<a href=\"https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample\" target=\"_blank\" rel=\"noopener\">配置文件示例</a></li>\n<li>环境变量：ETCD_CONFIG_FILE</li>\n</ul>\n<h2 id=\"分析参数\"><a href=\"#分析参数\" class=\"headerlink\" title=\"分析参数\"></a>分析参数</h2><hr>\n<p><strong>–enable-pprof</strong></p>\n<ul>\n<li>通过HTTP服务器启用运行时分析数据。地址位于客户端<code>URL+“/debug/pprof/”</code></li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_ENABLE_PPROF</li>\n</ul>\n<p><strong>–metrics</strong></p>\n<ul>\n<li>设置导出指标的详细程度，specify ‘extensive’ to include server side grpc histogram metrics.</li>\n<li>默认值：basic</li>\n<li>环境变量：ETCD_METRICS</li>\n</ul>\n<p><strong>–listen-metrics-urls</strong></p>\n<ul>\n<li>可以响应<code>/metrics</code>和<code>/health</code>端点的其他URL列表</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_LISTEN_METRICS_URLS</li>\n</ul>\n<h2 id=\"权限参数\"><a href=\"#权限参数\" class=\"headerlink\" title=\"权限参数\"></a>权限参数</h2><hr>\n<p><strong>–auth-token</strong></p>\n<ul>\n<li>指定令牌类型和特定于令牌的选项，特别是对于JWT,格式为<code>type,var1=val1,var2=val2,...</code>,可能的类型是<code>simple</code>或者<code>jwt</code>.对于具体的签名方法jwt可能的变量为<code>sign-method</code>（可能的值为<code>&#39;ES256&#39;, &#39;ES384&#39;, &#39;ES512&#39;, &#39;HS256&#39;, &#39;HS384&#39;, &#39;HS512&#39;, &#39;RS256&#39;, &#39;RS384&#39;, &#39;RS512&#39;, &#39;PS256&#39;, &#39;PS384&#39;,&#39;PS512&#39;</code>）</li>\n<li>对于非对称算法（“ RS”，“ PS”，“ ES”），公钥是可选的，因为私钥包含足够的信息来签名和验证令牌。<code>pub-key</code>用于指定用于验证jwt的公钥的路径,<code>priv-key</code>用于指定用于对jwt进行签名的私钥的路径，<code>ttl</code>用于指定jwt令牌的TTL。</li>\n<li>JWT的示例选项：<code>-auth-token jwt，pub-key=app.rsa.pub，privkey=app.rsasign-method = RS512，ttl = 10m</code></li>\n<li>默认值：”simple”</li>\n<li>环境变量：ETCD_AUTH_TOKEN</li>\n</ul>\n<p><strong>–bcrypt-cost</strong></p>\n<ul>\n<li>指定用于哈希认证密码的bcrypt算法的成本/强度。 有效值在4到31之间。</li>\n<li>默认值：10</li>\n<li>环境变量：(不支持)</li>\n</ul>\n<h2 id=\"实验参数\"><a href=\"#实验参数\" class=\"headerlink\" title=\"实验参数\"></a>实验参数</h2><hr>\n<p><strong>–experimental-corrupt-check-time</strong></p>\n<ul>\n<li>群集损坏检查通过之间的时间间隔</li>\n<li>默认值：0s</li>\n<li>环境变量：ETCD_EXPERIMENTAL_CORRUPT_CHECK_TIME</li>\n</ul>\n<p><strong>–experimental-compaction-batch-limit</strong></p>\n<ul>\n<li>设置每个压缩批处理中删除的最大修订。</li>\n<li>默认值：1000</li>\n<li>环境变量：ETCD_EXPERIMENTAL_COMPACTION_BATCH_LIMIT</li>\n</ul>\n<p><strong>–experimental-peer-skip-client-san-verification</strong></p>\n<ul>\n<li>跳过客户端证书中对等连接的SAN字段验证。 这可能是有帮助的，例如 如果群集成员在NAT后面的不同网络中运行。在这种情况下，请确保使用基于私有证书颁发机构的对等证书.<code>--peer-cert-file, --peer-key-file, --peer-trusted-ca-file</code></li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_EXPERIMENTAL_PEER_SKIP_CLIENT_SAN_VERIFICATION</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md\" target=\"_blank\" rel=\"noopener\">Configuration flags</a><br>etcd通过配置文件，多命令行参数和环境变量进行配置，</p>\n<p>可重用的配置文件是YAML文件，其名称和值由一个或多个下面描述的命令行标志组成。为了使用此文件，请将文件路径指定为<code>--config-file</code>标志或<code>ETCD_CONFIG_FILE</code>环境变量的值。如果需要的话<a href=\"https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample\" target=\"_blank\" rel=\"noopener\">配置文件示例</a>可以作为入口点创建新的配置文件。</p>\n<p>在命令行上设置的选项优先于环境中的选项。 如果提供了配置文件，则其他命令行标志和环境变量将被忽略。例如，<code>etcd --config-file etcd.conf.yml.sample --data-dir /tmp</code>将会忽略<code>--data-dir</code>参数。</p>\n<p>参数<code>--my-flag</code>的环境变量的格式为<code>ETCD_MY_FLAG</code>.它适用于所有参数。</p>\n<p>客户端请求<a href=\"http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.txt\" target=\"_blank\" rel=\"noopener\">官方的etcd端口</a>为2379,2380是节点通信端口。可以将etcd端口设置为接受TLS流量，非TLS流量，或同时接受TLS和非TLS流量。</p>\n<p>要在Linux启动时使用自定义设置自动启动etcd，强烈建议使用<a href=\"freedesktop.org/wiki/Software/systemd/\">systemd</a>单元。</p>\n<h2 id=\"成员标记\"><a href=\"#成员标记\" class=\"headerlink\" title=\"成员标记\"></a>成员标记</h2><hr>\n<p><strong>–name</strong></p>\n<ul>\n<li>人类可读的该成员的名字</li>\n<li>默认值：”default”</li>\n<li>环境变量：ETCD_NAME</li>\n<li>该值被该节点吃的<code>--initial-cluster</code>参数引用(例如 <code>default=http://localhost:2380</code>).如果使用<a href=\"\">静态引导程序</a>，则需要与标志中使用的键匹配。当使用发现服务时，每一个成员需要有唯一的名字。<code>Hostname</code>或者<code>machine-id</code>是好的选择。</li>\n</ul>\n<p><strong>–data-dir</strong></p>\n<ul>\n<li>数据目录的路径</li>\n<li>默认值：”${name}.etcd”</li>\n<li>环境变量：ETCD_DATA_DIR</li>\n</ul>\n<p><strong>–wal-dir</strong></p>\n<ul>\n<li>专用的wal目录的路径。如果这个参数被设置，etcd将会写WAL文件到walDir而不是dataDir，允许使用专用磁盘，并有助于避免日志记录和其他IO操作之间的io竞争。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_WAL_DIR</li>\n</ul>\n<p><strong>–snapshot-count</strong></p>\n<ul>\n<li>触发一个快照到磁盘的已提交交易的数量</li>\n<li>默认值：”100000”</li>\n<li>环境变量：ETCD_SNAPSHOP_COUNT</li>\n</ul>\n<p><strong>–heartbeat-interval</strong></p>\n<ul>\n<li>心跳间隔(毫秒为单位)</li>\n<li>默认值:”100”</li>\n<li>环境变量：ETCD_HEARTBEAT_INTERVAL</li>\n</ul>\n<p><strong>–election-timeout</strong></p>\n<ul>\n<li>选举超时时间(毫秒为单位)，从<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md\" target=\"_blank\" rel=\"noopener\">文档/tuning.md</a>发现更多细节</li>\n<li>默认值：”1000”</li>\n<li>环境变量：ETCD_ELECTION_TIMEOUT</li>\n</ul>\n<p><strong>–listen-peer-urls</strong></p>\n<ul>\n<li>监听在对等节点流量上的URL列表，该参数告诉etcd在指定的协议://IP:port组合上接受来自其对等方的传入请求。协议可以是http或者https。或者，使用<code>unix://&lt;file-path&gt;</code>或者<code>unixs://&lt;file-path&gt;</code>到unix sockets。如果将0.0.0.0作为IP，etcd将监听在所有的接口上的给定端口。如果给定了Ip和端口，etcd将监听指定的接口和端口。可以使用多个URL指定要监听的地址和端口的数量。 etcd将响应来自任何列出的地址和端口的请求。</li>\n<li>默认值：”<a href=\"http://localhost:2380\" target=\"_blank\" rel=\"noopener\">http://localhost:2380</a>“</li>\n<li>环境变量:ETCD_LISTEN_PEER_URLS</li>\n<li>示例：”<a href=\"http://10.0.0.1:2380\" target=\"_blank\" rel=\"noopener\">http://10.0.0.1:2380</a>“</li>\n<li>无效的示例：”<a href=\"http://example.com:2380\" target=\"_blank\" rel=\"noopener\">http://example.com:2380</a>“(绑定的域名是无效的)</li>\n</ul>\n<p><strong>–listen-client-urls</strong></p>\n<ul>\n<li>监听在客户端流量上的URL列表，该参数告诉etcd在指定的协议://IP:port组合上接受来自客户端的传入请求。协议可以是http或者https。或者，使用<code>unix://&lt;file-path&gt;</code>或者<code>unixs://&lt;file-path&gt;</code>到unix sockets。如果将0.0.0.0作为IP，etcd将监听在所有的接口上的给定端口。如果给定了Ip和端口，etcd将监听指定的接口和端口。可以使用多个URL指定要监听的地址和端口的数量。 etcd将响应来自任何列出的地址和端口的请求。</li>\n<li>默认值：”<a href=\"http://localhost:2379\" target=\"_blank\" rel=\"noopener\">http://localhost:2379</a>“</li>\n<li>环境变量:ETCD_LISTEN_CLIENT_URLS</li>\n<li>示例：”<a href=\"http://10.0.0.1:2379\" target=\"_blank\" rel=\"noopener\">http://10.0.0.1:2379</a>“</li>\n<li>无效的示例：”<a href=\"http://example.com:2379\" target=\"_blank\" rel=\"noopener\">http://example.com:2379</a>“(绑定的域名是无效的)</li>\n</ul>\n<p><strong>–max-snapshots</strong></p>\n<ul>\n<li>保留的快照文件最大数量（0为无限）</li>\n<li>默认值：5</li>\n<li>环境变量：ETCD_MAX_SNAPSHOTS</li>\n<li>Windows用户的默认设置是无限制的，建议手动设置到5（或出于安全性的考虑）。</li>\n</ul>\n<p><strong>–max-wals</strong></p>\n<ul>\n<li>保留的wal文件最大数量（0为无限）</li>\n<li>默认值：5</li>\n<li>环境变量：ETCD_MAX_WALS</li>\n<li>Windows用户的默认设置是无限制的，建议手动设置到5（或出于安全性的考虑）。</li>\n</ul>\n<p><strong>–cors</strong></p>\n<ul>\n<li>以逗号分隔的CORS来源白名单（跨来源资源共享）。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CORS</li>\n</ul>\n<p><strong>–quota-backent-bytes</strong></p>\n<ul>\n<li>后端大小超过给定配额时引发警报（0默认为低空间配额）。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_QUOTA_BACKEND_BYTES</li>\n</ul>\n<p><strong>–backend-batch-limit</strong></p>\n<ul>\n<li>BackendBatchLimit是提交后端事务之前的最大数量的操作。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_BACKEND_BATCH_LIMIT</li>\n</ul>\n<p><strong>–backend-bbolt-freelist-type</strong></p>\n<ul>\n<li>etcd后端（bboltdb）使用的自由列表类型（支持数组和映射的类型）。</li>\n<li>默认值：map</li>\n<li>环境变量：ETCD_BACKEND_BBOLT_FREELIST_TYPE</li>\n</ul>\n<p><strong>–backend-batch-interval</strong></p>\n<ul>\n<li>BackendBatchInterval是提交后端事务之前的最长时间。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_BACKEND_BATCH_INTERVAL</li>\n</ul>\n<p><strong>–max-txn-ops</strong></p>\n<ul>\n<li>交易中允许的最大操作数。</li>\n<li>默认值：128</li>\n<li>环境变量：ETCD_MAX_TXN_OPS</li>\n</ul>\n<p><strong>–max-request-bytes</strong></p>\n<ul>\n<li>服务器将接受的最大客户端请求大小（以字节为单位）。</li>\n<li>默认值：1572864</li>\n<li>环境变量：ETCD_MAX_REQUEST_BYTES</li>\n</ul>\n<p><strong>–grpc-keepalive-min-time</strong></p>\n<ul>\n<li>客户端在ping服务器之前应等待的最小持续时间间隔。</li>\n<li>默认值：5s</li>\n<li>环境变量：ETCD_GRPC_KEEPALIVE_MIN_TIME</li>\n</ul>\n<p><strong>–grpc-keepalive-interval</strong></p>\n<ul>\n<li>服务器到客户端ping的频率持续时间，以检查连接是否有效（0禁用）。</li>\n<li>默认值：2h</li>\n<li>环境变量：ETCD_GRPC_KEEPALIVE_INTERVAL</li>\n</ul>\n<p><strong>–grpc-keepalive-timeout</strong></p>\n<ul>\n<li>关闭无响应的连接之前的额外等待时间（0禁用）。</li>\n<li>默认值：20s</li>\n<li>环境变量：ETCD_GRPC_KEEPALIVE_TIMEOUT</li>\n</ul>\n<h2 id=\"集群参数\"><a href=\"#集群参数\" class=\"headerlink\" title=\"集群参数\"></a>集群参数</h2><hr>\n<p><code>--initial-advertise-peer-urls</code>,<code>--initial-cluster</code>,<code>--initial-cluster-state</code>,和<code>--initial-cluster-token</code>参数用于启动(<a href=\"\">静态启动</a>,<a href=\"\">发现服务启动</a>或者<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">运行时重新配置</a>)一个新成员，当重启已经存在的成员时将忽略。<br>前缀为<code>--discovery</code>的参数在使用<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/\">发现服务</a>时需要被设置。</p>\n<p><strong>–initial-advertise-peer-urls</strong></p>\n<ul>\n<li>此成员的对等URL的列表，以通告到集群的其余部分。 这些地址用于在集群周围传送etcd数据。 所有集群成员必须至少有一个路由。 这些URL可以包含域名。</li>\n<li>默认值：”<a href=\"http://localhost:2380\" target=\"_blank\" rel=\"noopener\">http://localhost:2380</a>“</li>\n<li>环境变量：ETCD_INITIAL_ADVERTISE_PEER_URLS</li>\n<li>示例：”<a href=\"http://example.com:2380\" target=\"_blank\" rel=\"noopener\">http://example.com:2380</a>, <a href=\"http://10.0.0.1:2380\" target=\"_blank\" rel=\"noopener\">http://10.0.0.1:2380</a>“</li>\n</ul>\n<p><strong>–initial-cluster</strong></p>\n<ul>\n<li>启动集群的初始化配置</li>\n<li>默认值：”default=<a href=\"http://localhost:2380\" target=\"_blank\" rel=\"noopener\">http://localhost:2380</a>“</li>\n<li>环境变量：ETCD_INITIAL_CLUSTER</li>\n<li>关键是所提供的每个节点的<code>--name</code>参数的值。 默认值使用<code>default</code>作为密钥，因为这是<code>--name</code>参数的默认值。</li>\n</ul>\n<p><strong>–initial-cluster-state</strong></p>\n<ul>\n<li>初始群集状态（“新”或“现有”）。 对于在初始静态或DNS引导过程中存在的所有成员，将其设置为<code>new</code>。 如果此选项设置为<code>existing</code>，则etcd将尝试加入现存集群。 如果设置了错误的值，etcd将尝试启动，但会安全地失败。</li>\n<li>默认值：”new:</li>\n<li>环境变量：ETCD_INITIAL_CLUSTER_STATE</li>\n</ul>\n<p><strong>–initial-cluster-token</strong></p>\n<ul>\n<li>引导期间etcd群集的初始集群令牌。</li>\n<li>默认值：”etcd-cluster”</li>\n<li>环境变量：ETCD_INITIAL_CLUSTER_TOKEN</li>\n</ul>\n<p><strong>–advertise-client-urls</strong></p>\n<ul>\n<li>此成员的客户端URL的列表，这些URL广播给集群的其余部分。 这些URL可以包含域名。</li>\n<li>默认值：<a href=\"http://localhost:2379\" target=\"_blank\" rel=\"noopener\">http://localhost:2379</a></li>\n<li>环境变量：ETCD_ADVERTISE_CLIENT_URLS</li>\n<li>示例：”<a href=\"http://example.com:2379\" target=\"_blank\" rel=\"noopener\">http://example.com:2379</a>, <a href=\"http://10.0.0.1:2379\" target=\"_blank\" rel=\"noopener\">http://10.0.0.1:2379</a>“</li>\n<li>如果从集群成员中发布诸如<code>http://localhost:2379</code>之类的URL并使用etcd的代理功能，请小心。这将导致循环，因为代理将向其自身转发请求，直到其资源（内存，文件描述符）最终耗尽为止。</li>\n</ul>\n<p><strong>–discovery</strong></p>\n<ul>\n<li>发现URL用于引导启动集群</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_DISCOVERY</li>\n</ul>\n<p><strong>–discovery-srv</strong></p>\n<ul>\n<li>用于引导集群的DNS srv域。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_DISCOVERY_SRV</li>\n</ul>\n<p><strong>–discovery-srv-name</strong></p>\n<ul>\n<li>使用DNS引导时查询的DNS srv名称的后缀。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_DISCOVERY_SRV_NAME</li>\n</ul>\n<p><strong>–discovery-fallback</strong></p>\n<ul>\n<li>发现服务失败时的预期行为(“退出”或“代理”)。“代理”仅支持v2 API。</li>\n<li>默认值： “proxy”</li>\n<li>环境变量：ETCD_DISCOVERY_FALLBACK</li>\n</ul>\n<p><strong>–discovery-proxy</strong></p>\n<ul>\n<li>HTTP代理，用于发现服务的流量。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_DISCOVERY_PROXY</li>\n</ul>\n<p><strong>–strict-reconfig-check</strong></p>\n<ul>\n<li>拒绝可能导致quorum丢失的重新配置请求。</li>\n<li>默认值：true</li>\n<li>环境变量：ETCD_STRICT_RECONFIG_CHECK</li>\n</ul>\n<p><strong>–auto-compaction-retention</strong></p>\n<ul>\n<li>mvcc密钥值存储的自动压缩保留时间（小时）。 0表示禁用自动压缩。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_AUTO_COMPACTION_RETENTION</li>\n</ul>\n<p><strong>–auto-compaction-mode</strong></p>\n<ul>\n<li>解释“自动压缩保留”之一：“定期”，“修订”。 基于期限的保留的“定期”，如果未提供时间单位（例如“ 5m”），则默认为小时。 “修订”用于基于修订号的保留。</li>\n<li>默认值：periodic</li>\n<li>环境变量：ETCD_AUTO_COMPACTION_MODE</li>\n</ul>\n<p><strong>–enable-v2</strong></p>\n<ul>\n<li>接受etcd V2客户端请求</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_ENABLE_V2</li>\n</ul>\n<h2 id=\"代理参数\"><a href=\"#代理参数\" class=\"headerlink\" title=\"代理参数\"></a>代理参数</h2><hr>\n<p>–proxy前缀标志将etcd配置为以代理模式运行。 “代理”仅支持v2 API。</p>\n<p><strong>–proxy</strong></p>\n<ul>\n<li>代理模式设置(”off”,”readonly”或者”on”)</li>\n<li>默认值：”off”</li>\n<li>环境变量：ETCD_PROXY</li>\n</ul>\n<p><strong>–proxy-failure-wait</strong></p>\n<ul>\n<li>在重新考虑端点请求之前，端点将保持故障状态的时间（以毫秒为单位）。</li>\n<li>默认值：5000</li>\n<li>环境变量：ETCD_PROXY_FAILURE_WAIT</li>\n</ul>\n<p><strong>–proxy-refresh-interval</strong></p>\n<ul>\n<li>节点刷新间隔的时间（以毫秒为单位）。</li>\n<li>默认值：30000</li>\n<li>环境变量：ETCD_PROXY_REFRESH_INTERVAL</li>\n</ul>\n<p><strong>–proxy-dial-timeout</strong></p>\n<ul>\n<li>拨号超时的时间（以毫秒为单位），或0以禁用超时</li>\n<li>默认值：1000</li>\n<li>环境变量：ETCD_PROXY_DIAL_TIMEOUT</li>\n</ul>\n<p><strong>–proxy-write-timeout</strong></p>\n<ul>\n<li>写入超时的时间（以毫秒为单位）或禁用超时的时间为0。</li>\n<li>默认值：5000</li>\n<li>环境变量：ETCD_PROXY_WRITE_TIMEOUT</li>\n</ul>\n<p><strong>–proxy-read-timeout</strong></p>\n<ul>\n<li>读取超时的时间（以毫秒为单位），或者为0以禁用超时。</li>\n<li>如果使用Watch，请勿更改此值，因为会使用较长的轮询请求。</li>\n<li>默认值：0</li>\n<li>环境变量：ETCD_PROXY_READ_TIMEOUT</li>\n</ul>\n<h2 id=\"安全参数\"><a href=\"#安全参数\" class=\"headerlink\" title=\"安全参数\"></a>安全参数</h2><hr>\n<p>安全参数有助于<a href=\"\">构建一个安全的etcd集群</a><br><strong>–ca-file</strong><br><strong>DEPRECATED</strong></p>\n<ul>\n<li>客户端服务器TLS CA文件的路径。 <code>--ca-file ca.crt</code>可以替换为<code>--trusted-ca-file ca.crt --client-cert-auth</code>，而etcd将执行相同的操作。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CA_FILE</li>\n</ul>\n<p><strong>–cert-file</strong></p>\n<ul>\n<li>客户端服务器TLS证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CERT_FILE</li>\n</ul>\n<p><strong>–key-file</strong></p>\n<ul>\n<li>客户端服务器TLS秘钥文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_KEY_FILE</li>\n</ul>\n<p><strong>–client-cert-auth</strong></p>\n<ul>\n<li>开启客户端证书认证</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_CLIENT_CERT_AUTH</li>\n<li>CN 权限认证不支持gRPC-网关</li>\n</ul>\n<p><strong>–client-crl-file</strong></p>\n<ul>\n<li>客户端被撤销的TLS证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CLIENT_CERT_ALLOWED_HOSTNAME</li>\n</ul>\n<p><strong>–client-cert-allowed-hostname</strong></p>\n<ul>\n<li>允许客户端证书身份验证的TLS名称。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CLIENT_CERT_ALLOWED_HOSTNAME</li>\n</ul>\n<p><strong>–trusted-ca-file</strong></p>\n<ul>\n<li>客户端服务器受信任的TLS CA证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_TRUSTED_CA_FILE</li>\n</ul>\n<p><strong>–auto-tls</strong></p>\n<ul>\n<li>客户端TLS使用自动生成的证书</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_AUTO_TLS</li>\n</ul>\n<p><strong>–peer-ca-file</strong><br><strong>已淘汰</strong></p>\n<ul>\n<li>节点TLS CA文件的路径.<code>--peer-ca-file</code>可以替换为<code>--peer-trusted-ca-file ca.crt --peer-client-cert-auth</code>，而etcd将执行相同的操作。</li>\n<li>默认值：”“</li>\n<li>环境变量：ETCD_PEER_CA_FILE</li>\n</ul>\n<p><strong>–peer-cert-file</strong></p>\n<ul>\n<li>对等服务器TLS证书文件的路径。 这是对等节点通信证书，在服务器和客户端都可以使用。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_CERT_FILE</li>\n</ul>\n<p><strong>–peer-key-file</strong></p>\n<ul>\n<li>对等服务器TLS秘钥文件的路径。 这是对等节点通信秘钥，在服务器和客户端都可以使用。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_KEY_FILE</li>\n</ul>\n<p><strong>–peer-client-cert-auth</strong></p>\n<ul>\n<li>启动节点客户端证书认证</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_PEER_CLIENT_CERT_AUTH</li>\n</ul>\n<p><strong>–peer-crl-file</strong></p>\n<ul>\n<li>节点被撤销的TLS证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_CRL_FILE</li>\n</ul>\n<p><strong>–peer-trusted-ca-file</strong></p>\n<ul>\n<li>节点受信任的TLS CA证书文件的路径</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_TRUSTED_CA_FILE</li>\n</ul>\n<p><strong>–peer-auto-tls</strong></p>\n<ul>\n<li>节点使用自动生成的证书</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_PEER_AUTO_TLS</li>\n</ul>\n<p><strong>–peer-cert-allowed-cn</strong></p>\n<ul>\n<li>允许使用CommonName进行对等身份验证。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_CERT_ALLOWED_CN</li>\n</ul>\n<p><strong>–peer-cert-allowed-hostname</strong></p>\n<ul>\n<li>允许的TLS证书名称用于对等身份验证。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_PEER_CERT_ALLOWED_HOSTNAME</li>\n</ul>\n<p><strong>–cipher-suites</strong></p>\n<ul>\n<li>以逗号分隔的服务器/客户端和对等方之间受支持的TLS密码套件列表。</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_CIPHER_SUITES</li>\n</ul>\n<h2 id=\"日志参数\"><a href=\"#日志参数\" class=\"headerlink\" title=\"日志参数\"></a>日志参数</h2><hr>\n<p><strong>–logger</strong></p>\n<p><strong>v3.4可以使用，警告：<code>--logger=capnslog</code>在v3.5被抛弃使用</strong></p>\n<ul>\n<li>指定“ zap”用于结构化日志记录或“ capnslog”。 </li>\n<li>默认值：capnslog</li>\n<li>环境变量：ETCD_LOGGER</li>\n</ul>\n<p><strong>–log-outputs</strong></p>\n<ul>\n<li>指定“ stdout”或“ stderr”以跳过日志记录，即使在systemd或逗号分隔的输出目标列表下运行时也是如此。</li>\n<li>默认值：defalut</li>\n<li>环境变量：ETCD_LOG_OUTPUTS</li>\n<li><code>default</code>在zap logger迁移期间对v3.4使用<code>stderr</code>配置</li>\n</ul>\n<p><strong>–log-level</strong><br><strong>v3.4可以使用</strong></p>\n<ul>\n<li>配置日志等级，仅支持<code>debug,info,warn,error,panic,fatal</code></li>\n<li>默认值：info</li>\n<li>环境变量：ETCD_LOG_LEVEL</li>\n<li><code>default</code>使用<code>info</code>.</li>\n</ul>\n<p><strong>–debug</strong><br><strong>警告：在v3.5被抛弃使用</strong></p>\n<ul>\n<li>将所有子程序包的默认日志级别降为DEBUG。</li>\n<li>默认值：false(所有的包使用INFO)</li>\n<li>环境变量：ETCD_DEBUG</li>\n</ul>\n<p><strong>–log-package-levels</strong><br><strong>警告：在v3.5被抛弃使用</strong></p>\n<ul>\n<li>将各个etcd子软件包设置为特定的日志级别。 一个例子是<code>etcdserver = WARNING，security = DEBUG</code></li>\n<li>默认值：””(所有的包使用INFO)</li>\n<li>环境变量：ETCD_LOG_PACKAGE_LEVELS</li>\n</ul>\n<h2 id=\"风险参数\"><a href=\"#风险参数\" class=\"headerlink\" title=\"风险参数\"></a>风险参数</h2><hr>\n<p>使用不安全标志时请小心，因为它将破坏共识协议提供的保证。 例如，如果群集中的其他成员仍然存在，可能会<code>panic</code>。 使用这些标志时，请遵循说明。<br><strong>–force-new-cluster</strong></p>\n<ul>\n<li>强制创建一个新的单成员群集。 它提交配置更改，以强制删除群集中的所有现有成员并添加自身，但是强烈建议不要这样做。 请查看<a href=\"\">灾难恢复文档</a>以了解首选的v3恢复过程。</li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_FORCE_NEW_CLUSTER</li>\n</ul>\n<h2 id=\"杂项参数\"><a href=\"#杂项参数\" class=\"headerlink\" title=\"杂项参数\"></a>杂项参数</h2><hr>\n<p><strong>–version</strong></p>\n<ul>\n<li>打印版本并退出</li>\n<li>默认值：false</li>\n</ul>\n<p><strong>–config-file</strong></p>\n<ul>\n<li>从文件加载服务器配置。 请注意，如果提供了配置文件，则其他命令行标志和环境变量将被忽略。</li>\n<li>默认值：””</li>\n<li>示例：<a href=\"https://github.com/etcd-io/etcd/blob/master/etcd.conf.yml.sample\" target=\"_blank\" rel=\"noopener\">配置文件示例</a></li>\n<li>环境变量：ETCD_CONFIG_FILE</li>\n</ul>\n<h2 id=\"分析参数\"><a href=\"#分析参数\" class=\"headerlink\" title=\"分析参数\"></a>分析参数</h2><hr>\n<p><strong>–enable-pprof</strong></p>\n<ul>\n<li>通过HTTP服务器启用运行时分析数据。地址位于客户端<code>URL+“/debug/pprof/”</code></li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_ENABLE_PPROF</li>\n</ul>\n<p><strong>–metrics</strong></p>\n<ul>\n<li>设置导出指标的详细程度，specify ‘extensive’ to include server side grpc histogram metrics.</li>\n<li>默认值：basic</li>\n<li>环境变量：ETCD_METRICS</li>\n</ul>\n<p><strong>–listen-metrics-urls</strong></p>\n<ul>\n<li>可以响应<code>/metrics</code>和<code>/health</code>端点的其他URL列表</li>\n<li>默认值：””</li>\n<li>环境变量：ETCD_LISTEN_METRICS_URLS</li>\n</ul>\n<h2 id=\"权限参数\"><a href=\"#权限参数\" class=\"headerlink\" title=\"权限参数\"></a>权限参数</h2><hr>\n<p><strong>–auth-token</strong></p>\n<ul>\n<li>指定令牌类型和特定于令牌的选项，特别是对于JWT,格式为<code>type,var1=val1,var2=val2,...</code>,可能的类型是<code>simple</code>或者<code>jwt</code>.对于具体的签名方法jwt可能的变量为<code>sign-method</code>（可能的值为<code>&#39;ES256&#39;, &#39;ES384&#39;, &#39;ES512&#39;, &#39;HS256&#39;, &#39;HS384&#39;, &#39;HS512&#39;, &#39;RS256&#39;, &#39;RS384&#39;, &#39;RS512&#39;, &#39;PS256&#39;, &#39;PS384&#39;,&#39;PS512&#39;</code>）</li>\n<li>对于非对称算法（“ RS”，“ PS”，“ ES”），公钥是可选的，因为私钥包含足够的信息来签名和验证令牌。<code>pub-key</code>用于指定用于验证jwt的公钥的路径,<code>priv-key</code>用于指定用于对jwt进行签名的私钥的路径，<code>ttl</code>用于指定jwt令牌的TTL。</li>\n<li>JWT的示例选项：<code>-auth-token jwt，pub-key=app.rsa.pub，privkey=app.rsasign-method = RS512，ttl = 10m</code></li>\n<li>默认值：”simple”</li>\n<li>环境变量：ETCD_AUTH_TOKEN</li>\n</ul>\n<p><strong>–bcrypt-cost</strong></p>\n<ul>\n<li>指定用于哈希认证密码的bcrypt算法的成本/强度。 有效值在4到31之间。</li>\n<li>默认值：10</li>\n<li>环境变量：(不支持)</li>\n</ul>\n<h2 id=\"实验参数\"><a href=\"#实验参数\" class=\"headerlink\" title=\"实验参数\"></a>实验参数</h2><hr>\n<p><strong>–experimental-corrupt-check-time</strong></p>\n<ul>\n<li>群集损坏检查通过之间的时间间隔</li>\n<li>默认值：0s</li>\n<li>环境变量：ETCD_EXPERIMENTAL_CORRUPT_CHECK_TIME</li>\n</ul>\n<p><strong>–experimental-compaction-batch-limit</strong></p>\n<ul>\n<li>设置每个压缩批处理中删除的最大修订。</li>\n<li>默认值：1000</li>\n<li>环境变量：ETCD_EXPERIMENTAL_COMPACTION_BATCH_LIMIT</li>\n</ul>\n<p><strong>–experimental-peer-skip-client-san-verification</strong></p>\n<ul>\n<li>跳过客户端证书中对等连接的SAN字段验证。 这可能是有帮助的，例如 如果群集成员在NAT后面的不同网络中运行。在这种情况下，请确保使用基于私有证书颁发机构的对等证书.<code>--peer-cert-file, --peer-key-file, --peer-trusted-ca-file</code></li>\n<li>默认值：false</li>\n<li>环境变量：ETCD_EXPERIMENTAL_PEER_SKIP_CLIENT_SAN_VERIFICATION</li>\n</ul>\n"},{"title":"etcd网关","date":"2019-11-24T07:24:56.000Z","_content":"原文地址:[L4 gateway](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/gateway.md)\n## 什么是etcd网关\n* * *\netcd网关是一个简单的TCP代理，可将网络数据转发到etcd集群。网关是无状态且透明的； 它既不会检查客户端请求，也不会干扰群集响应。\n网关支持多个etcd服务器端点，并采用简单的循环策略。 它仅路由到可用端点，并向其客户端隐藏故障。 将来可能会支持其他重试策略，例如加权轮询。\n\n## 什么时候使用etcd网关\n* * *\n每个访问etcd的应用程序必须首先具有etcd集群客户端端点的地址。 如果同一服务器上的多个应用程序访问相同的etcd集群，则每个应用程序仍需要知道etcd集群的广播的客户端端点。 如果将etcd集群重新配置为具有不同的端点，则每个应用程序可能还需要更新其端点列表。 这种大规模的重新配置既乏味又容易出错。\netcd网关通过充当稳定的本地端点来解决此问题。 典型的etcd网关配置是，每台计算机运行一台网关，侦听本地地址，并且每个etcd应用程序都连接到其本地网关。 结果只是网关需要更新其端点，而不是更新每个应用程序。\n总之，为了自动传播集群端点更改，etcd网关在为访问同一etcd集群的多个应用程序服务的每台机器上运行。\n\n## 什么时候不该使用etcd网关\n\n* 提升性能\n该网关不是为提高etcd群集性能而设计的。 它不提供缓存，监视合并或批处理。 etcd团队正在开发一种缓存代理，旨在提高群集的可伸缩性。\n* 在集群管理系统运行时\n像Kubernetes这样的高级集群管理系统本身就支持服务发现。 应用程序可以使用系统管理的DNS名称或虚拟IP地址访问etcd集群。 例如，kube-proxy等效于etcd网关。\n## 启动etcd网关\n* * *\n考虑一个具有以下静态端点的etcd集群：\n\n|名字|地址|主机名|\n|---|---|---|\n|infra0|10.0.1.10|infra0.example.com|\n|infra1|10.0.1.11|infra1.example.com|\n|infra2|10.0.1.12|infra2.example.com|\n\n通过以下命令使用静态端点启动etcd网关:\n```\n$ etcd gateway start --endpoints=infra0.example.com,infra1.example.com,infra2.example.com\n2016-08-16 11:21:18.867350 I | tcpproxy: ready to proxy client requests to [...]\n```\n或者，如果使用DNS进行服务发现，请考虑DNS SRV条目：\n```\n$ dig +noall +answer SRV _etcd-client._tcp.example.com\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.\n```\n```\n$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com\ninfra0.example.com.  300  IN  A  10.0.1.10\ninfra1.example.com.  300  IN  A  10.0.1.11\ninfra2.example.com.  300  IN  A  10.0.1.12\n```\n启动etcd网关，以使用以下命令从DNS SRV条目中获取端点：\n```\n$ etcd gateway start --discovery-srv=example.com\n2016-08-16 11:21:18.867350 I | tcpproxy: ready to proxy client requests to [...]\n```\n\n## 配置参数\n* * *\n#### **etcd 集群**\n\n**--endpoints**\n\n* 以逗号分隔的用于转发客户端连接的etcd服务器目标列表。\n* 默认：`127.0.0.1:2379`\n* 无效的例子:`https://127.0.0.1:2379`(网关不适用于TLS 终端)\n\n**--discovery-srv**\n\n* 用于通过SRV记录引导群集终结点的DNS域。\n* 默认值：未设置\n\n#### **网络**\n**--listen-addr**\n\n* 接收客户端请求绑定的接口和端口\n* 默认:`127.0.0.1:23790`\n\n**--retry-delay**\n\n* 重试连接到失败的端点之前的延迟时间。\n* 默认值：1m0s\n* 无效例子：\"123\"(期望之外的时间格式)\n\n#### **安全**\n**--insecure-discovery**\n\n* 接受不安全或容易受到中间人攻击的SRV记录。\n* 默认值：false\n\n**--trusted-ca-file**\n\n* etcd集群的客户端TLS CA文件的路径。 用于认证端点。\n* 默认值：未设置","source":"_posts/blog/etcd/etcd网关.md","raw":"---\ntitle: etcd网关\ndate: 2019-11-24 15:24:56\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址:[L4 gateway](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/gateway.md)\n## 什么是etcd网关\n* * *\netcd网关是一个简单的TCP代理，可将网络数据转发到etcd集群。网关是无状态且透明的； 它既不会检查客户端请求，也不会干扰群集响应。\n网关支持多个etcd服务器端点，并采用简单的循环策略。 它仅路由到可用端点，并向其客户端隐藏故障。 将来可能会支持其他重试策略，例如加权轮询。\n\n## 什么时候使用etcd网关\n* * *\n每个访问etcd的应用程序必须首先具有etcd集群客户端端点的地址。 如果同一服务器上的多个应用程序访问相同的etcd集群，则每个应用程序仍需要知道etcd集群的广播的客户端端点。 如果将etcd集群重新配置为具有不同的端点，则每个应用程序可能还需要更新其端点列表。 这种大规模的重新配置既乏味又容易出错。\netcd网关通过充当稳定的本地端点来解决此问题。 典型的etcd网关配置是，每台计算机运行一台网关，侦听本地地址，并且每个etcd应用程序都连接到其本地网关。 结果只是网关需要更新其端点，而不是更新每个应用程序。\n总之，为了自动传播集群端点更改，etcd网关在为访问同一etcd集群的多个应用程序服务的每台机器上运行。\n\n## 什么时候不该使用etcd网关\n\n* 提升性能\n该网关不是为提高etcd群集性能而设计的。 它不提供缓存，监视合并或批处理。 etcd团队正在开发一种缓存代理，旨在提高群集的可伸缩性。\n* 在集群管理系统运行时\n像Kubernetes这样的高级集群管理系统本身就支持服务发现。 应用程序可以使用系统管理的DNS名称或虚拟IP地址访问etcd集群。 例如，kube-proxy等效于etcd网关。\n## 启动etcd网关\n* * *\n考虑一个具有以下静态端点的etcd集群：\n\n|名字|地址|主机名|\n|---|---|---|\n|infra0|10.0.1.10|infra0.example.com|\n|infra1|10.0.1.11|infra1.example.com|\n|infra2|10.0.1.12|infra2.example.com|\n\n通过以下命令使用静态端点启动etcd网关:\n```\n$ etcd gateway start --endpoints=infra0.example.com,infra1.example.com,infra2.example.com\n2016-08-16 11:21:18.867350 I | tcpproxy: ready to proxy client requests to [...]\n```\n或者，如果使用DNS进行服务发现，请考虑DNS SRV条目：\n```\n$ dig +noall +answer SRV _etcd-client._tcp.example.com\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.\n```\n```\n$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com\ninfra0.example.com.  300  IN  A  10.0.1.10\ninfra1.example.com.  300  IN  A  10.0.1.11\ninfra2.example.com.  300  IN  A  10.0.1.12\n```\n启动etcd网关，以使用以下命令从DNS SRV条目中获取端点：\n```\n$ etcd gateway start --discovery-srv=example.com\n2016-08-16 11:21:18.867350 I | tcpproxy: ready to proxy client requests to [...]\n```\n\n## 配置参数\n* * *\n#### **etcd 集群**\n\n**--endpoints**\n\n* 以逗号分隔的用于转发客户端连接的etcd服务器目标列表。\n* 默认：`127.0.0.1:2379`\n* 无效的例子:`https://127.0.0.1:2379`(网关不适用于TLS 终端)\n\n**--discovery-srv**\n\n* 用于通过SRV记录引导群集终结点的DNS域。\n* 默认值：未设置\n\n#### **网络**\n**--listen-addr**\n\n* 接收客户端请求绑定的接口和端口\n* 默认:`127.0.0.1:23790`\n\n**--retry-delay**\n\n* 重试连接到失败的端点之前的延迟时间。\n* 默认值：1m0s\n* 无效例子：\"123\"(期望之外的时间格式)\n\n#### **安全**\n**--insecure-discovery**\n\n* 接受不安全或容易受到中间人攻击的SRV记录。\n* 默认值：false\n\n**--trusted-ca-file**\n\n* etcd集群的客户端TLS CA文件的路径。 用于认证端点。\n* 默认值：未设置","slug":"blog/etcd/etcd网关","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyh2001dk0vqaarz98db","content":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/gateway.md\" target=\"_blank\" rel=\"noopener\">L4 gateway</a></p>\n<h2 id=\"什么是etcd网关\"><a href=\"#什么是etcd网关\" class=\"headerlink\" title=\"什么是etcd网关\"></a>什么是etcd网关</h2><hr>\n<p>etcd网关是一个简单的TCP代理，可将网络数据转发到etcd集群。网关是无状态且透明的； 它既不会检查客户端请求，也不会干扰群集响应。<br>网关支持多个etcd服务器端点，并采用简单的循环策略。 它仅路由到可用端点，并向其客户端隐藏故障。 将来可能会支持其他重试策略，例如加权轮询。</p>\n<h2 id=\"什么时候使用etcd网关\"><a href=\"#什么时候使用etcd网关\" class=\"headerlink\" title=\"什么时候使用etcd网关\"></a>什么时候使用etcd网关</h2><hr>\n<p>每个访问etcd的应用程序必须首先具有etcd集群客户端端点的地址。 如果同一服务器上的多个应用程序访问相同的etcd集群，则每个应用程序仍需要知道etcd集群的广播的客户端端点。 如果将etcd集群重新配置为具有不同的端点，则每个应用程序可能还需要更新其端点列表。 这种大规模的重新配置既乏味又容易出错。<br>etcd网关通过充当稳定的本地端点来解决此问题。 典型的etcd网关配置是，每台计算机运行一台网关，侦听本地地址，并且每个etcd应用程序都连接到其本地网关。 结果只是网关需要更新其端点，而不是更新每个应用程序。<br>总之，为了自动传播集群端点更改，etcd网关在为访问同一etcd集群的多个应用程序服务的每台机器上运行。</p>\n<h2 id=\"什么时候不该使用etcd网关\"><a href=\"#什么时候不该使用etcd网关\" class=\"headerlink\" title=\"什么时候不该使用etcd网关\"></a>什么时候不该使用etcd网关</h2><ul>\n<li>提升性能<br>该网关不是为提高etcd群集性能而设计的。 它不提供缓存，监视合并或批处理。 etcd团队正在开发一种缓存代理，旨在提高群集的可伸缩性。</li>\n<li>在集群管理系统运行时<br>像Kubernetes这样的高级集群管理系统本身就支持服务发现。 应用程序可以使用系统管理的DNS名称或虚拟IP地址访问etcd集群。 例如，kube-proxy等效于etcd网关。<h2 id=\"启动etcd网关\"><a href=\"#启动etcd网关\" class=\"headerlink\" title=\"启动etcd网关\"></a>启动etcd网关</h2></li>\n</ul>\n<hr>\n<p>考虑一个具有以下静态端点的etcd集群：</p>\n<table>\n<thead>\n<tr>\n<th>名字</th>\n<th>地址</th>\n<th>主机名</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>infra0</td>\n<td>10.0.1.10</td>\n<td>infra0.example.com</td>\n</tr>\n<tr>\n<td>infra1</td>\n<td>10.0.1.11</td>\n<td>infra1.example.com</td>\n</tr>\n<tr>\n<td>infra2</td>\n<td>10.0.1.12</td>\n<td>infra2.example.com</td>\n</tr>\n</tbody></table>\n<p>通过以下命令使用静态端点启动etcd网关:</p>\n<pre><code>$ etcd gateway start --endpoints=infra0.example.com,infra1.example.com,infra2.example.com\n2016-08-16 11:21:18.867350 I | tcpproxy: ready to proxy client requests to [...]</code></pre><p>或者，如果使用DNS进行服务发现，请考虑DNS SRV条目：</p>\n<pre><code>$ dig +noall +answer SRV _etcd-client._tcp.example.com\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.</code></pre><pre><code>$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com\ninfra0.example.com.  300  IN  A  10.0.1.10\ninfra1.example.com.  300  IN  A  10.0.1.11\ninfra2.example.com.  300  IN  A  10.0.1.12</code></pre><p>启动etcd网关，以使用以下命令从DNS SRV条目中获取端点：</p>\n<pre><code>$ etcd gateway start --discovery-srv=example.com\n2016-08-16 11:21:18.867350 I | tcpproxy: ready to proxy client requests to [...]</code></pre><h2 id=\"配置参数\"><a href=\"#配置参数\" class=\"headerlink\" title=\"配置参数\"></a>配置参数</h2><hr>\n<h4 id=\"etcd-集群\"><a href=\"#etcd-集群\" class=\"headerlink\" title=\"etcd 集群\"></a><strong>etcd 集群</strong></h4><p><strong>–endpoints</strong></p>\n<ul>\n<li>以逗号分隔的用于转发客户端连接的etcd服务器目标列表。</li>\n<li>默认：<code>127.0.0.1:2379</code></li>\n<li>无效的例子:<code>https://127.0.0.1:2379</code>(网关不适用于TLS 终端)</li>\n</ul>\n<p><strong>–discovery-srv</strong></p>\n<ul>\n<li>用于通过SRV记录引导群集终结点的DNS域。</li>\n<li>默认值：未设置</li>\n</ul>\n<h4 id=\"网络\"><a href=\"#网络\" class=\"headerlink\" title=\"网络\"></a><strong>网络</strong></h4><p><strong>–listen-addr</strong></p>\n<ul>\n<li>接收客户端请求绑定的接口和端口</li>\n<li>默认:<code>127.0.0.1:23790</code></li>\n</ul>\n<p><strong>–retry-delay</strong></p>\n<ul>\n<li>重试连接到失败的端点之前的延迟时间。</li>\n<li>默认值：1m0s</li>\n<li>无效例子：”123”(期望之外的时间格式)</li>\n</ul>\n<h4 id=\"安全\"><a href=\"#安全\" class=\"headerlink\" title=\"安全\"></a><strong>安全</strong></h4><p><strong>–insecure-discovery</strong></p>\n<ul>\n<li>接受不安全或容易受到中间人攻击的SRV记录。</li>\n<li>默认值：false</li>\n</ul>\n<p><strong>–trusted-ca-file</strong></p>\n<ul>\n<li>etcd集群的客户端TLS CA文件的路径。 用于认证端点。</li>\n<li>默认值：未设置</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/gateway.md\" target=\"_blank\" rel=\"noopener\">L4 gateway</a></p>\n<h2 id=\"什么是etcd网关\"><a href=\"#什么是etcd网关\" class=\"headerlink\" title=\"什么是etcd网关\"></a>什么是etcd网关</h2><hr>\n<p>etcd网关是一个简单的TCP代理，可将网络数据转发到etcd集群。网关是无状态且透明的； 它既不会检查客户端请求，也不会干扰群集响应。<br>网关支持多个etcd服务器端点，并采用简单的循环策略。 它仅路由到可用端点，并向其客户端隐藏故障。 将来可能会支持其他重试策略，例如加权轮询。</p>\n<h2 id=\"什么时候使用etcd网关\"><a href=\"#什么时候使用etcd网关\" class=\"headerlink\" title=\"什么时候使用etcd网关\"></a>什么时候使用etcd网关</h2><hr>\n<p>每个访问etcd的应用程序必须首先具有etcd集群客户端端点的地址。 如果同一服务器上的多个应用程序访问相同的etcd集群，则每个应用程序仍需要知道etcd集群的广播的客户端端点。 如果将etcd集群重新配置为具有不同的端点，则每个应用程序可能还需要更新其端点列表。 这种大规模的重新配置既乏味又容易出错。<br>etcd网关通过充当稳定的本地端点来解决此问题。 典型的etcd网关配置是，每台计算机运行一台网关，侦听本地地址，并且每个etcd应用程序都连接到其本地网关。 结果只是网关需要更新其端点，而不是更新每个应用程序。<br>总之，为了自动传播集群端点更改，etcd网关在为访问同一etcd集群的多个应用程序服务的每台机器上运行。</p>\n<h2 id=\"什么时候不该使用etcd网关\"><a href=\"#什么时候不该使用etcd网关\" class=\"headerlink\" title=\"什么时候不该使用etcd网关\"></a>什么时候不该使用etcd网关</h2><ul>\n<li>提升性能<br>该网关不是为提高etcd群集性能而设计的。 它不提供缓存，监视合并或批处理。 etcd团队正在开发一种缓存代理，旨在提高群集的可伸缩性。</li>\n<li>在集群管理系统运行时<br>像Kubernetes这样的高级集群管理系统本身就支持服务发现。 应用程序可以使用系统管理的DNS名称或虚拟IP地址访问etcd集群。 例如，kube-proxy等效于etcd网关。<h2 id=\"启动etcd网关\"><a href=\"#启动etcd网关\" class=\"headerlink\" title=\"启动etcd网关\"></a>启动etcd网关</h2></li>\n</ul>\n<hr>\n<p>考虑一个具有以下静态端点的etcd集群：</p>\n<table>\n<thead>\n<tr>\n<th>名字</th>\n<th>地址</th>\n<th>主机名</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>infra0</td>\n<td>10.0.1.10</td>\n<td>infra0.example.com</td>\n</tr>\n<tr>\n<td>infra1</td>\n<td>10.0.1.11</td>\n<td>infra1.example.com</td>\n</tr>\n<tr>\n<td>infra2</td>\n<td>10.0.1.12</td>\n<td>infra2.example.com</td>\n</tr>\n</tbody></table>\n<p>通过以下命令使用静态端点启动etcd网关:</p>\n<pre><code>$ etcd gateway start --endpoints=infra0.example.com,infra1.example.com,infra2.example.com\n2016-08-16 11:21:18.867350 I | tcpproxy: ready to proxy client requests to [...]</code></pre><p>或者，如果使用DNS进行服务发现，请考虑DNS SRV条目：</p>\n<pre><code>$ dig +noall +answer SRV _etcd-client._tcp.example.com\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.</code></pre><pre><code>$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com\ninfra0.example.com.  300  IN  A  10.0.1.10\ninfra1.example.com.  300  IN  A  10.0.1.11\ninfra2.example.com.  300  IN  A  10.0.1.12</code></pre><p>启动etcd网关，以使用以下命令从DNS SRV条目中获取端点：</p>\n<pre><code>$ etcd gateway start --discovery-srv=example.com\n2016-08-16 11:21:18.867350 I | tcpproxy: ready to proxy client requests to [...]</code></pre><h2 id=\"配置参数\"><a href=\"#配置参数\" class=\"headerlink\" title=\"配置参数\"></a>配置参数</h2><hr>\n<h4 id=\"etcd-集群\"><a href=\"#etcd-集群\" class=\"headerlink\" title=\"etcd 集群\"></a><strong>etcd 集群</strong></h4><p><strong>–endpoints</strong></p>\n<ul>\n<li>以逗号分隔的用于转发客户端连接的etcd服务器目标列表。</li>\n<li>默认：<code>127.0.0.1:2379</code></li>\n<li>无效的例子:<code>https://127.0.0.1:2379</code>(网关不适用于TLS 终端)</li>\n</ul>\n<p><strong>–discovery-srv</strong></p>\n<ul>\n<li>用于通过SRV记录引导群集终结点的DNS域。</li>\n<li>默认值：未设置</li>\n</ul>\n<h4 id=\"网络\"><a href=\"#网络\" class=\"headerlink\" title=\"网络\"></a><strong>网络</strong></h4><p><strong>–listen-addr</strong></p>\n<ul>\n<li>接收客户端请求绑定的接口和端口</li>\n<li>默认:<code>127.0.0.1:23790</code></li>\n</ul>\n<p><strong>–retry-delay</strong></p>\n<ul>\n<li>重试连接到失败的端点之前的延迟时间。</li>\n<li>默认值：1m0s</li>\n<li>无效例子：”123”(期望之外的时间格式)</li>\n</ul>\n<h4 id=\"安全\"><a href=\"#安全\" class=\"headerlink\" title=\"安全\"></a><strong>安全</strong></h4><p><strong>–insecure-discovery</strong></p>\n<ul>\n<li>接受不安全或容易受到中间人攻击的SRV记录。</li>\n<li>默认值：false</li>\n</ul>\n<p><strong>–trusted-ca-file</strong></p>\n<ul>\n<li>etcd集群的客户端TLS CA文件的路径。 用于认证端点。</li>\n<li>默认值：未设置</li>\n</ul>\n"},{"title":"HTTP_JSON_API通过gRPC网关","date":"2019-11-23T04:32:38.000Z","_content":"原文地址:[HTTP JSON API through the gRPC gateway](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_grpc_gateway.md)\netcd v3 使用 gRPC 作为消息协议。etcd项目包括一个基于gRPC的[Go客户端](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%AE%A2%E6%88%B7%E7%AB%AFv3/)和一个命令行工具，[etcdctl](https://github.com/etcd-io/etcd/tree/master/etcdctl),通过gRPC与etcd集群进行交互.对于没有gRPC支持的语言，etcd提供JSON [gRPC网关](https://github.com/grpc-ecosystem/grpc-gateway)，这个网关提供一个RESTful风格的代理可以将HTTP/JSON请求转换为gRPC消息。\n### 使用 gRPC网关\n这个网关接受一个到etcd's buffer协议消息定义的JSON格式的映射,注意`Key`和`Value`字段定义为byte 数组，因此JSON必须使用base64编码,下面的例子使用`curl`,但是每个HTTP/JSON客户端的工作原理都和例子相同。\n**注意**\ngRPC网关节点从etcd v3.3发生变化：\n\n* etcd v3.2以及之前版本只使用`[CLIENT-URL]/v3alpha/*`。\n* etcd v3.3使用`[CLIENT-URL]/v3beta/*`保持`[CLIENT-URL]/v3alpha/*`使用。\n* etcd v3.4使用`[CLIENT-URL]/v3/*`保持`[CLIENT-URL]/v3beta/*`使用。\n    * `[CLIENT-URL]/v3alpha/*`被抛弃使用。\n* etcd v3.5以及最新版本只使用`[CLIENT-URL]/v3/*`。\n    * `[CLIENT-URL]/v3beta/*`被抛弃使用。\n\n### 存储和获取Keys\n使用`/v3/kv/range`和`/v3/kv/put`服务读和写Keys:\n```\n<<COMMENThttps://www.base64encode.org/foo is 'Zm9v' in Base64bar is 'YmFy'COMMENT\n\ncurl -L http://localhost:2379/v3/kv/put \\\n  -X POST -d '{\"key\": \"Zm9v\", \"value\": \"YmFy\"}'# {\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"2\",\"raft_term\":\"3\"}}\n\ncurl -L http://localhost:2379/v3/kv/range \\\n  -X POST -d '{\"key\": \"Zm9v\"}'# {\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"2\",\"raft_term\":\"3\"},\"kvs\":[{\"key\":\"Zm9v\",\"create_revision\":\"2\",\"mod_revision\":\"2\",\"version\":\"1\",\"value\":\"YmFy\"}],\"count\":\"1\"}\n\n# get all keys prefixed with \"foo\"\ncurl -L http://localhost:2379/v3/kv/range \\\n  -X POST -d '{\"key\": \"Zm9v\", \"range_end\": \"Zm9w\"}'# {\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"2\",\"raft_term\":\"3\"},\"kvs\":[{\"key\":\"Zm9v\",\"create_revision\":\"2\",\"mod_revision\":\"2\",\"version\":\"1\",\"value\":\"YmFy\"}],\"count\":\"1\"}\n```\n### 查看 Keys\n使用`/v3/watch`服务查看Keys:\n```\ncurl -N http://localhost:2379/v3/watch \\\n  -X POST -d '{\"create_request\": {\"key\":\"Zm9v\"} }' &# {\"result\":{\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"1\",\"raft_term\":\"2\"},\"created\":true}}\n\ncurl -L http://localhost:2379/v3/kv/put \\\n  -X POST -d '{\"key\": \"Zm9v\", \"value\": \"YmFy\"}' >/dev/null 2>&1# {\"result\":{\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"2\",\"raft_term\":\"2\"},\"events\":[{\"kv\":{\"key\":\"Zm9v\",\"create_revision\":\"2\",\"mod_revision\":\"2\",\"version\":\"1\",\"value\":\"YmFy\"}}]}}\n```\n### 交易\n使用``/v3/kv/txn`发行一个交易：\n```\n# 目标创建\ncurl -L http://localhost:2379/v3/kv/txn \\\n  -X POST \\\n  -d '{\"compare\":[{\"target\":\"CREATE\",\"key\":\"Zm9v\",\"createRevision\":\"2\"}],\"success\":[{\"requestPut\":{\"key\":\"Zm9v\",\"value\":\"YmFy\"}}]}'# {\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"3\",\"raft_term\":\"2\"},\"succeeded\":true,\"responses\":[{\"response_put\":{\"header\":{\"revision\":\"3\"}}}]}\n```\n```\n# 目标版本\ncurl -L http://localhost:2379/v3/kv/txn \\\n  -X POST \\\n  -d '{\"compare\":[{\"version\":\"4\",\"result\":\"EQUAL\",\"target\":\"VERSION\",\"key\":\"Zm9v\"}],\"success\":[{\"requestRange\":{\"key\":\"Zm9v\"}}]}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"6\",\"raft_term\":\"3\"},\"succeeded\":true,\"responses\":[{\"response_range\":{\"header\":{\"revision\":\"6\"},\"kvs\":[{\"key\":\"Zm9v\",\"create_revision\":\"2\",\"mod_revision\":\"6\",\"version\":\"4\",\"value\":\"YmF6\"}],\"count\":\"1\"}}]}\n```\n### 权限\n使用`/v3/auth`设置权限：\n```\n# 创建root用户\ncurl -L http://localhost:2379/v3/auth/user/add \\\n  -X POST -d '{\"name\": \"root\", \"password\": \"pass\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"}}\n\n# 创建root角色\ncurl -L http://localhost:2379/v3/auth/role/add \\\n  -X POST -d '{\"name\": \"root\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"}}\n\n# 授予root角色\ncurl -L http://localhost:2379/v3/auth/user/grant \\\n  -X POST -d '{\"user\": \"root\", \"role\": \"root\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"}}\n\n# 开启认证\ncurl -L http://localhost:2379/v3/auth/enable -X POST -d '{}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"}}\n```\n通过`/v3/auth/authenticate`服务使用一个认证令牌进行认证:\n```\n# 为根用户获取认证令牌\ncurl -L http://localhost:2379/v3/auth/authenticate \\\n  -X POST -d '{\"name\": \"root\", \"password\": \"pass\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"},\"token\":\"sssvIpwfnLAcWAQH.9\"}\n```\n使用认证证书设置认证头部到认证令牌获取Keys：\n```\ncurl -L http://localhost:2379/v3/kv/put \\\n  -H 'Authorization : sssvIpwfnLAcWAQH.9' \\\n  -X POST -d '{\"key\": \"Zm9v\", \"value\": \"YmFy\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"2\",\"raft_term\":\"2\"}}\n```","source":"_posts/blog/etcd/HTTP_JSON_API通过gRPC网关.md","raw":"---\ntitle: HTTP_JSON_API通过gRPC网关\ndate: 2019-11-23 12:32:38\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址:[HTTP JSON API through the gRPC gateway](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_grpc_gateway.md)\netcd v3 使用 gRPC 作为消息协议。etcd项目包括一个基于gRPC的[Go客户端](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%AE%A2%E6%88%B7%E7%AB%AFv3/)和一个命令行工具，[etcdctl](https://github.com/etcd-io/etcd/tree/master/etcdctl),通过gRPC与etcd集群进行交互.对于没有gRPC支持的语言，etcd提供JSON [gRPC网关](https://github.com/grpc-ecosystem/grpc-gateway)，这个网关提供一个RESTful风格的代理可以将HTTP/JSON请求转换为gRPC消息。\n### 使用 gRPC网关\n这个网关接受一个到etcd's buffer协议消息定义的JSON格式的映射,注意`Key`和`Value`字段定义为byte 数组，因此JSON必须使用base64编码,下面的例子使用`curl`,但是每个HTTP/JSON客户端的工作原理都和例子相同。\n**注意**\ngRPC网关节点从etcd v3.3发生变化：\n\n* etcd v3.2以及之前版本只使用`[CLIENT-URL]/v3alpha/*`。\n* etcd v3.3使用`[CLIENT-URL]/v3beta/*`保持`[CLIENT-URL]/v3alpha/*`使用。\n* etcd v3.4使用`[CLIENT-URL]/v3/*`保持`[CLIENT-URL]/v3beta/*`使用。\n    * `[CLIENT-URL]/v3alpha/*`被抛弃使用。\n* etcd v3.5以及最新版本只使用`[CLIENT-URL]/v3/*`。\n    * `[CLIENT-URL]/v3beta/*`被抛弃使用。\n\n### 存储和获取Keys\n使用`/v3/kv/range`和`/v3/kv/put`服务读和写Keys:\n```\n<<COMMENThttps://www.base64encode.org/foo is 'Zm9v' in Base64bar is 'YmFy'COMMENT\n\ncurl -L http://localhost:2379/v3/kv/put \\\n  -X POST -d '{\"key\": \"Zm9v\", \"value\": \"YmFy\"}'# {\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"2\",\"raft_term\":\"3\"}}\n\ncurl -L http://localhost:2379/v3/kv/range \\\n  -X POST -d '{\"key\": \"Zm9v\"}'# {\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"2\",\"raft_term\":\"3\"},\"kvs\":[{\"key\":\"Zm9v\",\"create_revision\":\"2\",\"mod_revision\":\"2\",\"version\":\"1\",\"value\":\"YmFy\"}],\"count\":\"1\"}\n\n# get all keys prefixed with \"foo\"\ncurl -L http://localhost:2379/v3/kv/range \\\n  -X POST -d '{\"key\": \"Zm9v\", \"range_end\": \"Zm9w\"}'# {\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"2\",\"raft_term\":\"3\"},\"kvs\":[{\"key\":\"Zm9v\",\"create_revision\":\"2\",\"mod_revision\":\"2\",\"version\":\"1\",\"value\":\"YmFy\"}],\"count\":\"1\"}\n```\n### 查看 Keys\n使用`/v3/watch`服务查看Keys:\n```\ncurl -N http://localhost:2379/v3/watch \\\n  -X POST -d '{\"create_request\": {\"key\":\"Zm9v\"} }' &# {\"result\":{\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"1\",\"raft_term\":\"2\"},\"created\":true}}\n\ncurl -L http://localhost:2379/v3/kv/put \\\n  -X POST -d '{\"key\": \"Zm9v\", \"value\": \"YmFy\"}' >/dev/null 2>&1# {\"result\":{\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"2\",\"raft_term\":\"2\"},\"events\":[{\"kv\":{\"key\":\"Zm9v\",\"create_revision\":\"2\",\"mod_revision\":\"2\",\"version\":\"1\",\"value\":\"YmFy\"}}]}}\n```\n### 交易\n使用``/v3/kv/txn`发行一个交易：\n```\n# 目标创建\ncurl -L http://localhost:2379/v3/kv/txn \\\n  -X POST \\\n  -d '{\"compare\":[{\"target\":\"CREATE\",\"key\":\"Zm9v\",\"createRevision\":\"2\"}],\"success\":[{\"requestPut\":{\"key\":\"Zm9v\",\"value\":\"YmFy\"}}]}'# {\"header\":{\"cluster_id\":\"12585971608760269493\",\"member_id\":\"13847567121247652255\",\"revision\":\"3\",\"raft_term\":\"2\"},\"succeeded\":true,\"responses\":[{\"response_put\":{\"header\":{\"revision\":\"3\"}}}]}\n```\n```\n# 目标版本\ncurl -L http://localhost:2379/v3/kv/txn \\\n  -X POST \\\n  -d '{\"compare\":[{\"version\":\"4\",\"result\":\"EQUAL\",\"target\":\"VERSION\",\"key\":\"Zm9v\"}],\"success\":[{\"requestRange\":{\"key\":\"Zm9v\"}}]}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"6\",\"raft_term\":\"3\"},\"succeeded\":true,\"responses\":[{\"response_range\":{\"header\":{\"revision\":\"6\"},\"kvs\":[{\"key\":\"Zm9v\",\"create_revision\":\"2\",\"mod_revision\":\"6\",\"version\":\"4\",\"value\":\"YmF6\"}],\"count\":\"1\"}}]}\n```\n### 权限\n使用`/v3/auth`设置权限：\n```\n# 创建root用户\ncurl -L http://localhost:2379/v3/auth/user/add \\\n  -X POST -d '{\"name\": \"root\", \"password\": \"pass\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"}}\n\n# 创建root角色\ncurl -L http://localhost:2379/v3/auth/role/add \\\n  -X POST -d '{\"name\": \"root\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"}}\n\n# 授予root角色\ncurl -L http://localhost:2379/v3/auth/user/grant \\\n  -X POST -d '{\"user\": \"root\", \"role\": \"root\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"}}\n\n# 开启认证\ncurl -L http://localhost:2379/v3/auth/enable -X POST -d '{}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"}}\n```\n通过`/v3/auth/authenticate`服务使用一个认证令牌进行认证:\n```\n# 为根用户获取认证令牌\ncurl -L http://localhost:2379/v3/auth/authenticate \\\n  -X POST -d '{\"name\": \"root\", \"password\": \"pass\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"1\",\"raft_term\":\"2\"},\"token\":\"sssvIpwfnLAcWAQH.9\"}\n```\n使用认证证书设置认证头部到认证令牌获取Keys：\n```\ncurl -L http://localhost:2379/v3/kv/put \\\n  -H 'Authorization : sssvIpwfnLAcWAQH.9' \\\n  -X POST -d '{\"key\": \"Zm9v\", \"value\": \"YmFy\"}'# {\"header\":{\"cluster_id\":\"14841639068965178418\",\"member_id\":\"10276657743932975437\",\"revision\":\"2\",\"raft_term\":\"2\"}}\n```","slug":"blog/etcd/HTTP_JSON_API通过gRPC网关","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyh4001hk0vq09t6g8d2","content":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_grpc_gateway.md\" target=\"_blank\" rel=\"noopener\">HTTP JSON API through the gRPC gateway</a><br>etcd v3 使用 gRPC 作为消息协议。etcd项目包括一个基于gRPC的<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%AE%A2%E6%88%B7%E7%AB%AFv3/\">Go客户端</a>和一个命令行工具，<a href=\"https://github.com/etcd-io/etcd/tree/master/etcdctl\" target=\"_blank\" rel=\"noopener\">etcdctl</a>,通过gRPC与etcd集群进行交互.对于没有gRPC支持的语言，etcd提供JSON <a href=\"https://github.com/grpc-ecosystem/grpc-gateway\" target=\"_blank\" rel=\"noopener\">gRPC网关</a>，这个网关提供一个RESTful风格的代理可以将HTTP/JSON请求转换为gRPC消息。</p>\n<h3 id=\"使用-gRPC网关\"><a href=\"#使用-gRPC网关\" class=\"headerlink\" title=\"使用 gRPC网关\"></a>使用 gRPC网关</h3><p>这个网关接受一个到etcd’s buffer协议消息定义的JSON格式的映射,注意<code>Key</code>和<code>Value</code>字段定义为byte 数组，因此JSON必须使用base64编码,下面的例子使用<code>curl</code>,但是每个HTTP/JSON客户端的工作原理都和例子相同。<br><strong>注意</strong><br>gRPC网关节点从etcd v3.3发生变化：</p>\n<ul>\n<li>etcd v3.2以及之前版本只使用<code>[CLIENT-URL]/v3alpha/*</code>。</li>\n<li>etcd v3.3使用<code>[CLIENT-URL]/v3beta/*</code>保持<code>[CLIENT-URL]/v3alpha/*</code>使用。</li>\n<li>etcd v3.4使用<code>[CLIENT-URL]/v3/*</code>保持<code>[CLIENT-URL]/v3beta/*</code>使用。<ul>\n<li><code>[CLIENT-URL]/v3alpha/*</code>被抛弃使用。</li>\n</ul>\n</li>\n<li>etcd v3.5以及最新版本只使用<code>[CLIENT-URL]/v3/*</code>。<ul>\n<li><code>[CLIENT-URL]/v3beta/*</code>被抛弃使用。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"存储和获取Keys\"><a href=\"#存储和获取Keys\" class=\"headerlink\" title=\"存储和获取Keys\"></a>存储和获取Keys</h3><p>使用<code>/v3/kv/range</code>和<code>/v3/kv/put</code>服务读和写Keys:</p>\n<pre><code>&lt;&lt;COMMENThttps://www.base64encode.org/foo is &#39;Zm9v&#39; in Base64bar is &#39;YmFy&#39;COMMENT\n\ncurl -L http://localhost:2379/v3/kv/put \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;, &quot;value&quot;: &quot;YmFy&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;3&quot;}}\n\ncurl -L http://localhost:2379/v3/kv/range \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;3&quot;},&quot;kvs&quot;:[{&quot;key&quot;:&quot;Zm9v&quot;,&quot;create_revision&quot;:&quot;2&quot;,&quot;mod_revision&quot;:&quot;2&quot;,&quot;version&quot;:&quot;1&quot;,&quot;value&quot;:&quot;YmFy&quot;}],&quot;count&quot;:&quot;1&quot;}\n\n# get all keys prefixed with &quot;foo&quot;\ncurl -L http://localhost:2379/v3/kv/range \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;, &quot;range_end&quot;: &quot;Zm9w&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;3&quot;},&quot;kvs&quot;:[{&quot;key&quot;:&quot;Zm9v&quot;,&quot;create_revision&quot;:&quot;2&quot;,&quot;mod_revision&quot;:&quot;2&quot;,&quot;version&quot;:&quot;1&quot;,&quot;value&quot;:&quot;YmFy&quot;}],&quot;count&quot;:&quot;1&quot;}</code></pre><h3 id=\"查看-Keys\"><a href=\"#查看-Keys\" class=\"headerlink\" title=\"查看 Keys\"></a>查看 Keys</h3><p>使用<code>/v3/watch</code>服务查看Keys:</p>\n<pre><code>curl -N http://localhost:2379/v3/watch \\\n  -X POST -d &#39;{&quot;create_request&quot;: {&quot;key&quot;:&quot;Zm9v&quot;} }&#39; &amp;# {&quot;result&quot;:{&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;},&quot;created&quot;:true}}\n\ncurl -L http://localhost:2379/v3/kv/put \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;, &quot;value&quot;: &quot;YmFy&quot;}&#39; &gt;/dev/null 2&gt;&amp;1# {&quot;result&quot;:{&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;2&quot;},&quot;events&quot;:[{&quot;kv&quot;:{&quot;key&quot;:&quot;Zm9v&quot;,&quot;create_revision&quot;:&quot;2&quot;,&quot;mod_revision&quot;:&quot;2&quot;,&quot;version&quot;:&quot;1&quot;,&quot;value&quot;:&quot;YmFy&quot;}}]}}</code></pre><h3 id=\"交易\"><a href=\"#交易\" class=\"headerlink\" title=\"交易\"></a>交易</h3><p>使用``/v3/kv/txn`发行一个交易：</p>\n<pre><code># 目标创建\ncurl -L http://localhost:2379/v3/kv/txn \\\n  -X POST \\\n  -d &#39;{&quot;compare&quot;:[{&quot;target&quot;:&quot;CREATE&quot;,&quot;key&quot;:&quot;Zm9v&quot;,&quot;createRevision&quot;:&quot;2&quot;}],&quot;success&quot;:[{&quot;requestPut&quot;:{&quot;key&quot;:&quot;Zm9v&quot;,&quot;value&quot;:&quot;YmFy&quot;}}]}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;3&quot;,&quot;raft_term&quot;:&quot;2&quot;},&quot;succeeded&quot;:true,&quot;responses&quot;:[{&quot;response_put&quot;:{&quot;header&quot;:{&quot;revision&quot;:&quot;3&quot;}}}]}</code></pre><pre><code># 目标版本\ncurl -L http://localhost:2379/v3/kv/txn \\\n  -X POST \\\n  -d &#39;{&quot;compare&quot;:[{&quot;version&quot;:&quot;4&quot;,&quot;result&quot;:&quot;EQUAL&quot;,&quot;target&quot;:&quot;VERSION&quot;,&quot;key&quot;:&quot;Zm9v&quot;}],&quot;success&quot;:[{&quot;requestRange&quot;:{&quot;key&quot;:&quot;Zm9v&quot;}}]}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;6&quot;,&quot;raft_term&quot;:&quot;3&quot;},&quot;succeeded&quot;:true,&quot;responses&quot;:[{&quot;response_range&quot;:{&quot;header&quot;:{&quot;revision&quot;:&quot;6&quot;},&quot;kvs&quot;:[{&quot;key&quot;:&quot;Zm9v&quot;,&quot;create_revision&quot;:&quot;2&quot;,&quot;mod_revision&quot;:&quot;6&quot;,&quot;version&quot;:&quot;4&quot;,&quot;value&quot;:&quot;YmF6&quot;}],&quot;count&quot;:&quot;1&quot;}}]}</code></pre><h3 id=\"权限\"><a href=\"#权限\" class=\"headerlink\" title=\"权限\"></a>权限</h3><p>使用<code>/v3/auth</code>设置权限：</p>\n<pre><code># 创建root用户\ncurl -L http://localhost:2379/v3/auth/user/add \\\n  -X POST -d &#39;{&quot;name&quot;: &quot;root&quot;, &quot;password&quot;: &quot;pass&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;}}\n\n# 创建root角色\ncurl -L http://localhost:2379/v3/auth/role/add \\\n  -X POST -d &#39;{&quot;name&quot;: &quot;root&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;}}\n\n# 授予root角色\ncurl -L http://localhost:2379/v3/auth/user/grant \\\n  -X POST -d &#39;{&quot;user&quot;: &quot;root&quot;, &quot;role&quot;: &quot;root&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;}}\n\n# 开启认证\ncurl -L http://localhost:2379/v3/auth/enable -X POST -d &#39;{}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;}}</code></pre><p>通过<code>/v3/auth/authenticate</code>服务使用一个认证令牌进行认证:</p>\n<pre><code># 为根用户获取认证令牌\ncurl -L http://localhost:2379/v3/auth/authenticate \\\n  -X POST -d &#39;{&quot;name&quot;: &quot;root&quot;, &quot;password&quot;: &quot;pass&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;},&quot;token&quot;:&quot;sssvIpwfnLAcWAQH.9&quot;}</code></pre><p>使用认证证书设置认证头部到认证令牌获取Keys：</p>\n<pre><code>curl -L http://localhost:2379/v3/kv/put \\\n  -H &#39;Authorization : sssvIpwfnLAcWAQH.9&#39; \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;, &quot;value&quot;: &quot;YmFy&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;2&quot;}}</code></pre>","site":{"data":{}},"excerpt":"","more":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_grpc_gateway.md\" target=\"_blank\" rel=\"noopener\">HTTP JSON API through the gRPC gateway</a><br>etcd v3 使用 gRPC 作为消息协议。etcd项目包括一个基于gRPC的<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%AE%A2%E6%88%B7%E7%AB%AFv3/\">Go客户端</a>和一个命令行工具，<a href=\"https://github.com/etcd-io/etcd/tree/master/etcdctl\" target=\"_blank\" rel=\"noopener\">etcdctl</a>,通过gRPC与etcd集群进行交互.对于没有gRPC支持的语言，etcd提供JSON <a href=\"https://github.com/grpc-ecosystem/grpc-gateway\" target=\"_blank\" rel=\"noopener\">gRPC网关</a>，这个网关提供一个RESTful风格的代理可以将HTTP/JSON请求转换为gRPC消息。</p>\n<h3 id=\"使用-gRPC网关\"><a href=\"#使用-gRPC网关\" class=\"headerlink\" title=\"使用 gRPC网关\"></a>使用 gRPC网关</h3><p>这个网关接受一个到etcd’s buffer协议消息定义的JSON格式的映射,注意<code>Key</code>和<code>Value</code>字段定义为byte 数组，因此JSON必须使用base64编码,下面的例子使用<code>curl</code>,但是每个HTTP/JSON客户端的工作原理都和例子相同。<br><strong>注意</strong><br>gRPC网关节点从etcd v3.3发生变化：</p>\n<ul>\n<li>etcd v3.2以及之前版本只使用<code>[CLIENT-URL]/v3alpha/*</code>。</li>\n<li>etcd v3.3使用<code>[CLIENT-URL]/v3beta/*</code>保持<code>[CLIENT-URL]/v3alpha/*</code>使用。</li>\n<li>etcd v3.4使用<code>[CLIENT-URL]/v3/*</code>保持<code>[CLIENT-URL]/v3beta/*</code>使用。<ul>\n<li><code>[CLIENT-URL]/v3alpha/*</code>被抛弃使用。</li>\n</ul>\n</li>\n<li>etcd v3.5以及最新版本只使用<code>[CLIENT-URL]/v3/*</code>。<ul>\n<li><code>[CLIENT-URL]/v3beta/*</code>被抛弃使用。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"存储和获取Keys\"><a href=\"#存储和获取Keys\" class=\"headerlink\" title=\"存储和获取Keys\"></a>存储和获取Keys</h3><p>使用<code>/v3/kv/range</code>和<code>/v3/kv/put</code>服务读和写Keys:</p>\n<pre><code>&lt;&lt;COMMENThttps://www.base64encode.org/foo is &#39;Zm9v&#39; in Base64bar is &#39;YmFy&#39;COMMENT\n\ncurl -L http://localhost:2379/v3/kv/put \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;, &quot;value&quot;: &quot;YmFy&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;3&quot;}}\n\ncurl -L http://localhost:2379/v3/kv/range \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;3&quot;},&quot;kvs&quot;:[{&quot;key&quot;:&quot;Zm9v&quot;,&quot;create_revision&quot;:&quot;2&quot;,&quot;mod_revision&quot;:&quot;2&quot;,&quot;version&quot;:&quot;1&quot;,&quot;value&quot;:&quot;YmFy&quot;}],&quot;count&quot;:&quot;1&quot;}\n\n# get all keys prefixed with &quot;foo&quot;\ncurl -L http://localhost:2379/v3/kv/range \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;, &quot;range_end&quot;: &quot;Zm9w&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;3&quot;},&quot;kvs&quot;:[{&quot;key&quot;:&quot;Zm9v&quot;,&quot;create_revision&quot;:&quot;2&quot;,&quot;mod_revision&quot;:&quot;2&quot;,&quot;version&quot;:&quot;1&quot;,&quot;value&quot;:&quot;YmFy&quot;}],&quot;count&quot;:&quot;1&quot;}</code></pre><h3 id=\"查看-Keys\"><a href=\"#查看-Keys\" class=\"headerlink\" title=\"查看 Keys\"></a>查看 Keys</h3><p>使用<code>/v3/watch</code>服务查看Keys:</p>\n<pre><code>curl -N http://localhost:2379/v3/watch \\\n  -X POST -d &#39;{&quot;create_request&quot;: {&quot;key&quot;:&quot;Zm9v&quot;} }&#39; &amp;# {&quot;result&quot;:{&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;},&quot;created&quot;:true}}\n\ncurl -L http://localhost:2379/v3/kv/put \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;, &quot;value&quot;: &quot;YmFy&quot;}&#39; &gt;/dev/null 2&gt;&amp;1# {&quot;result&quot;:{&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;2&quot;},&quot;events&quot;:[{&quot;kv&quot;:{&quot;key&quot;:&quot;Zm9v&quot;,&quot;create_revision&quot;:&quot;2&quot;,&quot;mod_revision&quot;:&quot;2&quot;,&quot;version&quot;:&quot;1&quot;,&quot;value&quot;:&quot;YmFy&quot;}}]}}</code></pre><h3 id=\"交易\"><a href=\"#交易\" class=\"headerlink\" title=\"交易\"></a>交易</h3><p>使用``/v3/kv/txn`发行一个交易：</p>\n<pre><code># 目标创建\ncurl -L http://localhost:2379/v3/kv/txn \\\n  -X POST \\\n  -d &#39;{&quot;compare&quot;:[{&quot;target&quot;:&quot;CREATE&quot;,&quot;key&quot;:&quot;Zm9v&quot;,&quot;createRevision&quot;:&quot;2&quot;}],&quot;success&quot;:[{&quot;requestPut&quot;:{&quot;key&quot;:&quot;Zm9v&quot;,&quot;value&quot;:&quot;YmFy&quot;}}]}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;12585971608760269493&quot;,&quot;member_id&quot;:&quot;13847567121247652255&quot;,&quot;revision&quot;:&quot;3&quot;,&quot;raft_term&quot;:&quot;2&quot;},&quot;succeeded&quot;:true,&quot;responses&quot;:[{&quot;response_put&quot;:{&quot;header&quot;:{&quot;revision&quot;:&quot;3&quot;}}}]}</code></pre><pre><code># 目标版本\ncurl -L http://localhost:2379/v3/kv/txn \\\n  -X POST \\\n  -d &#39;{&quot;compare&quot;:[{&quot;version&quot;:&quot;4&quot;,&quot;result&quot;:&quot;EQUAL&quot;,&quot;target&quot;:&quot;VERSION&quot;,&quot;key&quot;:&quot;Zm9v&quot;}],&quot;success&quot;:[{&quot;requestRange&quot;:{&quot;key&quot;:&quot;Zm9v&quot;}}]}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;6&quot;,&quot;raft_term&quot;:&quot;3&quot;},&quot;succeeded&quot;:true,&quot;responses&quot;:[{&quot;response_range&quot;:{&quot;header&quot;:{&quot;revision&quot;:&quot;6&quot;},&quot;kvs&quot;:[{&quot;key&quot;:&quot;Zm9v&quot;,&quot;create_revision&quot;:&quot;2&quot;,&quot;mod_revision&quot;:&quot;6&quot;,&quot;version&quot;:&quot;4&quot;,&quot;value&quot;:&quot;YmF6&quot;}],&quot;count&quot;:&quot;1&quot;}}]}</code></pre><h3 id=\"权限\"><a href=\"#权限\" class=\"headerlink\" title=\"权限\"></a>权限</h3><p>使用<code>/v3/auth</code>设置权限：</p>\n<pre><code># 创建root用户\ncurl -L http://localhost:2379/v3/auth/user/add \\\n  -X POST -d &#39;{&quot;name&quot;: &quot;root&quot;, &quot;password&quot;: &quot;pass&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;}}\n\n# 创建root角色\ncurl -L http://localhost:2379/v3/auth/role/add \\\n  -X POST -d &#39;{&quot;name&quot;: &quot;root&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;}}\n\n# 授予root角色\ncurl -L http://localhost:2379/v3/auth/user/grant \\\n  -X POST -d &#39;{&quot;user&quot;: &quot;root&quot;, &quot;role&quot;: &quot;root&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;}}\n\n# 开启认证\ncurl -L http://localhost:2379/v3/auth/enable -X POST -d &#39;{}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;}}</code></pre><p>通过<code>/v3/auth/authenticate</code>服务使用一个认证令牌进行认证:</p>\n<pre><code># 为根用户获取认证令牌\ncurl -L http://localhost:2379/v3/auth/authenticate \\\n  -X POST -d &#39;{&quot;name&quot;: &quot;root&quot;, &quot;password&quot;: &quot;pass&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;1&quot;,&quot;raft_term&quot;:&quot;2&quot;},&quot;token&quot;:&quot;sssvIpwfnLAcWAQH.9&quot;}</code></pre><p>使用认证证书设置认证头部到认证令牌获取Keys：</p>\n<pre><code>curl -L http://localhost:2379/v3/kv/put \\\n  -H &#39;Authorization : sssvIpwfnLAcWAQH.9&#39; \\\n  -X POST -d &#39;{&quot;key&quot;: &quot;Zm9v&quot;, &quot;value&quot;: &quot;YmFy&quot;}&#39;# {&quot;header&quot;:{&quot;cluster_id&quot;:&quot;14841639068965178418&quot;,&quot;member_id&quot;:&quot;10276657743932975437&quot;,&quot;revision&quot;:&quot;2&quot;,&quot;raft_term&quot;:&quot;2&quot;}}</code></pre>"},{"title":"TLS","date":"2019-11-25T11:58:39.000Z","_content":"原文地址：[TLS](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/security.md)\netcd支持用于客户端到服务器以及对等方（服务器到服务器/集群）通信的自动TLS以及通过客户端证书的身份验证.\n要启动并运行，首先要获得一个成员的CA证书和签名密钥对。 建议为集群中的每个成员创建并签名一个新的密钥对。\n为了方便起见，[cfssl](https://github.com/cloudflare/cfssl)工具提供了一个简单的接口来生成证书，我们在[此处](https://github.com/etcd-io/etcd/blob/master/hack/tls-setup)提供了使用该工具的示例。 或者，尝试使用本指南[生成自签名密钥对](https://github.com/coreos/docs/blob/master/os/generate-self-signed-certificates.md)。\n\n## 基本设置\n* * *\netcd通过命令行参数或环境变量采用了几种与证书相关的配置选项：\n\n**客户端到服务器的通信：**\n`--cert-file=<path>`:用于SSL/TLS**与**etcd的连接的证书。设置此选项后，advertise-client-urls可以使用HTTPS模式。\n`--key-file=<path>`:证书的密钥。 必须未加密。\n`--client-cert-auth`:设置此选项后，etcd将检查所有传入的HTTPS请求以查找由受信任CA签名的客户端证书，不提供有效客户端证书的请求将失败。 如果启用了身份验证，则证书将为“公用名”字段指定的用户名提供凭据。\n`--trusted-ca-file=<path>`:受信任的证书颁发机构。\n`--auto-tls`:使用自动生成的自签名证书进行与客户端的TLS连接。\n\n**对等节点(服务器到服务器/集群)间的通信：**\n对等节点选项的工作方式与客户端到服务器的选项相同：\n`--peer-cert-file=<path>`:用于SSL/TLS**与**对等节点之间的连接的证书。这将用于监听对等方地址以及向其他对等方发送请求。\n`--peer-key-file=<path>`:证书的密钥。 必须未加密。\n`--peer-client-cert-auth`:设置此选项后，etcd将检查所有传入的对等节点请求以查找由受信任CA签名的客户端证书.\n`--peer-trusted-ca-file=<path>`:受信任的证书颁发机构。\n`--peer-auto-tls`:使用自动生成的自签名证书进行与对等节点之间的TLS连接。\n如果提供了客户端到服务器或对等节点证书，则还必须设置密钥。 所有这些配置选项也可以通过环境变量`ETCD_CA_FILE`，`ETCD_PEER_CA_FILE`等获得。\n`--cipher-suites`:服务器/客户端与对等方之间受支持的TLS密码套件的逗号分隔列表（空将由Go自动填充）。从`v3.2.22+,v3.3.7+`和`v3.4+`起可用。\n\n## 示例1：客户端通过HTTPS与服务器进行加密传输\n* * *\n为此，请准备好CA证书（`ca.crt`）和签名密钥对（`server.crt`，`server.key`）。\n让我们配置etcd以逐步提供简单的HTTPS传输安全性：\n```\n$ etcd --name infra0 --data-dir infra0 \\\n  --cert-file=/path/to/server.crt --key-file=/path/to/server.key \\\n  --advertise-client-urls=https://127.0.0.1:2379 --listen-client-urls=https://127.0.0.1:2379\n```\n这应该可以正常启动，并且可以通过对etcd用HTTPS方式来测试配置：\n```\n$ curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v\n```\n该命令应显示握手成功。 由于我们使用具有自己的证书颁发机构的自签名证书，因此必须使用`--cacert`选项将CA传递给curl。 另一种可能性是将CA证书添加到系统的受信任证书目录（通常在`/etc/pki/tls/certs`或`/etc/ssl/certs`中）。\n**OSX10.9+的用户：**OSX 10.9+上的curl 7.30.0无法理解在命令行中传递的证书。可以替代的方法是将虚拟`ca.crt`直接导入到钥匙串中，或添加`-k`标志来`curl`以忽略错误。要在没有-k标志的情况下进行测试，请运行打开的`./fixtures/ca/ca.crt`并按照提示进行操作。测试后请删除此证书！如果有解决方法，请告诉我们。\n\n## 示例2：使用HTTPS客户端证书的客户端到服务器身份验证\n* * *\n目前，我们已经为etcd客户端提供了验证服务器身份并提供传输安全性的功能。 但是，我们也可以使用客户端证书来防止对etcd的未经授权的访问。\n客户端将向服务器提供其证书，服务器将检查证书是否由提供的CA签名并决定是否满足请求。\n为此，需要第一个示例中提到的相同文件，以及由同一证书颁发机构签名的客户端密钥对（`client.crt`，`client.key`）。\n```\n$ etcd --name infra0 --data-dir infra0 \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca.crt --cert-file=/path/to/server.crt --key-file=/path/to/server.key \\\n  --advertise-client-urls https://127.0.0.1:2379 --listen-client-urls https://127.0.0.1:2379\n```\n现在，对该服务器尝试与上述相同的请求：\n```\n$ curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v\n```\n该请求应该是被服务器拒绝：\n```\n...\nroutines:SSL3_READ_BYTES:sslv3 alert bad certificate\n...\n```\n为了使其成功，我们需要将CA签名的客户端证书提供给服务器：\n```\n$ curl --cacert /path/to/ca.crt --cert /path/to/client.crt --key /path/to/client.key \\\n  -L https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v\n```\n输出应包括：\n```\n...\nSSLv3, TLS handshake, CERT verify (15):\n...\nTLS handshake, Finished (20)\n```\n以及服务器的响应：\n```\n{\n    \"action\": \"set\",\n    \"node\": {\n        \"createdIndex\": 12,\n        \"key\": \"/foo\",\n        \"modifiedIndex\": 12,\n        \"value\": \"bar\"\n    }\n}\n```\n指定密码套件以阻止[较弱的TLS密码套件](https://github.com/etcd-io/etcd/issues/8320)。\n当使用无效密码套件请求客户端问候时，TLS握手将失败。\n例如：\n```\n$ etcd \\\n  --cert-file ./server.crt \\\n  --key-file ./server.key \\\n  --trusted-ca-file ./ca.crt \\\n  --cipher-suites TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n```\n然后，客户端请求必须指定服务器中指定的密码套件之一：\n```\n# 有效的加密套件\n$ curl \\\n  --cacert ./ca.crt \\\n  --cert ./server.crt \\\n  --key ./server.key \\\n  -L [CLIENT-URL]/metrics \\\n  --ciphers ECDHE-RSA-AES128-GCM-SHA256\n\n# 成功请求\netcd_server_version{server_version=\"3.2.22\"} 1\n...\n```\n```\n# 无效的加密套件\n$ curl \\\n  --cacert ./ca.crt \\\n  --cert ./server.crt \\\n  --key ./server.key \\\n  -L [CLIENT-URL]/metrics \\\n  --ciphers ECDHE-RSA-DES-CBC3-SHA\n\n# 请求失败\n(35) error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure\n```\n\n## 示例3：集群中的传输安全性和客户端证书\n* * *\n\netcd支持与上述对等节点通信相同的模型，这意味着集群中etcd成员之间的通信。\n假设我们有这个`ca.crt`和两个由此CA签名的成员，它们具有自己的密钥对（`member1.crt`和`member1.key`，`member2.crt`和`member2.key`），我们按以下方式启动etcd：\n```\nDISCOVERY_URL=... # from https://discovery.etcd.io/new\n\n# member1\n$ etcd --name infra1 --data-dir infra1 \\\n  --peer-client-cert-auth --peer-trusted-ca-file=/path/to/ca.crt --peer-cert-file=/path/to/member1.crt --peer-key-file=/path/to/member1.key \\\n  --initial-advertise-peer-urls=https://10.0.1.10:2380 --listen-peer-urls=https://10.0.1.10:2380 \\\n  --discovery ${DISCOVERY_URL}\n\n# member2\n$ etcd --name infra2 --data-dir infra2 \\\n  --peer-client-cert-auth --peer-trusted-ca-file=/path/to/ca.crt --peer-cert-file=/path/to/member2.crt --peer-key-file=/path/to/member2.key \\\n  --initial-advertise-peer-urls=https://10.0.1.11:2380 --listen-peer-urls=https://10.0.1.11:2380 \\\n  --discovery ${DISCOVERY_URL}\n```\netcd成员将形成一个集群，并且集群中成员之间的所有通信都将使用客户端证书进行加密和身份验证。 etcd的输出将显示它连接以使用HTTPS的地址。\n\n## 示例4：自动自签名传输安全性\n* * *\n\n对于需要通信加密而不是身份验证的情况，etcd支持使用自动生成的自签名证书来加密其消息。 因为不需要在etcd之外管理证书和密钥，所以这简化了部署。\n配置etcd以使用带有`--auto-tls`和`--peer-auto-tls`标志的自签名证书进行客户端和对等节点连接：\n```\nDISCOVERY_URL=... # from https://discovery.etcd.io/new\n\n# member1\n$ etcd --name infra1 --data-dir infra1 \\\n  --auto-tls --peer-auto-tls \\\n  --initial-advertise-peer-urls=https://10.0.1.10:2380 --listen-peer-urls=https://10.0.1.10:2380 \\\n  --discovery ${DISCOVERY_URL}\n\n# member2\n$ etcd --name infra2 --data-dir infra2 \\\n  --auto-tls --peer-auto-tls \\\n  --initial-advertise-peer-urls=https://10.0.1.11:2380 --listen-peer-urls=https://10.0.1.11:2380 \\\n  --discovery ${DISCOVERY_URL}\n```\n自签名证书不会验证身份，因此curl将返回错误：\n```\ncurl: (60) SSL certificate problem: Invalid certificate chain\n```\n要禁用证书链检查，请使用`-k`标志调用`curl`：\n```\n$ curl -k https://127.0.0.1:2379/v2/keys/foo -Xput -d value=bar -v\n```\n\n## DNS SRV的注意事项\n* * *\n\n如果连接是安全的，则`etcd proxy`从其客户端TLS终端，并使用`--peer-key-file`和`--peer-cert-file`中指定的代理自身的密钥/证书与etcd成员进行通信。\n\n代理通过给定成员的`--advertise-client-urls`和`--advertise-peer-urls`与etcd成员进行通信。 它将客户端请求转发到etcd成员广播的客户端URL，并通过etcd成员广播的对等URL同步初始集群配置。\n\n为etcd成员启用客户端身份验证后，管理员必须确保代理的`--peer-cert-file`选项中指定的对等节点证书对该身份验证有效。如果启用了对等节点身份验证，则代理的对等节点证书也必须对对等节点身份验证有效。\n\n## TLS 身份验证的注意事项\n* * *\n\n从[v3.2.0开始，TLS证书将在每个客户端连接上重新加载](https://github.com/etcd-io/etcd/pull/7829)。 这在不停止etcd服务器而替换到期证书时很有用； 可以通过用新证书覆盖旧证书来完成。 刷新每个连接的证书应该没有太多的开销，但是将来可以通过缓存层进行改进。 示例测试可以在[这里](https://github.com/coreos/etcd/blob/b041ce5d514a4b4aaeefbffb008f0c7570a18986/integration/v3_grpc_test.go#L1601-L1757)找到。\n\n从[v3.2.0开始，服务器使用错误的IP `SAN`拒绝传入的对等证书](https://github.com/etcd-io/etcd/pull/7687)。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中包含任何IP地址，则服务器仅在远程IP地址与这些IP地址之一匹配时才对对等节点身份验证。 这是为了防止未经授权的端点加入群集。 例如，对等节点B的CSR（带有cfssl）为：\n```\n{\n  \"CN\": \"etcd peer\",\n  \"hosts\": [\n    \"*.example.default.svc\",\n    \"*.example.default.svc.cluster.local\",\n    \"10.138.0.27\"\n  ],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"CA\",\n      \"ST\": \"San Francisco\"\n    }\n  ]\n}\n```\n当对等节点B的实际IP地址是`10.138.0.2`，而不是`10.138.0.27`。 当对等节点B尝试加入集群时，对等节点A将拒绝B，并显示错误x509：证书对`10.138.0.27`有效，而不对`10.138.0.2`有效，因为B的远程IP地址与“使用者备用名称（SAN）”字段中的地址不匹配。\n\n从[v3.2.0开始，服务器在检查SAN时解析TLS DNSNames](https://github.com/etcd-io/etcd/pull/7767)。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中仅包含DNS名称（不包含IP地址），则仅当这些DNS名称上的正向查找（`dig b.com`）具有与远程IP匹配的IP时，服务器才对对等身份验证 地址。 例如，对等B的CSR（带有`cfssl`）为：\n```\n{\n  \"CN\": \"etcd peer\",\n  \"hosts\": [\n    \"b.com\"\n  ],\n```\n当对等节点B的远程IP地址为`10.138.0.2`时。 当对等节点B尝试加入集群时，对等节点A查找传入的主机`b.com`以获取IP地址列表（例如`dig b.com`）。如果列表不包含IP `10.138.0.2`，则出现错误`tls: 10.138.0.2 does not match any of DNSNames [\"b.com\"]`.\n\n从[v3.2.2开始，如果IP匹配，服务器将接受连接，而无需检查DNS条目](https://github.com/etcd-io/etcd/pull/8223)。 例如，如果对等节点证书在“使用者备用名称（SAN）”字段中包含IP地址和DNS名称，并且远程IP地址与这些IP地址之一匹配，则服务器仅接受连接而无需进一步检查DNS名称。 例如，对等节点B的CSR（带有`cfssl`）为：\n```\n{\n  \"CN\": \"etcd peer\",\n  \"hosts\": [\n    \"invalid.domain\",\n    \"10.138.0.2\"\n  ],\n```\n当对等节点B的远程IP地址是`10.138.0.2`并且`invalid.domain`是无效的主机时。 当对等节点B尝试加入集群时，对等节点A成功地对节点B进行了身份验证，因为“使用者备用名称（SAN）”字段具有有效的匹配IP地址。 有关更多详细信息，请参见问题[＃8206](https://github.com/etcd-io/etcd/issues/8206)。\n\n从[v3.2.5开始，服务器支持在通配符DNS `SAN`上进行反向查找](https://github.com/etcd-io/etcd/pull/8281)。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中仅包含DNS名称（不包含IP地址），则服务器首先对远程IP地址进行反向查找，以获取映射到该地址的名称列表（例如`nslookup IPADDR`）。如果这些名称的名称与对等节点证书的DNS名称（通过完全匹配或通配符匹配）匹配，则接受连接。 如果没有匹配项，则服务器将对等节点证书中的每个DNS条目进行正向查找（例如，如果条目为`*.example.default.svc`，则查找`example.default.svc`），并且仅在主机的解析地址具有匹配的IP时接受连接 地址和对等节点的远程IP地址。 例如，对等B的CSR（带有`cfssl`）为：\n```\n{\n  \"CN\": \"etcd peer\",\n  \"hosts\": [\n    \"*.example.default.svc\",\n    \"*.example.default.svc.cluster.local\"\n  ],\n```\n当对等节点B的远程IP地址为`10.138.0.2`时。 当对等节点B尝试加入集群时，对等节点A反向查找IP `10.138.0.2`以获取主机名列表。 并且，“主题备用名称”（SAN）字段中的主机名必须与对等节点B的证书DNS名称完全匹配或与通配符匹配。 如果反向/正向查找均无效，则返回错误`\"tls: \"10.138.0.2\" does not match any of DNSNames [\"*.example.default.svc\",\"*.example.default.svc.cluster.local\"]`。有关更多详细信息，请参见问题[＃8268](https://github.com/etcd-io/etcd/issues/8268)。\n\n[v3.3.0](https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.3.md)添加了[etcd --peer-cert-allowed-cn](https://github.com/etcd-io/etcd/pull/8616)参数，以支持[基于CN（通用名称）的对等节点连接的身份验证](https://github.com/etcd-io/etcd/issues/8262)。 Kubernetes TLS引导涉及为etcd成员和其他系统组件（例如API服务器，kubelet等）生成动态证书。 为每个组件维护不同的CA可提供对etcd集群的更严格的访问控制，但通常很乏味。 指定`--peer-cert-allowed-cn`标志时，即使具有共享的CA，节点也只能以匹配的通用名称加入。 例如，三节点群集中的每个成员都设置有CSR（使用`cfssl`），如下所示：\n```\n{\n  \"CN\": \"etcd.local\",\n  \"hosts\": [\n    \"m1.etcd.local\",\n    \"127.0.0.1\",\n    \"localhost\"\n  ],\n```\n```\n{\n  \"CN\": \"etcd.local\",\n  \"hosts\": [\n    \"m2.etcd.local\",\n    \"127.0.0.1\",\n    \"localhost\"\n  ],\n```\n```\n{\n  \"CN\": \"etcd.local\",\n  \"hosts\": [\n    \"m3.etcd.local\",\n    \"127.0.0.1\",\n    \"localhost\"\n  ],\n```\n如果给定`--peer-cert-allowed-cn etcd.local`，则只有具有相同通用名称的对等方将被认证。 CSR中具有不同CN或`--peer-cert-allowed-cn`的节点将被拒绝：\n```\n$ etcd --peer-cert-allowed-cn m1.etcd.local\n\nI | embed: rejected connection from \"127.0.0.1:48044\" (error \"CommonName authentication failed\", ServerName \"m1.etcd.local\")\nI | embed: rejected connection from \"127.0.0.1:55702\" (error \"remote error: tls: bad certificate\", ServerName \"m3.etcd.local\")\n```\n每个进程都应以以下内容开始：\n```\netcd --peer-cert-allowed-cn etcd.local\n\nI | pkg/netutil: resolving m3.etcd.local:32380 to 127.0.0.1:32380\nI | pkg/netutil: resolving m2.etcd.local:22380 to 127.0.0.1:22380\nI | pkg/netutil: resolving m1.etcd.local:2380 to 127.0.0.1:2380\nI | etcdserver: published {Name:m3 ClientURLs:[https://m3.etcd.local:32379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | etcdserver: published {Name:m1 ClientURLs:[https://m1.etcd.local:2379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | etcdserver: published {Name:m2 ClientURLs:[https://m2.etcd.local:22379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | embed: serving client requests on 127.0.0.1:32379\nI | embed: serving client requests on 127.0.0.1:22379\nI | embed: serving client requests on 127.0.0.1:2379\n```\n[v3.2.19](https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.2.md)和[v3.3.4](https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.3.md)修复了[当证书SAN字段仅包含IP地址但不包含域名时TLS重新加载的问题](https://github.com/etcd-io/etcd/issues/9541)。 例如，如下设置了具有CSR（具有`cfssl`）的成员：\n```\n{\n  \"CN\": \"etcd.local\",\n  \"hosts\": [\n    \"127.0.0.1\"\n  ],\n```\n在Go中，仅当服务器的`（* tls.Config）.Certificates`字段不为空或`（* tls.ClientHelloInfo）.ServerName`不为空且具有有效SNI时，服务器才会调用`（* tls.Config）.GetCertificate`来重新加载TLS 来自客户。 以前，etcd始终填充`（* tls.Config）`。在初始客户端TLS握手上的证书为非空。 因此，总是希望客户端提供匹配的SNI，以便通过TLS验证并触发`（* tls.Config）.GetCertificate`以重新加载TLS数据。\n\n但是，其SAN字段[仅包括IP地址不包含任何域名的证书](https://github.com/etcd-io/etcd/issues/9541)将请求`* tls.ClientHelloInfo`带有空的`ServerName`字段，从而无法在初始TLS握手时触发TLS重新加载；当需要在线更换过期证书时，这将成为一个问题。\n\n现在,`（* tls.Config）.Certificates`在初始TLS客户端握手时创建为空，首先触发`（* tls.Config）.GetCertificate`，然后在每个新的TLS连接上填充其余证书，即使客户端SNI为 为空（例如，证书仅包括IP）。\n\n## 主机白名单的注意事项\n* * *\n\n`etcd --host-whitelist`参数指定HTTP客户端请求中可接受的主机名。 客户端源策略可以防止对不安全的etcd服务器的“[DNS重新绑定](https://en.wikipedia.org/wiki/DNS_rebinding)”攻击。 也就是说，任何网站都可以简单地创建一个授权的DNS名称，并将DNS定向到“`localhost`”（或任何其他地址）。 然后，侦听“`localhost`”上的etcd服务器的所有HTTP端点都可以访问，因此容易受到DNS重新绑定攻击。 有关更多详细信息，请参见[CVE-2018-5702](https://bugs.chromium.org/p/project-zero/issues/detail?id=1447#c2)。\n\n客户原始策略的工作方式如下：\n\n1. 如果客户端通过HTTPS连接是安全的，则允许使用任何主机名。\n2. 如果客户端连接不安全且“` HostWhitelist`”不为空，则仅允许其Host字段列在白名单中的HTTP请求。\n\n请注意，无论是否启用身份验证，都会实施客户端来源策略，以进行更严格的控制。\n\n默认情况下，`etcd --host-whitelist`和`embed.Config.HostWhitelist`设置为空以允许所有主机名。请注意，在指定主机名时，不会自动添加回送地址。 要允许环回接口，请手动将其添加到白名单（例如“ `localhost`”，“`127.0.0.1`”等）。\n\n## 常见问题\n* * *\n\n使用TLS客户端身份验证时，我看到SSLv3警报握手失败？\n`golang`的`crypto/tls`软件包在使用之前检查证书公钥的密钥用法。要使用证书公共密钥进行客户端身份验证，我们需要在创建证书公共密钥时将`clientAuth`添加到“`Extended Key Usage`”中。\n\n这是操作方法：\n\n将以下部分添加到openssl.cnf中：\n```\n[ ssl_client ]\n...\n  extendedKeyUsage = clientAuth\n...\n\n```\n创建证书时，请确保在`-extensions`参数中引用它：\n```\n$ openssl ca -config openssl.cnf -policy policy_anything -extensions ssl_client -out certs/machine.crt -infiles machine.csr\n```\n\n通过对等证书身份验证，我收到“证书对127.0.0.1有效，而不对$我的Ip有效”\n\n确保使用主题名称（成员的公共IP地址）对证书进行签名。 例如，`etcd-ca`工具为其`new-cert`命令提供了`--ip=`选项。\n\n需要在其使用者名称中为成员的FQDN签署证书，使用使用者备用名称（简称IP SAN）添加IP地址。 `etcd-ca`工具为其`new-cert`命令提供了`--domain=`选项，`openssl`也可以做到[这](http://wiki.cacert.org/FAQ/subjectAltName)一点。","source":"_posts/blog/etcd/TLS.md","raw":"---\ntitle: TLS\ndate: 2019-11-25 19:58:39\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址：[TLS](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/security.md)\netcd支持用于客户端到服务器以及对等方（服务器到服务器/集群）通信的自动TLS以及通过客户端证书的身份验证.\n要启动并运行，首先要获得一个成员的CA证书和签名密钥对。 建议为集群中的每个成员创建并签名一个新的密钥对。\n为了方便起见，[cfssl](https://github.com/cloudflare/cfssl)工具提供了一个简单的接口来生成证书，我们在[此处](https://github.com/etcd-io/etcd/blob/master/hack/tls-setup)提供了使用该工具的示例。 或者，尝试使用本指南[生成自签名密钥对](https://github.com/coreos/docs/blob/master/os/generate-self-signed-certificates.md)。\n\n## 基本设置\n* * *\netcd通过命令行参数或环境变量采用了几种与证书相关的配置选项：\n\n**客户端到服务器的通信：**\n`--cert-file=<path>`:用于SSL/TLS**与**etcd的连接的证书。设置此选项后，advertise-client-urls可以使用HTTPS模式。\n`--key-file=<path>`:证书的密钥。 必须未加密。\n`--client-cert-auth`:设置此选项后，etcd将检查所有传入的HTTPS请求以查找由受信任CA签名的客户端证书，不提供有效客户端证书的请求将失败。 如果启用了身份验证，则证书将为“公用名”字段指定的用户名提供凭据。\n`--trusted-ca-file=<path>`:受信任的证书颁发机构。\n`--auto-tls`:使用自动生成的自签名证书进行与客户端的TLS连接。\n\n**对等节点(服务器到服务器/集群)间的通信：**\n对等节点选项的工作方式与客户端到服务器的选项相同：\n`--peer-cert-file=<path>`:用于SSL/TLS**与**对等节点之间的连接的证书。这将用于监听对等方地址以及向其他对等方发送请求。\n`--peer-key-file=<path>`:证书的密钥。 必须未加密。\n`--peer-client-cert-auth`:设置此选项后，etcd将检查所有传入的对等节点请求以查找由受信任CA签名的客户端证书.\n`--peer-trusted-ca-file=<path>`:受信任的证书颁发机构。\n`--peer-auto-tls`:使用自动生成的自签名证书进行与对等节点之间的TLS连接。\n如果提供了客户端到服务器或对等节点证书，则还必须设置密钥。 所有这些配置选项也可以通过环境变量`ETCD_CA_FILE`，`ETCD_PEER_CA_FILE`等获得。\n`--cipher-suites`:服务器/客户端与对等方之间受支持的TLS密码套件的逗号分隔列表（空将由Go自动填充）。从`v3.2.22+,v3.3.7+`和`v3.4+`起可用。\n\n## 示例1：客户端通过HTTPS与服务器进行加密传输\n* * *\n为此，请准备好CA证书（`ca.crt`）和签名密钥对（`server.crt`，`server.key`）。\n让我们配置etcd以逐步提供简单的HTTPS传输安全性：\n```\n$ etcd --name infra0 --data-dir infra0 \\\n  --cert-file=/path/to/server.crt --key-file=/path/to/server.key \\\n  --advertise-client-urls=https://127.0.0.1:2379 --listen-client-urls=https://127.0.0.1:2379\n```\n这应该可以正常启动，并且可以通过对etcd用HTTPS方式来测试配置：\n```\n$ curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v\n```\n该命令应显示握手成功。 由于我们使用具有自己的证书颁发机构的自签名证书，因此必须使用`--cacert`选项将CA传递给curl。 另一种可能性是将CA证书添加到系统的受信任证书目录（通常在`/etc/pki/tls/certs`或`/etc/ssl/certs`中）。\n**OSX10.9+的用户：**OSX 10.9+上的curl 7.30.0无法理解在命令行中传递的证书。可以替代的方法是将虚拟`ca.crt`直接导入到钥匙串中，或添加`-k`标志来`curl`以忽略错误。要在没有-k标志的情况下进行测试，请运行打开的`./fixtures/ca/ca.crt`并按照提示进行操作。测试后请删除此证书！如果有解决方法，请告诉我们。\n\n## 示例2：使用HTTPS客户端证书的客户端到服务器身份验证\n* * *\n目前，我们已经为etcd客户端提供了验证服务器身份并提供传输安全性的功能。 但是，我们也可以使用客户端证书来防止对etcd的未经授权的访问。\n客户端将向服务器提供其证书，服务器将检查证书是否由提供的CA签名并决定是否满足请求。\n为此，需要第一个示例中提到的相同文件，以及由同一证书颁发机构签名的客户端密钥对（`client.crt`，`client.key`）。\n```\n$ etcd --name infra0 --data-dir infra0 \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca.crt --cert-file=/path/to/server.crt --key-file=/path/to/server.key \\\n  --advertise-client-urls https://127.0.0.1:2379 --listen-client-urls https://127.0.0.1:2379\n```\n现在，对该服务器尝试与上述相同的请求：\n```\n$ curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v\n```\n该请求应该是被服务器拒绝：\n```\n...\nroutines:SSL3_READ_BYTES:sslv3 alert bad certificate\n...\n```\n为了使其成功，我们需要将CA签名的客户端证书提供给服务器：\n```\n$ curl --cacert /path/to/ca.crt --cert /path/to/client.crt --key /path/to/client.key \\\n  -L https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v\n```\n输出应包括：\n```\n...\nSSLv3, TLS handshake, CERT verify (15):\n...\nTLS handshake, Finished (20)\n```\n以及服务器的响应：\n```\n{\n    \"action\": \"set\",\n    \"node\": {\n        \"createdIndex\": 12,\n        \"key\": \"/foo\",\n        \"modifiedIndex\": 12,\n        \"value\": \"bar\"\n    }\n}\n```\n指定密码套件以阻止[较弱的TLS密码套件](https://github.com/etcd-io/etcd/issues/8320)。\n当使用无效密码套件请求客户端问候时，TLS握手将失败。\n例如：\n```\n$ etcd \\\n  --cert-file ./server.crt \\\n  --key-file ./server.key \\\n  --trusted-ca-file ./ca.crt \\\n  --cipher-suites TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n```\n然后，客户端请求必须指定服务器中指定的密码套件之一：\n```\n# 有效的加密套件\n$ curl \\\n  --cacert ./ca.crt \\\n  --cert ./server.crt \\\n  --key ./server.key \\\n  -L [CLIENT-URL]/metrics \\\n  --ciphers ECDHE-RSA-AES128-GCM-SHA256\n\n# 成功请求\netcd_server_version{server_version=\"3.2.22\"} 1\n...\n```\n```\n# 无效的加密套件\n$ curl \\\n  --cacert ./ca.crt \\\n  --cert ./server.crt \\\n  --key ./server.key \\\n  -L [CLIENT-URL]/metrics \\\n  --ciphers ECDHE-RSA-DES-CBC3-SHA\n\n# 请求失败\n(35) error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure\n```\n\n## 示例3：集群中的传输安全性和客户端证书\n* * *\n\netcd支持与上述对等节点通信相同的模型，这意味着集群中etcd成员之间的通信。\n假设我们有这个`ca.crt`和两个由此CA签名的成员，它们具有自己的密钥对（`member1.crt`和`member1.key`，`member2.crt`和`member2.key`），我们按以下方式启动etcd：\n```\nDISCOVERY_URL=... # from https://discovery.etcd.io/new\n\n# member1\n$ etcd --name infra1 --data-dir infra1 \\\n  --peer-client-cert-auth --peer-trusted-ca-file=/path/to/ca.crt --peer-cert-file=/path/to/member1.crt --peer-key-file=/path/to/member1.key \\\n  --initial-advertise-peer-urls=https://10.0.1.10:2380 --listen-peer-urls=https://10.0.1.10:2380 \\\n  --discovery ${DISCOVERY_URL}\n\n# member2\n$ etcd --name infra2 --data-dir infra2 \\\n  --peer-client-cert-auth --peer-trusted-ca-file=/path/to/ca.crt --peer-cert-file=/path/to/member2.crt --peer-key-file=/path/to/member2.key \\\n  --initial-advertise-peer-urls=https://10.0.1.11:2380 --listen-peer-urls=https://10.0.1.11:2380 \\\n  --discovery ${DISCOVERY_URL}\n```\netcd成员将形成一个集群，并且集群中成员之间的所有通信都将使用客户端证书进行加密和身份验证。 etcd的输出将显示它连接以使用HTTPS的地址。\n\n## 示例4：自动自签名传输安全性\n* * *\n\n对于需要通信加密而不是身份验证的情况，etcd支持使用自动生成的自签名证书来加密其消息。 因为不需要在etcd之外管理证书和密钥，所以这简化了部署。\n配置etcd以使用带有`--auto-tls`和`--peer-auto-tls`标志的自签名证书进行客户端和对等节点连接：\n```\nDISCOVERY_URL=... # from https://discovery.etcd.io/new\n\n# member1\n$ etcd --name infra1 --data-dir infra1 \\\n  --auto-tls --peer-auto-tls \\\n  --initial-advertise-peer-urls=https://10.0.1.10:2380 --listen-peer-urls=https://10.0.1.10:2380 \\\n  --discovery ${DISCOVERY_URL}\n\n# member2\n$ etcd --name infra2 --data-dir infra2 \\\n  --auto-tls --peer-auto-tls \\\n  --initial-advertise-peer-urls=https://10.0.1.11:2380 --listen-peer-urls=https://10.0.1.11:2380 \\\n  --discovery ${DISCOVERY_URL}\n```\n自签名证书不会验证身份，因此curl将返回错误：\n```\ncurl: (60) SSL certificate problem: Invalid certificate chain\n```\n要禁用证书链检查，请使用`-k`标志调用`curl`：\n```\n$ curl -k https://127.0.0.1:2379/v2/keys/foo -Xput -d value=bar -v\n```\n\n## DNS SRV的注意事项\n* * *\n\n如果连接是安全的，则`etcd proxy`从其客户端TLS终端，并使用`--peer-key-file`和`--peer-cert-file`中指定的代理自身的密钥/证书与etcd成员进行通信。\n\n代理通过给定成员的`--advertise-client-urls`和`--advertise-peer-urls`与etcd成员进行通信。 它将客户端请求转发到etcd成员广播的客户端URL，并通过etcd成员广播的对等URL同步初始集群配置。\n\n为etcd成员启用客户端身份验证后，管理员必须确保代理的`--peer-cert-file`选项中指定的对等节点证书对该身份验证有效。如果启用了对等节点身份验证，则代理的对等节点证书也必须对对等节点身份验证有效。\n\n## TLS 身份验证的注意事项\n* * *\n\n从[v3.2.0开始，TLS证书将在每个客户端连接上重新加载](https://github.com/etcd-io/etcd/pull/7829)。 这在不停止etcd服务器而替换到期证书时很有用； 可以通过用新证书覆盖旧证书来完成。 刷新每个连接的证书应该没有太多的开销，但是将来可以通过缓存层进行改进。 示例测试可以在[这里](https://github.com/coreos/etcd/blob/b041ce5d514a4b4aaeefbffb008f0c7570a18986/integration/v3_grpc_test.go#L1601-L1757)找到。\n\n从[v3.2.0开始，服务器使用错误的IP `SAN`拒绝传入的对等证书](https://github.com/etcd-io/etcd/pull/7687)。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中包含任何IP地址，则服务器仅在远程IP地址与这些IP地址之一匹配时才对对等节点身份验证。 这是为了防止未经授权的端点加入群集。 例如，对等节点B的CSR（带有cfssl）为：\n```\n{\n  \"CN\": \"etcd peer\",\n  \"hosts\": [\n    \"*.example.default.svc\",\n    \"*.example.default.svc.cluster.local\",\n    \"10.138.0.27\"\n  ],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"CA\",\n      \"ST\": \"San Francisco\"\n    }\n  ]\n}\n```\n当对等节点B的实际IP地址是`10.138.0.2`，而不是`10.138.0.27`。 当对等节点B尝试加入集群时，对等节点A将拒绝B，并显示错误x509：证书对`10.138.0.27`有效，而不对`10.138.0.2`有效，因为B的远程IP地址与“使用者备用名称（SAN）”字段中的地址不匹配。\n\n从[v3.2.0开始，服务器在检查SAN时解析TLS DNSNames](https://github.com/etcd-io/etcd/pull/7767)。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中仅包含DNS名称（不包含IP地址），则仅当这些DNS名称上的正向查找（`dig b.com`）具有与远程IP匹配的IP时，服务器才对对等身份验证 地址。 例如，对等B的CSR（带有`cfssl`）为：\n```\n{\n  \"CN\": \"etcd peer\",\n  \"hosts\": [\n    \"b.com\"\n  ],\n```\n当对等节点B的远程IP地址为`10.138.0.2`时。 当对等节点B尝试加入集群时，对等节点A查找传入的主机`b.com`以获取IP地址列表（例如`dig b.com`）。如果列表不包含IP `10.138.0.2`，则出现错误`tls: 10.138.0.2 does not match any of DNSNames [\"b.com\"]`.\n\n从[v3.2.2开始，如果IP匹配，服务器将接受连接，而无需检查DNS条目](https://github.com/etcd-io/etcd/pull/8223)。 例如，如果对等节点证书在“使用者备用名称（SAN）”字段中包含IP地址和DNS名称，并且远程IP地址与这些IP地址之一匹配，则服务器仅接受连接而无需进一步检查DNS名称。 例如，对等节点B的CSR（带有`cfssl`）为：\n```\n{\n  \"CN\": \"etcd peer\",\n  \"hosts\": [\n    \"invalid.domain\",\n    \"10.138.0.2\"\n  ],\n```\n当对等节点B的远程IP地址是`10.138.0.2`并且`invalid.domain`是无效的主机时。 当对等节点B尝试加入集群时，对等节点A成功地对节点B进行了身份验证，因为“使用者备用名称（SAN）”字段具有有效的匹配IP地址。 有关更多详细信息，请参见问题[＃8206](https://github.com/etcd-io/etcd/issues/8206)。\n\n从[v3.2.5开始，服务器支持在通配符DNS `SAN`上进行反向查找](https://github.com/etcd-io/etcd/pull/8281)。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中仅包含DNS名称（不包含IP地址），则服务器首先对远程IP地址进行反向查找，以获取映射到该地址的名称列表（例如`nslookup IPADDR`）。如果这些名称的名称与对等节点证书的DNS名称（通过完全匹配或通配符匹配）匹配，则接受连接。 如果没有匹配项，则服务器将对等节点证书中的每个DNS条目进行正向查找（例如，如果条目为`*.example.default.svc`，则查找`example.default.svc`），并且仅在主机的解析地址具有匹配的IP时接受连接 地址和对等节点的远程IP地址。 例如，对等B的CSR（带有`cfssl`）为：\n```\n{\n  \"CN\": \"etcd peer\",\n  \"hosts\": [\n    \"*.example.default.svc\",\n    \"*.example.default.svc.cluster.local\"\n  ],\n```\n当对等节点B的远程IP地址为`10.138.0.2`时。 当对等节点B尝试加入集群时，对等节点A反向查找IP `10.138.0.2`以获取主机名列表。 并且，“主题备用名称”（SAN）字段中的主机名必须与对等节点B的证书DNS名称完全匹配或与通配符匹配。 如果反向/正向查找均无效，则返回错误`\"tls: \"10.138.0.2\" does not match any of DNSNames [\"*.example.default.svc\",\"*.example.default.svc.cluster.local\"]`。有关更多详细信息，请参见问题[＃8268](https://github.com/etcd-io/etcd/issues/8268)。\n\n[v3.3.0](https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.3.md)添加了[etcd --peer-cert-allowed-cn](https://github.com/etcd-io/etcd/pull/8616)参数，以支持[基于CN（通用名称）的对等节点连接的身份验证](https://github.com/etcd-io/etcd/issues/8262)。 Kubernetes TLS引导涉及为etcd成员和其他系统组件（例如API服务器，kubelet等）生成动态证书。 为每个组件维护不同的CA可提供对etcd集群的更严格的访问控制，但通常很乏味。 指定`--peer-cert-allowed-cn`标志时，即使具有共享的CA，节点也只能以匹配的通用名称加入。 例如，三节点群集中的每个成员都设置有CSR（使用`cfssl`），如下所示：\n```\n{\n  \"CN\": \"etcd.local\",\n  \"hosts\": [\n    \"m1.etcd.local\",\n    \"127.0.0.1\",\n    \"localhost\"\n  ],\n```\n```\n{\n  \"CN\": \"etcd.local\",\n  \"hosts\": [\n    \"m2.etcd.local\",\n    \"127.0.0.1\",\n    \"localhost\"\n  ],\n```\n```\n{\n  \"CN\": \"etcd.local\",\n  \"hosts\": [\n    \"m3.etcd.local\",\n    \"127.0.0.1\",\n    \"localhost\"\n  ],\n```\n如果给定`--peer-cert-allowed-cn etcd.local`，则只有具有相同通用名称的对等方将被认证。 CSR中具有不同CN或`--peer-cert-allowed-cn`的节点将被拒绝：\n```\n$ etcd --peer-cert-allowed-cn m1.etcd.local\n\nI | embed: rejected connection from \"127.0.0.1:48044\" (error \"CommonName authentication failed\", ServerName \"m1.etcd.local\")\nI | embed: rejected connection from \"127.0.0.1:55702\" (error \"remote error: tls: bad certificate\", ServerName \"m3.etcd.local\")\n```\n每个进程都应以以下内容开始：\n```\netcd --peer-cert-allowed-cn etcd.local\n\nI | pkg/netutil: resolving m3.etcd.local:32380 to 127.0.0.1:32380\nI | pkg/netutil: resolving m2.etcd.local:22380 to 127.0.0.1:22380\nI | pkg/netutil: resolving m1.etcd.local:2380 to 127.0.0.1:2380\nI | etcdserver: published {Name:m3 ClientURLs:[https://m3.etcd.local:32379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | etcdserver: published {Name:m1 ClientURLs:[https://m1.etcd.local:2379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | etcdserver: published {Name:m2 ClientURLs:[https://m2.etcd.local:22379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | embed: serving client requests on 127.0.0.1:32379\nI | embed: serving client requests on 127.0.0.1:22379\nI | embed: serving client requests on 127.0.0.1:2379\n```\n[v3.2.19](https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.2.md)和[v3.3.4](https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.3.md)修复了[当证书SAN字段仅包含IP地址但不包含域名时TLS重新加载的问题](https://github.com/etcd-io/etcd/issues/9541)。 例如，如下设置了具有CSR（具有`cfssl`）的成员：\n```\n{\n  \"CN\": \"etcd.local\",\n  \"hosts\": [\n    \"127.0.0.1\"\n  ],\n```\n在Go中，仅当服务器的`（* tls.Config）.Certificates`字段不为空或`（* tls.ClientHelloInfo）.ServerName`不为空且具有有效SNI时，服务器才会调用`（* tls.Config）.GetCertificate`来重新加载TLS 来自客户。 以前，etcd始终填充`（* tls.Config）`。在初始客户端TLS握手上的证书为非空。 因此，总是希望客户端提供匹配的SNI，以便通过TLS验证并触发`（* tls.Config）.GetCertificate`以重新加载TLS数据。\n\n但是，其SAN字段[仅包括IP地址不包含任何域名的证书](https://github.com/etcd-io/etcd/issues/9541)将请求`* tls.ClientHelloInfo`带有空的`ServerName`字段，从而无法在初始TLS握手时触发TLS重新加载；当需要在线更换过期证书时，这将成为一个问题。\n\n现在,`（* tls.Config）.Certificates`在初始TLS客户端握手时创建为空，首先触发`（* tls.Config）.GetCertificate`，然后在每个新的TLS连接上填充其余证书，即使客户端SNI为 为空（例如，证书仅包括IP）。\n\n## 主机白名单的注意事项\n* * *\n\n`etcd --host-whitelist`参数指定HTTP客户端请求中可接受的主机名。 客户端源策略可以防止对不安全的etcd服务器的“[DNS重新绑定](https://en.wikipedia.org/wiki/DNS_rebinding)”攻击。 也就是说，任何网站都可以简单地创建一个授权的DNS名称，并将DNS定向到“`localhost`”（或任何其他地址）。 然后，侦听“`localhost`”上的etcd服务器的所有HTTP端点都可以访问，因此容易受到DNS重新绑定攻击。 有关更多详细信息，请参见[CVE-2018-5702](https://bugs.chromium.org/p/project-zero/issues/detail?id=1447#c2)。\n\n客户原始策略的工作方式如下：\n\n1. 如果客户端通过HTTPS连接是安全的，则允许使用任何主机名。\n2. 如果客户端连接不安全且“` HostWhitelist`”不为空，则仅允许其Host字段列在白名单中的HTTP请求。\n\n请注意，无论是否启用身份验证，都会实施客户端来源策略，以进行更严格的控制。\n\n默认情况下，`etcd --host-whitelist`和`embed.Config.HostWhitelist`设置为空以允许所有主机名。请注意，在指定主机名时，不会自动添加回送地址。 要允许环回接口，请手动将其添加到白名单（例如“ `localhost`”，“`127.0.0.1`”等）。\n\n## 常见问题\n* * *\n\n使用TLS客户端身份验证时，我看到SSLv3警报握手失败？\n`golang`的`crypto/tls`软件包在使用之前检查证书公钥的密钥用法。要使用证书公共密钥进行客户端身份验证，我们需要在创建证书公共密钥时将`clientAuth`添加到“`Extended Key Usage`”中。\n\n这是操作方法：\n\n将以下部分添加到openssl.cnf中：\n```\n[ ssl_client ]\n...\n  extendedKeyUsage = clientAuth\n...\n\n```\n创建证书时，请确保在`-extensions`参数中引用它：\n```\n$ openssl ca -config openssl.cnf -policy policy_anything -extensions ssl_client -out certs/machine.crt -infiles machine.csr\n```\n\n通过对等证书身份验证，我收到“证书对127.0.0.1有效，而不对$我的Ip有效”\n\n确保使用主题名称（成员的公共IP地址）对证书进行签名。 例如，`etcd-ca`工具为其`new-cert`命令提供了`--ip=`选项。\n\n需要在其使用者名称中为成员的FQDN签署证书，使用使用者备用名称（简称IP SAN）添加IP地址。 `etcd-ca`工具为其`new-cert`命令提供了`--domain=`选项，`openssl`也可以做到[这](http://wiki.cacert.org/FAQ/subjectAltName)一点。","slug":"blog/etcd/TLS","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyh9001lk0vq6v2u3rja","content":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/security.md\" target=\"_blank\" rel=\"noopener\">TLS</a><br>etcd支持用于客户端到服务器以及对等方（服务器到服务器/集群）通信的自动TLS以及通过客户端证书的身份验证.<br>要启动并运行，首先要获得一个成员的CA证书和签名密钥对。 建议为集群中的每个成员创建并签名一个新的密钥对。<br>为了方便起见，<a href=\"https://github.com/cloudflare/cfssl\" target=\"_blank\" rel=\"noopener\">cfssl</a>工具提供了一个简单的接口来生成证书，我们在<a href=\"https://github.com/etcd-io/etcd/blob/master/hack/tls-setup\" target=\"_blank\" rel=\"noopener\">此处</a>提供了使用该工具的示例。 或者，尝试使用本指南<a href=\"https://github.com/coreos/docs/blob/master/os/generate-self-signed-certificates.md\" target=\"_blank\" rel=\"noopener\">生成自签名密钥对</a>。</p>\n<h2 id=\"基本设置\"><a href=\"#基本设置\" class=\"headerlink\" title=\"基本设置\"></a>基本设置</h2><hr>\n<p>etcd通过命令行参数或环境变量采用了几种与证书相关的配置选项：</p>\n<p><strong>客户端到服务器的通信：</strong><br><code>--cert-file=&lt;path&gt;</code>:用于SSL/TLS<strong>与</strong>etcd的连接的证书。设置此选项后，advertise-client-urls可以使用HTTPS模式。<br><code>--key-file=&lt;path&gt;</code>:证书的密钥。 必须未加密。<br><code>--client-cert-auth</code>:设置此选项后，etcd将检查所有传入的HTTPS请求以查找由受信任CA签名的客户端证书，不提供有效客户端证书的请求将失败。 如果启用了身份验证，则证书将为“公用名”字段指定的用户名提供凭据。<br><code>--trusted-ca-file=&lt;path&gt;</code>:受信任的证书颁发机构。<br><code>--auto-tls</code>:使用自动生成的自签名证书进行与客户端的TLS连接。</p>\n<p><strong>对等节点(服务器到服务器/集群)间的通信：</strong><br>对等节点选项的工作方式与客户端到服务器的选项相同：<br><code>--peer-cert-file=&lt;path&gt;</code>:用于SSL/TLS<strong>与</strong>对等节点之间的连接的证书。这将用于监听对等方地址以及向其他对等方发送请求。<br><code>--peer-key-file=&lt;path&gt;</code>:证书的密钥。 必须未加密。<br><code>--peer-client-cert-auth</code>:设置此选项后，etcd将检查所有传入的对等节点请求以查找由受信任CA签名的客户端证书.<br><code>--peer-trusted-ca-file=&lt;path&gt;</code>:受信任的证书颁发机构。<br><code>--peer-auto-tls</code>:使用自动生成的自签名证书进行与对等节点之间的TLS连接。<br>如果提供了客户端到服务器或对等节点证书，则还必须设置密钥。 所有这些配置选项也可以通过环境变量<code>ETCD_CA_FILE</code>，<code>ETCD_PEER_CA_FILE</code>等获得。<br><code>--cipher-suites</code>:服务器/客户端与对等方之间受支持的TLS密码套件的逗号分隔列表（空将由Go自动填充）。从<code>v3.2.22+,v3.3.7+</code>和<code>v3.4+</code>起可用。</p>\n<h2 id=\"示例1：客户端通过HTTPS与服务器进行加密传输\"><a href=\"#示例1：客户端通过HTTPS与服务器进行加密传输\" class=\"headerlink\" title=\"示例1：客户端通过HTTPS与服务器进行加密传输\"></a>示例1：客户端通过HTTPS与服务器进行加密传输</h2><hr>\n<p>为此，请准备好CA证书（<code>ca.crt</code>）和签名密钥对（<code>server.crt</code>，<code>server.key</code>）。<br>让我们配置etcd以逐步提供简单的HTTPS传输安全性：</p>\n<pre><code>$ etcd --name infra0 --data-dir infra0 \\\n  --cert-file=/path/to/server.crt --key-file=/path/to/server.key \\\n  --advertise-client-urls=https://127.0.0.1:2379 --listen-client-urls=https://127.0.0.1:2379</code></pre><p>这应该可以正常启动，并且可以通过对etcd用HTTPS方式来测试配置：</p>\n<pre><code>$ curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v</code></pre><p>该命令应显示握手成功。 由于我们使用具有自己的证书颁发机构的自签名证书，因此必须使用<code>--cacert</code>选项将CA传递给curl。 另一种可能性是将CA证书添加到系统的受信任证书目录（通常在<code>/etc/pki/tls/certs</code>或<code>/etc/ssl/certs</code>中）。<br><strong>OSX10.9+的用户：</strong>OSX 10.9+上的curl 7.30.0无法理解在命令行中传递的证书。可以替代的方法是将虚拟<code>ca.crt</code>直接导入到钥匙串中，或添加<code>-k</code>标志来<code>curl</code>以忽略错误。要在没有-k标志的情况下进行测试，请运行打开的<code>./fixtures/ca/ca.crt</code>并按照提示进行操作。测试后请删除此证书！如果有解决方法，请告诉我们。</p>\n<h2 id=\"示例2：使用HTTPS客户端证书的客户端到服务器身份验证\"><a href=\"#示例2：使用HTTPS客户端证书的客户端到服务器身份验证\" class=\"headerlink\" title=\"示例2：使用HTTPS客户端证书的客户端到服务器身份验证\"></a>示例2：使用HTTPS客户端证书的客户端到服务器身份验证</h2><hr>\n<p>目前，我们已经为etcd客户端提供了验证服务器身份并提供传输安全性的功能。 但是，我们也可以使用客户端证书来防止对etcd的未经授权的访问。<br>客户端将向服务器提供其证书，服务器将检查证书是否由提供的CA签名并决定是否满足请求。<br>为此，需要第一个示例中提到的相同文件，以及由同一证书颁发机构签名的客户端密钥对（<code>client.crt</code>，<code>client.key</code>）。</p>\n<pre><code>$ etcd --name infra0 --data-dir infra0 \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca.crt --cert-file=/path/to/server.crt --key-file=/path/to/server.key \\\n  --advertise-client-urls https://127.0.0.1:2379 --listen-client-urls https://127.0.0.1:2379</code></pre><p>现在，对该服务器尝试与上述相同的请求：</p>\n<pre><code>$ curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v</code></pre><p>该请求应该是被服务器拒绝：</p>\n<pre><code>...\nroutines:SSL3_READ_BYTES:sslv3 alert bad certificate\n...</code></pre><p>为了使其成功，我们需要将CA签名的客户端证书提供给服务器：</p>\n<pre><code>$ curl --cacert /path/to/ca.crt --cert /path/to/client.crt --key /path/to/client.key \\\n  -L https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v</code></pre><p>输出应包括：</p>\n<pre><code>...\nSSLv3, TLS handshake, CERT verify (15):\n...\nTLS handshake, Finished (20)</code></pre><p>以及服务器的响应：</p>\n<pre><code>{\n    &quot;action&quot;: &quot;set&quot;,\n    &quot;node&quot;: {\n        &quot;createdIndex&quot;: 12,\n        &quot;key&quot;: &quot;/foo&quot;,\n        &quot;modifiedIndex&quot;: 12,\n        &quot;value&quot;: &quot;bar&quot;\n    }\n}</code></pre><p>指定密码套件以阻止<a href=\"https://github.com/etcd-io/etcd/issues/8320\" target=\"_blank\" rel=\"noopener\">较弱的TLS密码套件</a>。<br>当使用无效密码套件请求客户端问候时，TLS握手将失败。<br>例如：</p>\n<pre><code>$ etcd \\\n  --cert-file ./server.crt \\\n  --key-file ./server.key \\\n  --trusted-ca-file ./ca.crt \\\n  --cipher-suites TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384</code></pre><p>然后，客户端请求必须指定服务器中指定的密码套件之一：</p>\n<pre><code># 有效的加密套件\n$ curl \\\n  --cacert ./ca.crt \\\n  --cert ./server.crt \\\n  --key ./server.key \\\n  -L [CLIENT-URL]/metrics \\\n  --ciphers ECDHE-RSA-AES128-GCM-SHA256\n\n# 成功请求\netcd_server_version{server_version=&quot;3.2.22&quot;} 1\n...</code></pre><pre><code># 无效的加密套件\n$ curl \\\n  --cacert ./ca.crt \\\n  --cert ./server.crt \\\n  --key ./server.key \\\n  -L [CLIENT-URL]/metrics \\\n  --ciphers ECDHE-RSA-DES-CBC3-SHA\n\n# 请求失败\n(35) error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure</code></pre><h2 id=\"示例3：集群中的传输安全性和客户端证书\"><a href=\"#示例3：集群中的传输安全性和客户端证书\" class=\"headerlink\" title=\"示例3：集群中的传输安全性和客户端证书\"></a>示例3：集群中的传输安全性和客户端证书</h2><hr>\n<p>etcd支持与上述对等节点通信相同的模型，这意味着集群中etcd成员之间的通信。<br>假设我们有这个<code>ca.crt</code>和两个由此CA签名的成员，它们具有自己的密钥对（<code>member1.crt</code>和<code>member1.key</code>，<code>member2.crt</code>和<code>member2.key</code>），我们按以下方式启动etcd：</p>\n<pre><code>DISCOVERY_URL=... # from https://discovery.etcd.io/new\n\n# member1\n$ etcd --name infra1 --data-dir infra1 \\\n  --peer-client-cert-auth --peer-trusted-ca-file=/path/to/ca.crt --peer-cert-file=/path/to/member1.crt --peer-key-file=/path/to/member1.key \\\n  --initial-advertise-peer-urls=https://10.0.1.10:2380 --listen-peer-urls=https://10.0.1.10:2380 \\\n  --discovery ${DISCOVERY_URL}\n\n# member2\n$ etcd --name infra2 --data-dir infra2 \\\n  --peer-client-cert-auth --peer-trusted-ca-file=/path/to/ca.crt --peer-cert-file=/path/to/member2.crt --peer-key-file=/path/to/member2.key \\\n  --initial-advertise-peer-urls=https://10.0.1.11:2380 --listen-peer-urls=https://10.0.1.11:2380 \\\n  --discovery ${DISCOVERY_URL}</code></pre><p>etcd成员将形成一个集群，并且集群中成员之间的所有通信都将使用客户端证书进行加密和身份验证。 etcd的输出将显示它连接以使用HTTPS的地址。</p>\n<h2 id=\"示例4：自动自签名传输安全性\"><a href=\"#示例4：自动自签名传输安全性\" class=\"headerlink\" title=\"示例4：自动自签名传输安全性\"></a>示例4：自动自签名传输安全性</h2><hr>\n<p>对于需要通信加密而不是身份验证的情况，etcd支持使用自动生成的自签名证书来加密其消息。 因为不需要在etcd之外管理证书和密钥，所以这简化了部署。<br>配置etcd以使用带有<code>--auto-tls</code>和<code>--peer-auto-tls</code>标志的自签名证书进行客户端和对等节点连接：</p>\n<pre><code>DISCOVERY_URL=... # from https://discovery.etcd.io/new\n\n# member1\n$ etcd --name infra1 --data-dir infra1 \\\n  --auto-tls --peer-auto-tls \\\n  --initial-advertise-peer-urls=https://10.0.1.10:2380 --listen-peer-urls=https://10.0.1.10:2380 \\\n  --discovery ${DISCOVERY_URL}\n\n# member2\n$ etcd --name infra2 --data-dir infra2 \\\n  --auto-tls --peer-auto-tls \\\n  --initial-advertise-peer-urls=https://10.0.1.11:2380 --listen-peer-urls=https://10.0.1.11:2380 \\\n  --discovery ${DISCOVERY_URL}</code></pre><p>自签名证书不会验证身份，因此curl将返回错误：</p>\n<pre><code>curl: (60) SSL certificate problem: Invalid certificate chain</code></pre><p>要禁用证书链检查，请使用<code>-k</code>标志调用<code>curl</code>：</p>\n<pre><code>$ curl -k https://127.0.0.1:2379/v2/keys/foo -Xput -d value=bar -v</code></pre><h2 id=\"DNS-SRV的注意事项\"><a href=\"#DNS-SRV的注意事项\" class=\"headerlink\" title=\"DNS SRV的注意事项\"></a>DNS SRV的注意事项</h2><hr>\n<p>如果连接是安全的，则<code>etcd proxy</code>从其客户端TLS终端，并使用<code>--peer-key-file</code>和<code>--peer-cert-file</code>中指定的代理自身的密钥/证书与etcd成员进行通信。</p>\n<p>代理通过给定成员的<code>--advertise-client-urls</code>和<code>--advertise-peer-urls</code>与etcd成员进行通信。 它将客户端请求转发到etcd成员广播的客户端URL，并通过etcd成员广播的对等URL同步初始集群配置。</p>\n<p>为etcd成员启用客户端身份验证后，管理员必须确保代理的<code>--peer-cert-file</code>选项中指定的对等节点证书对该身份验证有效。如果启用了对等节点身份验证，则代理的对等节点证书也必须对对等节点身份验证有效。</p>\n<h2 id=\"TLS-身份验证的注意事项\"><a href=\"#TLS-身份验证的注意事项\" class=\"headerlink\" title=\"TLS 身份验证的注意事项\"></a>TLS 身份验证的注意事项</h2><hr>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/7829\" target=\"_blank\" rel=\"noopener\">v3.2.0开始，TLS证书将在每个客户端连接上重新加载</a>。 这在不停止etcd服务器而替换到期证书时很有用； 可以通过用新证书覆盖旧证书来完成。 刷新每个连接的证书应该没有太多的开销，但是将来可以通过缓存层进行改进。 示例测试可以在<a href=\"https://github.com/coreos/etcd/blob/b041ce5d514a4b4aaeefbffb008f0c7570a18986/integration/v3_grpc_test.go#L1601-L1757\" target=\"_blank\" rel=\"noopener\">这里</a>找到。</p>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/7687\" target=\"_blank\" rel=\"noopener\">v3.2.0开始，服务器使用错误的IP <code>SAN</code>拒绝传入的对等证书</a>。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中包含任何IP地址，则服务器仅在远程IP地址与这些IP地址之一匹配时才对对等节点身份验证。 这是为了防止未经授权的端点加入群集。 例如，对等节点B的CSR（带有cfssl）为：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd peer&quot;,\n  &quot;hosts&quot;: [\n    &quot;*.example.default.svc&quot;,\n    &quot;*.example.default.svc.cluster.local&quot;,\n    &quot;10.138.0.27&quot;\n  ],\n  &quot;key&quot;: {\n    &quot;algo&quot;: &quot;rsa&quot;,\n    &quot;size&quot;: 2048\n  },\n  &quot;names&quot;: [\n    {\n      &quot;C&quot;: &quot;US&quot;,\n      &quot;L&quot;: &quot;CA&quot;,\n      &quot;ST&quot;: &quot;San Francisco&quot;\n    }\n  ]\n}</code></pre><p>当对等节点B的实际IP地址是<code>10.138.0.2</code>，而不是<code>10.138.0.27</code>。 当对等节点B尝试加入集群时，对等节点A将拒绝B，并显示错误x509：证书对<code>10.138.0.27</code>有效，而不对<code>10.138.0.2</code>有效，因为B的远程IP地址与“使用者备用名称（SAN）”字段中的地址不匹配。</p>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/7767\" target=\"_blank\" rel=\"noopener\">v3.2.0开始，服务器在检查SAN时解析TLS DNSNames</a>。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中仅包含DNS名称（不包含IP地址），则仅当这些DNS名称上的正向查找（<code>dig b.com</code>）具有与远程IP匹配的IP时，服务器才对对等身份验证 地址。 例如，对等B的CSR（带有<code>cfssl</code>）为：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd peer&quot;,\n  &quot;hosts&quot;: [\n    &quot;b.com&quot;\n  ],</code></pre><p>当对等节点B的远程IP地址为<code>10.138.0.2</code>时。 当对等节点B尝试加入集群时，对等节点A查找传入的主机<code>b.com</code>以获取IP地址列表（例如<code>dig b.com</code>）。如果列表不包含IP <code>10.138.0.2</code>，则出现错误<code>tls: 10.138.0.2 does not match any of DNSNames [&quot;b.com&quot;]</code>.</p>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/8223\" target=\"_blank\" rel=\"noopener\">v3.2.2开始，如果IP匹配，服务器将接受连接，而无需检查DNS条目</a>。 例如，如果对等节点证书在“使用者备用名称（SAN）”字段中包含IP地址和DNS名称，并且远程IP地址与这些IP地址之一匹配，则服务器仅接受连接而无需进一步检查DNS名称。 例如，对等节点B的CSR（带有<code>cfssl</code>）为：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd peer&quot;,\n  &quot;hosts&quot;: [\n    &quot;invalid.domain&quot;,\n    &quot;10.138.0.2&quot;\n  ],</code></pre><p>当对等节点B的远程IP地址是<code>10.138.0.2</code>并且<code>invalid.domain</code>是无效的主机时。 当对等节点B尝试加入集群时，对等节点A成功地对节点B进行了身份验证，因为“使用者备用名称（SAN）”字段具有有效的匹配IP地址。 有关更多详细信息，请参见问题<a href=\"https://github.com/etcd-io/etcd/issues/8206\" target=\"_blank\" rel=\"noopener\">＃8206</a>。</p>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/8281\" target=\"_blank\" rel=\"noopener\">v3.2.5开始，服务器支持在通配符DNS <code>SAN</code>上进行反向查找</a>。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中仅包含DNS名称（不包含IP地址），则服务器首先对远程IP地址进行反向查找，以获取映射到该地址的名称列表（例如<code>nslookup IPADDR</code>）。如果这些名称的名称与对等节点证书的DNS名称（通过完全匹配或通配符匹配）匹配，则接受连接。 如果没有匹配项，则服务器将对等节点证书中的每个DNS条目进行正向查找（例如，如果条目为<code>*.example.default.svc</code>，则查找<code>example.default.svc</code>），并且仅在主机的解析地址具有匹配的IP时接受连接 地址和对等节点的远程IP地址。 例如，对等B的CSR（带有<code>cfssl</code>）为：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd peer&quot;,\n  &quot;hosts&quot;: [\n    &quot;*.example.default.svc&quot;,\n    &quot;*.example.default.svc.cluster.local&quot;\n  ],</code></pre><p>当对等节点B的远程IP地址为<code>10.138.0.2</code>时。 当对等节点B尝试加入集群时，对等节点A反向查找IP <code>10.138.0.2</code>以获取主机名列表。 并且，“主题备用名称”（SAN）字段中的主机名必须与对等节点B的证书DNS名称完全匹配或与通配符匹配。 如果反向/正向查找均无效，则返回错误<code>&quot;tls: &quot;10.138.0.2&quot; does not match any of DNSNames [&quot;*.example.default.svc&quot;,&quot;*.example.default.svc.cluster.local&quot;]</code>。有关更多详细信息，请参见问题<a href=\"https://github.com/etcd-io/etcd/issues/8268\" target=\"_blank\" rel=\"noopener\">＃8268</a>。</p>\n<p><a href=\"https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.3.md\" target=\"_blank\" rel=\"noopener\">v3.3.0</a>添加了<a href=\"https://github.com/etcd-io/etcd/pull/8616\" target=\"_blank\" rel=\"noopener\">etcd –peer-cert-allowed-cn</a>参数，以支持<a href=\"https://github.com/etcd-io/etcd/issues/8262\" target=\"_blank\" rel=\"noopener\">基于CN（通用名称）的对等节点连接的身份验证</a>。 Kubernetes TLS引导涉及为etcd成员和其他系统组件（例如API服务器，kubelet等）生成动态证书。 为每个组件维护不同的CA可提供对etcd集群的更严格的访问控制，但通常很乏味。 指定<code>--peer-cert-allowed-cn</code>标志时，即使具有共享的CA，节点也只能以匹配的通用名称加入。 例如，三节点群集中的每个成员都设置有CSR（使用<code>cfssl</code>），如下所示：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd.local&quot;,\n  &quot;hosts&quot;: [\n    &quot;m1.etcd.local&quot;,\n    &quot;127.0.0.1&quot;,\n    &quot;localhost&quot;\n  ],</code></pre><pre><code>{\n  &quot;CN&quot;: &quot;etcd.local&quot;,\n  &quot;hosts&quot;: [\n    &quot;m2.etcd.local&quot;,\n    &quot;127.0.0.1&quot;,\n    &quot;localhost&quot;\n  ],</code></pre><pre><code>{\n  &quot;CN&quot;: &quot;etcd.local&quot;,\n  &quot;hosts&quot;: [\n    &quot;m3.etcd.local&quot;,\n    &quot;127.0.0.1&quot;,\n    &quot;localhost&quot;\n  ],</code></pre><p>如果给定<code>--peer-cert-allowed-cn etcd.local</code>，则只有具有相同通用名称的对等方将被认证。 CSR中具有不同CN或<code>--peer-cert-allowed-cn</code>的节点将被拒绝：</p>\n<pre><code>$ etcd --peer-cert-allowed-cn m1.etcd.local\n\nI | embed: rejected connection from &quot;127.0.0.1:48044&quot; (error &quot;CommonName authentication failed&quot;, ServerName &quot;m1.etcd.local&quot;)\nI | embed: rejected connection from &quot;127.0.0.1:55702&quot; (error &quot;remote error: tls: bad certificate&quot;, ServerName &quot;m3.etcd.local&quot;)</code></pre><p>每个进程都应以以下内容开始：</p>\n<pre><code>etcd --peer-cert-allowed-cn etcd.local\n\nI | pkg/netutil: resolving m3.etcd.local:32380 to 127.0.0.1:32380\nI | pkg/netutil: resolving m2.etcd.local:22380 to 127.0.0.1:22380\nI | pkg/netutil: resolving m1.etcd.local:2380 to 127.0.0.1:2380\nI | etcdserver: published {Name:m3 ClientURLs:[https://m3.etcd.local:32379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | etcdserver: published {Name:m1 ClientURLs:[https://m1.etcd.local:2379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | etcdserver: published {Name:m2 ClientURLs:[https://m2.etcd.local:22379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | embed: serving client requests on 127.0.0.1:32379\nI | embed: serving client requests on 127.0.0.1:22379\nI | embed: serving client requests on 127.0.0.1:2379</code></pre><p><a href=\"https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.2.md\" target=\"_blank\" rel=\"noopener\">v3.2.19</a>和<a href=\"https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.3.md\" target=\"_blank\" rel=\"noopener\">v3.3.4</a>修复了<a href=\"https://github.com/etcd-io/etcd/issues/9541\" target=\"_blank\" rel=\"noopener\">当证书SAN字段仅包含IP地址但不包含域名时TLS重新加载的问题</a>。 例如，如下设置了具有CSR（具有<code>cfssl</code>）的成员：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd.local&quot;,\n  &quot;hosts&quot;: [\n    &quot;127.0.0.1&quot;\n  ],</code></pre><p>在Go中，仅当服务器的<code>（* tls.Config）.Certificates</code>字段不为空或<code>（* tls.ClientHelloInfo）.ServerName</code>不为空且具有有效SNI时，服务器才会调用<code>（* tls.Config）.GetCertificate</code>来重新加载TLS 来自客户。 以前，etcd始终填充<code>（* tls.Config）</code>。在初始客户端TLS握手上的证书为非空。 因此，总是希望客户端提供匹配的SNI，以便通过TLS验证并触发<code>（* tls.Config）.GetCertificate</code>以重新加载TLS数据。</p>\n<p>但是，其SAN字段<a href=\"https://github.com/etcd-io/etcd/issues/9541\" target=\"_blank\" rel=\"noopener\">仅包括IP地址不包含任何域名的证书</a>将请求<code>* tls.ClientHelloInfo</code>带有空的<code>ServerName</code>字段，从而无法在初始TLS握手时触发TLS重新加载；当需要在线更换过期证书时，这将成为一个问题。</p>\n<p>现在,<code>（* tls.Config）.Certificates</code>在初始TLS客户端握手时创建为空，首先触发<code>（* tls.Config）.GetCertificate</code>，然后在每个新的TLS连接上填充其余证书，即使客户端SNI为 为空（例如，证书仅包括IP）。</p>\n<h2 id=\"主机白名单的注意事项\"><a href=\"#主机白名单的注意事项\" class=\"headerlink\" title=\"主机白名单的注意事项\"></a>主机白名单的注意事项</h2><hr>\n<p><code>etcd --host-whitelist</code>参数指定HTTP客户端请求中可接受的主机名。 客户端源策略可以防止对不安全的etcd服务器的“<a href=\"https://en.wikipedia.org/wiki/DNS_rebinding\" target=\"_blank\" rel=\"noopener\">DNS重新绑定</a>”攻击。 也就是说，任何网站都可以简单地创建一个授权的DNS名称，并将DNS定向到“<code>localhost</code>”（或任何其他地址）。 然后，侦听“<code>localhost</code>”上的etcd服务器的所有HTTP端点都可以访问，因此容易受到DNS重新绑定攻击。 有关更多详细信息，请参见<a href=\"https://bugs.chromium.org/p/project-zero/issues/detail?id=1447#c2\" target=\"_blank\" rel=\"noopener\">CVE-2018-5702</a>。</p>\n<p>客户原始策略的工作方式如下：</p>\n<ol>\n<li>如果客户端通过HTTPS连接是安全的，则允许使用任何主机名。</li>\n<li>如果客户端连接不安全且“<code>HostWhitelist</code>”不为空，则仅允许其Host字段列在白名单中的HTTP请求。</li>\n</ol>\n<p>请注意，无论是否启用身份验证，都会实施客户端来源策略，以进行更严格的控制。</p>\n<p>默认情况下，<code>etcd --host-whitelist</code>和<code>embed.Config.HostWhitelist</code>设置为空以允许所有主机名。请注意，在指定主机名时，不会自动添加回送地址。 要允许环回接口，请手动将其添加到白名单（例如“ <code>localhost</code>”，“<code>127.0.0.1</code>”等）。</p>\n<h2 id=\"常见问题\"><a href=\"#常见问题\" class=\"headerlink\" title=\"常见问题\"></a>常见问题</h2><hr>\n<p>使用TLS客户端身份验证时，我看到SSLv3警报握手失败？<br><code>golang</code>的<code>crypto/tls</code>软件包在使用之前检查证书公钥的密钥用法。要使用证书公共密钥进行客户端身份验证，我们需要在创建证书公共密钥时将<code>clientAuth</code>添加到“<code>Extended Key Usage</code>”中。</p>\n<p>这是操作方法：</p>\n<p>将以下部分添加到openssl.cnf中：</p>\n<pre><code>[ ssl_client ]\n...\n  extendedKeyUsage = clientAuth\n...\n</code></pre><p>创建证书时，请确保在<code>-extensions</code>参数中引用它：</p>\n<pre><code>$ openssl ca -config openssl.cnf -policy policy_anything -extensions ssl_client -out certs/machine.crt -infiles machine.csr</code></pre><p>通过对等证书身份验证，我收到“证书对127.0.0.1有效，而不对$我的Ip有效”</p>\n<p>确保使用主题名称（成员的公共IP地址）对证书进行签名。 例如，<code>etcd-ca</code>工具为其<code>new-cert</code>命令提供了<code>--ip=</code>选项。</p>\n<p>需要在其使用者名称中为成员的FQDN签署证书，使用使用者备用名称（简称IP SAN）添加IP地址。 <code>etcd-ca</code>工具为其<code>new-cert</code>命令提供了<code>--domain=</code>选项，<code>openssl</code>也可以做到<a href=\"http://wiki.cacert.org/FAQ/subjectAltName\" target=\"_blank\" rel=\"noopener\">这</a>一点。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/security.md\" target=\"_blank\" rel=\"noopener\">TLS</a><br>etcd支持用于客户端到服务器以及对等方（服务器到服务器/集群）通信的自动TLS以及通过客户端证书的身份验证.<br>要启动并运行，首先要获得一个成员的CA证书和签名密钥对。 建议为集群中的每个成员创建并签名一个新的密钥对。<br>为了方便起见，<a href=\"https://github.com/cloudflare/cfssl\" target=\"_blank\" rel=\"noopener\">cfssl</a>工具提供了一个简单的接口来生成证书，我们在<a href=\"https://github.com/etcd-io/etcd/blob/master/hack/tls-setup\" target=\"_blank\" rel=\"noopener\">此处</a>提供了使用该工具的示例。 或者，尝试使用本指南<a href=\"https://github.com/coreos/docs/blob/master/os/generate-self-signed-certificates.md\" target=\"_blank\" rel=\"noopener\">生成自签名密钥对</a>。</p>\n<h2 id=\"基本设置\"><a href=\"#基本设置\" class=\"headerlink\" title=\"基本设置\"></a>基本设置</h2><hr>\n<p>etcd通过命令行参数或环境变量采用了几种与证书相关的配置选项：</p>\n<p><strong>客户端到服务器的通信：</strong><br><code>--cert-file=&lt;path&gt;</code>:用于SSL/TLS<strong>与</strong>etcd的连接的证书。设置此选项后，advertise-client-urls可以使用HTTPS模式。<br><code>--key-file=&lt;path&gt;</code>:证书的密钥。 必须未加密。<br><code>--client-cert-auth</code>:设置此选项后，etcd将检查所有传入的HTTPS请求以查找由受信任CA签名的客户端证书，不提供有效客户端证书的请求将失败。 如果启用了身份验证，则证书将为“公用名”字段指定的用户名提供凭据。<br><code>--trusted-ca-file=&lt;path&gt;</code>:受信任的证书颁发机构。<br><code>--auto-tls</code>:使用自动生成的自签名证书进行与客户端的TLS连接。</p>\n<p><strong>对等节点(服务器到服务器/集群)间的通信：</strong><br>对等节点选项的工作方式与客户端到服务器的选项相同：<br><code>--peer-cert-file=&lt;path&gt;</code>:用于SSL/TLS<strong>与</strong>对等节点之间的连接的证书。这将用于监听对等方地址以及向其他对等方发送请求。<br><code>--peer-key-file=&lt;path&gt;</code>:证书的密钥。 必须未加密。<br><code>--peer-client-cert-auth</code>:设置此选项后，etcd将检查所有传入的对等节点请求以查找由受信任CA签名的客户端证书.<br><code>--peer-trusted-ca-file=&lt;path&gt;</code>:受信任的证书颁发机构。<br><code>--peer-auto-tls</code>:使用自动生成的自签名证书进行与对等节点之间的TLS连接。<br>如果提供了客户端到服务器或对等节点证书，则还必须设置密钥。 所有这些配置选项也可以通过环境变量<code>ETCD_CA_FILE</code>，<code>ETCD_PEER_CA_FILE</code>等获得。<br><code>--cipher-suites</code>:服务器/客户端与对等方之间受支持的TLS密码套件的逗号分隔列表（空将由Go自动填充）。从<code>v3.2.22+,v3.3.7+</code>和<code>v3.4+</code>起可用。</p>\n<h2 id=\"示例1：客户端通过HTTPS与服务器进行加密传输\"><a href=\"#示例1：客户端通过HTTPS与服务器进行加密传输\" class=\"headerlink\" title=\"示例1：客户端通过HTTPS与服务器进行加密传输\"></a>示例1：客户端通过HTTPS与服务器进行加密传输</h2><hr>\n<p>为此，请准备好CA证书（<code>ca.crt</code>）和签名密钥对（<code>server.crt</code>，<code>server.key</code>）。<br>让我们配置etcd以逐步提供简单的HTTPS传输安全性：</p>\n<pre><code>$ etcd --name infra0 --data-dir infra0 \\\n  --cert-file=/path/to/server.crt --key-file=/path/to/server.key \\\n  --advertise-client-urls=https://127.0.0.1:2379 --listen-client-urls=https://127.0.0.1:2379</code></pre><p>这应该可以正常启动，并且可以通过对etcd用HTTPS方式来测试配置：</p>\n<pre><code>$ curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v</code></pre><p>该命令应显示握手成功。 由于我们使用具有自己的证书颁发机构的自签名证书，因此必须使用<code>--cacert</code>选项将CA传递给curl。 另一种可能性是将CA证书添加到系统的受信任证书目录（通常在<code>/etc/pki/tls/certs</code>或<code>/etc/ssl/certs</code>中）。<br><strong>OSX10.9+的用户：</strong>OSX 10.9+上的curl 7.30.0无法理解在命令行中传递的证书。可以替代的方法是将虚拟<code>ca.crt</code>直接导入到钥匙串中，或添加<code>-k</code>标志来<code>curl</code>以忽略错误。要在没有-k标志的情况下进行测试，请运行打开的<code>./fixtures/ca/ca.crt</code>并按照提示进行操作。测试后请删除此证书！如果有解决方法，请告诉我们。</p>\n<h2 id=\"示例2：使用HTTPS客户端证书的客户端到服务器身份验证\"><a href=\"#示例2：使用HTTPS客户端证书的客户端到服务器身份验证\" class=\"headerlink\" title=\"示例2：使用HTTPS客户端证书的客户端到服务器身份验证\"></a>示例2：使用HTTPS客户端证书的客户端到服务器身份验证</h2><hr>\n<p>目前，我们已经为etcd客户端提供了验证服务器身份并提供传输安全性的功能。 但是，我们也可以使用客户端证书来防止对etcd的未经授权的访问。<br>客户端将向服务器提供其证书，服务器将检查证书是否由提供的CA签名并决定是否满足请求。<br>为此，需要第一个示例中提到的相同文件，以及由同一证书颁发机构签名的客户端密钥对（<code>client.crt</code>，<code>client.key</code>）。</p>\n<pre><code>$ etcd --name infra0 --data-dir infra0 \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca.crt --cert-file=/path/to/server.crt --key-file=/path/to/server.key \\\n  --advertise-client-urls https://127.0.0.1:2379 --listen-client-urls https://127.0.0.1:2379</code></pre><p>现在，对该服务器尝试与上述相同的请求：</p>\n<pre><code>$ curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v</code></pre><p>该请求应该是被服务器拒绝：</p>\n<pre><code>...\nroutines:SSL3_READ_BYTES:sslv3 alert bad certificate\n...</code></pre><p>为了使其成功，我们需要将CA签名的客户端证书提供给服务器：</p>\n<pre><code>$ curl --cacert /path/to/ca.crt --cert /path/to/client.crt --key /path/to/client.key \\\n  -L https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v</code></pre><p>输出应包括：</p>\n<pre><code>...\nSSLv3, TLS handshake, CERT verify (15):\n...\nTLS handshake, Finished (20)</code></pre><p>以及服务器的响应：</p>\n<pre><code>{\n    &quot;action&quot;: &quot;set&quot;,\n    &quot;node&quot;: {\n        &quot;createdIndex&quot;: 12,\n        &quot;key&quot;: &quot;/foo&quot;,\n        &quot;modifiedIndex&quot;: 12,\n        &quot;value&quot;: &quot;bar&quot;\n    }\n}</code></pre><p>指定密码套件以阻止<a href=\"https://github.com/etcd-io/etcd/issues/8320\" target=\"_blank\" rel=\"noopener\">较弱的TLS密码套件</a>。<br>当使用无效密码套件请求客户端问候时，TLS握手将失败。<br>例如：</p>\n<pre><code>$ etcd \\\n  --cert-file ./server.crt \\\n  --key-file ./server.key \\\n  --trusted-ca-file ./ca.crt \\\n  --cipher-suites TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384</code></pre><p>然后，客户端请求必须指定服务器中指定的密码套件之一：</p>\n<pre><code># 有效的加密套件\n$ curl \\\n  --cacert ./ca.crt \\\n  --cert ./server.crt \\\n  --key ./server.key \\\n  -L [CLIENT-URL]/metrics \\\n  --ciphers ECDHE-RSA-AES128-GCM-SHA256\n\n# 成功请求\netcd_server_version{server_version=&quot;3.2.22&quot;} 1\n...</code></pre><pre><code># 无效的加密套件\n$ curl \\\n  --cacert ./ca.crt \\\n  --cert ./server.crt \\\n  --key ./server.key \\\n  -L [CLIENT-URL]/metrics \\\n  --ciphers ECDHE-RSA-DES-CBC3-SHA\n\n# 请求失败\n(35) error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure</code></pre><h2 id=\"示例3：集群中的传输安全性和客户端证书\"><a href=\"#示例3：集群中的传输安全性和客户端证书\" class=\"headerlink\" title=\"示例3：集群中的传输安全性和客户端证书\"></a>示例3：集群中的传输安全性和客户端证书</h2><hr>\n<p>etcd支持与上述对等节点通信相同的模型，这意味着集群中etcd成员之间的通信。<br>假设我们有这个<code>ca.crt</code>和两个由此CA签名的成员，它们具有自己的密钥对（<code>member1.crt</code>和<code>member1.key</code>，<code>member2.crt</code>和<code>member2.key</code>），我们按以下方式启动etcd：</p>\n<pre><code>DISCOVERY_URL=... # from https://discovery.etcd.io/new\n\n# member1\n$ etcd --name infra1 --data-dir infra1 \\\n  --peer-client-cert-auth --peer-trusted-ca-file=/path/to/ca.crt --peer-cert-file=/path/to/member1.crt --peer-key-file=/path/to/member1.key \\\n  --initial-advertise-peer-urls=https://10.0.1.10:2380 --listen-peer-urls=https://10.0.1.10:2380 \\\n  --discovery ${DISCOVERY_URL}\n\n# member2\n$ etcd --name infra2 --data-dir infra2 \\\n  --peer-client-cert-auth --peer-trusted-ca-file=/path/to/ca.crt --peer-cert-file=/path/to/member2.crt --peer-key-file=/path/to/member2.key \\\n  --initial-advertise-peer-urls=https://10.0.1.11:2380 --listen-peer-urls=https://10.0.1.11:2380 \\\n  --discovery ${DISCOVERY_URL}</code></pre><p>etcd成员将形成一个集群，并且集群中成员之间的所有通信都将使用客户端证书进行加密和身份验证。 etcd的输出将显示它连接以使用HTTPS的地址。</p>\n<h2 id=\"示例4：自动自签名传输安全性\"><a href=\"#示例4：自动自签名传输安全性\" class=\"headerlink\" title=\"示例4：自动自签名传输安全性\"></a>示例4：自动自签名传输安全性</h2><hr>\n<p>对于需要通信加密而不是身份验证的情况，etcd支持使用自动生成的自签名证书来加密其消息。 因为不需要在etcd之外管理证书和密钥，所以这简化了部署。<br>配置etcd以使用带有<code>--auto-tls</code>和<code>--peer-auto-tls</code>标志的自签名证书进行客户端和对等节点连接：</p>\n<pre><code>DISCOVERY_URL=... # from https://discovery.etcd.io/new\n\n# member1\n$ etcd --name infra1 --data-dir infra1 \\\n  --auto-tls --peer-auto-tls \\\n  --initial-advertise-peer-urls=https://10.0.1.10:2380 --listen-peer-urls=https://10.0.1.10:2380 \\\n  --discovery ${DISCOVERY_URL}\n\n# member2\n$ etcd --name infra2 --data-dir infra2 \\\n  --auto-tls --peer-auto-tls \\\n  --initial-advertise-peer-urls=https://10.0.1.11:2380 --listen-peer-urls=https://10.0.1.11:2380 \\\n  --discovery ${DISCOVERY_URL}</code></pre><p>自签名证书不会验证身份，因此curl将返回错误：</p>\n<pre><code>curl: (60) SSL certificate problem: Invalid certificate chain</code></pre><p>要禁用证书链检查，请使用<code>-k</code>标志调用<code>curl</code>：</p>\n<pre><code>$ curl -k https://127.0.0.1:2379/v2/keys/foo -Xput -d value=bar -v</code></pre><h2 id=\"DNS-SRV的注意事项\"><a href=\"#DNS-SRV的注意事项\" class=\"headerlink\" title=\"DNS SRV的注意事项\"></a>DNS SRV的注意事项</h2><hr>\n<p>如果连接是安全的，则<code>etcd proxy</code>从其客户端TLS终端，并使用<code>--peer-key-file</code>和<code>--peer-cert-file</code>中指定的代理自身的密钥/证书与etcd成员进行通信。</p>\n<p>代理通过给定成员的<code>--advertise-client-urls</code>和<code>--advertise-peer-urls</code>与etcd成员进行通信。 它将客户端请求转发到etcd成员广播的客户端URL，并通过etcd成员广播的对等URL同步初始集群配置。</p>\n<p>为etcd成员启用客户端身份验证后，管理员必须确保代理的<code>--peer-cert-file</code>选项中指定的对等节点证书对该身份验证有效。如果启用了对等节点身份验证，则代理的对等节点证书也必须对对等节点身份验证有效。</p>\n<h2 id=\"TLS-身份验证的注意事项\"><a href=\"#TLS-身份验证的注意事项\" class=\"headerlink\" title=\"TLS 身份验证的注意事项\"></a>TLS 身份验证的注意事项</h2><hr>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/7829\" target=\"_blank\" rel=\"noopener\">v3.2.0开始，TLS证书将在每个客户端连接上重新加载</a>。 这在不停止etcd服务器而替换到期证书时很有用； 可以通过用新证书覆盖旧证书来完成。 刷新每个连接的证书应该没有太多的开销，但是将来可以通过缓存层进行改进。 示例测试可以在<a href=\"https://github.com/coreos/etcd/blob/b041ce5d514a4b4aaeefbffb008f0c7570a18986/integration/v3_grpc_test.go#L1601-L1757\" target=\"_blank\" rel=\"noopener\">这里</a>找到。</p>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/7687\" target=\"_blank\" rel=\"noopener\">v3.2.0开始，服务器使用错误的IP <code>SAN</code>拒绝传入的对等证书</a>。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中包含任何IP地址，则服务器仅在远程IP地址与这些IP地址之一匹配时才对对等节点身份验证。 这是为了防止未经授权的端点加入群集。 例如，对等节点B的CSR（带有cfssl）为：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd peer&quot;,\n  &quot;hosts&quot;: [\n    &quot;*.example.default.svc&quot;,\n    &quot;*.example.default.svc.cluster.local&quot;,\n    &quot;10.138.0.27&quot;\n  ],\n  &quot;key&quot;: {\n    &quot;algo&quot;: &quot;rsa&quot;,\n    &quot;size&quot;: 2048\n  },\n  &quot;names&quot;: [\n    {\n      &quot;C&quot;: &quot;US&quot;,\n      &quot;L&quot;: &quot;CA&quot;,\n      &quot;ST&quot;: &quot;San Francisco&quot;\n    }\n  ]\n}</code></pre><p>当对等节点B的实际IP地址是<code>10.138.0.2</code>，而不是<code>10.138.0.27</code>。 当对等节点B尝试加入集群时，对等节点A将拒绝B，并显示错误x509：证书对<code>10.138.0.27</code>有效，而不对<code>10.138.0.2</code>有效，因为B的远程IP地址与“使用者备用名称（SAN）”字段中的地址不匹配。</p>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/7767\" target=\"_blank\" rel=\"noopener\">v3.2.0开始，服务器在检查SAN时解析TLS DNSNames</a>。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中仅包含DNS名称（不包含IP地址），则仅当这些DNS名称上的正向查找（<code>dig b.com</code>）具有与远程IP匹配的IP时，服务器才对对等身份验证 地址。 例如，对等B的CSR（带有<code>cfssl</code>）为：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd peer&quot;,\n  &quot;hosts&quot;: [\n    &quot;b.com&quot;\n  ],</code></pre><p>当对等节点B的远程IP地址为<code>10.138.0.2</code>时。 当对等节点B尝试加入集群时，对等节点A查找传入的主机<code>b.com</code>以获取IP地址列表（例如<code>dig b.com</code>）。如果列表不包含IP <code>10.138.0.2</code>，则出现错误<code>tls: 10.138.0.2 does not match any of DNSNames [&quot;b.com&quot;]</code>.</p>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/8223\" target=\"_blank\" rel=\"noopener\">v3.2.2开始，如果IP匹配，服务器将接受连接，而无需检查DNS条目</a>。 例如，如果对等节点证书在“使用者备用名称（SAN）”字段中包含IP地址和DNS名称，并且远程IP地址与这些IP地址之一匹配，则服务器仅接受连接而无需进一步检查DNS名称。 例如，对等节点B的CSR（带有<code>cfssl</code>）为：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd peer&quot;,\n  &quot;hosts&quot;: [\n    &quot;invalid.domain&quot;,\n    &quot;10.138.0.2&quot;\n  ],</code></pre><p>当对等节点B的远程IP地址是<code>10.138.0.2</code>并且<code>invalid.domain</code>是无效的主机时。 当对等节点B尝试加入集群时，对等节点A成功地对节点B进行了身份验证，因为“使用者备用名称（SAN）”字段具有有效的匹配IP地址。 有关更多详细信息，请参见问题<a href=\"https://github.com/etcd-io/etcd/issues/8206\" target=\"_blank\" rel=\"noopener\">＃8206</a>。</p>\n<p>从<a href=\"https://github.com/etcd-io/etcd/pull/8281\" target=\"_blank\" rel=\"noopener\">v3.2.5开始，服务器支持在通配符DNS <code>SAN</code>上进行反向查找</a>。 例如，如果对等节点证书在“使用者备用名称”（SAN）字段中仅包含DNS名称（不包含IP地址），则服务器首先对远程IP地址进行反向查找，以获取映射到该地址的名称列表（例如<code>nslookup IPADDR</code>）。如果这些名称的名称与对等节点证书的DNS名称（通过完全匹配或通配符匹配）匹配，则接受连接。 如果没有匹配项，则服务器将对等节点证书中的每个DNS条目进行正向查找（例如，如果条目为<code>*.example.default.svc</code>，则查找<code>example.default.svc</code>），并且仅在主机的解析地址具有匹配的IP时接受连接 地址和对等节点的远程IP地址。 例如，对等B的CSR（带有<code>cfssl</code>）为：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd peer&quot;,\n  &quot;hosts&quot;: [\n    &quot;*.example.default.svc&quot;,\n    &quot;*.example.default.svc.cluster.local&quot;\n  ],</code></pre><p>当对等节点B的远程IP地址为<code>10.138.0.2</code>时。 当对等节点B尝试加入集群时，对等节点A反向查找IP <code>10.138.0.2</code>以获取主机名列表。 并且，“主题备用名称”（SAN）字段中的主机名必须与对等节点B的证书DNS名称完全匹配或与通配符匹配。 如果反向/正向查找均无效，则返回错误<code>&quot;tls: &quot;10.138.0.2&quot; does not match any of DNSNames [&quot;*.example.default.svc&quot;,&quot;*.example.default.svc.cluster.local&quot;]</code>。有关更多详细信息，请参见问题<a href=\"https://github.com/etcd-io/etcd/issues/8268\" target=\"_blank\" rel=\"noopener\">＃8268</a>。</p>\n<p><a href=\"https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.3.md\" target=\"_blank\" rel=\"noopener\">v3.3.0</a>添加了<a href=\"https://github.com/etcd-io/etcd/pull/8616\" target=\"_blank\" rel=\"noopener\">etcd –peer-cert-allowed-cn</a>参数，以支持<a href=\"https://github.com/etcd-io/etcd/issues/8262\" target=\"_blank\" rel=\"noopener\">基于CN（通用名称）的对等节点连接的身份验证</a>。 Kubernetes TLS引导涉及为etcd成员和其他系统组件（例如API服务器，kubelet等）生成动态证书。 为每个组件维护不同的CA可提供对etcd集群的更严格的访问控制，但通常很乏味。 指定<code>--peer-cert-allowed-cn</code>标志时，即使具有共享的CA，节点也只能以匹配的通用名称加入。 例如，三节点群集中的每个成员都设置有CSR（使用<code>cfssl</code>），如下所示：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd.local&quot;,\n  &quot;hosts&quot;: [\n    &quot;m1.etcd.local&quot;,\n    &quot;127.0.0.1&quot;,\n    &quot;localhost&quot;\n  ],</code></pre><pre><code>{\n  &quot;CN&quot;: &quot;etcd.local&quot;,\n  &quot;hosts&quot;: [\n    &quot;m2.etcd.local&quot;,\n    &quot;127.0.0.1&quot;,\n    &quot;localhost&quot;\n  ],</code></pre><pre><code>{\n  &quot;CN&quot;: &quot;etcd.local&quot;,\n  &quot;hosts&quot;: [\n    &quot;m3.etcd.local&quot;,\n    &quot;127.0.0.1&quot;,\n    &quot;localhost&quot;\n  ],</code></pre><p>如果给定<code>--peer-cert-allowed-cn etcd.local</code>，则只有具有相同通用名称的对等方将被认证。 CSR中具有不同CN或<code>--peer-cert-allowed-cn</code>的节点将被拒绝：</p>\n<pre><code>$ etcd --peer-cert-allowed-cn m1.etcd.local\n\nI | embed: rejected connection from &quot;127.0.0.1:48044&quot; (error &quot;CommonName authentication failed&quot;, ServerName &quot;m1.etcd.local&quot;)\nI | embed: rejected connection from &quot;127.0.0.1:55702&quot; (error &quot;remote error: tls: bad certificate&quot;, ServerName &quot;m3.etcd.local&quot;)</code></pre><p>每个进程都应以以下内容开始：</p>\n<pre><code>etcd --peer-cert-allowed-cn etcd.local\n\nI | pkg/netutil: resolving m3.etcd.local:32380 to 127.0.0.1:32380\nI | pkg/netutil: resolving m2.etcd.local:22380 to 127.0.0.1:22380\nI | pkg/netutil: resolving m1.etcd.local:2380 to 127.0.0.1:2380\nI | etcdserver: published {Name:m3 ClientURLs:[https://m3.etcd.local:32379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | etcdserver: published {Name:m1 ClientURLs:[https://m1.etcd.local:2379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | etcdserver: published {Name:m2 ClientURLs:[https://m2.etcd.local:22379]} to cluster 9db03f09b20de32b\nI | embed: ready to serve client requests\nI | embed: serving client requests on 127.0.0.1:32379\nI | embed: serving client requests on 127.0.0.1:22379\nI | embed: serving client requests on 127.0.0.1:2379</code></pre><p><a href=\"https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.2.md\" target=\"_blank\" rel=\"noopener\">v3.2.19</a>和<a href=\"https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.3.md\" target=\"_blank\" rel=\"noopener\">v3.3.4</a>修复了<a href=\"https://github.com/etcd-io/etcd/issues/9541\" target=\"_blank\" rel=\"noopener\">当证书SAN字段仅包含IP地址但不包含域名时TLS重新加载的问题</a>。 例如，如下设置了具有CSR（具有<code>cfssl</code>）的成员：</p>\n<pre><code>{\n  &quot;CN&quot;: &quot;etcd.local&quot;,\n  &quot;hosts&quot;: [\n    &quot;127.0.0.1&quot;\n  ],</code></pre><p>在Go中，仅当服务器的<code>（* tls.Config）.Certificates</code>字段不为空或<code>（* tls.ClientHelloInfo）.ServerName</code>不为空且具有有效SNI时，服务器才会调用<code>（* tls.Config）.GetCertificate</code>来重新加载TLS 来自客户。 以前，etcd始终填充<code>（* tls.Config）</code>。在初始客户端TLS握手上的证书为非空。 因此，总是希望客户端提供匹配的SNI，以便通过TLS验证并触发<code>（* tls.Config）.GetCertificate</code>以重新加载TLS数据。</p>\n<p>但是，其SAN字段<a href=\"https://github.com/etcd-io/etcd/issues/9541\" target=\"_blank\" rel=\"noopener\">仅包括IP地址不包含任何域名的证书</a>将请求<code>* tls.ClientHelloInfo</code>带有空的<code>ServerName</code>字段，从而无法在初始TLS握手时触发TLS重新加载；当需要在线更换过期证书时，这将成为一个问题。</p>\n<p>现在,<code>（* tls.Config）.Certificates</code>在初始TLS客户端握手时创建为空，首先触发<code>（* tls.Config）.GetCertificate</code>，然后在每个新的TLS连接上填充其余证书，即使客户端SNI为 为空（例如，证书仅包括IP）。</p>\n<h2 id=\"主机白名单的注意事项\"><a href=\"#主机白名单的注意事项\" class=\"headerlink\" title=\"主机白名单的注意事项\"></a>主机白名单的注意事项</h2><hr>\n<p><code>etcd --host-whitelist</code>参数指定HTTP客户端请求中可接受的主机名。 客户端源策略可以防止对不安全的etcd服务器的“<a href=\"https://en.wikipedia.org/wiki/DNS_rebinding\" target=\"_blank\" rel=\"noopener\">DNS重新绑定</a>”攻击。 也就是说，任何网站都可以简单地创建一个授权的DNS名称，并将DNS定向到“<code>localhost</code>”（或任何其他地址）。 然后，侦听“<code>localhost</code>”上的etcd服务器的所有HTTP端点都可以访问，因此容易受到DNS重新绑定攻击。 有关更多详细信息，请参见<a href=\"https://bugs.chromium.org/p/project-zero/issues/detail?id=1447#c2\" target=\"_blank\" rel=\"noopener\">CVE-2018-5702</a>。</p>\n<p>客户原始策略的工作方式如下：</p>\n<ol>\n<li>如果客户端通过HTTPS连接是安全的，则允许使用任何主机名。</li>\n<li>如果客户端连接不安全且“<code>HostWhitelist</code>”不为空，则仅允许其Host字段列在白名单中的HTTP请求。</li>\n</ol>\n<p>请注意，无论是否启用身份验证，都会实施客户端来源策略，以进行更严格的控制。</p>\n<p>默认情况下，<code>etcd --host-whitelist</code>和<code>embed.Config.HostWhitelist</code>设置为空以允许所有主机名。请注意，在指定主机名时，不会自动添加回送地址。 要允许环回接口，请手动将其添加到白名单（例如“ <code>localhost</code>”，“<code>127.0.0.1</code>”等）。</p>\n<h2 id=\"常见问题\"><a href=\"#常见问题\" class=\"headerlink\" title=\"常见问题\"></a>常见问题</h2><hr>\n<p>使用TLS客户端身份验证时，我看到SSLv3警报握手失败？<br><code>golang</code>的<code>crypto/tls</code>软件包在使用之前检查证书公钥的密钥用法。要使用证书公共密钥进行客户端身份验证，我们需要在创建证书公共密钥时将<code>clientAuth</code>添加到“<code>Extended Key Usage</code>”中。</p>\n<p>这是操作方法：</p>\n<p>将以下部分添加到openssl.cnf中：</p>\n<pre><code>[ ssl_client ]\n...\n  extendedKeyUsage = clientAuth\n...\n</code></pre><p>创建证书时，请确保在<code>-extensions</code>参数中引用它：</p>\n<pre><code>$ openssl ca -config openssl.cnf -policy policy_anything -extensions ssl_client -out certs/machine.crt -infiles machine.csr</code></pre><p>通过对等证书身份验证，我收到“证书对127.0.0.1有效，而不对$我的Ip有效”</p>\n<p>确保使用主题名称（成员的公共IP地址）对证书进行签名。 例如，<code>etcd-ca</code>工具为其<code>new-cert</code>命令提供了<code>--ip=</code>选项。</p>\n<p>需要在其使用者名称中为成员的FQDN签署证书，使用使用者备用名称（简称IP SAN）添加IP地址。 <code>etcd-ca</code>工具为其<code>new-cert</code>命令提供了<code>--domain=</code>选项，<code>openssl</code>也可以做到<a href=\"http://wiki.cacert.org/FAQ/subjectAltName\" target=\"_blank\" rel=\"noopener\">这</a>一点。</p>\n"},{"title":"gRPC代理","date":"2019-11-24T06:35:15.000Z","_content":"原文地址:[gRPC proxy](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/grpc_proxy.md)\ngRPC代理是在gRPC层（L7）运行的无状态etcd反向代理。代理旨在减少核心etcd群集上的总处理负载。对于水平可伸缩性，它合并了监视和租约API请求。 为了保护集群免受滥用客户端的侵害，它会缓存关键范围请求。\ngRPC代理支持多个etcd服务器端点。 代理启动时，它会随机选择一个etcd服务器端点来使用.该端点将处理所有请求，直到代理检测到端点故障为止。 如果gRPC代理检测到端点故障，它将切换到其他端点（如果有）以向其客户端隐藏故障。 将来可能会支持其他重试策略，例如加权轮询。\n\n## 可扩展的监视 API\n* * *\ngRPC代理将同一键或范围上的多个客户端监视程序（c-watcher）合并为连接到etcd服务器的单个监视程序（s-watcher）。 代理将所有事件从S-watcher广播到其c-watcher。\n假设N个客户端监视相同的密钥，则一个gRPC代理可以将etcd服务器上的监视负载从N减少到1。用户可以部署多个gRPC代理来进一步分配服务器负载。\n在以下示例中，三个客户端监视键A。gRPC代理将三个监视程序合并，从而创建一个附加到etcd服务器的监视程序。\n```\n            +-------------+\n            | etcd 服务器 |\n            +------+------+\n                   ^ 监视 key A (s-watcher)\n                   |\n           +-------+-----+\n           | gRPC 代理  | <-------+\n           |             |         |\n           ++-----+------+         |监视 key A (c-watcher)\n监视 key A ^     ^ 监视 key A    |\n(c-watcher) |     | (c-watcher)    |\n    +-------+-+  ++--------+  +----+----+\n    |  客户端 |  |  客户端 |  |  客户端 |\n    |         |  |         |  |         |\n    +---------+  +---------+  +---------+\n\n```\n### 局限性\n为了有效地将多个客户端监视程序合并为一个监视程序，gRPC代理在可能的情况下将新的c-watcher合并为现有的s-watcher。 由于网络延迟或缓冲的未传递事件，此合并的s-watcher可能与etcd服务器不同步。 如果未指定监视版本，则gRPC代理将不能保证c-watcher从最近的存储修订版本开始监视。 例如，如果客户端从具有修订版1000的etcd服务器监视，则该监视程序将从修订版1000开始。如果客户端从gRPC代理监视，则可以从修订版990开始监视。\n类似的限制也适用于取消。 取消观察者后，etcd服务器的修订版可能大于取消响应修订版。\n对于大多数用例，这两个限制不应引起问题。 将来，可能会有其他选项强制观察者绕过gRPC代理以获得更准确的修订响应。\n\n## 可扩展的租约 API\n* * *\n为了保持其租约有效，客户端必须至少向一个etcd服务器建立一个gRPC流，以发送定期的心跳信号。 如果etcd工作负载涉及大量租约活动分布在许多客户端上，则这些流可能会导致CPU使用率过高。 为了减少核心群集上的流总数，该代理支持租约流合并。\n假设N个客户端正在更新租约，则单个gRPC代理将etcd服务器上的流负载从N减少到1。部署中可能具有其他gRPC代理，以进一步在多个代理之间分配流。\n在以下示例中，三个客户端更新了三个独立的租约（L1，L2和L3）。 gRPC代理将三个客户端租约流（c-stream）合并为连接到etcd服务器的单个租约保持活动流（s-stream）。 代理将客户端租用心跳从c流转发到s流，然后将响应返回到相应的c流。\n```\n          +-------------+\n          | etcd 服务器 |\n          +------+------+\n                 ^\n                 | 心跳 L1, L2, L3\n                 | (s-stream)\n                 v\n         +-------+-----+\n         | gRPC 代理  +<-----------+\n         +---+------+--+            | 心跳 L3\n             ^      ^               | (c-stream)\n心跳 L1 |      | 心跳 L2  |\n(c-stream)   v      v (c-stream)    v\n      +------+-+  +-+------+  +-----+--+\n      | 客户端 |  | 客户端 |  | 客户端 |\n      +--------+  +--------+  +--------+\n\n```\n### 客户保护滥用\ngRPC代理在不违反一致性要求时会缓存请求的响应。 这可以保护etcd服务器免遭严密for循环中滥用客户端的侵害。\n\n## 启动etcd gRPC代理\n* * *\n考虑一个etcd集群包括以下几个静态端点：\n|名字|地址|主机名|\n|---|---|---|\n|infra0|10.0.1.10|infra0.example.com|\n|infra1|10.0.1.11|infra1.example.com|\n|infra2|10.0.1.12|infra2.example.com|\n通过以下命令使用静态节点启动gRPC代理：\n```\n$ etcd grpc-proxy start --endpoints=infra0.example.com,infra1.example.com,infra2.example.com --listen-addr=127.0.0.1:2379\n```\netcd gRPC启动并监听端口2379.它将客户端请求转发到上面提供的三个端点之一。\n通过代理发送请求：\n```\n$ ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 put foo bar\nOK\n$ ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 get foo\nfoo\nbar\n```\n\n## 客户端端点同步和名称解析\n* * *\n代理支持通过写入用户定义的端点来注册其端点以进行发现。 这有两个目的。 首先，它允许客户端将其端点与一组代理端点同步，以实现高可用性。 其次，它是etcd [gRPC命名](https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/)的端点提供程序。\n通过提供用户定义的前缀来注册代理：\n```\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23790 \\\n  --advertise-client-url=127.0.0.1:23790 \\\n  --resolver-prefix=\"___grpc_proxy_endpoint\" \\\n  --resolver-ttl=60\n\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23791 \\\n  --advertise-client-url=127.0.0.1:23791 \\\n  --resolver-prefix=\"___grpc_proxy_endpoint\" \\\n  --resolver-ttl=60\n```\n代理将会列出成员列表中的所有成员：\n```\nETCDCTL_API=3 etcdctl --endpoints=http://localhost:23790 member list --write-out table\n\n+----+---------+--------------------------------+------------+-----------------+\n| ID | STATUS  |              NAME              | PEER ADDRS |  CLIENT ADDRS   |\n+----+---------+--------------------------------+------------+-----------------+\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23791 |\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23790 |\n+----+---------+--------------------------------+------------+-----------------+\n```\n这使客户端可以通过Sync自动发现代理端点：\n```\ncli, err := clientv3.New(clientv3.Config{\n    Endpoints: []string{\"http://localhost:23790\"},\n})\nif err != nil {\n    log.Fatal(err)\n}\ndefer cli.Close()\n\n// fetch registered grpc-proxy endpoints\nif err := cli.Sync(context.Background()); err != nil {\n    log.Fatal(err)\n}\n```\n注意，如果配置的代理没有解析程序前缀，\n```\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23792 \\\n  --advertise-client-url=127.0.0.1:23792\n```\ngrpc-proxy的成员列表API返回其自己的`advertise-client-url`：\n```\nETCDCTL_API=3 etcdctl --endpoints=http://localhost:23792 member list --write-out table\n\n+----+---------+--------------------------------+------------+-----------------+\n| ID | STATUS  |              NAME              | PEER ADDRS |  CLIENT ADDRS   |\n+----+---------+--------------------------------+------------+-----------------+\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23792 |\n+----+---------+--------------------------------+------------+-----------------+\n```\n## 命名空间\n* * *\n假设一个应用程序期望对整个键空间有完全控制，但是etcd集群与其他应用程序共享。 为了使所有应用程序都不会相互干扰地运行，代理可以对etcd键空间进行分区，以便客户端可以访问完整的键空间。 当给代理提供标志`--namespace`时，所有进入代理的客户端请求都将转换为在键上具有用户定义的前缀。 对etcd集群的访问将在前缀下，而来自代理的响应将删除该前缀；对于客户端，显然根本没有前缀。\n要为代理命名空间，请通过`--namespace`启动：\n```\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23790 \\\n  --namespace=my-prefix/\n```\n现在，对代理的访问在etcd集群上透明地加上前缀：\n```\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:23790 put my-key abc\n# OK\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:23790 get my-key\n# my-key\n# abc\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:2379 get my-prefix/my-key\n# my-prefix/my-key\n# abc\n```\n\n## TLS终端\n* * *\n使用来自安全etcd群集的TLS的gRPC代理终端为未加密的本地端点提供服务.\n使用客户端https启动单个成员etcd集群尝试：\n```\n$ etcd --listen-client-urls https://localhost:2379 --advertise-client-urls https://localhost:2379 --cert-file=peer.crt --key-file=peer.key --trusted-ca-file=ca.crt --client-cert-auth\n```\n确认客户端端口正在提供https：\n```\n# fails\n$ ETCDCTL_API=3 etcdctl --endpoints=http://localhost:2379 endpoint status\n# works\n$ ETCDCTL_API=3 etcdctl --endpoints=https://localhost:2379 --cert=client.crt --key=client.key --cacert=ca.crt endpoint status\n```\n接下来，通过使用客户端证书连接到etcd端点`https://localhost2379`在`localhost:12379`上启动gRPC代理：\n```\n$ etcd grpc-proxy start --endpoints=https://localhost:2379 --listen-addr localhost:12379 --cert client.crt --key client.key --cacert=ca.crt --insecure-skip-tls-verify &\n```\n最后，通过在http上将密钥放入代理来测试TLS终端：\n```\n$ ETCDCTL_API=3 etcdctl --endpoints=http://localhost:12379 put abc def\n# OK\n```\n## 指标和健康\n* * *\ngRPC代理为`--endpoints`定义的etcd成员公开了`/health`和Prometheus`/metrics`端点。 另一种方法是定义一个附加URL，该URL将使用`--metrics-addr`参数来响应`/metrics`和`/health`端点。\n```\n$ etcd grpc-proxy start \\\n  --endpoints https://localhost:2379 \\\n  --metrics-addr https://0.0.0.0:4443 \\\n  --listen-addr 127.0.0.1:23790 \\\n  --key client.key \\\n  --key-file proxy-server.key \\\n  --cert client.crt \\\n  --cert-file proxy-server.crt \\\n  --cacert ca.pem \\\n  --trusted-ca-file proxy-ca.pem\n```\n### 已知问题\n代理的主接口同时服务于HTTP2和HTTP/1.1。如果如上例所示，使用TLS设置了代理，则在监听接口上使用诸如cURL之类的客户端时，将要求在返回`/metrics`或`/health`的请求上将协议显式设置为HTTP/1.1。通过使用`--metrics-addr`参数，辅助接口将没有此要求。\n```\n $ curl --cacert proxy-ca.pem --key proxy-client.key --cert proxy-client.crt https://127.0.0.1:23790/metrics --http1.1\n```","source":"_posts/blog/etcd/gRPC代理.md","raw":"---\ntitle: gRPC代理\ndate: 2019-11-24 14:35:15\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址:[gRPC proxy](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/grpc_proxy.md)\ngRPC代理是在gRPC层（L7）运行的无状态etcd反向代理。代理旨在减少核心etcd群集上的总处理负载。对于水平可伸缩性，它合并了监视和租约API请求。 为了保护集群免受滥用客户端的侵害，它会缓存关键范围请求。\ngRPC代理支持多个etcd服务器端点。 代理启动时，它会随机选择一个etcd服务器端点来使用.该端点将处理所有请求，直到代理检测到端点故障为止。 如果gRPC代理检测到端点故障，它将切换到其他端点（如果有）以向其客户端隐藏故障。 将来可能会支持其他重试策略，例如加权轮询。\n\n## 可扩展的监视 API\n* * *\ngRPC代理将同一键或范围上的多个客户端监视程序（c-watcher）合并为连接到etcd服务器的单个监视程序（s-watcher）。 代理将所有事件从S-watcher广播到其c-watcher。\n假设N个客户端监视相同的密钥，则一个gRPC代理可以将etcd服务器上的监视负载从N减少到1。用户可以部署多个gRPC代理来进一步分配服务器负载。\n在以下示例中，三个客户端监视键A。gRPC代理将三个监视程序合并，从而创建一个附加到etcd服务器的监视程序。\n```\n            +-------------+\n            | etcd 服务器 |\n            +------+------+\n                   ^ 监视 key A (s-watcher)\n                   |\n           +-------+-----+\n           | gRPC 代理  | <-------+\n           |             |         |\n           ++-----+------+         |监视 key A (c-watcher)\n监视 key A ^     ^ 监视 key A    |\n(c-watcher) |     | (c-watcher)    |\n    +-------+-+  ++--------+  +----+----+\n    |  客户端 |  |  客户端 |  |  客户端 |\n    |         |  |         |  |         |\n    +---------+  +---------+  +---------+\n\n```\n### 局限性\n为了有效地将多个客户端监视程序合并为一个监视程序，gRPC代理在可能的情况下将新的c-watcher合并为现有的s-watcher。 由于网络延迟或缓冲的未传递事件，此合并的s-watcher可能与etcd服务器不同步。 如果未指定监视版本，则gRPC代理将不能保证c-watcher从最近的存储修订版本开始监视。 例如，如果客户端从具有修订版1000的etcd服务器监视，则该监视程序将从修订版1000开始。如果客户端从gRPC代理监视，则可以从修订版990开始监视。\n类似的限制也适用于取消。 取消观察者后，etcd服务器的修订版可能大于取消响应修订版。\n对于大多数用例，这两个限制不应引起问题。 将来，可能会有其他选项强制观察者绕过gRPC代理以获得更准确的修订响应。\n\n## 可扩展的租约 API\n* * *\n为了保持其租约有效，客户端必须至少向一个etcd服务器建立一个gRPC流，以发送定期的心跳信号。 如果etcd工作负载涉及大量租约活动分布在许多客户端上，则这些流可能会导致CPU使用率过高。 为了减少核心群集上的流总数，该代理支持租约流合并。\n假设N个客户端正在更新租约，则单个gRPC代理将etcd服务器上的流负载从N减少到1。部署中可能具有其他gRPC代理，以进一步在多个代理之间分配流。\n在以下示例中，三个客户端更新了三个独立的租约（L1，L2和L3）。 gRPC代理将三个客户端租约流（c-stream）合并为连接到etcd服务器的单个租约保持活动流（s-stream）。 代理将客户端租用心跳从c流转发到s流，然后将响应返回到相应的c流。\n```\n          +-------------+\n          | etcd 服务器 |\n          +------+------+\n                 ^\n                 | 心跳 L1, L2, L3\n                 | (s-stream)\n                 v\n         +-------+-----+\n         | gRPC 代理  +<-----------+\n         +---+------+--+            | 心跳 L3\n             ^      ^               | (c-stream)\n心跳 L1 |      | 心跳 L2  |\n(c-stream)   v      v (c-stream)    v\n      +------+-+  +-+------+  +-----+--+\n      | 客户端 |  | 客户端 |  | 客户端 |\n      +--------+  +--------+  +--------+\n\n```\n### 客户保护滥用\ngRPC代理在不违反一致性要求时会缓存请求的响应。 这可以保护etcd服务器免遭严密for循环中滥用客户端的侵害。\n\n## 启动etcd gRPC代理\n* * *\n考虑一个etcd集群包括以下几个静态端点：\n|名字|地址|主机名|\n|---|---|---|\n|infra0|10.0.1.10|infra0.example.com|\n|infra1|10.0.1.11|infra1.example.com|\n|infra2|10.0.1.12|infra2.example.com|\n通过以下命令使用静态节点启动gRPC代理：\n```\n$ etcd grpc-proxy start --endpoints=infra0.example.com,infra1.example.com,infra2.example.com --listen-addr=127.0.0.1:2379\n```\netcd gRPC启动并监听端口2379.它将客户端请求转发到上面提供的三个端点之一。\n通过代理发送请求：\n```\n$ ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 put foo bar\nOK\n$ ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 get foo\nfoo\nbar\n```\n\n## 客户端端点同步和名称解析\n* * *\n代理支持通过写入用户定义的端点来注册其端点以进行发现。 这有两个目的。 首先，它允许客户端将其端点与一组代理端点同步，以实现高可用性。 其次，它是etcd [gRPC命名](https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/)的端点提供程序。\n通过提供用户定义的前缀来注册代理：\n```\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23790 \\\n  --advertise-client-url=127.0.0.1:23790 \\\n  --resolver-prefix=\"___grpc_proxy_endpoint\" \\\n  --resolver-ttl=60\n\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23791 \\\n  --advertise-client-url=127.0.0.1:23791 \\\n  --resolver-prefix=\"___grpc_proxy_endpoint\" \\\n  --resolver-ttl=60\n```\n代理将会列出成员列表中的所有成员：\n```\nETCDCTL_API=3 etcdctl --endpoints=http://localhost:23790 member list --write-out table\n\n+----+---------+--------------------------------+------------+-----------------+\n| ID | STATUS  |              NAME              | PEER ADDRS |  CLIENT ADDRS   |\n+----+---------+--------------------------------+------------+-----------------+\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23791 |\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23790 |\n+----+---------+--------------------------------+------------+-----------------+\n```\n这使客户端可以通过Sync自动发现代理端点：\n```\ncli, err := clientv3.New(clientv3.Config{\n    Endpoints: []string{\"http://localhost:23790\"},\n})\nif err != nil {\n    log.Fatal(err)\n}\ndefer cli.Close()\n\n// fetch registered grpc-proxy endpoints\nif err := cli.Sync(context.Background()); err != nil {\n    log.Fatal(err)\n}\n```\n注意，如果配置的代理没有解析程序前缀，\n```\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23792 \\\n  --advertise-client-url=127.0.0.1:23792\n```\ngrpc-proxy的成员列表API返回其自己的`advertise-client-url`：\n```\nETCDCTL_API=3 etcdctl --endpoints=http://localhost:23792 member list --write-out table\n\n+----+---------+--------------------------------+------------+-----------------+\n| ID | STATUS  |              NAME              | PEER ADDRS |  CLIENT ADDRS   |\n+----+---------+--------------------------------+------------+-----------------+\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23792 |\n+----+---------+--------------------------------+------------+-----------------+\n```\n## 命名空间\n* * *\n假设一个应用程序期望对整个键空间有完全控制，但是etcd集群与其他应用程序共享。 为了使所有应用程序都不会相互干扰地运行，代理可以对etcd键空间进行分区，以便客户端可以访问完整的键空间。 当给代理提供标志`--namespace`时，所有进入代理的客户端请求都将转换为在键上具有用户定义的前缀。 对etcd集群的访问将在前缀下，而来自代理的响应将删除该前缀；对于客户端，显然根本没有前缀。\n要为代理命名空间，请通过`--namespace`启动：\n```\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23790 \\\n  --namespace=my-prefix/\n```\n现在，对代理的访问在etcd集群上透明地加上前缀：\n```\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:23790 put my-key abc\n# OK\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:23790 get my-key\n# my-key\n# abc\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:2379 get my-prefix/my-key\n# my-prefix/my-key\n# abc\n```\n\n## TLS终端\n* * *\n使用来自安全etcd群集的TLS的gRPC代理终端为未加密的本地端点提供服务.\n使用客户端https启动单个成员etcd集群尝试：\n```\n$ etcd --listen-client-urls https://localhost:2379 --advertise-client-urls https://localhost:2379 --cert-file=peer.crt --key-file=peer.key --trusted-ca-file=ca.crt --client-cert-auth\n```\n确认客户端端口正在提供https：\n```\n# fails\n$ ETCDCTL_API=3 etcdctl --endpoints=http://localhost:2379 endpoint status\n# works\n$ ETCDCTL_API=3 etcdctl --endpoints=https://localhost:2379 --cert=client.crt --key=client.key --cacert=ca.crt endpoint status\n```\n接下来，通过使用客户端证书连接到etcd端点`https://localhost2379`在`localhost:12379`上启动gRPC代理：\n```\n$ etcd grpc-proxy start --endpoints=https://localhost:2379 --listen-addr localhost:12379 --cert client.crt --key client.key --cacert=ca.crt --insecure-skip-tls-verify &\n```\n最后，通过在http上将密钥放入代理来测试TLS终端：\n```\n$ ETCDCTL_API=3 etcdctl --endpoints=http://localhost:12379 put abc def\n# OK\n```\n## 指标和健康\n* * *\ngRPC代理为`--endpoints`定义的etcd成员公开了`/health`和Prometheus`/metrics`端点。 另一种方法是定义一个附加URL，该URL将使用`--metrics-addr`参数来响应`/metrics`和`/health`端点。\n```\n$ etcd grpc-proxy start \\\n  --endpoints https://localhost:2379 \\\n  --metrics-addr https://0.0.0.0:4443 \\\n  --listen-addr 127.0.0.1:23790 \\\n  --key client.key \\\n  --key-file proxy-server.key \\\n  --cert client.crt \\\n  --cert-file proxy-server.crt \\\n  --cacert ca.pem \\\n  --trusted-ca-file proxy-ca.pem\n```\n### 已知问题\n代理的主接口同时服务于HTTP2和HTTP/1.1。如果如上例所示，使用TLS设置了代理，则在监听接口上使用诸如cURL之类的客户端时，将要求在返回`/metrics`或`/health`的请求上将协议显式设置为HTTP/1.1。通过使用`--metrics-addr`参数，辅助接口将没有此要求。\n```\n $ curl --cacert proxy-ca.pem --key proxy-client.key --cert proxy-client.crt https://127.0.0.1:23790/metrics --http1.1\n```","slug":"blog/etcd/gRPC代理","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyhg001ok0vq6uxxdwcd","content":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/grpc_proxy.md\" target=\"_blank\" rel=\"noopener\">gRPC proxy</a><br>gRPC代理是在gRPC层（L7）运行的无状态etcd反向代理。代理旨在减少核心etcd群集上的总处理负载。对于水平可伸缩性，它合并了监视和租约API请求。 为了保护集群免受滥用客户端的侵害，它会缓存关键范围请求。<br>gRPC代理支持多个etcd服务器端点。 代理启动时，它会随机选择一个etcd服务器端点来使用.该端点将处理所有请求，直到代理检测到端点故障为止。 如果gRPC代理检测到端点故障，它将切换到其他端点（如果有）以向其客户端隐藏故障。 将来可能会支持其他重试策略，例如加权轮询。</p>\n<h2 id=\"可扩展的监视-API\"><a href=\"#可扩展的监视-API\" class=\"headerlink\" title=\"可扩展的监视 API\"></a>可扩展的监视 API</h2><hr>\n<p>gRPC代理将同一键或范围上的多个客户端监视程序（c-watcher）合并为连接到etcd服务器的单个监视程序（s-watcher）。 代理将所有事件从S-watcher广播到其c-watcher。<br>假设N个客户端监视相同的密钥，则一个gRPC代理可以将etcd服务器上的监视负载从N减少到1。用户可以部署多个gRPC代理来进一步分配服务器负载。<br>在以下示例中，三个客户端监视键A。gRPC代理将三个监视程序合并，从而创建一个附加到etcd服务器的监视程序。</p>\n<pre><code>            +-------------+\n            | etcd 服务器 |\n            +------+------+\n                   ^ 监视 key A (s-watcher)\n                   |\n           +-------+-----+\n           | gRPC 代理  | &lt;-------+\n           |             |         |\n           ++-----+------+         |监视 key A (c-watcher)\n监视 key A ^     ^ 监视 key A    |\n(c-watcher) |     | (c-watcher)    |\n    +-------+-+  ++--------+  +----+----+\n    |  客户端 |  |  客户端 |  |  客户端 |\n    |         |  |         |  |         |\n    +---------+  +---------+  +---------+\n</code></pre><h3 id=\"局限性\"><a href=\"#局限性\" class=\"headerlink\" title=\"局限性\"></a>局限性</h3><p>为了有效地将多个客户端监视程序合并为一个监视程序，gRPC代理在可能的情况下将新的c-watcher合并为现有的s-watcher。 由于网络延迟或缓冲的未传递事件，此合并的s-watcher可能与etcd服务器不同步。 如果未指定监视版本，则gRPC代理将不能保证c-watcher从最近的存储修订版本开始监视。 例如，如果客户端从具有修订版1000的etcd服务器监视，则该监视程序将从修订版1000开始。如果客户端从gRPC代理监视，则可以从修订版990开始监视。<br>类似的限制也适用于取消。 取消观察者后，etcd服务器的修订版可能大于取消响应修订版。<br>对于大多数用例，这两个限制不应引起问题。 将来，可能会有其他选项强制观察者绕过gRPC代理以获得更准确的修订响应。</p>\n<h2 id=\"可扩展的租约-API\"><a href=\"#可扩展的租约-API\" class=\"headerlink\" title=\"可扩展的租约 API\"></a>可扩展的租约 API</h2><hr>\n<p>为了保持其租约有效，客户端必须至少向一个etcd服务器建立一个gRPC流，以发送定期的心跳信号。 如果etcd工作负载涉及大量租约活动分布在许多客户端上，则这些流可能会导致CPU使用率过高。 为了减少核心群集上的流总数，该代理支持租约流合并。<br>假设N个客户端正在更新租约，则单个gRPC代理将etcd服务器上的流负载从N减少到1。部署中可能具有其他gRPC代理，以进一步在多个代理之间分配流。<br>在以下示例中，三个客户端更新了三个独立的租约（L1，L2和L3）。 gRPC代理将三个客户端租约流（c-stream）合并为连接到etcd服务器的单个租约保持活动流（s-stream）。 代理将客户端租用心跳从c流转发到s流，然后将响应返回到相应的c流。</p>\n<pre><code>          +-------------+\n          | etcd 服务器 |\n          +------+------+\n                 ^\n                 | 心跳 L1, L2, L3\n                 | (s-stream)\n                 v\n         +-------+-----+\n         | gRPC 代理  +&lt;-----------+\n         +---+------+--+            | 心跳 L3\n             ^      ^               | (c-stream)\n心跳 L1 |      | 心跳 L2  |\n(c-stream)   v      v (c-stream)    v\n      +------+-+  +-+------+  +-----+--+\n      | 客户端 |  | 客户端 |  | 客户端 |\n      +--------+  +--------+  +--------+\n</code></pre><h3 id=\"客户保护滥用\"><a href=\"#客户保护滥用\" class=\"headerlink\" title=\"客户保护滥用\"></a>客户保护滥用</h3><p>gRPC代理在不违反一致性要求时会缓存请求的响应。 这可以保护etcd服务器免遭严密for循环中滥用客户端的侵害。</p>\n<h2 id=\"启动etcd-gRPC代理\"><a href=\"#启动etcd-gRPC代理\" class=\"headerlink\" title=\"启动etcd gRPC代理\"></a>启动etcd gRPC代理</h2><hr>\n<p>考虑一个etcd集群包括以下几个静态端点：<br>|名字|地址|主机名|<br>|—|—|—|<br>|infra0|10.0.1.10|infra0.example.com|<br>|infra1|10.0.1.11|infra1.example.com|<br>|infra2|10.0.1.12|infra2.example.com|<br>通过以下命令使用静态节点启动gRPC代理：</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=infra0.example.com,infra1.example.com,infra2.example.com --listen-addr=127.0.0.1:2379</code></pre><p>etcd gRPC启动并监听端口2379.它将客户端请求转发到上面提供的三个端点之一。<br>通过代理发送请求：</p>\n<pre><code>$ ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 put foo bar\nOK\n$ ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 get foo\nfoo\nbar</code></pre><h2 id=\"客户端端点同步和名称解析\"><a href=\"#客户端端点同步和名称解析\" class=\"headerlink\" title=\"客户端端点同步和名称解析\"></a>客户端端点同步和名称解析</h2><hr>\n<p>代理支持通过写入用户定义的端点来注册其端点以进行发现。 这有两个目的。 首先，它允许客户端将其端点与一组代理端点同步，以实现高可用性。 其次，它是etcd <a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/\">gRPC命名</a>的端点提供程序。<br>通过提供用户定义的前缀来注册代理：</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23790 \\\n  --advertise-client-url=127.0.0.1:23790 \\\n  --resolver-prefix=&quot;___grpc_proxy_endpoint&quot; \\\n  --resolver-ttl=60\n\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23791 \\\n  --advertise-client-url=127.0.0.1:23791 \\\n  --resolver-prefix=&quot;___grpc_proxy_endpoint&quot; \\\n  --resolver-ttl=60</code></pre><p>代理将会列出成员列表中的所有成员：</p>\n<pre><code>ETCDCTL_API=3 etcdctl --endpoints=http://localhost:23790 member list --write-out table\n\n+----+---------+--------------------------------+------------+-----------------+\n| ID | STATUS  |              NAME              | PEER ADDRS |  CLIENT ADDRS   |\n+----+---------+--------------------------------+------------+-----------------+\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23791 |\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23790 |\n+----+---------+--------------------------------+------------+-----------------+</code></pre><p>这使客户端可以通过Sync自动发现代理端点：</p>\n<pre><code>cli, err := clientv3.New(clientv3.Config{\n    Endpoints: []string{&quot;http://localhost:23790&quot;},\n})\nif err != nil {\n    log.Fatal(err)\n}\ndefer cli.Close()\n\n// fetch registered grpc-proxy endpoints\nif err := cli.Sync(context.Background()); err != nil {\n    log.Fatal(err)\n}</code></pre><p>注意，如果配置的代理没有解析程序前缀，</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23792 \\\n  --advertise-client-url=127.0.0.1:23792</code></pre><p>grpc-proxy的成员列表API返回其自己的<code>advertise-client-url</code>：</p>\n<pre><code>ETCDCTL_API=3 etcdctl --endpoints=http://localhost:23792 member list --write-out table\n\n+----+---------+--------------------------------+------------+-----------------+\n| ID | STATUS  |              NAME              | PEER ADDRS |  CLIENT ADDRS   |\n+----+---------+--------------------------------+------------+-----------------+\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23792 |\n+----+---------+--------------------------------+------------+-----------------+</code></pre><h2 id=\"命名空间\"><a href=\"#命名空间\" class=\"headerlink\" title=\"命名空间\"></a>命名空间</h2><hr>\n<p>假设一个应用程序期望对整个键空间有完全控制，但是etcd集群与其他应用程序共享。 为了使所有应用程序都不会相互干扰地运行，代理可以对etcd键空间进行分区，以便客户端可以访问完整的键空间。 当给代理提供标志<code>--namespace</code>时，所有进入代理的客户端请求都将转换为在键上具有用户定义的前缀。 对etcd集群的访问将在前缀下，而来自代理的响应将删除该前缀；对于客户端，显然根本没有前缀。<br>要为代理命名空间，请通过<code>--namespace</code>启动：</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23790 \\\n  --namespace=my-prefix/</code></pre><p>现在，对代理的访问在etcd集群上透明地加上前缀：</p>\n<pre><code>$ ETCDCTL_API=3 etcdctl --endpoints=localhost:23790 put my-key abc\n# OK\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:23790 get my-key\n# my-key\n# abc\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:2379 get my-prefix/my-key\n# my-prefix/my-key\n# abc</code></pre><h2 id=\"TLS终端\"><a href=\"#TLS终端\" class=\"headerlink\" title=\"TLS终端\"></a>TLS终端</h2><hr>\n<p>使用来自安全etcd群集的TLS的gRPC代理终端为未加密的本地端点提供服务.<br>使用客户端https启动单个成员etcd集群尝试：</p>\n<pre><code>$ etcd --listen-client-urls https://localhost:2379 --advertise-client-urls https://localhost:2379 --cert-file=peer.crt --key-file=peer.key --trusted-ca-file=ca.crt --client-cert-auth</code></pre><p>确认客户端端口正在提供https：</p>\n<pre><code># fails\n$ ETCDCTL_API=3 etcdctl --endpoints=http://localhost:2379 endpoint status\n# works\n$ ETCDCTL_API=3 etcdctl --endpoints=https://localhost:2379 --cert=client.crt --key=client.key --cacert=ca.crt endpoint status</code></pre><p>接下来，通过使用客户端证书连接到etcd端点<code>https://localhost2379</code>在<code>localhost:12379</code>上启动gRPC代理：</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=https://localhost:2379 --listen-addr localhost:12379 --cert client.crt --key client.key --cacert=ca.crt --insecure-skip-tls-verify &amp;</code></pre><p>最后，通过在http上将密钥放入代理来测试TLS终端：</p>\n<pre><code>$ ETCDCTL_API=3 etcdctl --endpoints=http://localhost:12379 put abc def\n# OK</code></pre><h2 id=\"指标和健康\"><a href=\"#指标和健康\" class=\"headerlink\" title=\"指标和健康\"></a>指标和健康</h2><hr>\n<p>gRPC代理为<code>--endpoints</code>定义的etcd成员公开了<code>/health</code>和Prometheus<code>/metrics</code>端点。 另一种方法是定义一个附加URL，该URL将使用<code>--metrics-addr</code>参数来响应<code>/metrics</code>和<code>/health</code>端点。</p>\n<pre><code>$ etcd grpc-proxy start \\\n  --endpoints https://localhost:2379 \\\n  --metrics-addr https://0.0.0.0:4443 \\\n  --listen-addr 127.0.0.1:23790 \\\n  --key client.key \\\n  --key-file proxy-server.key \\\n  --cert client.crt \\\n  --cert-file proxy-server.crt \\\n  --cacert ca.pem \\\n  --trusted-ca-file proxy-ca.pem</code></pre><h3 id=\"已知问题\"><a href=\"#已知问题\" class=\"headerlink\" title=\"已知问题\"></a>已知问题</h3><p>代理的主接口同时服务于HTTP2和HTTP/1.1。如果如上例所示，使用TLS设置了代理，则在监听接口上使用诸如cURL之类的客户端时，将要求在返回<code>/metrics</code>或<code>/health</code>的请求上将协议显式设置为HTTP/1.1。通过使用<code>--metrics-addr</code>参数，辅助接口将没有此要求。</p>\n<pre><code> $ curl --cacert proxy-ca.pem --key proxy-client.key --cert proxy-client.crt https://127.0.0.1:23790/metrics --http1.1</code></pre>","site":{"data":{}},"excerpt":"","more":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/grpc_proxy.md\" target=\"_blank\" rel=\"noopener\">gRPC proxy</a><br>gRPC代理是在gRPC层（L7）运行的无状态etcd反向代理。代理旨在减少核心etcd群集上的总处理负载。对于水平可伸缩性，它合并了监视和租约API请求。 为了保护集群免受滥用客户端的侵害，它会缓存关键范围请求。<br>gRPC代理支持多个etcd服务器端点。 代理启动时，它会随机选择一个etcd服务器端点来使用.该端点将处理所有请求，直到代理检测到端点故障为止。 如果gRPC代理检测到端点故障，它将切换到其他端点（如果有）以向其客户端隐藏故障。 将来可能会支持其他重试策略，例如加权轮询。</p>\n<h2 id=\"可扩展的监视-API\"><a href=\"#可扩展的监视-API\" class=\"headerlink\" title=\"可扩展的监视 API\"></a>可扩展的监视 API</h2><hr>\n<p>gRPC代理将同一键或范围上的多个客户端监视程序（c-watcher）合并为连接到etcd服务器的单个监视程序（s-watcher）。 代理将所有事件从S-watcher广播到其c-watcher。<br>假设N个客户端监视相同的密钥，则一个gRPC代理可以将etcd服务器上的监视负载从N减少到1。用户可以部署多个gRPC代理来进一步分配服务器负载。<br>在以下示例中，三个客户端监视键A。gRPC代理将三个监视程序合并，从而创建一个附加到etcd服务器的监视程序。</p>\n<pre><code>            +-------------+\n            | etcd 服务器 |\n            +------+------+\n                   ^ 监视 key A (s-watcher)\n                   |\n           +-------+-----+\n           | gRPC 代理  | &lt;-------+\n           |             |         |\n           ++-----+------+         |监视 key A (c-watcher)\n监视 key A ^     ^ 监视 key A    |\n(c-watcher) |     | (c-watcher)    |\n    +-------+-+  ++--------+  +----+----+\n    |  客户端 |  |  客户端 |  |  客户端 |\n    |         |  |         |  |         |\n    +---------+  +---------+  +---------+\n</code></pre><h3 id=\"局限性\"><a href=\"#局限性\" class=\"headerlink\" title=\"局限性\"></a>局限性</h3><p>为了有效地将多个客户端监视程序合并为一个监视程序，gRPC代理在可能的情况下将新的c-watcher合并为现有的s-watcher。 由于网络延迟或缓冲的未传递事件，此合并的s-watcher可能与etcd服务器不同步。 如果未指定监视版本，则gRPC代理将不能保证c-watcher从最近的存储修订版本开始监视。 例如，如果客户端从具有修订版1000的etcd服务器监视，则该监视程序将从修订版1000开始。如果客户端从gRPC代理监视，则可以从修订版990开始监视。<br>类似的限制也适用于取消。 取消观察者后，etcd服务器的修订版可能大于取消响应修订版。<br>对于大多数用例，这两个限制不应引起问题。 将来，可能会有其他选项强制观察者绕过gRPC代理以获得更准确的修订响应。</p>\n<h2 id=\"可扩展的租约-API\"><a href=\"#可扩展的租约-API\" class=\"headerlink\" title=\"可扩展的租约 API\"></a>可扩展的租约 API</h2><hr>\n<p>为了保持其租约有效，客户端必须至少向一个etcd服务器建立一个gRPC流，以发送定期的心跳信号。 如果etcd工作负载涉及大量租约活动分布在许多客户端上，则这些流可能会导致CPU使用率过高。 为了减少核心群集上的流总数，该代理支持租约流合并。<br>假设N个客户端正在更新租约，则单个gRPC代理将etcd服务器上的流负载从N减少到1。部署中可能具有其他gRPC代理，以进一步在多个代理之间分配流。<br>在以下示例中，三个客户端更新了三个独立的租约（L1，L2和L3）。 gRPC代理将三个客户端租约流（c-stream）合并为连接到etcd服务器的单个租约保持活动流（s-stream）。 代理将客户端租用心跳从c流转发到s流，然后将响应返回到相应的c流。</p>\n<pre><code>          +-------------+\n          | etcd 服务器 |\n          +------+------+\n                 ^\n                 | 心跳 L1, L2, L3\n                 | (s-stream)\n                 v\n         +-------+-----+\n         | gRPC 代理  +&lt;-----------+\n         +---+------+--+            | 心跳 L3\n             ^      ^               | (c-stream)\n心跳 L1 |      | 心跳 L2  |\n(c-stream)   v      v (c-stream)    v\n      +------+-+  +-+------+  +-----+--+\n      | 客户端 |  | 客户端 |  | 客户端 |\n      +--------+  +--------+  +--------+\n</code></pre><h3 id=\"客户保护滥用\"><a href=\"#客户保护滥用\" class=\"headerlink\" title=\"客户保护滥用\"></a>客户保护滥用</h3><p>gRPC代理在不违反一致性要求时会缓存请求的响应。 这可以保护etcd服务器免遭严密for循环中滥用客户端的侵害。</p>\n<h2 id=\"启动etcd-gRPC代理\"><a href=\"#启动etcd-gRPC代理\" class=\"headerlink\" title=\"启动etcd gRPC代理\"></a>启动etcd gRPC代理</h2><hr>\n<p>考虑一个etcd集群包括以下几个静态端点：<br>|名字|地址|主机名|<br>|—|—|—|<br>|infra0|10.0.1.10|infra0.example.com|<br>|infra1|10.0.1.11|infra1.example.com|<br>|infra2|10.0.1.12|infra2.example.com|<br>通过以下命令使用静态节点启动gRPC代理：</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=infra0.example.com,infra1.example.com,infra2.example.com --listen-addr=127.0.0.1:2379</code></pre><p>etcd gRPC启动并监听端口2379.它将客户端请求转发到上面提供的三个端点之一。<br>通过代理发送请求：</p>\n<pre><code>$ ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 put foo bar\nOK\n$ ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 get foo\nfoo\nbar</code></pre><h2 id=\"客户端端点同步和名称解析\"><a href=\"#客户端端点同步和名称解析\" class=\"headerlink\" title=\"客户端端点同步和名称解析\"></a>客户端端点同步和名称解析</h2><hr>\n<p>代理支持通过写入用户定义的端点来注册其端点以进行发现。 这有两个目的。 首先，它允许客户端将其端点与一组代理端点同步，以实现高可用性。 其次，它是etcd <a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/\">gRPC命名</a>的端点提供程序。<br>通过提供用户定义的前缀来注册代理：</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23790 \\\n  --advertise-client-url=127.0.0.1:23790 \\\n  --resolver-prefix=&quot;___grpc_proxy_endpoint&quot; \\\n  --resolver-ttl=60\n\n$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23791 \\\n  --advertise-client-url=127.0.0.1:23791 \\\n  --resolver-prefix=&quot;___grpc_proxy_endpoint&quot; \\\n  --resolver-ttl=60</code></pre><p>代理将会列出成员列表中的所有成员：</p>\n<pre><code>ETCDCTL_API=3 etcdctl --endpoints=http://localhost:23790 member list --write-out table\n\n+----+---------+--------------------------------+------------+-----------------+\n| ID | STATUS  |              NAME              | PEER ADDRS |  CLIENT ADDRS   |\n+----+---------+--------------------------------+------------+-----------------+\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23791 |\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23790 |\n+----+---------+--------------------------------+------------+-----------------+</code></pre><p>这使客户端可以通过Sync自动发现代理端点：</p>\n<pre><code>cli, err := clientv3.New(clientv3.Config{\n    Endpoints: []string{&quot;http://localhost:23790&quot;},\n})\nif err != nil {\n    log.Fatal(err)\n}\ndefer cli.Close()\n\n// fetch registered grpc-proxy endpoints\nif err := cli.Sync(context.Background()); err != nil {\n    log.Fatal(err)\n}</code></pre><p>注意，如果配置的代理没有解析程序前缀，</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23792 \\\n  --advertise-client-url=127.0.0.1:23792</code></pre><p>grpc-proxy的成员列表API返回其自己的<code>advertise-client-url</code>：</p>\n<pre><code>ETCDCTL_API=3 etcdctl --endpoints=http://localhost:23792 member list --write-out table\n\n+----+---------+--------------------------------+------------+-----------------+\n| ID | STATUS  |              NAME              | PEER ADDRS |  CLIENT ADDRS   |\n+----+---------+--------------------------------+------------+-----------------+\n|  0 | started | Gyu-Hos-MBP.sfo.coreos.systems |            | 127.0.0.1:23792 |\n+----+---------+--------------------------------+------------+-----------------+</code></pre><h2 id=\"命名空间\"><a href=\"#命名空间\" class=\"headerlink\" title=\"命名空间\"></a>命名空间</h2><hr>\n<p>假设一个应用程序期望对整个键空间有完全控制，但是etcd集群与其他应用程序共享。 为了使所有应用程序都不会相互干扰地运行，代理可以对etcd键空间进行分区，以便客户端可以访问完整的键空间。 当给代理提供标志<code>--namespace</code>时，所有进入代理的客户端请求都将转换为在键上具有用户定义的前缀。 对etcd集群的访问将在前缀下，而来自代理的响应将删除该前缀；对于客户端，显然根本没有前缀。<br>要为代理命名空间，请通过<code>--namespace</code>启动：</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=localhost:2379 \\\n  --listen-addr=127.0.0.1:23790 \\\n  --namespace=my-prefix/</code></pre><p>现在，对代理的访问在etcd集群上透明地加上前缀：</p>\n<pre><code>$ ETCDCTL_API=3 etcdctl --endpoints=localhost:23790 put my-key abc\n# OK\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:23790 get my-key\n# my-key\n# abc\n$ ETCDCTL_API=3 etcdctl --endpoints=localhost:2379 get my-prefix/my-key\n# my-prefix/my-key\n# abc</code></pre><h2 id=\"TLS终端\"><a href=\"#TLS终端\" class=\"headerlink\" title=\"TLS终端\"></a>TLS终端</h2><hr>\n<p>使用来自安全etcd群集的TLS的gRPC代理终端为未加密的本地端点提供服务.<br>使用客户端https启动单个成员etcd集群尝试：</p>\n<pre><code>$ etcd --listen-client-urls https://localhost:2379 --advertise-client-urls https://localhost:2379 --cert-file=peer.crt --key-file=peer.key --trusted-ca-file=ca.crt --client-cert-auth</code></pre><p>确认客户端端口正在提供https：</p>\n<pre><code># fails\n$ ETCDCTL_API=3 etcdctl --endpoints=http://localhost:2379 endpoint status\n# works\n$ ETCDCTL_API=3 etcdctl --endpoints=https://localhost:2379 --cert=client.crt --key=client.key --cacert=ca.crt endpoint status</code></pre><p>接下来，通过使用客户端证书连接到etcd端点<code>https://localhost2379</code>在<code>localhost:12379</code>上启动gRPC代理：</p>\n<pre><code>$ etcd grpc-proxy start --endpoints=https://localhost:2379 --listen-addr localhost:12379 --cert client.crt --key client.key --cacert=ca.crt --insecure-skip-tls-verify &amp;</code></pre><p>最后，通过在http上将密钥放入代理来测试TLS终端：</p>\n<pre><code>$ ETCDCTL_API=3 etcdctl --endpoints=http://localhost:12379 put abc def\n# OK</code></pre><h2 id=\"指标和健康\"><a href=\"#指标和健康\" class=\"headerlink\" title=\"指标和健康\"></a>指标和健康</h2><hr>\n<p>gRPC代理为<code>--endpoints</code>定义的etcd成员公开了<code>/health</code>和Prometheus<code>/metrics</code>端点。 另一种方法是定义一个附加URL，该URL将使用<code>--metrics-addr</code>参数来响应<code>/metrics</code>和<code>/health</code>端点。</p>\n<pre><code>$ etcd grpc-proxy start \\\n  --endpoints https://localhost:2379 \\\n  --metrics-addr https://0.0.0.0:4443 \\\n  --listen-addr 127.0.0.1:23790 \\\n  --key client.key \\\n  --key-file proxy-server.key \\\n  --cert client.crt \\\n  --cert-file proxy-server.crt \\\n  --cacert ca.pem \\\n  --trusted-ca-file proxy-ca.pem</code></pre><h3 id=\"已知问题\"><a href=\"#已知问题\" class=\"headerlink\" title=\"已知问题\"></a>已知问题</h3><p>代理的主接口同时服务于HTTP2和HTTP/1.1。如果如上例所示，使用TLS设置了代理，则在监听接口上使用诸如cURL之类的客户端时，将要求在返回<code>/metrics</code>或<code>/health</code>的请求上将协议显式设置为HTTP/1.1。通过使用<code>--metrics-addr</code>参数，辅助接口将没有此要求。</p>\n<pre><code> $ curl --cacert proxy-ca.pem --key proxy-client.key --cert proxy-client.crt https://127.0.0.1:23790/metrics --http1.1</code></pre>"},{"title":"gRPC命名与发现","date":"2019-11-23T04:32:31.000Z","_content":"原文地址:[gRPC naming and discovery](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/grpc_naming.md)\netcd提供一个gRPC解析器支持备用的命名系统，该命名系统从etcd获取主机以发现gRPC服务。以下机制基于监视对以服务名称为前缀的Key的更新。\n通过go-grpc使用etcd发现服务\n\n* * *\netcd客户端提供一个gRPC解析器通过etcd后端解析gRPC主机,解析器通过etcd客户端初始化并指定了解析目标:\n```\nimport (\n        \"go.etcd.io/etcd/clientv3\"\n        etcdnaming \"go.etcd.io/etcd/clientv3/naming\"\n\n        \"google.golang.org/grpc\"\n)\n\n...\n\ncli, cerr := clientv3.NewFromURL(\"http://localhost:2379\")\nr := &etcdnaming.GRPCResolver{Client: cli}\nb := grpc.RoundRobin(r)\nconn, gerr := grpc.Dial(\"my-service\", grpc.WithBalancer(b), grpc.WithBlock(), ...)\n```\n### 管理服务主机\netcd解析器对于解析目标前缀下所有Keys后面跟一个\"/\"(例如\"my-service/\"),使用JSON编码go-grpc`naming.Update`值作为潜在的服务主机。通过创建一个新的Key将主机添加到服务中，通过删除Keys将主机从服务中删除。\n### 添加一个主机\n一个新的主机可以通过`etcdctl`添加到服务中：\n```\nETCDCTL_API=3 etcdctl put my-service/1.2.3.4 '{\"Addr\":\"1.2.3.4\",\"Metadata\":\"...\"}'\n```\netcd客户端的`GRPCResolver.Update`方法也可以通过key匹配`Addr`注册一个新的主机到服务中：\n```\nr.Update(context.TODO(), \"my-service\", naming.Update{Op: naming.Add, Addr: \"1.2.3.4\", Metadata: \"...\"})\n```\n### 删除一个主机\n通过etcdctl可以从服务中删除一个主机:\n```\nETCDCTL_API=3 etcdctl del my-service/1.2.3.4\n```\netcd 客户端的`GRPCResolver.Update`方法也可以删除一个主机：\n```\nr.Update(context.TODO(), \"my-service\", naming.Update{Op: naming.Delete, Addr: \"1.2.3.4\"})\n```\n### 注册一个主机并绑定一个租约\n注册一个主机ging绑定一个租约确保如果主机不能维护保持存活的心跳(例如机器宕机)，该主机将会从服务中移除。\n```\nlease=`ETCDCTL_API=3 etcdctl lease grant 5 | cut -f2 -d' '`\nETCDCTL_API=3 etcdctl put --lease=$lease my-service/1.2.3.4 '{\"Addr\":\"1.2.3.4\",\"Metadata\":\"...\"}'\nETCDCTL_API=3 etcdctl lease keep-alive $lease\n```","source":"_posts/blog/etcd/gRPC命名与发现.md","raw":"---\ntitle: gRPC命名与发现\ndate: 2019-11-23 12:32:31\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址:[gRPC naming and discovery](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/grpc_naming.md)\netcd提供一个gRPC解析器支持备用的命名系统，该命名系统从etcd获取主机以发现gRPC服务。以下机制基于监视对以服务名称为前缀的Key的更新。\n通过go-grpc使用etcd发现服务\n\n* * *\netcd客户端提供一个gRPC解析器通过etcd后端解析gRPC主机,解析器通过etcd客户端初始化并指定了解析目标:\n```\nimport (\n        \"go.etcd.io/etcd/clientv3\"\n        etcdnaming \"go.etcd.io/etcd/clientv3/naming\"\n\n        \"google.golang.org/grpc\"\n)\n\n...\n\ncli, cerr := clientv3.NewFromURL(\"http://localhost:2379\")\nr := &etcdnaming.GRPCResolver{Client: cli}\nb := grpc.RoundRobin(r)\nconn, gerr := grpc.Dial(\"my-service\", grpc.WithBalancer(b), grpc.WithBlock(), ...)\n```\n### 管理服务主机\netcd解析器对于解析目标前缀下所有Keys后面跟一个\"/\"(例如\"my-service/\"),使用JSON编码go-grpc`naming.Update`值作为潜在的服务主机。通过创建一个新的Key将主机添加到服务中，通过删除Keys将主机从服务中删除。\n### 添加一个主机\n一个新的主机可以通过`etcdctl`添加到服务中：\n```\nETCDCTL_API=3 etcdctl put my-service/1.2.3.4 '{\"Addr\":\"1.2.3.4\",\"Metadata\":\"...\"}'\n```\netcd客户端的`GRPCResolver.Update`方法也可以通过key匹配`Addr`注册一个新的主机到服务中：\n```\nr.Update(context.TODO(), \"my-service\", naming.Update{Op: naming.Add, Addr: \"1.2.3.4\", Metadata: \"...\"})\n```\n### 删除一个主机\n通过etcdctl可以从服务中删除一个主机:\n```\nETCDCTL_API=3 etcdctl del my-service/1.2.3.4\n```\netcd 客户端的`GRPCResolver.Update`方法也可以删除一个主机：\n```\nr.Update(context.TODO(), \"my-service\", naming.Update{Op: naming.Delete, Addr: \"1.2.3.4\"})\n```\n### 注册一个主机并绑定一个租约\n注册一个主机ging绑定一个租约确保如果主机不能维护保持存活的心跳(例如机器宕机)，该主机将会从服务中移除。\n```\nlease=`ETCDCTL_API=3 etcdctl lease grant 5 | cut -f2 -d' '`\nETCDCTL_API=3 etcdctl put --lease=$lease my-service/1.2.3.4 '{\"Addr\":\"1.2.3.4\",\"Metadata\":\"...\"}'\nETCDCTL_API=3 etcdctl lease keep-alive $lease\n```","slug":"blog/etcd/gRPC命名与发现","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyhi001rk0vqc0kv07qv","content":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/grpc_naming.md\" target=\"_blank\" rel=\"noopener\">gRPC naming and discovery</a><br>etcd提供一个gRPC解析器支持备用的命名系统，该命名系统从etcd获取主机以发现gRPC服务。以下机制基于监视对以服务名称为前缀的Key的更新。<br>通过go-grpc使用etcd发现服务</p>\n<hr>\n<p>etcd客户端提供一个gRPC解析器通过etcd后端解析gRPC主机,解析器通过etcd客户端初始化并指定了解析目标:</p>\n<pre><code>import (\n        &quot;go.etcd.io/etcd/clientv3&quot;\n        etcdnaming &quot;go.etcd.io/etcd/clientv3/naming&quot;\n\n        &quot;google.golang.org/grpc&quot;\n)\n\n...\n\ncli, cerr := clientv3.NewFromURL(&quot;http://localhost:2379&quot;)\nr := &amp;etcdnaming.GRPCResolver{Client: cli}\nb := grpc.RoundRobin(r)\nconn, gerr := grpc.Dial(&quot;my-service&quot;, grpc.WithBalancer(b), grpc.WithBlock(), ...)</code></pre><h3 id=\"管理服务主机\"><a href=\"#管理服务主机\" class=\"headerlink\" title=\"管理服务主机\"></a>管理服务主机</h3><p>etcd解析器对于解析目标前缀下所有Keys后面跟一个”/“(例如”my-service/“),使用JSON编码go-grpc<code>naming.Update</code>值作为潜在的服务主机。通过创建一个新的Key将主机添加到服务中，通过删除Keys将主机从服务中删除。</p>\n<h3 id=\"添加一个主机\"><a href=\"#添加一个主机\" class=\"headerlink\" title=\"添加一个主机\"></a>添加一个主机</h3><p>一个新的主机可以通过<code>etcdctl</code>添加到服务中：</p>\n<pre><code>ETCDCTL_API=3 etcdctl put my-service/1.2.3.4 &#39;{&quot;Addr&quot;:&quot;1.2.3.4&quot;,&quot;Metadata&quot;:&quot;...&quot;}&#39;</code></pre><p>etcd客户端的<code>GRPCResolver.Update</code>方法也可以通过key匹配<code>Addr</code>注册一个新的主机到服务中：</p>\n<pre><code>r.Update(context.TODO(), &quot;my-service&quot;, naming.Update{Op: naming.Add, Addr: &quot;1.2.3.4&quot;, Metadata: &quot;...&quot;})</code></pre><h3 id=\"删除一个主机\"><a href=\"#删除一个主机\" class=\"headerlink\" title=\"删除一个主机\"></a>删除一个主机</h3><p>通过etcdctl可以从服务中删除一个主机:</p>\n<pre><code>ETCDCTL_API=3 etcdctl del my-service/1.2.3.4</code></pre><p>etcd 客户端的<code>GRPCResolver.Update</code>方法也可以删除一个主机：</p>\n<pre><code>r.Update(context.TODO(), &quot;my-service&quot;, naming.Update{Op: naming.Delete, Addr: &quot;1.2.3.4&quot;})</code></pre><h3 id=\"注册一个主机并绑定一个租约\"><a href=\"#注册一个主机并绑定一个租约\" class=\"headerlink\" title=\"注册一个主机并绑定一个租约\"></a>注册一个主机并绑定一个租约</h3><p>注册一个主机ging绑定一个租约确保如果主机不能维护保持存活的心跳(例如机器宕机)，该主机将会从服务中移除。</p>\n<pre><code>lease=`ETCDCTL_API=3 etcdctl lease grant 5 | cut -f2 -d&#39; &#39;`\nETCDCTL_API=3 etcdctl put --lease=$lease my-service/1.2.3.4 &#39;{&quot;Addr&quot;:&quot;1.2.3.4&quot;,&quot;Metadata&quot;:&quot;...&quot;}&#39;\nETCDCTL_API=3 etcdctl lease keep-alive $lease</code></pre>","site":{"data":{}},"excerpt":"","more":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/grpc_naming.md\" target=\"_blank\" rel=\"noopener\">gRPC naming and discovery</a><br>etcd提供一个gRPC解析器支持备用的命名系统，该命名系统从etcd获取主机以发现gRPC服务。以下机制基于监视对以服务名称为前缀的Key的更新。<br>通过go-grpc使用etcd发现服务</p>\n<hr>\n<p>etcd客户端提供一个gRPC解析器通过etcd后端解析gRPC主机,解析器通过etcd客户端初始化并指定了解析目标:</p>\n<pre><code>import (\n        &quot;go.etcd.io/etcd/clientv3&quot;\n        etcdnaming &quot;go.etcd.io/etcd/clientv3/naming&quot;\n\n        &quot;google.golang.org/grpc&quot;\n)\n\n...\n\ncli, cerr := clientv3.NewFromURL(&quot;http://localhost:2379&quot;)\nr := &amp;etcdnaming.GRPCResolver{Client: cli}\nb := grpc.RoundRobin(r)\nconn, gerr := grpc.Dial(&quot;my-service&quot;, grpc.WithBalancer(b), grpc.WithBlock(), ...)</code></pre><h3 id=\"管理服务主机\"><a href=\"#管理服务主机\" class=\"headerlink\" title=\"管理服务主机\"></a>管理服务主机</h3><p>etcd解析器对于解析目标前缀下所有Keys后面跟一个”/“(例如”my-service/“),使用JSON编码go-grpc<code>naming.Update</code>值作为潜在的服务主机。通过创建一个新的Key将主机添加到服务中，通过删除Keys将主机从服务中删除。</p>\n<h3 id=\"添加一个主机\"><a href=\"#添加一个主机\" class=\"headerlink\" title=\"添加一个主机\"></a>添加一个主机</h3><p>一个新的主机可以通过<code>etcdctl</code>添加到服务中：</p>\n<pre><code>ETCDCTL_API=3 etcdctl put my-service/1.2.3.4 &#39;{&quot;Addr&quot;:&quot;1.2.3.4&quot;,&quot;Metadata&quot;:&quot;...&quot;}&#39;</code></pre><p>etcd客户端的<code>GRPCResolver.Update</code>方法也可以通过key匹配<code>Addr</code>注册一个新的主机到服务中：</p>\n<pre><code>r.Update(context.TODO(), &quot;my-service&quot;, naming.Update{Op: naming.Add, Addr: &quot;1.2.3.4&quot;, Metadata: &quot;...&quot;})</code></pre><h3 id=\"删除一个主机\"><a href=\"#删除一个主机\" class=\"headerlink\" title=\"删除一个主机\"></a>删除一个主机</h3><p>通过etcdctl可以从服务中删除一个主机:</p>\n<pre><code>ETCDCTL_API=3 etcdctl del my-service/1.2.3.4</code></pre><p>etcd 客户端的<code>GRPCResolver.Update</code>方法也可以删除一个主机：</p>\n<pre><code>r.Update(context.TODO(), &quot;my-service&quot;, naming.Update{Op: naming.Delete, Addr: &quot;1.2.3.4&quot;})</code></pre><h3 id=\"注册一个主机并绑定一个租约\"><a href=\"#注册一个主机并绑定一个租约\" class=\"headerlink\" title=\"注册一个主机并绑定一个租约\"></a>注册一个主机并绑定一个租约</h3><p>注册一个主机ging绑定一个租约确保如果主机不能维护保持存活的心跳(例如机器宕机)，该主机将会从服务中移除。</p>\n<pre><code>lease=`ETCDCTL_API=3 etcdctl lease grant 5 | cut -f2 -d&#39; &#39;`\nETCDCTL_API=3 etcdctl put --lease=$lease my-service/1.2.3.4 &#39;{&quot;Addr&quot;:&quot;1.2.3.4&quot;,&quot;Metadata&quot;:&quot;...&quot;}&#39;\nETCDCTL_API=3 etcdctl lease keep-alive $lease</code></pre>"},{"title":"单机集群","date":"2019-11-23T04:31:58.000Z","_content":"原文地址：[Setting up local clusters](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/local_cluster.md)\n## 设置单节点集群\n对于测试环境与开发环境，最快速与简单的方式是配置一个本地集群。对于生产环境，参考[集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/)部分。\n### 本地单节点集群\n##### 启动一个集群\n运行以下命令来部署一个单节点的etcd集群:\n```\n$ ./etcd\n...\n```\n如果`etcd`二进制文件不在当前工作目录，那可能位于`$GOPATH/bin/etcd`或者是`/usr/local/bin/etcd`.合适地运行命令。\n运行的`etcd`成员在`localhost:2379`监听客户端的请求。\n##### 与集群进行交互\n使用`etcdctl`与运行中的集群进行交互操作\n    1.     例子：在集群中存储一个键值对：\n    \n```\n$ ./etcdctl put foo bar\nOK\n```\n如果`OK`被打印在控制台，说明已经成功存储Key-Value对。\n    2.     获取键`foo`对应的值：\n    \n```\n$ ./etcdctl get foo\nbar\n```\n如果`bar`被返回，说明与`etcd`集群的交互操作和期望中的相同。\n### 本地多节点集群\n##### 启动一个集群\n在`etcd`的`git`仓库中存在一个`Procfile`文件提供一种简单的方式可以对本地多节点集群进行配置。在启动多节点集群之前，将工作目录导向`etcd`的根目录并执行以下操作：\n\n    1.    安装`goreman`控制基于`Procfile`的应用：\n    ```\n    $ go get github.com/mattn/goreman\n    ```\n    2.    使用 `etcd`的配置文件`Procfile`通过`goreman`启动一个集群：\n    ```\n    $ goreman -f Procfile start\n    ```\n    集群成员已经启动了，并在`localhost:2379`,localhost:22379`,localhost:32379`监听客户端的请求。\n##### 与集群进行交互\n使用`etcdctl`与运行中的集群进行交互操作:\n\n    1. 打印成员列表：\n    $ `etcdctl --write-out=table --endpoints=localhost:2379 member list`\n    `etcd`集群中的成员列表显示如下：\n    \n| ID | STATUS |NAME| PEER ADDRS|CLIENT ADDRS|\n| :----------------- | :---- | :---- | :------------------- | :-------------------- |                \n|8211f1d0f64f3269   |started|infra1|http://127.0.0.1:2380|http://127.0.0.1:2379|\n|91bc3c398fb3c146|started|infra1|http://127.0.0.1:22380|http://127.0.0.1:22379|\n|fd422379fda50e48|started|infra1|http://127.0.0.1:32380|http://127.0.0.1:32379|\n\n    2.     例子：在集群中存储一个Key-Value对：\n    \n```\n$ ./etcdctl put foo bar\nOK\n```\n如果`OK`被打印在控制台，说明已经成功存储键-值对。\n##### 容错测试\n关闭一个成员然后尝试通过键获取值来进行容错测试：\n1. 获取一个运行中的成员的名字然后停止它：\n    `Procfile`列出了多节点集群的属性信息。例如，名称为`etcd2`的运行中的成员。\n2. 停止该成员：\n```\n#kill etcd2\n$ goreman run stop etcd2\n```\n3.存储一个键：\n```\n$ etcdctl put key hello\nOK\n```\n4.获取前一步所存储的键： \n```\n$ etcdctl get key\nhello\n```\n5.从已经停止的成员处获取键：\n```\n$ etcdctl --endpoints=localhost:22379 get key\n```\n该命令应该由于连接失败展示一个错误：\n```\n2017/06/18 23:07:35 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:22379: getsockopt: connection refused\"; Reconnecting to \"localhost:22379\"\nError:  grpc: timed out trying to connect\n```\n6.重启停止的成员：\n```\n$ goreman run restart etcd2\n```\n7.从重启的成员处获取键：\n```\n$ etcdctl --endpoints=localhost:22379 get key\nhello\n```\n重启的成员重新建立了连接.`etcdctl`将能够成功地从重启的成员处接受键,读[与etcd进行交互](https://newonexd.github.io/2019/11/23/blog/etcd/%E4%B8%8Eetcd%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92/)部分学习更多关于与etcd交互的内容。","source":"_posts/blog/etcd/单机集群.md","raw":"---\ntitle: 单机集群\ndate: 2019-11-23 12:31:58\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址：[Setting up local clusters](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/local_cluster.md)\n## 设置单节点集群\n对于测试环境与开发环境，最快速与简单的方式是配置一个本地集群。对于生产环境，参考[集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/)部分。\n### 本地单节点集群\n##### 启动一个集群\n运行以下命令来部署一个单节点的etcd集群:\n```\n$ ./etcd\n...\n```\n如果`etcd`二进制文件不在当前工作目录，那可能位于`$GOPATH/bin/etcd`或者是`/usr/local/bin/etcd`.合适地运行命令。\n运行的`etcd`成员在`localhost:2379`监听客户端的请求。\n##### 与集群进行交互\n使用`etcdctl`与运行中的集群进行交互操作\n    1.     例子：在集群中存储一个键值对：\n    \n```\n$ ./etcdctl put foo bar\nOK\n```\n如果`OK`被打印在控制台，说明已经成功存储Key-Value对。\n    2.     获取键`foo`对应的值：\n    \n```\n$ ./etcdctl get foo\nbar\n```\n如果`bar`被返回，说明与`etcd`集群的交互操作和期望中的相同。\n### 本地多节点集群\n##### 启动一个集群\n在`etcd`的`git`仓库中存在一个`Procfile`文件提供一种简单的方式可以对本地多节点集群进行配置。在启动多节点集群之前，将工作目录导向`etcd`的根目录并执行以下操作：\n\n    1.    安装`goreman`控制基于`Procfile`的应用：\n    ```\n    $ go get github.com/mattn/goreman\n    ```\n    2.    使用 `etcd`的配置文件`Procfile`通过`goreman`启动一个集群：\n    ```\n    $ goreman -f Procfile start\n    ```\n    集群成员已经启动了，并在`localhost:2379`,localhost:22379`,localhost:32379`监听客户端的请求。\n##### 与集群进行交互\n使用`etcdctl`与运行中的集群进行交互操作:\n\n    1. 打印成员列表：\n    $ `etcdctl --write-out=table --endpoints=localhost:2379 member list`\n    `etcd`集群中的成员列表显示如下：\n    \n| ID | STATUS |NAME| PEER ADDRS|CLIENT ADDRS|\n| :----------------- | :---- | :---- | :------------------- | :-------------------- |                \n|8211f1d0f64f3269   |started|infra1|http://127.0.0.1:2380|http://127.0.0.1:2379|\n|91bc3c398fb3c146|started|infra1|http://127.0.0.1:22380|http://127.0.0.1:22379|\n|fd422379fda50e48|started|infra1|http://127.0.0.1:32380|http://127.0.0.1:32379|\n\n    2.     例子：在集群中存储一个Key-Value对：\n    \n```\n$ ./etcdctl put foo bar\nOK\n```\n如果`OK`被打印在控制台，说明已经成功存储键-值对。\n##### 容错测试\n关闭一个成员然后尝试通过键获取值来进行容错测试：\n1. 获取一个运行中的成员的名字然后停止它：\n    `Procfile`列出了多节点集群的属性信息。例如，名称为`etcd2`的运行中的成员。\n2. 停止该成员：\n```\n#kill etcd2\n$ goreman run stop etcd2\n```\n3.存储一个键：\n```\n$ etcdctl put key hello\nOK\n```\n4.获取前一步所存储的键： \n```\n$ etcdctl get key\nhello\n```\n5.从已经停止的成员处获取键：\n```\n$ etcdctl --endpoints=localhost:22379 get key\n```\n该命令应该由于连接失败展示一个错误：\n```\n2017/06/18 23:07:35 grpc: Conn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp 127.0.0.1:22379: getsockopt: connection refused\"; Reconnecting to \"localhost:22379\"\nError:  grpc: timed out trying to connect\n```\n6.重启停止的成员：\n```\n$ goreman run restart etcd2\n```\n7.从重启的成员处获取键：\n```\n$ etcdctl --endpoints=localhost:22379 get key\nhello\n```\n重启的成员重新建立了连接.`etcdctl`将能够成功地从重启的成员处接受键,读[与etcd进行交互](https://newonexd.github.io/2019/11/23/blog/etcd/%E4%B8%8Eetcd%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92/)部分学习更多关于与etcd交互的内容。","slug":"blog/etcd/单机集群","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyhk001vk0vq74u3eokf","content":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/local_cluster.md\" target=\"_blank\" rel=\"noopener\">Setting up local clusters</a></p>\n<h2 id=\"设置单节点集群\"><a href=\"#设置单节点集群\" class=\"headerlink\" title=\"设置单节点集群\"></a>设置单节点集群</h2><p>对于测试环境与开发环境，最快速与简单的方式是配置一个本地集群。对于生产环境，参考<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/\">集群</a>部分。</p>\n<h3 id=\"本地单节点集群\"><a href=\"#本地单节点集群\" class=\"headerlink\" title=\"本地单节点集群\"></a>本地单节点集群</h3><h5 id=\"启动一个集群\"><a href=\"#启动一个集群\" class=\"headerlink\" title=\"启动一个集群\"></a>启动一个集群</h5><p>运行以下命令来部署一个单节点的etcd集群:</p>\n<pre><code>$ ./etcd\n...</code></pre><p>如果<code>etcd</code>二进制文件不在当前工作目录，那可能位于<code>$GOPATH/bin/etcd</code>或者是<code>/usr/local/bin/etcd</code>.合适地运行命令。<br>运行的<code>etcd</code>成员在<code>localhost:2379</code>监听客户端的请求。</p>\n<h5 id=\"与集群进行交互\"><a href=\"#与集群进行交互\" class=\"headerlink\" title=\"与集群进行交互\"></a>与集群进行交互</h5><p>使用<code>etcdctl</code>与运行中的集群进行交互操作<br>    1.     例子：在集群中存储一个键值对：</p>\n<pre><code>$ ./etcdctl put foo bar\nOK</code></pre><p>如果<code>OK</code>被打印在控制台，说明已经成功存储Key-Value对。<br>    2.     获取键<code>foo</code>对应的值：</p>\n<pre><code>$ ./etcdctl get foo\nbar</code></pre><p>如果<code>bar</code>被返回，说明与<code>etcd</code>集群的交互操作和期望中的相同。</p>\n<h3 id=\"本地多节点集群\"><a href=\"#本地多节点集群\" class=\"headerlink\" title=\"本地多节点集群\"></a>本地多节点集群</h3><h5 id=\"启动一个集群-1\"><a href=\"#启动一个集群-1\" class=\"headerlink\" title=\"启动一个集群\"></a>启动一个集群</h5><p>在<code>etcd</code>的<code>git</code>仓库中存在一个<code>Procfile</code>文件提供一种简单的方式可以对本地多节点集群进行配置。在启动多节点集群之前，将工作目录导向<code>etcd</code>的根目录并执行以下操作：</p>\n<pre><code>1.    安装`goreman`控制基于`Procfile`的应用：\n```\n$ go get github.com/mattn/goreman\n```\n2.    使用 `etcd`的配置文件`Procfile`通过`goreman`启动一个集群：\n```\n$ goreman -f Procfile start\n```\n集群成员已经启动了，并在`localhost:2379`,localhost:22379`,localhost:32379`监听客户端的请求。</code></pre><h5 id=\"与集群进行交互-1\"><a href=\"#与集群进行交互-1\" class=\"headerlink\" title=\"与集群进行交互\"></a>与集群进行交互</h5><p>使用<code>etcdctl</code>与运行中的集群进行交互操作:</p>\n<pre><code>1. 打印成员列表：\n$ `etcdctl --write-out=table --endpoints=localhost:2379 member list`\n`etcd`集群中的成员列表显示如下：</code></pre><table>\n<thead>\n<tr>\n<th align=\"left\">ID</th>\n<th align=\"left\">STATUS</th>\n<th align=\"left\">NAME</th>\n<th align=\"left\">PEER ADDRS</th>\n<th align=\"left\">CLIENT ADDRS</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">8211f1d0f64f3269</td>\n<td align=\"left\">started</td>\n<td align=\"left\">infra1</td>\n<td align=\"left\"><a href=\"http://127.0.0.1:2380\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:2380</a></td>\n<td align=\"left\"><a href=\"http://127.0.0.1:2379\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:2379</a></td>\n</tr>\n<tr>\n<td align=\"left\">91bc3c398fb3c146</td>\n<td align=\"left\">started</td>\n<td align=\"left\">infra1</td>\n<td align=\"left\"><a href=\"http://127.0.0.1:22380\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:22380</a></td>\n<td align=\"left\"><a href=\"http://127.0.0.1:22379\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:22379</a></td>\n</tr>\n<tr>\n<td align=\"left\">fd422379fda50e48</td>\n<td align=\"left\">started</td>\n<td align=\"left\">infra1</td>\n<td align=\"left\"><a href=\"http://127.0.0.1:32380\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:32380</a></td>\n<td align=\"left\"><a href=\"http://127.0.0.1:32379\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:32379</a></td>\n</tr>\n</tbody></table>\n<pre><code>2.     例子：在集群中存储一个Key-Value对：</code></pre><pre><code>$ ./etcdctl put foo bar\nOK</code></pre><p>如果<code>OK</code>被打印在控制台，说明已经成功存储键-值对。</p>\n<h5 id=\"容错测试\"><a href=\"#容错测试\" class=\"headerlink\" title=\"容错测试\"></a>容错测试</h5><p>关闭一个成员然后尝试通过键获取值来进行容错测试：</p>\n<ol>\n<li>获取一个运行中的成员的名字然后停止它：<br> <code>Procfile</code>列出了多节点集群的属性信息。例如，名称为<code>etcd2</code>的运行中的成员。</li>\n<li>停止该成员：<pre><code>#kill etcd2\n$ goreman run stop etcd2</code></pre></li>\n<li>存储一个键：<pre><code>$ etcdctl put key hello\nOK</code></pre></li>\n<li>获取前一步所存储的键： <pre><code>$ etcdctl get key\nhello</code></pre></li>\n<li>从已经停止的成员处获取键：<pre><code>$ etcdctl --endpoints=localhost:22379 get key</code></pre>该命令应该由于连接失败展示一个错误：<pre><code>2017/06/18 23:07:35 grpc: Conn.resetTransport failed to create client transport: connection error: desc = &quot;transport: dial tcp 127.0.0.1:22379: getsockopt: connection refused&quot;; Reconnecting to &quot;localhost:22379&quot;\nError:  grpc: timed out trying to connect</code></pre></li>\n<li>重启停止的成员：<pre><code>$ goreman run restart etcd2</code></pre></li>\n<li>从重启的成员处获取键：<pre><code>$ etcdctl --endpoints=localhost:22379 get key\nhello</code></pre>重启的成员重新建立了连接.<code>etcdctl</code>将能够成功地从重启的成员处接受键,读<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E4%B8%8Eetcd%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92/\">与etcd进行交互</a>部分学习更多关于与etcd交互的内容。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/local_cluster.md\" target=\"_blank\" rel=\"noopener\">Setting up local clusters</a></p>\n<h2 id=\"设置单节点集群\"><a href=\"#设置单节点集群\" class=\"headerlink\" title=\"设置单节点集群\"></a>设置单节点集群</h2><p>对于测试环境与开发环境，最快速与简单的方式是配置一个本地集群。对于生产环境，参考<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/\">集群</a>部分。</p>\n<h3 id=\"本地单节点集群\"><a href=\"#本地单节点集群\" class=\"headerlink\" title=\"本地单节点集群\"></a>本地单节点集群</h3><h5 id=\"启动一个集群\"><a href=\"#启动一个集群\" class=\"headerlink\" title=\"启动一个集群\"></a>启动一个集群</h5><p>运行以下命令来部署一个单节点的etcd集群:</p>\n<pre><code>$ ./etcd\n...</code></pre><p>如果<code>etcd</code>二进制文件不在当前工作目录，那可能位于<code>$GOPATH/bin/etcd</code>或者是<code>/usr/local/bin/etcd</code>.合适地运行命令。<br>运行的<code>etcd</code>成员在<code>localhost:2379</code>监听客户端的请求。</p>\n<h5 id=\"与集群进行交互\"><a href=\"#与集群进行交互\" class=\"headerlink\" title=\"与集群进行交互\"></a>与集群进行交互</h5><p>使用<code>etcdctl</code>与运行中的集群进行交互操作<br>    1.     例子：在集群中存储一个键值对：</p>\n<pre><code>$ ./etcdctl put foo bar\nOK</code></pre><p>如果<code>OK</code>被打印在控制台，说明已经成功存储Key-Value对。<br>    2.     获取键<code>foo</code>对应的值：</p>\n<pre><code>$ ./etcdctl get foo\nbar</code></pre><p>如果<code>bar</code>被返回，说明与<code>etcd</code>集群的交互操作和期望中的相同。</p>\n<h3 id=\"本地多节点集群\"><a href=\"#本地多节点集群\" class=\"headerlink\" title=\"本地多节点集群\"></a>本地多节点集群</h3><h5 id=\"启动一个集群-1\"><a href=\"#启动一个集群-1\" class=\"headerlink\" title=\"启动一个集群\"></a>启动一个集群</h5><p>在<code>etcd</code>的<code>git</code>仓库中存在一个<code>Procfile</code>文件提供一种简单的方式可以对本地多节点集群进行配置。在启动多节点集群之前，将工作目录导向<code>etcd</code>的根目录并执行以下操作：</p>\n<pre><code>1.    安装`goreman`控制基于`Procfile`的应用：\n```\n$ go get github.com/mattn/goreman\n```\n2.    使用 `etcd`的配置文件`Procfile`通过`goreman`启动一个集群：\n```\n$ goreman -f Procfile start\n```\n集群成员已经启动了，并在`localhost:2379`,localhost:22379`,localhost:32379`监听客户端的请求。</code></pre><h5 id=\"与集群进行交互-1\"><a href=\"#与集群进行交互-1\" class=\"headerlink\" title=\"与集群进行交互\"></a>与集群进行交互</h5><p>使用<code>etcdctl</code>与运行中的集群进行交互操作:</p>\n<pre><code>1. 打印成员列表：\n$ `etcdctl --write-out=table --endpoints=localhost:2379 member list`\n`etcd`集群中的成员列表显示如下：</code></pre><table>\n<thead>\n<tr>\n<th align=\"left\">ID</th>\n<th align=\"left\">STATUS</th>\n<th align=\"left\">NAME</th>\n<th align=\"left\">PEER ADDRS</th>\n<th align=\"left\">CLIENT ADDRS</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">8211f1d0f64f3269</td>\n<td align=\"left\">started</td>\n<td align=\"left\">infra1</td>\n<td align=\"left\"><a href=\"http://127.0.0.1:2380\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:2380</a></td>\n<td align=\"left\"><a href=\"http://127.0.0.1:2379\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:2379</a></td>\n</tr>\n<tr>\n<td align=\"left\">91bc3c398fb3c146</td>\n<td align=\"left\">started</td>\n<td align=\"left\">infra1</td>\n<td align=\"left\"><a href=\"http://127.0.0.1:22380\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:22380</a></td>\n<td align=\"left\"><a href=\"http://127.0.0.1:22379\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:22379</a></td>\n</tr>\n<tr>\n<td align=\"left\">fd422379fda50e48</td>\n<td align=\"left\">started</td>\n<td align=\"left\">infra1</td>\n<td align=\"left\"><a href=\"http://127.0.0.1:32380\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:32380</a></td>\n<td align=\"left\"><a href=\"http://127.0.0.1:32379\" target=\"_blank\" rel=\"noopener\">http://127.0.0.1:32379</a></td>\n</tr>\n</tbody></table>\n<pre><code>2.     例子：在集群中存储一个Key-Value对：</code></pre><pre><code>$ ./etcdctl put foo bar\nOK</code></pre><p>如果<code>OK</code>被打印在控制台，说明已经成功存储键-值对。</p>\n<h5 id=\"容错测试\"><a href=\"#容错测试\" class=\"headerlink\" title=\"容错测试\"></a>容错测试</h5><p>关闭一个成员然后尝试通过键获取值来进行容错测试：</p>\n<ol>\n<li>获取一个运行中的成员的名字然后停止它：<br> <code>Procfile</code>列出了多节点集群的属性信息。例如，名称为<code>etcd2</code>的运行中的成员。</li>\n<li>停止该成员：<pre><code>#kill etcd2\n$ goreman run stop etcd2</code></pre></li>\n<li>存储一个键：<pre><code>$ etcdctl put key hello\nOK</code></pre></li>\n<li>获取前一步所存储的键： <pre><code>$ etcdctl get key\nhello</code></pre></li>\n<li>从已经停止的成员处获取键：<pre><code>$ etcdctl --endpoints=localhost:22379 get key</code></pre>该命令应该由于连接失败展示一个错误：<pre><code>2017/06/18 23:07:35 grpc: Conn.resetTransport failed to create client transport: connection error: desc = &quot;transport: dial tcp 127.0.0.1:22379: getsockopt: connection refused&quot;; Reconnecting to &quot;localhost:22379&quot;\nError:  grpc: timed out trying to connect</code></pre></li>\n<li>重启停止的成员：<pre><code>$ goreman run restart etcd2</code></pre></li>\n<li>从重启的成员处获取键：<pre><code>$ etcdctl --endpoints=localhost:22379 get key\nhello</code></pre>重启的成员重新建立了连接.<code>etcdctl</code>将能够成功地从重启的成员处接受键,读<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E4%B8%8Eetcd%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92/\">与etcd进行交互</a>部分学习更多关于与etcd交互的内容。</li>\n</ol>\n"},{"title":"与etcd进行交互","date":"2019-11-23T04:32:15.000Z","_content":"原文地址：[Interacting with etcd](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/interacting_v3.md)\n## 与etcd进行交互\n用户更多的是通过putting或者是getting从etcd获取一个键对应的值。这一部分描述了如何通过etcdctl做这些工作。etcdctl是一个与etcd服务器进行交互的命令行工具.这里的描述适用于gRPC APIs或者是客户端库的APIs。\n用于与etcd交互的API版本可以通过环境变量`ETCDCTL_API`设置为2或者3.默认情况下，分支为(3.4)的主版本使用V3 的API，而早期的版本(3.3或者更早)默认使用V2 API。\n注意使用V2 API所创建的任何Key不能够通过V3 API进行访问。而V3 API `etcdctl get`获取V2 的Key将返回0并退出，这是预料之中的情况。\n```\nexport ETCDCTL_API=3\n```\n### 发现版本\n使用合适的命令在执行不同版本的etcd时etcdctl和服务器API的版本将会有用。\n这里的命令可用于发现版本信息：\n```\n$ etcdctl version\netcdctl version:3.1.0-alpha.0+git\nAPI version:3.1\n```\n### 写入一个KEY\n应用程序通过向etcd集群写入Keys来存储Keys，每次存储的Key将会通过Raft协议实现一致性与可扩展性复制到所有的etcd集群成员中。\n这里的命令是将Key`foo`的值存储到`bar`上：\n```\n$ etcdctl put foo bar\nOK\n```\n给Key附上一个租约，Key将在一个具体的时间间隔被设置。\n这里的命令是在10秒后将Key`foo`的值存储到`bar`上：\n```\n$ etcdctl put foo1 bar1 --lease=1234abcd\n```\n注意：以上命令中租约ID为1234abcd将会在租约创建10秒后将id返回，这个id将附在Key上。\n### 读取Keys\n应用程序可以从一个etcd集群中读取Key，可能会查询到单个Key，或者是一个范围内的Key。\n比如etcd集群中存储以下Key：\n```\nfoo = bar\nfoo1 = bar1\nfoo2 = bar2\nfoo3 = bar3\n```\n这里的命令是读取Key`foo`对应的值：\n```\n$ etcdctl get foo\nfoo\nbar\n```\n这里的命令是读取Key`foo`对应的十六进制的值:\n```\n$ etcdctl get foo --hex\n\\x66\\x6f\\x6f       #Key\n\\x62\\x61\\x72       #Value\n```\n这里的命令是只读取Key`foo`对应的值：\n```\n$ etcdctl get foo --print-value-only\nbar\n```\n这里的命令是读取从Key`foo`到Key`foo3`范围内对应的值：\n```\n$ etcdctl get foo foo3\nfoo\nbar\nfoo1\nbar1\nfoo2\nbar2\n```\n注意这里Key为`foo3`不包括在内因为这里的范围是半开区间`[foo,foo3)`，不包括`foo3`。\n\n这里的命令是获取前缀为`foo`的Key的范围内所有的值：\n```\n$ etcdctl get --prefix foo\nfoo\nbar\nfoo1\nbar1\nfoo2\nbar2\nfoo3\nbar3\n```\n这里的命令是获取前缀为`foo`的Key的范围内所有的值,并且限制结果集为2：\n```\n$ etcdctl get --prefix --limit=2 foo\nfoo\nbar\nfoo1\nbar1\n```\n### 读取之前版本的Keys：\n应用程度可能希望读取一个被替代的版本的Key。例如，一个应用程序可能想要通过读取一个先前版本的Key来回滚到一个老的配置。另外，一个应用程序可能想要通过访问Key的历史记录对多个Key通过多个请求获取一致性的结果。由于对etcd集群中键值对的每一次修改都会增加对在etcd集群中的全局修订存储，应用程序可以通过提供一个老的版本来读取被替代的Keys。\n比如一个etcd集群中存在以下的Keys：\n```\nfoo = bar         # revision = 2\nfoo1 = bar1       # revision = 3\nfoo = bar_new     # revision = 4\nfoo1 = bar1_new   # revision = 5\n```\n这里的例子是访问过去版本的Keys：\n```\n$ etcdctl get --prefix foo # access the most recent versions of keys\nfoo\nbar_new\nfoo1\nbar1_new\n\n$ etcdctl get --prefix --rev=4 foo # access the versions of keys at revision 4\nfoo\nbar_new\nfoo1\nbar1\n\n$ etcdctl get --prefix --rev=3 foo # access the versions of keys at revision 3\nfoo\nbar\nfoo1\nbar1\n\n$ etcdctl get --prefix --rev=2 foo # access the versions of keys at revision 2\nfoo\nbar\n\n$ etcdctl get --prefix --rev=1 foo # access the versions of keys at revision 1\n```\n### 读取大于或等于一个具体的Key的比特值的Key：\n应用程序可能想要读取大于或等于一个具体的Key的byte值的Key。\n一个etcd集群中有以下的Keys：\n```\na = 123\nb = 456\nz = 789\n```\n这里的命令是读取大于或等于Key `b`的byte值的Key：\n```\n$ etcdctl get --from-key b\nb\n456\nz\n789\n```\n### 删除 Keys\n应用程序可以从etcd集群中删除一个Key或者删除一个范围内的Key：\n一个etcd集群中有以下的Keys：\n```\nfoo = bar\nfoo1 = bar1\nfoo3 = bar3\nzoo = val\nzoo1 = val1\nzoo2 = val2\na = 123\nb = 456\nz = 789\n```\n这里的命令是删除Key`foo`:\n```\n$ etcdctl del foo\n1 # 1 个 key 被删除\n```\n这里的命令是删除从Key`foo`到Key`foo9`范围内的Key:\n```\n$ etcdctl del foo foo9\n2 # 2 个 keys 被删除\n```\n这里的命令是删除Key`zoo`并将已删除的键值对返回:\n```\n$ etcdctl del --prev-kv zoo\n1   # 1 个 key 被删除\nzoo # 被删除的Key\nval # 被删除的Key所对应的Value\n```\n这里的命令是删除前缀为`zoo`的Keys:\n```\n$ etcdctl del --prefix zoo\n2 # 2 个 key 被删除\n```\n这里的命令是读取大于或等于Key `b`的byte值的Keys：\n```\n$ etcdctl del --from-key b\n2 # 2 个 key 被删除\n```\n### 观察key的变化\n应用程序可以监视一个Key或者一个范围内的Keys的每一次更新。\n这里的命令是观察key`foo`:\n```\n$ etcdctl watch foo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n```\n这里的命令是观察十六进制的key`foo`:\n```\n$ etcdctl watch foo --hex\n# 在另一个终端执行: etcdctl put foo bar\nPUT\n\\x66\\x6f\\x6f          # Key\n\\x62\\x61\\x72          # Value\n```\n这里的命令是观察从Key`foo`到Key`foo9`范围内的Key：\n```\n$ etcdctl watch foo foo9\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put foo1 bar1\nPUT\nfoo1\nbar1\n```\n这里的命令是观察前缀为`foo`的Key的范围内所有的值：\n```\n$ etcdctl watch --prefix foo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put fooz1 barz1\nPUT\nfooz1\nbarz1\n```\n这里的命令是观察多个Keys`foo`和`zoo`:\n```\n$ etcdctl watch -i\n$ watch foo\n$ watch zoo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put zoo val\nPUT\nzoo\nval\n```\n### 观察Keys的历史版本\n应用程序可能想要观察etcd中Keys的更新历史。例如，应用程序可能想获取key的所有修改；如果应用程序保持与etcd的连接，那么命令`watch`已经足够。然而，如果应用程序或者etcd宕机，一次更新可能就会失败，应用程序可能不能实时接收Key的更新。为了保证更新可以被交付，应用程序必须通过观察到Keys的历史更新。为了做到这些，应用程序要指定观察的历史版本，就像读取历史版本的Keys：\n我们首先完成以下操作：\n```\n$ etcdctl put foo bar         # revision = 2\nOK\n$ etcdctl put foo1 bar1       # revision = 3\nOK\n$ etcdctl put foo bar_new     # revision = 4\nOK\n$ etcdctl put foo1 bar1_new   # revision = 5\nOK\n```\n这里有个例子观察历史更新：\n```\n# watch for changes on key `foo` since revision 2\n$ etcdctl watch --rev=2 foo\nPUT\nfoo\nbar\nPUT\nfoo\nbar_new\n```\n```\n# watch for changes on key `foo` since revision 3\n$ etcdctl watch --rev=3 foo\nPUT\nfoo\nbar_new\n```\n这里有例子只观察最后一次的更新：\n```\n# watch for changes on key `foo` and return last revision value along with modified value\n$ etcdctl watch --prev-kv foo\n# 在另一个终端执行 etcdctl put foo bar_latest\nPUT\nfoo         # key\nbar_new     # last value of foo key before modification\nfoo         # key\nbar_latest  # value of foo key after modification\n```\n### 观察进度\n应用程序可能想要检查观察者进度以确定最新的观察者流的状态。例如，如果观察者更新的缓存，那么就可以通过原子读取与修改进度进行比较知道缓存内容是否已经过时。\n进度请求可以通过`progress`命令与观察者session进行交互在一个观察者流中告诉服务器发送一个进度提示更新.\n```\n$ etcdctl watch -i\n$ watch a\n$ progress\nprogress notify: 1\n# 在另一个终端执行: etcdctl put x 0\n# 在另一个终端执行: etcdctl put y 1\n$ progress\nprogress notify: 3\n```\n注意，在进度提示响应中的修改号来自观察者流连接到的本地etcd服务器。如果该节点被分区并且不是该分区的一部分，这个进度提示修改版本可能会低于由未分区的etcd服务器节点返回的修改版本。\n### 压缩修改\n正如我们提到的，etcd保持修改信息所以应用可以读取过去版本的Keys，然而，为了避免无数的修改历史累积，对过去的修改进行压缩是很重要的。在压缩后，etcd移除了历史修改，释放资源为以后使用。在压缩修改版本之前所有的被修改的替代版本数据将不能获取。\n这里的命令是对修改进行压缩：\n```\n$ etcdctl compact 5\ncompacted revision 5\n\n# any revisions before the compacted one are not accessible\n$ etcdctl get --rev=4 foo\nError:  rpc error: code = 11 desc = etcdserver: mvcc: required revision has been compacted\n```\n注意：etcd服务器的当前版本可以使用json格式的命令通过(存在或不存在的)key发现。例如下面的通过查看在etcd服务器中不存在的myKey:\n```\n$ etcdctl get mykey -w=json\n{\"header\":{\"cluster_id\":14841639068965178418,\"member_id\":10276657743932975437,\"revision\":15,\"raft_term\":4}}\n```\n### 授予租约\n应用程序可以为etcd集群上的Keys授予一个租约。当Key附上租约后，它的生命周期会绑定到租约的生命周期并由存活时间(TTL)进行管理。每一个租约都有一个由应用程序授予的最小的TTL值.这个租约实际的TTL值至少是最小的TTL值，由etcd集群决定。一旦超过租约的TTL，租约将会超时并删除附上的所有的Keys。\n这里有命令授予一个租约：\n```\n# grant a lease with 60 second TTL\n$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n\n# attach key foo to lease 32695410dcc0ca06\n$ etcdctl put --lease=32695410dcc0ca06 foo bar\nOK\n```\n### 撤销租约\n应用程序可以根据租约ID撤销租约，撤销一个租约将删除附上的所有的Keys。\n例如我们完成下面的操作：\n```\n$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n$ etcdctl put --lease=32695410dcc0ca06 foo bar\nOK\n```\n这里的命令可以撤销该租约：\n```\n$ etcdctl lease revoke 32695410dcc0ca06\nlease 32695410dcc0ca06 revoked\n\n$ etcdctl get foo\n# empty response since foo is deleted due to lease revocation\n```\n### 保持租约存活\n应用程序可以通过刷新租约的TTL使它不会超时保证租约存活。\n例如我们完成下面的操作：\n```\n$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n```\n这里有命令保持租约存活：\n```\n$ etcdctl lease keep-alive 32695410dcc0ca06\nlease 32695410dcc0ca06 keepalived with TTL(60)\nlease 32695410dcc0ca06 keepalived with TTL(60)\nlease 32695410dcc0ca06 keepalived with TTL(60)\n...\n```\n### 获取租约信息\n应用程序可能想知道关于租约的信息，所以可以通过重新创建或者检查租约是否仍然生存或已经超时。应用程序可能也想知道一个具体的租约上所附的Key。\n例如我们完成下面的操作：\n```\n# grant a lease with 500 second TTL\n$ etcdctl lease grant 500\nlease 694d5765fc71500b granted with TTL(500s)\n\n# attach key zoo1 to lease 694d5765fc71500b\n$ etcdctl put zoo1 val1 --lease=694d5765fc71500b\nOK\n\n# attach key zoo2 to lease 694d5765fc71500b\n$ etcdctl put zoo2 val2 --lease=694d5765fc71500b\nOK\n```\n这里有命令获取关于租约的信息:\n```\n$ etcdctl lease timetolive 694d5765fc71500b\nlease 694d5765fc71500b granted with TTL(500s), remaining(258s)\n```\n这里有命令获取租约上所依附的关于Keys的信息：\n```\n$ etcdctl lease timetolive --keys 694d5765fc71500b\nlease 694d5765fc71500b granted with TTL(500s), remaining(132s), attached keys([zoo2 zoo1])\n\n# if the lease has expired or does not exist it will give the below response:\nError:  etcdserver: requested lease not found\n```","source":"_posts/blog/etcd/与etcd进行交互.md","raw":"---\ntitle: 与etcd进行交互\ndate: 2019-11-23 12:32:15\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址：[Interacting with etcd](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/interacting_v3.md)\n## 与etcd进行交互\n用户更多的是通过putting或者是getting从etcd获取一个键对应的值。这一部分描述了如何通过etcdctl做这些工作。etcdctl是一个与etcd服务器进行交互的命令行工具.这里的描述适用于gRPC APIs或者是客户端库的APIs。\n用于与etcd交互的API版本可以通过环境变量`ETCDCTL_API`设置为2或者3.默认情况下，分支为(3.4)的主版本使用V3 的API，而早期的版本(3.3或者更早)默认使用V2 API。\n注意使用V2 API所创建的任何Key不能够通过V3 API进行访问。而V3 API `etcdctl get`获取V2 的Key将返回0并退出，这是预料之中的情况。\n```\nexport ETCDCTL_API=3\n```\n### 发现版本\n使用合适的命令在执行不同版本的etcd时etcdctl和服务器API的版本将会有用。\n这里的命令可用于发现版本信息：\n```\n$ etcdctl version\netcdctl version:3.1.0-alpha.0+git\nAPI version:3.1\n```\n### 写入一个KEY\n应用程序通过向etcd集群写入Keys来存储Keys，每次存储的Key将会通过Raft协议实现一致性与可扩展性复制到所有的etcd集群成员中。\n这里的命令是将Key`foo`的值存储到`bar`上：\n```\n$ etcdctl put foo bar\nOK\n```\n给Key附上一个租约，Key将在一个具体的时间间隔被设置。\n这里的命令是在10秒后将Key`foo`的值存储到`bar`上：\n```\n$ etcdctl put foo1 bar1 --lease=1234abcd\n```\n注意：以上命令中租约ID为1234abcd将会在租约创建10秒后将id返回，这个id将附在Key上。\n### 读取Keys\n应用程序可以从一个etcd集群中读取Key，可能会查询到单个Key，或者是一个范围内的Key。\n比如etcd集群中存储以下Key：\n```\nfoo = bar\nfoo1 = bar1\nfoo2 = bar2\nfoo3 = bar3\n```\n这里的命令是读取Key`foo`对应的值：\n```\n$ etcdctl get foo\nfoo\nbar\n```\n这里的命令是读取Key`foo`对应的十六进制的值:\n```\n$ etcdctl get foo --hex\n\\x66\\x6f\\x6f       #Key\n\\x62\\x61\\x72       #Value\n```\n这里的命令是只读取Key`foo`对应的值：\n```\n$ etcdctl get foo --print-value-only\nbar\n```\n这里的命令是读取从Key`foo`到Key`foo3`范围内对应的值：\n```\n$ etcdctl get foo foo3\nfoo\nbar\nfoo1\nbar1\nfoo2\nbar2\n```\n注意这里Key为`foo3`不包括在内因为这里的范围是半开区间`[foo,foo3)`，不包括`foo3`。\n\n这里的命令是获取前缀为`foo`的Key的范围内所有的值：\n```\n$ etcdctl get --prefix foo\nfoo\nbar\nfoo1\nbar1\nfoo2\nbar2\nfoo3\nbar3\n```\n这里的命令是获取前缀为`foo`的Key的范围内所有的值,并且限制结果集为2：\n```\n$ etcdctl get --prefix --limit=2 foo\nfoo\nbar\nfoo1\nbar1\n```\n### 读取之前版本的Keys：\n应用程度可能希望读取一个被替代的版本的Key。例如，一个应用程序可能想要通过读取一个先前版本的Key来回滚到一个老的配置。另外，一个应用程序可能想要通过访问Key的历史记录对多个Key通过多个请求获取一致性的结果。由于对etcd集群中键值对的每一次修改都会增加对在etcd集群中的全局修订存储，应用程序可以通过提供一个老的版本来读取被替代的Keys。\n比如一个etcd集群中存在以下的Keys：\n```\nfoo = bar         # revision = 2\nfoo1 = bar1       # revision = 3\nfoo = bar_new     # revision = 4\nfoo1 = bar1_new   # revision = 5\n```\n这里的例子是访问过去版本的Keys：\n```\n$ etcdctl get --prefix foo # access the most recent versions of keys\nfoo\nbar_new\nfoo1\nbar1_new\n\n$ etcdctl get --prefix --rev=4 foo # access the versions of keys at revision 4\nfoo\nbar_new\nfoo1\nbar1\n\n$ etcdctl get --prefix --rev=3 foo # access the versions of keys at revision 3\nfoo\nbar\nfoo1\nbar1\n\n$ etcdctl get --prefix --rev=2 foo # access the versions of keys at revision 2\nfoo\nbar\n\n$ etcdctl get --prefix --rev=1 foo # access the versions of keys at revision 1\n```\n### 读取大于或等于一个具体的Key的比特值的Key：\n应用程序可能想要读取大于或等于一个具体的Key的byte值的Key。\n一个etcd集群中有以下的Keys：\n```\na = 123\nb = 456\nz = 789\n```\n这里的命令是读取大于或等于Key `b`的byte值的Key：\n```\n$ etcdctl get --from-key b\nb\n456\nz\n789\n```\n### 删除 Keys\n应用程序可以从etcd集群中删除一个Key或者删除一个范围内的Key：\n一个etcd集群中有以下的Keys：\n```\nfoo = bar\nfoo1 = bar1\nfoo3 = bar3\nzoo = val\nzoo1 = val1\nzoo2 = val2\na = 123\nb = 456\nz = 789\n```\n这里的命令是删除Key`foo`:\n```\n$ etcdctl del foo\n1 # 1 个 key 被删除\n```\n这里的命令是删除从Key`foo`到Key`foo9`范围内的Key:\n```\n$ etcdctl del foo foo9\n2 # 2 个 keys 被删除\n```\n这里的命令是删除Key`zoo`并将已删除的键值对返回:\n```\n$ etcdctl del --prev-kv zoo\n1   # 1 个 key 被删除\nzoo # 被删除的Key\nval # 被删除的Key所对应的Value\n```\n这里的命令是删除前缀为`zoo`的Keys:\n```\n$ etcdctl del --prefix zoo\n2 # 2 个 key 被删除\n```\n这里的命令是读取大于或等于Key `b`的byte值的Keys：\n```\n$ etcdctl del --from-key b\n2 # 2 个 key 被删除\n```\n### 观察key的变化\n应用程序可以监视一个Key或者一个范围内的Keys的每一次更新。\n这里的命令是观察key`foo`:\n```\n$ etcdctl watch foo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n```\n这里的命令是观察十六进制的key`foo`:\n```\n$ etcdctl watch foo --hex\n# 在另一个终端执行: etcdctl put foo bar\nPUT\n\\x66\\x6f\\x6f          # Key\n\\x62\\x61\\x72          # Value\n```\n这里的命令是观察从Key`foo`到Key`foo9`范围内的Key：\n```\n$ etcdctl watch foo foo9\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put foo1 bar1\nPUT\nfoo1\nbar1\n```\n这里的命令是观察前缀为`foo`的Key的范围内所有的值：\n```\n$ etcdctl watch --prefix foo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put fooz1 barz1\nPUT\nfooz1\nbarz1\n```\n这里的命令是观察多个Keys`foo`和`zoo`:\n```\n$ etcdctl watch -i\n$ watch foo\n$ watch zoo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put zoo val\nPUT\nzoo\nval\n```\n### 观察Keys的历史版本\n应用程序可能想要观察etcd中Keys的更新历史。例如，应用程序可能想获取key的所有修改；如果应用程序保持与etcd的连接，那么命令`watch`已经足够。然而，如果应用程序或者etcd宕机，一次更新可能就会失败，应用程序可能不能实时接收Key的更新。为了保证更新可以被交付，应用程序必须通过观察到Keys的历史更新。为了做到这些，应用程序要指定观察的历史版本，就像读取历史版本的Keys：\n我们首先完成以下操作：\n```\n$ etcdctl put foo bar         # revision = 2\nOK\n$ etcdctl put foo1 bar1       # revision = 3\nOK\n$ etcdctl put foo bar_new     # revision = 4\nOK\n$ etcdctl put foo1 bar1_new   # revision = 5\nOK\n```\n这里有个例子观察历史更新：\n```\n# watch for changes on key `foo` since revision 2\n$ etcdctl watch --rev=2 foo\nPUT\nfoo\nbar\nPUT\nfoo\nbar_new\n```\n```\n# watch for changes on key `foo` since revision 3\n$ etcdctl watch --rev=3 foo\nPUT\nfoo\nbar_new\n```\n这里有例子只观察最后一次的更新：\n```\n# watch for changes on key `foo` and return last revision value along with modified value\n$ etcdctl watch --prev-kv foo\n# 在另一个终端执行 etcdctl put foo bar_latest\nPUT\nfoo         # key\nbar_new     # last value of foo key before modification\nfoo         # key\nbar_latest  # value of foo key after modification\n```\n### 观察进度\n应用程序可能想要检查观察者进度以确定最新的观察者流的状态。例如，如果观察者更新的缓存，那么就可以通过原子读取与修改进度进行比较知道缓存内容是否已经过时。\n进度请求可以通过`progress`命令与观察者session进行交互在一个观察者流中告诉服务器发送一个进度提示更新.\n```\n$ etcdctl watch -i\n$ watch a\n$ progress\nprogress notify: 1\n# 在另一个终端执行: etcdctl put x 0\n# 在另一个终端执行: etcdctl put y 1\n$ progress\nprogress notify: 3\n```\n注意，在进度提示响应中的修改号来自观察者流连接到的本地etcd服务器。如果该节点被分区并且不是该分区的一部分，这个进度提示修改版本可能会低于由未分区的etcd服务器节点返回的修改版本。\n### 压缩修改\n正如我们提到的，etcd保持修改信息所以应用可以读取过去版本的Keys，然而，为了避免无数的修改历史累积，对过去的修改进行压缩是很重要的。在压缩后，etcd移除了历史修改，释放资源为以后使用。在压缩修改版本之前所有的被修改的替代版本数据将不能获取。\n这里的命令是对修改进行压缩：\n```\n$ etcdctl compact 5\ncompacted revision 5\n\n# any revisions before the compacted one are not accessible\n$ etcdctl get --rev=4 foo\nError:  rpc error: code = 11 desc = etcdserver: mvcc: required revision has been compacted\n```\n注意：etcd服务器的当前版本可以使用json格式的命令通过(存在或不存在的)key发现。例如下面的通过查看在etcd服务器中不存在的myKey:\n```\n$ etcdctl get mykey -w=json\n{\"header\":{\"cluster_id\":14841639068965178418,\"member_id\":10276657743932975437,\"revision\":15,\"raft_term\":4}}\n```\n### 授予租约\n应用程序可以为etcd集群上的Keys授予一个租约。当Key附上租约后，它的生命周期会绑定到租约的生命周期并由存活时间(TTL)进行管理。每一个租约都有一个由应用程序授予的最小的TTL值.这个租约实际的TTL值至少是最小的TTL值，由etcd集群决定。一旦超过租约的TTL，租约将会超时并删除附上的所有的Keys。\n这里有命令授予一个租约：\n```\n# grant a lease with 60 second TTL\n$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n\n# attach key foo to lease 32695410dcc0ca06\n$ etcdctl put --lease=32695410dcc0ca06 foo bar\nOK\n```\n### 撤销租约\n应用程序可以根据租约ID撤销租约，撤销一个租约将删除附上的所有的Keys。\n例如我们完成下面的操作：\n```\n$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n$ etcdctl put --lease=32695410dcc0ca06 foo bar\nOK\n```\n这里的命令可以撤销该租约：\n```\n$ etcdctl lease revoke 32695410dcc0ca06\nlease 32695410dcc0ca06 revoked\n\n$ etcdctl get foo\n# empty response since foo is deleted due to lease revocation\n```\n### 保持租约存活\n应用程序可以通过刷新租约的TTL使它不会超时保证租约存活。\n例如我们完成下面的操作：\n```\n$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n```\n这里有命令保持租约存活：\n```\n$ etcdctl lease keep-alive 32695410dcc0ca06\nlease 32695410dcc0ca06 keepalived with TTL(60)\nlease 32695410dcc0ca06 keepalived with TTL(60)\nlease 32695410dcc0ca06 keepalived with TTL(60)\n...\n```\n### 获取租约信息\n应用程序可能想知道关于租约的信息，所以可以通过重新创建或者检查租约是否仍然生存或已经超时。应用程序可能也想知道一个具体的租约上所附的Key。\n例如我们完成下面的操作：\n```\n# grant a lease with 500 second TTL\n$ etcdctl lease grant 500\nlease 694d5765fc71500b granted with TTL(500s)\n\n# attach key zoo1 to lease 694d5765fc71500b\n$ etcdctl put zoo1 val1 --lease=694d5765fc71500b\nOK\n\n# attach key zoo2 to lease 694d5765fc71500b\n$ etcdctl put zoo2 val2 --lease=694d5765fc71500b\nOK\n```\n这里有命令获取关于租约的信息:\n```\n$ etcdctl lease timetolive 694d5765fc71500b\nlease 694d5765fc71500b granted with TTL(500s), remaining(258s)\n```\n这里有命令获取租约上所依附的关于Keys的信息：\n```\n$ etcdctl lease timetolive --keys 694d5765fc71500b\nlease 694d5765fc71500b granted with TTL(500s), remaining(132s), attached keys([zoo2 zoo1])\n\n# if the lease has expired or does not exist it will give the below response:\nError:  etcdserver: requested lease not found\n```","slug":"blog/etcd/与etcd进行交互","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyhm001yk0vq2mjieeoo","content":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/interacting_v3.md\" target=\"_blank\" rel=\"noopener\">Interacting with etcd</a></p>\n<h2 id=\"与etcd进行交互\"><a href=\"#与etcd进行交互\" class=\"headerlink\" title=\"与etcd进行交互\"></a>与etcd进行交互</h2><p>用户更多的是通过putting或者是getting从etcd获取一个键对应的值。这一部分描述了如何通过etcdctl做这些工作。etcdctl是一个与etcd服务器进行交互的命令行工具.这里的描述适用于gRPC APIs或者是客户端库的APIs。<br>用于与etcd交互的API版本可以通过环境变量<code>ETCDCTL_API</code>设置为2或者3.默认情况下，分支为(3.4)的主版本使用V3 的API，而早期的版本(3.3或者更早)默认使用V2 API。<br>注意使用V2 API所创建的任何Key不能够通过V3 API进行访问。而V3 API <code>etcdctl get</code>获取V2 的Key将返回0并退出，这是预料之中的情况。</p>\n<pre><code>export ETCDCTL_API=3</code></pre><h3 id=\"发现版本\"><a href=\"#发现版本\" class=\"headerlink\" title=\"发现版本\"></a>发现版本</h3><p>使用合适的命令在执行不同版本的etcd时etcdctl和服务器API的版本将会有用。<br>这里的命令可用于发现版本信息：</p>\n<pre><code>$ etcdctl version\netcdctl version:3.1.0-alpha.0+git\nAPI version:3.1</code></pre><h3 id=\"写入一个KEY\"><a href=\"#写入一个KEY\" class=\"headerlink\" title=\"写入一个KEY\"></a>写入一个KEY</h3><p>应用程序通过向etcd集群写入Keys来存储Keys，每次存储的Key将会通过Raft协议实现一致性与可扩展性复制到所有的etcd集群成员中。<br>这里的命令是将Key<code>foo</code>的值存储到<code>bar</code>上：</p>\n<pre><code>$ etcdctl put foo bar\nOK</code></pre><p>给Key附上一个租约，Key将在一个具体的时间间隔被设置。<br>这里的命令是在10秒后将Key<code>foo</code>的值存储到<code>bar</code>上：</p>\n<pre><code>$ etcdctl put foo1 bar1 --lease=1234abcd</code></pre><p>注意：以上命令中租约ID为1234abcd将会在租约创建10秒后将id返回，这个id将附在Key上。</p>\n<h3 id=\"读取Keys\"><a href=\"#读取Keys\" class=\"headerlink\" title=\"读取Keys\"></a>读取Keys</h3><p>应用程序可以从一个etcd集群中读取Key，可能会查询到单个Key，或者是一个范围内的Key。<br>比如etcd集群中存储以下Key：</p>\n<pre><code>foo = bar\nfoo1 = bar1\nfoo2 = bar2\nfoo3 = bar3</code></pre><p>这里的命令是读取Key<code>foo</code>对应的值：</p>\n<pre><code>$ etcdctl get foo\nfoo\nbar</code></pre><p>这里的命令是读取Key<code>foo</code>对应的十六进制的值:</p>\n<pre><code>$ etcdctl get foo --hex\n\\x66\\x6f\\x6f       #Key\n\\x62\\x61\\x72       #Value</code></pre><p>这里的命令是只读取Key<code>foo</code>对应的值：</p>\n<pre><code>$ etcdctl get foo --print-value-only\nbar</code></pre><p>这里的命令是读取从Key<code>foo</code>到Key<code>foo3</code>范围内对应的值：</p>\n<pre><code>$ etcdctl get foo foo3\nfoo\nbar\nfoo1\nbar1\nfoo2\nbar2</code></pre><p>注意这里Key为<code>foo3</code>不包括在内因为这里的范围是半开区间<code>[foo,foo3)</code>，不包括<code>foo3</code>。</p>\n<p>这里的命令是获取前缀为<code>foo</code>的Key的范围内所有的值：</p>\n<pre><code>$ etcdctl get --prefix foo\nfoo\nbar\nfoo1\nbar1\nfoo2\nbar2\nfoo3\nbar3</code></pre><p>这里的命令是获取前缀为<code>foo</code>的Key的范围内所有的值,并且限制结果集为2：</p>\n<pre><code>$ etcdctl get --prefix --limit=2 foo\nfoo\nbar\nfoo1\nbar1</code></pre><h3 id=\"读取之前版本的Keys：\"><a href=\"#读取之前版本的Keys：\" class=\"headerlink\" title=\"读取之前版本的Keys：\"></a>读取之前版本的Keys：</h3><p>应用程度可能希望读取一个被替代的版本的Key。例如，一个应用程序可能想要通过读取一个先前版本的Key来回滚到一个老的配置。另外，一个应用程序可能想要通过访问Key的历史记录对多个Key通过多个请求获取一致性的结果。由于对etcd集群中键值对的每一次修改都会增加对在etcd集群中的全局修订存储，应用程序可以通过提供一个老的版本来读取被替代的Keys。<br>比如一个etcd集群中存在以下的Keys：</p>\n<pre><code>foo = bar         # revision = 2\nfoo1 = bar1       # revision = 3\nfoo = bar_new     # revision = 4\nfoo1 = bar1_new   # revision = 5</code></pre><p>这里的例子是访问过去版本的Keys：</p>\n<pre><code>$ etcdctl get --prefix foo # access the most recent versions of keys\nfoo\nbar_new\nfoo1\nbar1_new\n\n$ etcdctl get --prefix --rev=4 foo # access the versions of keys at revision 4\nfoo\nbar_new\nfoo1\nbar1\n\n$ etcdctl get --prefix --rev=3 foo # access the versions of keys at revision 3\nfoo\nbar\nfoo1\nbar1\n\n$ etcdctl get --prefix --rev=2 foo # access the versions of keys at revision 2\nfoo\nbar\n\n$ etcdctl get --prefix --rev=1 foo # access the versions of keys at revision 1</code></pre><h3 id=\"读取大于或等于一个具体的Key的比特值的Key：\"><a href=\"#读取大于或等于一个具体的Key的比特值的Key：\" class=\"headerlink\" title=\"读取大于或等于一个具体的Key的比特值的Key：\"></a>读取大于或等于一个具体的Key的比特值的Key：</h3><p>应用程序可能想要读取大于或等于一个具体的Key的byte值的Key。<br>一个etcd集群中有以下的Keys：</p>\n<pre><code>a = 123\nb = 456\nz = 789</code></pre><p>这里的命令是读取大于或等于Key <code>b</code>的byte值的Key：</p>\n<pre><code>$ etcdctl get --from-key b\nb\n456\nz\n789</code></pre><h3 id=\"删除-Keys\"><a href=\"#删除-Keys\" class=\"headerlink\" title=\"删除 Keys\"></a>删除 Keys</h3><p>应用程序可以从etcd集群中删除一个Key或者删除一个范围内的Key：<br>一个etcd集群中有以下的Keys：</p>\n<pre><code>foo = bar\nfoo1 = bar1\nfoo3 = bar3\nzoo = val\nzoo1 = val1\nzoo2 = val2\na = 123\nb = 456\nz = 789</code></pre><p>这里的命令是删除Key<code>foo</code>:</p>\n<pre><code>$ etcdctl del foo\n1 # 1 个 key 被删除</code></pre><p>这里的命令是删除从Key<code>foo</code>到Key<code>foo9</code>范围内的Key:</p>\n<pre><code>$ etcdctl del foo foo9\n2 # 2 个 keys 被删除</code></pre><p>这里的命令是删除Key<code>zoo</code>并将已删除的键值对返回:</p>\n<pre><code>$ etcdctl del --prev-kv zoo\n1   # 1 个 key 被删除\nzoo # 被删除的Key\nval # 被删除的Key所对应的Value</code></pre><p>这里的命令是删除前缀为<code>zoo</code>的Keys:</p>\n<pre><code>$ etcdctl del --prefix zoo\n2 # 2 个 key 被删除</code></pre><p>这里的命令是读取大于或等于Key <code>b</code>的byte值的Keys：</p>\n<pre><code>$ etcdctl del --from-key b\n2 # 2 个 key 被删除</code></pre><h3 id=\"观察key的变化\"><a href=\"#观察key的变化\" class=\"headerlink\" title=\"观察key的变化\"></a>观察key的变化</h3><p>应用程序可以监视一个Key或者一个范围内的Keys的每一次更新。<br>这里的命令是观察key<code>foo</code>:</p>\n<pre><code>$ etcdctl watch foo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar</code></pre><p>这里的命令是观察十六进制的key<code>foo</code>:</p>\n<pre><code>$ etcdctl watch foo --hex\n# 在另一个终端执行: etcdctl put foo bar\nPUT\n\\x66\\x6f\\x6f          # Key\n\\x62\\x61\\x72          # Value</code></pre><p>这里的命令是观察从Key<code>foo</code>到Key<code>foo9</code>范围内的Key：</p>\n<pre><code>$ etcdctl watch foo foo9\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put foo1 bar1\nPUT\nfoo1\nbar1</code></pre><p>这里的命令是观察前缀为<code>foo</code>的Key的范围内所有的值：</p>\n<pre><code>$ etcdctl watch --prefix foo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put fooz1 barz1\nPUT\nfooz1\nbarz1</code></pre><p>这里的命令是观察多个Keys<code>foo</code>和<code>zoo</code>:</p>\n<pre><code>$ etcdctl watch -i\n$ watch foo\n$ watch zoo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put zoo val\nPUT\nzoo\nval</code></pre><h3 id=\"观察Keys的历史版本\"><a href=\"#观察Keys的历史版本\" class=\"headerlink\" title=\"观察Keys的历史版本\"></a>观察Keys的历史版本</h3><p>应用程序可能想要观察etcd中Keys的更新历史。例如，应用程序可能想获取key的所有修改；如果应用程序保持与etcd的连接，那么命令<code>watch</code>已经足够。然而，如果应用程序或者etcd宕机，一次更新可能就会失败，应用程序可能不能实时接收Key的更新。为了保证更新可以被交付，应用程序必须通过观察到Keys的历史更新。为了做到这些，应用程序要指定观察的历史版本，就像读取历史版本的Keys：<br>我们首先完成以下操作：</p>\n<pre><code>$ etcdctl put foo bar         # revision = 2\nOK\n$ etcdctl put foo1 bar1       # revision = 3\nOK\n$ etcdctl put foo bar_new     # revision = 4\nOK\n$ etcdctl put foo1 bar1_new   # revision = 5\nOK</code></pre><p>这里有个例子观察历史更新：</p>\n<pre><code># watch for changes on key `foo` since revision 2\n$ etcdctl watch --rev=2 foo\nPUT\nfoo\nbar\nPUT\nfoo\nbar_new</code></pre><pre><code># watch for changes on key `foo` since revision 3\n$ etcdctl watch --rev=3 foo\nPUT\nfoo\nbar_new</code></pre><p>这里有例子只观察最后一次的更新：</p>\n<pre><code># watch for changes on key `foo` and return last revision value along with modified value\n$ etcdctl watch --prev-kv foo\n# 在另一个终端执行 etcdctl put foo bar_latest\nPUT\nfoo         # key\nbar_new     # last value of foo key before modification\nfoo         # key\nbar_latest  # value of foo key after modification</code></pre><h3 id=\"观察进度\"><a href=\"#观察进度\" class=\"headerlink\" title=\"观察进度\"></a>观察进度</h3><p>应用程序可能想要检查观察者进度以确定最新的观察者流的状态。例如，如果观察者更新的缓存，那么就可以通过原子读取与修改进度进行比较知道缓存内容是否已经过时。<br>进度请求可以通过<code>progress</code>命令与观察者session进行交互在一个观察者流中告诉服务器发送一个进度提示更新.</p>\n<pre><code>$ etcdctl watch -i\n$ watch a\n$ progress\nprogress notify: 1\n# 在另一个终端执行: etcdctl put x 0\n# 在另一个终端执行: etcdctl put y 1\n$ progress\nprogress notify: 3</code></pre><p>注意，在进度提示响应中的修改号来自观察者流连接到的本地etcd服务器。如果该节点被分区并且不是该分区的一部分，这个进度提示修改版本可能会低于由未分区的etcd服务器节点返回的修改版本。</p>\n<h3 id=\"压缩修改\"><a href=\"#压缩修改\" class=\"headerlink\" title=\"压缩修改\"></a>压缩修改</h3><p>正如我们提到的，etcd保持修改信息所以应用可以读取过去版本的Keys，然而，为了避免无数的修改历史累积，对过去的修改进行压缩是很重要的。在压缩后，etcd移除了历史修改，释放资源为以后使用。在压缩修改版本之前所有的被修改的替代版本数据将不能获取。<br>这里的命令是对修改进行压缩：</p>\n<pre><code>$ etcdctl compact 5\ncompacted revision 5\n\n# any revisions before the compacted one are not accessible\n$ etcdctl get --rev=4 foo\nError:  rpc error: code = 11 desc = etcdserver: mvcc: required revision has been compacted</code></pre><p>注意：etcd服务器的当前版本可以使用json格式的命令通过(存在或不存在的)key发现。例如下面的通过查看在etcd服务器中不存在的myKey:</p>\n<pre><code>$ etcdctl get mykey -w=json\n{&quot;header&quot;:{&quot;cluster_id&quot;:14841639068965178418,&quot;member_id&quot;:10276657743932975437,&quot;revision&quot;:15,&quot;raft_term&quot;:4}}</code></pre><h3 id=\"授予租约\"><a href=\"#授予租约\" class=\"headerlink\" title=\"授予租约\"></a>授予租约</h3><p>应用程序可以为etcd集群上的Keys授予一个租约。当Key附上租约后，它的生命周期会绑定到租约的生命周期并由存活时间(TTL)进行管理。每一个租约都有一个由应用程序授予的最小的TTL值.这个租约实际的TTL值至少是最小的TTL值，由etcd集群决定。一旦超过租约的TTL，租约将会超时并删除附上的所有的Keys。<br>这里有命令授予一个租约：</p>\n<pre><code># grant a lease with 60 second TTL\n$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n\n# attach key foo to lease 32695410dcc0ca06\n$ etcdctl put --lease=32695410dcc0ca06 foo bar\nOK</code></pre><h3 id=\"撤销租约\"><a href=\"#撤销租约\" class=\"headerlink\" title=\"撤销租约\"></a>撤销租约</h3><p>应用程序可以根据租约ID撤销租约，撤销一个租约将删除附上的所有的Keys。<br>例如我们完成下面的操作：</p>\n<pre><code>$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n$ etcdctl put --lease=32695410dcc0ca06 foo bar\nOK</code></pre><p>这里的命令可以撤销该租约：</p>\n<pre><code>$ etcdctl lease revoke 32695410dcc0ca06\nlease 32695410dcc0ca06 revoked\n\n$ etcdctl get foo\n# empty response since foo is deleted due to lease revocation</code></pre><h3 id=\"保持租约存活\"><a href=\"#保持租约存活\" class=\"headerlink\" title=\"保持租约存活\"></a>保持租约存活</h3><p>应用程序可以通过刷新租约的TTL使它不会超时保证租约存活。<br>例如我们完成下面的操作：</p>\n<pre><code>$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)</code></pre><p>这里有命令保持租约存活：</p>\n<pre><code>$ etcdctl lease keep-alive 32695410dcc0ca06\nlease 32695410dcc0ca06 keepalived with TTL(60)\nlease 32695410dcc0ca06 keepalived with TTL(60)\nlease 32695410dcc0ca06 keepalived with TTL(60)\n...</code></pre><h3 id=\"获取租约信息\"><a href=\"#获取租约信息\" class=\"headerlink\" title=\"获取租约信息\"></a>获取租约信息</h3><p>应用程序可能想知道关于租约的信息，所以可以通过重新创建或者检查租约是否仍然生存或已经超时。应用程序可能也想知道一个具体的租约上所附的Key。<br>例如我们完成下面的操作：</p>\n<pre><code># grant a lease with 500 second TTL\n$ etcdctl lease grant 500\nlease 694d5765fc71500b granted with TTL(500s)\n\n# attach key zoo1 to lease 694d5765fc71500b\n$ etcdctl put zoo1 val1 --lease=694d5765fc71500b\nOK\n\n# attach key zoo2 to lease 694d5765fc71500b\n$ etcdctl put zoo2 val2 --lease=694d5765fc71500b\nOK</code></pre><p>这里有命令获取关于租约的信息:</p>\n<pre><code>$ etcdctl lease timetolive 694d5765fc71500b\nlease 694d5765fc71500b granted with TTL(500s), remaining(258s)</code></pre><p>这里有命令获取租约上所依附的关于Keys的信息：</p>\n<pre><code>$ etcdctl lease timetolive --keys 694d5765fc71500b\nlease 694d5765fc71500b granted with TTL(500s), remaining(132s), attached keys([zoo2 zoo1])\n\n# if the lease has expired or does not exist it will give the below response:\nError:  etcdserver: requested lease not found</code></pre>","site":{"data":{}},"excerpt":"","more":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/interacting_v3.md\" target=\"_blank\" rel=\"noopener\">Interacting with etcd</a></p>\n<h2 id=\"与etcd进行交互\"><a href=\"#与etcd进行交互\" class=\"headerlink\" title=\"与etcd进行交互\"></a>与etcd进行交互</h2><p>用户更多的是通过putting或者是getting从etcd获取一个键对应的值。这一部分描述了如何通过etcdctl做这些工作。etcdctl是一个与etcd服务器进行交互的命令行工具.这里的描述适用于gRPC APIs或者是客户端库的APIs。<br>用于与etcd交互的API版本可以通过环境变量<code>ETCDCTL_API</code>设置为2或者3.默认情况下，分支为(3.4)的主版本使用V3 的API，而早期的版本(3.3或者更早)默认使用V2 API。<br>注意使用V2 API所创建的任何Key不能够通过V3 API进行访问。而V3 API <code>etcdctl get</code>获取V2 的Key将返回0并退出，这是预料之中的情况。</p>\n<pre><code>export ETCDCTL_API=3</code></pre><h3 id=\"发现版本\"><a href=\"#发现版本\" class=\"headerlink\" title=\"发现版本\"></a>发现版本</h3><p>使用合适的命令在执行不同版本的etcd时etcdctl和服务器API的版本将会有用。<br>这里的命令可用于发现版本信息：</p>\n<pre><code>$ etcdctl version\netcdctl version:3.1.0-alpha.0+git\nAPI version:3.1</code></pre><h3 id=\"写入一个KEY\"><a href=\"#写入一个KEY\" class=\"headerlink\" title=\"写入一个KEY\"></a>写入一个KEY</h3><p>应用程序通过向etcd集群写入Keys来存储Keys，每次存储的Key将会通过Raft协议实现一致性与可扩展性复制到所有的etcd集群成员中。<br>这里的命令是将Key<code>foo</code>的值存储到<code>bar</code>上：</p>\n<pre><code>$ etcdctl put foo bar\nOK</code></pre><p>给Key附上一个租约，Key将在一个具体的时间间隔被设置。<br>这里的命令是在10秒后将Key<code>foo</code>的值存储到<code>bar</code>上：</p>\n<pre><code>$ etcdctl put foo1 bar1 --lease=1234abcd</code></pre><p>注意：以上命令中租约ID为1234abcd将会在租约创建10秒后将id返回，这个id将附在Key上。</p>\n<h3 id=\"读取Keys\"><a href=\"#读取Keys\" class=\"headerlink\" title=\"读取Keys\"></a>读取Keys</h3><p>应用程序可以从一个etcd集群中读取Key，可能会查询到单个Key，或者是一个范围内的Key。<br>比如etcd集群中存储以下Key：</p>\n<pre><code>foo = bar\nfoo1 = bar1\nfoo2 = bar2\nfoo3 = bar3</code></pre><p>这里的命令是读取Key<code>foo</code>对应的值：</p>\n<pre><code>$ etcdctl get foo\nfoo\nbar</code></pre><p>这里的命令是读取Key<code>foo</code>对应的十六进制的值:</p>\n<pre><code>$ etcdctl get foo --hex\n\\x66\\x6f\\x6f       #Key\n\\x62\\x61\\x72       #Value</code></pre><p>这里的命令是只读取Key<code>foo</code>对应的值：</p>\n<pre><code>$ etcdctl get foo --print-value-only\nbar</code></pre><p>这里的命令是读取从Key<code>foo</code>到Key<code>foo3</code>范围内对应的值：</p>\n<pre><code>$ etcdctl get foo foo3\nfoo\nbar\nfoo1\nbar1\nfoo2\nbar2</code></pre><p>注意这里Key为<code>foo3</code>不包括在内因为这里的范围是半开区间<code>[foo,foo3)</code>，不包括<code>foo3</code>。</p>\n<p>这里的命令是获取前缀为<code>foo</code>的Key的范围内所有的值：</p>\n<pre><code>$ etcdctl get --prefix foo\nfoo\nbar\nfoo1\nbar1\nfoo2\nbar2\nfoo3\nbar3</code></pre><p>这里的命令是获取前缀为<code>foo</code>的Key的范围内所有的值,并且限制结果集为2：</p>\n<pre><code>$ etcdctl get --prefix --limit=2 foo\nfoo\nbar\nfoo1\nbar1</code></pre><h3 id=\"读取之前版本的Keys：\"><a href=\"#读取之前版本的Keys：\" class=\"headerlink\" title=\"读取之前版本的Keys：\"></a>读取之前版本的Keys：</h3><p>应用程度可能希望读取一个被替代的版本的Key。例如，一个应用程序可能想要通过读取一个先前版本的Key来回滚到一个老的配置。另外，一个应用程序可能想要通过访问Key的历史记录对多个Key通过多个请求获取一致性的结果。由于对etcd集群中键值对的每一次修改都会增加对在etcd集群中的全局修订存储，应用程序可以通过提供一个老的版本来读取被替代的Keys。<br>比如一个etcd集群中存在以下的Keys：</p>\n<pre><code>foo = bar         # revision = 2\nfoo1 = bar1       # revision = 3\nfoo = bar_new     # revision = 4\nfoo1 = bar1_new   # revision = 5</code></pre><p>这里的例子是访问过去版本的Keys：</p>\n<pre><code>$ etcdctl get --prefix foo # access the most recent versions of keys\nfoo\nbar_new\nfoo1\nbar1_new\n\n$ etcdctl get --prefix --rev=4 foo # access the versions of keys at revision 4\nfoo\nbar_new\nfoo1\nbar1\n\n$ etcdctl get --prefix --rev=3 foo # access the versions of keys at revision 3\nfoo\nbar\nfoo1\nbar1\n\n$ etcdctl get --prefix --rev=2 foo # access the versions of keys at revision 2\nfoo\nbar\n\n$ etcdctl get --prefix --rev=1 foo # access the versions of keys at revision 1</code></pre><h3 id=\"读取大于或等于一个具体的Key的比特值的Key：\"><a href=\"#读取大于或等于一个具体的Key的比特值的Key：\" class=\"headerlink\" title=\"读取大于或等于一个具体的Key的比特值的Key：\"></a>读取大于或等于一个具体的Key的比特值的Key：</h3><p>应用程序可能想要读取大于或等于一个具体的Key的byte值的Key。<br>一个etcd集群中有以下的Keys：</p>\n<pre><code>a = 123\nb = 456\nz = 789</code></pre><p>这里的命令是读取大于或等于Key <code>b</code>的byte值的Key：</p>\n<pre><code>$ etcdctl get --from-key b\nb\n456\nz\n789</code></pre><h3 id=\"删除-Keys\"><a href=\"#删除-Keys\" class=\"headerlink\" title=\"删除 Keys\"></a>删除 Keys</h3><p>应用程序可以从etcd集群中删除一个Key或者删除一个范围内的Key：<br>一个etcd集群中有以下的Keys：</p>\n<pre><code>foo = bar\nfoo1 = bar1\nfoo3 = bar3\nzoo = val\nzoo1 = val1\nzoo2 = val2\na = 123\nb = 456\nz = 789</code></pre><p>这里的命令是删除Key<code>foo</code>:</p>\n<pre><code>$ etcdctl del foo\n1 # 1 个 key 被删除</code></pre><p>这里的命令是删除从Key<code>foo</code>到Key<code>foo9</code>范围内的Key:</p>\n<pre><code>$ etcdctl del foo foo9\n2 # 2 个 keys 被删除</code></pre><p>这里的命令是删除Key<code>zoo</code>并将已删除的键值对返回:</p>\n<pre><code>$ etcdctl del --prev-kv zoo\n1   # 1 个 key 被删除\nzoo # 被删除的Key\nval # 被删除的Key所对应的Value</code></pre><p>这里的命令是删除前缀为<code>zoo</code>的Keys:</p>\n<pre><code>$ etcdctl del --prefix zoo\n2 # 2 个 key 被删除</code></pre><p>这里的命令是读取大于或等于Key <code>b</code>的byte值的Keys：</p>\n<pre><code>$ etcdctl del --from-key b\n2 # 2 个 key 被删除</code></pre><h3 id=\"观察key的变化\"><a href=\"#观察key的变化\" class=\"headerlink\" title=\"观察key的变化\"></a>观察key的变化</h3><p>应用程序可以监视一个Key或者一个范围内的Keys的每一次更新。<br>这里的命令是观察key<code>foo</code>:</p>\n<pre><code>$ etcdctl watch foo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar</code></pre><p>这里的命令是观察十六进制的key<code>foo</code>:</p>\n<pre><code>$ etcdctl watch foo --hex\n# 在另一个终端执行: etcdctl put foo bar\nPUT\n\\x66\\x6f\\x6f          # Key\n\\x62\\x61\\x72          # Value</code></pre><p>这里的命令是观察从Key<code>foo</code>到Key<code>foo9</code>范围内的Key：</p>\n<pre><code>$ etcdctl watch foo foo9\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put foo1 bar1\nPUT\nfoo1\nbar1</code></pre><p>这里的命令是观察前缀为<code>foo</code>的Key的范围内所有的值：</p>\n<pre><code>$ etcdctl watch --prefix foo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put fooz1 barz1\nPUT\nfooz1\nbarz1</code></pre><p>这里的命令是观察多个Keys<code>foo</code>和<code>zoo</code>:</p>\n<pre><code>$ etcdctl watch -i\n$ watch foo\n$ watch zoo\n# 在另一个终端执行: etcdctl put foo bar\nPUT\nfoo\nbar\n# 在另一个终端执行: etcdctl put zoo val\nPUT\nzoo\nval</code></pre><h3 id=\"观察Keys的历史版本\"><a href=\"#观察Keys的历史版本\" class=\"headerlink\" title=\"观察Keys的历史版本\"></a>观察Keys的历史版本</h3><p>应用程序可能想要观察etcd中Keys的更新历史。例如，应用程序可能想获取key的所有修改；如果应用程序保持与etcd的连接，那么命令<code>watch</code>已经足够。然而，如果应用程序或者etcd宕机，一次更新可能就会失败，应用程序可能不能实时接收Key的更新。为了保证更新可以被交付，应用程序必须通过观察到Keys的历史更新。为了做到这些，应用程序要指定观察的历史版本，就像读取历史版本的Keys：<br>我们首先完成以下操作：</p>\n<pre><code>$ etcdctl put foo bar         # revision = 2\nOK\n$ etcdctl put foo1 bar1       # revision = 3\nOK\n$ etcdctl put foo bar_new     # revision = 4\nOK\n$ etcdctl put foo1 bar1_new   # revision = 5\nOK</code></pre><p>这里有个例子观察历史更新：</p>\n<pre><code># watch for changes on key `foo` since revision 2\n$ etcdctl watch --rev=2 foo\nPUT\nfoo\nbar\nPUT\nfoo\nbar_new</code></pre><pre><code># watch for changes on key `foo` since revision 3\n$ etcdctl watch --rev=3 foo\nPUT\nfoo\nbar_new</code></pre><p>这里有例子只观察最后一次的更新：</p>\n<pre><code># watch for changes on key `foo` and return last revision value along with modified value\n$ etcdctl watch --prev-kv foo\n# 在另一个终端执行 etcdctl put foo bar_latest\nPUT\nfoo         # key\nbar_new     # last value of foo key before modification\nfoo         # key\nbar_latest  # value of foo key after modification</code></pre><h3 id=\"观察进度\"><a href=\"#观察进度\" class=\"headerlink\" title=\"观察进度\"></a>观察进度</h3><p>应用程序可能想要检查观察者进度以确定最新的观察者流的状态。例如，如果观察者更新的缓存，那么就可以通过原子读取与修改进度进行比较知道缓存内容是否已经过时。<br>进度请求可以通过<code>progress</code>命令与观察者session进行交互在一个观察者流中告诉服务器发送一个进度提示更新.</p>\n<pre><code>$ etcdctl watch -i\n$ watch a\n$ progress\nprogress notify: 1\n# 在另一个终端执行: etcdctl put x 0\n# 在另一个终端执行: etcdctl put y 1\n$ progress\nprogress notify: 3</code></pre><p>注意，在进度提示响应中的修改号来自观察者流连接到的本地etcd服务器。如果该节点被分区并且不是该分区的一部分，这个进度提示修改版本可能会低于由未分区的etcd服务器节点返回的修改版本。</p>\n<h3 id=\"压缩修改\"><a href=\"#压缩修改\" class=\"headerlink\" title=\"压缩修改\"></a>压缩修改</h3><p>正如我们提到的，etcd保持修改信息所以应用可以读取过去版本的Keys，然而，为了避免无数的修改历史累积，对过去的修改进行压缩是很重要的。在压缩后，etcd移除了历史修改，释放资源为以后使用。在压缩修改版本之前所有的被修改的替代版本数据将不能获取。<br>这里的命令是对修改进行压缩：</p>\n<pre><code>$ etcdctl compact 5\ncompacted revision 5\n\n# any revisions before the compacted one are not accessible\n$ etcdctl get --rev=4 foo\nError:  rpc error: code = 11 desc = etcdserver: mvcc: required revision has been compacted</code></pre><p>注意：etcd服务器的当前版本可以使用json格式的命令通过(存在或不存在的)key发现。例如下面的通过查看在etcd服务器中不存在的myKey:</p>\n<pre><code>$ etcdctl get mykey -w=json\n{&quot;header&quot;:{&quot;cluster_id&quot;:14841639068965178418,&quot;member_id&quot;:10276657743932975437,&quot;revision&quot;:15,&quot;raft_term&quot;:4}}</code></pre><h3 id=\"授予租约\"><a href=\"#授予租约\" class=\"headerlink\" title=\"授予租约\"></a>授予租约</h3><p>应用程序可以为etcd集群上的Keys授予一个租约。当Key附上租约后，它的生命周期会绑定到租约的生命周期并由存活时间(TTL)进行管理。每一个租约都有一个由应用程序授予的最小的TTL值.这个租约实际的TTL值至少是最小的TTL值，由etcd集群决定。一旦超过租约的TTL，租约将会超时并删除附上的所有的Keys。<br>这里有命令授予一个租约：</p>\n<pre><code># grant a lease with 60 second TTL\n$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n\n# attach key foo to lease 32695410dcc0ca06\n$ etcdctl put --lease=32695410dcc0ca06 foo bar\nOK</code></pre><h3 id=\"撤销租约\"><a href=\"#撤销租约\" class=\"headerlink\" title=\"撤销租约\"></a>撤销租约</h3><p>应用程序可以根据租约ID撤销租约，撤销一个租约将删除附上的所有的Keys。<br>例如我们完成下面的操作：</p>\n<pre><code>$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)\n$ etcdctl put --lease=32695410dcc0ca06 foo bar\nOK</code></pre><p>这里的命令可以撤销该租约：</p>\n<pre><code>$ etcdctl lease revoke 32695410dcc0ca06\nlease 32695410dcc0ca06 revoked\n\n$ etcdctl get foo\n# empty response since foo is deleted due to lease revocation</code></pre><h3 id=\"保持租约存活\"><a href=\"#保持租约存活\" class=\"headerlink\" title=\"保持租约存活\"></a>保持租约存活</h3><p>应用程序可以通过刷新租约的TTL使它不会超时保证租约存活。<br>例如我们完成下面的操作：</p>\n<pre><code>$ etcdctl lease grant 60\nlease 32695410dcc0ca06 granted with TTL(60s)</code></pre><p>这里有命令保持租约存活：</p>\n<pre><code>$ etcdctl lease keep-alive 32695410dcc0ca06\nlease 32695410dcc0ca06 keepalived with TTL(60)\nlease 32695410dcc0ca06 keepalived with TTL(60)\nlease 32695410dcc0ca06 keepalived with TTL(60)\n...</code></pre><h3 id=\"获取租约信息\"><a href=\"#获取租约信息\" class=\"headerlink\" title=\"获取租约信息\"></a>获取租约信息</h3><p>应用程序可能想知道关于租约的信息，所以可以通过重新创建或者检查租约是否仍然生存或已经超时。应用程序可能也想知道一个具体的租约上所附的Key。<br>例如我们完成下面的操作：</p>\n<pre><code># grant a lease with 500 second TTL\n$ etcdctl lease grant 500\nlease 694d5765fc71500b granted with TTL(500s)\n\n# attach key zoo1 to lease 694d5765fc71500b\n$ etcdctl put zoo1 val1 --lease=694d5765fc71500b\nOK\n\n# attach key zoo2 to lease 694d5765fc71500b\n$ etcdctl put zoo2 val2 --lease=694d5765fc71500b\nOK</code></pre><p>这里有命令获取关于租约的信息:</p>\n<pre><code>$ etcdctl lease timetolive 694d5765fc71500b\nlease 694d5765fc71500b granted with TTL(500s), remaining(258s)</code></pre><p>这里有命令获取租约上所依附的关于Keys的信息：</p>\n<pre><code>$ etcdctl lease timetolive --keys 694d5765fc71500b\nlease 694d5765fc71500b granted with TTL(500s), remaining(132s), attached keys([zoo2 zoo1])\n\n# if the lease has expired or does not exist it will give the below response:\nError:  etcdserver: requested lease not found</code></pre>"},{"title":"在容器中运行etcd集群","date":"2019-11-24T07:38:50.000Z","_content":"原文地址：[Docker container](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/container.md#docker)\n以下指南显示了如何使用[静态引导过程](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/)在rkt和Docker上运行etcd。\n## **rkt**\n* * *\n**运行单节点的etcd**\n以下rkt run命令将在端口2379上公开etcd客户端API，并在端口2380上公开对等API。\n\n配置etcd时使用主机IP地址。\n```\nexport NODE1=192.168.1.21\n```\n信任CoreOS [App签名密钥](https://coreos.com/security/app-signing-key/)。\n```\nsudo rkt trust --prefix quay.io/coreos/etcd\n# gpg key fingerprint is: 18AD 5014 C99E F7E3 BA5F  6CE9 50BD D3E0 FC8A 365E\n```\n运行etcd v3.2版本或指定其他发行版本。\n```\nsudo rkt run --net=default:IP=${NODE1} quay.io/coreos/etcd:v3.2 -- -name=node1 -advertise-client-urls=http://${NODE1}:2379 -initial-advertise-peer-urls=http://${NODE1}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE1}:2380 -initial-cluster=node1=http://${NODE1}:2380\n```\n列出集群成员：\n```\netcdctl --endpoints=http://192.168.1.21:2379 member list\n```\n**运行3个节点的etcd**\n使用`-initial-cluster`参数在本地使用rkt设置3节点集群。\n```\nexport NODE1=172.16.28.21\nexport NODE2=172.16.28.22\nexport NODE3=172.16.28.23\n```\n```\n# node 1\nsudo rkt run --net=default:IP=${NODE1} quay.io/coreos/etcd:v3.2 -- -name=node1 -advertise-client-urls=http://${NODE1}:2379 -initial-advertise-peer-urls=http://${NODE1}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE1}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n\n# node 2\nsudo rkt run --net=default:IP=${NODE2} quay.io/coreos/etcd:v3.2 -- -name=node2 -advertise-client-urls=http://${NODE2}:2379 -initial-advertise-peer-urls=http://${NODE2}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE2}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n\n# node 3\nsudo rkt run --net=default:IP=${NODE3} quay.io/coreos/etcd:v3.2 -- -name=node3 -advertise-client-urls=http://${NODE3}:2379 -initial-advertise-peer-urls=http://${NODE3}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE3}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n```\n验证集群是否健康并且可以访问。\n```\nETCDCTL_API=3 etcdctl --endpoints=http://172.16.28.21:2379,http://172.16.28.22:2379,http://172.16.28.23:2379 endpoint health\n```\n### DNS\n通过本地解析器已知的DNS名称引用对等方的生产群集必须安装[主机的DNS配置](https://coreos.com/tectonic/docs/latest/tutorials/sandbox/index.html#customizing-rkt-options)。\n\n## **Docker**\n为了向Docker主机外部的客户端公开etcd API，请使用容器的主机IP地址。 请参阅[docker inspect](https://docs.docker.com/engine/reference/commandline/inspect/)了解有关如何获取IP地址的更多详细信息。 或者，为`docker run`命令指定`--net = host`标志，以跳过将容器放置在单独的网络堆栈内的操作。\n**运行单节点的etcd**\n适用主机Ip地址配置etcd：\n```\nexport NODE1=192.168.1.21\n```\n配置Docker卷存储etcd数据:\n```\ndocker volume create --name etcd-data\nexport DATA_DIR=\"etcd-data\"\n```\n运行最新版本的etcd：\n```\nREGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:latest \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name node1 \\\n  --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster node1=http://${NODE1}:2380\n```\n列出集群成员：\n```\netcdctl --endpoints=http://${NODE1}:2379 member list\n```\n**运行3个节点的etcd**\n```\nREGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\n# For each machine\nETCD_VERSION=latest\nTOKEN=my-etcd-token\nCLUSTER_STATE=new\nNAME_1=etcd-node-0\nNAME_2=etcd-node-1\nNAME_3=etcd-node-2\nHOST_1=10.20.30.1\nHOST_2=10.20.30.2\nHOST_3=10.20.30.3\nCLUSTER=${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2380,${NAME_3}=http://${HOST_3}:2380\nDATA_DIR=/var/lib/etcd\n\n# For node 1\nTHIS_NAME=${NAME_1}\nTHIS_IP=${HOST_1}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# For node 2\nTHIS_NAME=${NAME_2}\nTHIS_IP=${HOST_2}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# For node 3\nTHIS_NAME=${NAME_3}\nTHIS_IP=${HOST_3}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n```\n适用版本v3的`etcdctl`：\n```\ndocker exec etcd /bin/sh -c \"export ETCDCTL_API=3 && /usr/local/bin/etcdctl put foo bar\"\n```\n## Bare Metal\n* * *\n要在裸机上配置3节点etcd集群，[裸机存储库](https://github.com/poseidon/matchbox/tree/master/examples)中的示例可能会有用。\n#### 挂载一个证书卷：\netcd发布容器不包含默认的根证书。 要将HTTPS与受根权限信任的证书一起使用（例如，用于发现），请将证书目录安装到etcd容器中：\n```\nREGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=docker://gcr.io/etcd-development/etcd\n\nrkt run \\\n  --insecure-options=image \\\n  --volume etcd-ssl-certs-bundle,kind=host,source=/etc/ssl/certs/ca-certificates.crt \\\n  --mount volume=etcd-ssl-certs-bundle,target=/etc/ssl/certs/ca-certificates.crt \\\n  ${REGISTRY}:latest -- --name my-name \\\n  --initial-advertise-peer-urls http://localhost:2380 --listen-peer-urls http://localhost:2380 \\\n  --advertise-client-urls http://localhost:2379 --listen-client-urls http://localhost:2379 \\\n  --discovery https://discovery.etcd.io/c11fbcdc16972e45253491a24fcf45e1\n```\n```\nREGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=/etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt \\\n  ${REGISTRY}:latest \\\n  /usr/local/bin/etcd --name my-name \\\n  --initial-advertise-peer-urls http://localhost:2380 --listen-peer-urls http://localhost:2380 \\\n  --advertise-client-urls http://localhost:2379 --listen-client-urls http://localhost:2379 \\\n  --discovery https://discovery.etcd.io/86a9ff6c8cb8b4c4544c1a2f88f8b801\n```","source":"_posts/blog/etcd/在容器内运行etcd集群.md","raw":"---\ntitle: 在容器中运行etcd集群\ndate: 2019-11-24 15:38:50\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址：[Docker container](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/container.md#docker)\n以下指南显示了如何使用[静态引导过程](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/)在rkt和Docker上运行etcd。\n## **rkt**\n* * *\n**运行单节点的etcd**\n以下rkt run命令将在端口2379上公开etcd客户端API，并在端口2380上公开对等API。\n\n配置etcd时使用主机IP地址。\n```\nexport NODE1=192.168.1.21\n```\n信任CoreOS [App签名密钥](https://coreos.com/security/app-signing-key/)。\n```\nsudo rkt trust --prefix quay.io/coreos/etcd\n# gpg key fingerprint is: 18AD 5014 C99E F7E3 BA5F  6CE9 50BD D3E0 FC8A 365E\n```\n运行etcd v3.2版本或指定其他发行版本。\n```\nsudo rkt run --net=default:IP=${NODE1} quay.io/coreos/etcd:v3.2 -- -name=node1 -advertise-client-urls=http://${NODE1}:2379 -initial-advertise-peer-urls=http://${NODE1}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE1}:2380 -initial-cluster=node1=http://${NODE1}:2380\n```\n列出集群成员：\n```\netcdctl --endpoints=http://192.168.1.21:2379 member list\n```\n**运行3个节点的etcd**\n使用`-initial-cluster`参数在本地使用rkt设置3节点集群。\n```\nexport NODE1=172.16.28.21\nexport NODE2=172.16.28.22\nexport NODE3=172.16.28.23\n```\n```\n# node 1\nsudo rkt run --net=default:IP=${NODE1} quay.io/coreos/etcd:v3.2 -- -name=node1 -advertise-client-urls=http://${NODE1}:2379 -initial-advertise-peer-urls=http://${NODE1}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE1}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n\n# node 2\nsudo rkt run --net=default:IP=${NODE2} quay.io/coreos/etcd:v3.2 -- -name=node2 -advertise-client-urls=http://${NODE2}:2379 -initial-advertise-peer-urls=http://${NODE2}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE2}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n\n# node 3\nsudo rkt run --net=default:IP=${NODE3} quay.io/coreos/etcd:v3.2 -- -name=node3 -advertise-client-urls=http://${NODE3}:2379 -initial-advertise-peer-urls=http://${NODE3}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE3}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n```\n验证集群是否健康并且可以访问。\n```\nETCDCTL_API=3 etcdctl --endpoints=http://172.16.28.21:2379,http://172.16.28.22:2379,http://172.16.28.23:2379 endpoint health\n```\n### DNS\n通过本地解析器已知的DNS名称引用对等方的生产群集必须安装[主机的DNS配置](https://coreos.com/tectonic/docs/latest/tutorials/sandbox/index.html#customizing-rkt-options)。\n\n## **Docker**\n为了向Docker主机外部的客户端公开etcd API，请使用容器的主机IP地址。 请参阅[docker inspect](https://docs.docker.com/engine/reference/commandline/inspect/)了解有关如何获取IP地址的更多详细信息。 或者，为`docker run`命令指定`--net = host`标志，以跳过将容器放置在单独的网络堆栈内的操作。\n**运行单节点的etcd**\n适用主机Ip地址配置etcd：\n```\nexport NODE1=192.168.1.21\n```\n配置Docker卷存储etcd数据:\n```\ndocker volume create --name etcd-data\nexport DATA_DIR=\"etcd-data\"\n```\n运行最新版本的etcd：\n```\nREGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:latest \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name node1 \\\n  --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster node1=http://${NODE1}:2380\n```\n列出集群成员：\n```\netcdctl --endpoints=http://${NODE1}:2379 member list\n```\n**运行3个节点的etcd**\n```\nREGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\n# For each machine\nETCD_VERSION=latest\nTOKEN=my-etcd-token\nCLUSTER_STATE=new\nNAME_1=etcd-node-0\nNAME_2=etcd-node-1\nNAME_3=etcd-node-2\nHOST_1=10.20.30.1\nHOST_2=10.20.30.2\nHOST_3=10.20.30.3\nCLUSTER=${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2380,${NAME_3}=http://${HOST_3}:2380\nDATA_DIR=/var/lib/etcd\n\n# For node 1\nTHIS_NAME=${NAME_1}\nTHIS_IP=${HOST_1}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# For node 2\nTHIS_NAME=${NAME_2}\nTHIS_IP=${HOST_2}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# For node 3\nTHIS_NAME=${NAME_3}\nTHIS_IP=${HOST_3}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n```\n适用版本v3的`etcdctl`：\n```\ndocker exec etcd /bin/sh -c \"export ETCDCTL_API=3 && /usr/local/bin/etcdctl put foo bar\"\n```\n## Bare Metal\n* * *\n要在裸机上配置3节点etcd集群，[裸机存储库](https://github.com/poseidon/matchbox/tree/master/examples)中的示例可能会有用。\n#### 挂载一个证书卷：\netcd发布容器不包含默认的根证书。 要将HTTPS与受根权限信任的证书一起使用（例如，用于发现），请将证书目录安装到etcd容器中：\n```\nREGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=docker://gcr.io/etcd-development/etcd\n\nrkt run \\\n  --insecure-options=image \\\n  --volume etcd-ssl-certs-bundle,kind=host,source=/etc/ssl/certs/ca-certificates.crt \\\n  --mount volume=etcd-ssl-certs-bundle,target=/etc/ssl/certs/ca-certificates.crt \\\n  ${REGISTRY}:latest -- --name my-name \\\n  --initial-advertise-peer-urls http://localhost:2380 --listen-peer-urls http://localhost:2380 \\\n  --advertise-client-urls http://localhost:2379 --listen-client-urls http://localhost:2379 \\\n  --discovery https://discovery.etcd.io/c11fbcdc16972e45253491a24fcf45e1\n```\n```\nREGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=/etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt \\\n  ${REGISTRY}:latest \\\n  /usr/local/bin/etcd --name my-name \\\n  --initial-advertise-peer-urls http://localhost:2380 --listen-peer-urls http://localhost:2380 \\\n  --advertise-client-urls http://localhost:2379 --listen-client-urls http://localhost:2379 \\\n  --discovery https://discovery.etcd.io/86a9ff6c8cb8b4c4544c1a2f88f8b801\n```","slug":"blog/etcd/在容器内运行etcd集群","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyht0021k0vqhmnyczck","content":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/container.md#docker\" target=\"_blank\" rel=\"noopener\">Docker container</a><br>以下指南显示了如何使用<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/\">静态引导过程</a>在rkt和Docker上运行etcd。</p>\n<h2 id=\"rkt\"><a href=\"#rkt\" class=\"headerlink\" title=\"rkt\"></a><strong>rkt</strong></h2><hr>\n<p><strong>运行单节点的etcd</strong><br>以下rkt run命令将在端口2379上公开etcd客户端API，并在端口2380上公开对等API。</p>\n<p>配置etcd时使用主机IP地址。</p>\n<pre><code>export NODE1=192.168.1.21</code></pre><p>信任CoreOS <a href=\"https://coreos.com/security/app-signing-key/\" target=\"_blank\" rel=\"noopener\">App签名密钥</a>。</p>\n<pre><code>sudo rkt trust --prefix quay.io/coreos/etcd\n# gpg key fingerprint is: 18AD 5014 C99E F7E3 BA5F  6CE9 50BD D3E0 FC8A 365E</code></pre><p>运行etcd v3.2版本或指定其他发行版本。</p>\n<pre><code>sudo rkt run --net=default:IP=${NODE1} quay.io/coreos/etcd:v3.2 -- -name=node1 -advertise-client-urls=http://${NODE1}:2379 -initial-advertise-peer-urls=http://${NODE1}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE1}:2380 -initial-cluster=node1=http://${NODE1}:2380</code></pre><p>列出集群成员：</p>\n<pre><code>etcdctl --endpoints=http://192.168.1.21:2379 member list</code></pre><p><strong>运行3个节点的etcd</strong><br>使用<code>-initial-cluster</code>参数在本地使用rkt设置3节点集群。</p>\n<pre><code>export NODE1=172.16.28.21\nexport NODE2=172.16.28.22\nexport NODE3=172.16.28.23</code></pre><pre><code># node 1\nsudo rkt run --net=default:IP=${NODE1} quay.io/coreos/etcd:v3.2 -- -name=node1 -advertise-client-urls=http://${NODE1}:2379 -initial-advertise-peer-urls=http://${NODE1}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE1}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n\n# node 2\nsudo rkt run --net=default:IP=${NODE2} quay.io/coreos/etcd:v3.2 -- -name=node2 -advertise-client-urls=http://${NODE2}:2379 -initial-advertise-peer-urls=http://${NODE2}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE2}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n\n# node 3\nsudo rkt run --net=default:IP=${NODE3} quay.io/coreos/etcd:v3.2 -- -name=node3 -advertise-client-urls=http://${NODE3}:2379 -initial-advertise-peer-urls=http://${NODE3}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE3}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380</code></pre><p>验证集群是否健康并且可以访问。</p>\n<pre><code>ETCDCTL_API=3 etcdctl --endpoints=http://172.16.28.21:2379,http://172.16.28.22:2379,http://172.16.28.23:2379 endpoint health</code></pre><h3 id=\"DNS\"><a href=\"#DNS\" class=\"headerlink\" title=\"DNS\"></a>DNS</h3><p>通过本地解析器已知的DNS名称引用对等方的生产群集必须安装<a href=\"https://coreos.com/tectonic/docs/latest/tutorials/sandbox/index.html#customizing-rkt-options\" target=\"_blank\" rel=\"noopener\">主机的DNS配置</a>。</p>\n<h2 id=\"Docker\"><a href=\"#Docker\" class=\"headerlink\" title=\"Docker\"></a><strong>Docker</strong></h2><p>为了向Docker主机外部的客户端公开etcd API，请使用容器的主机IP地址。 请参阅<a href=\"https://docs.docker.com/engine/reference/commandline/inspect/\" target=\"_blank\" rel=\"noopener\">docker inspect</a>了解有关如何获取IP地址的更多详细信息。 或者，为<code>docker run</code>命令指定<code>--net = host</code>标志，以跳过将容器放置在单独的网络堆栈内的操作。<br><strong>运行单节点的etcd</strong><br>适用主机Ip地址配置etcd：</p>\n<pre><code>export NODE1=192.168.1.21</code></pre><p>配置Docker卷存储etcd数据:</p>\n<pre><code>docker volume create --name etcd-data\nexport DATA_DIR=&quot;etcd-data&quot;</code></pre><p>运行最新版本的etcd：</p>\n<pre><code>REGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:latest \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name node1 \\\n  --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster node1=http://${NODE1}:2380</code></pre><p>列出集群成员：</p>\n<pre><code>etcdctl --endpoints=http://${NODE1}:2379 member list</code></pre><p><strong>运行3个节点的etcd</strong></p>\n<pre><code>REGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\n# For each machine\nETCD_VERSION=latest\nTOKEN=my-etcd-token\nCLUSTER_STATE=new\nNAME_1=etcd-node-0\nNAME_2=etcd-node-1\nNAME_3=etcd-node-2\nHOST_1=10.20.30.1\nHOST_2=10.20.30.2\nHOST_3=10.20.30.3\nCLUSTER=${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2380,${NAME_3}=http://${HOST_3}:2380\nDATA_DIR=/var/lib/etcd\n\n# For node 1\nTHIS_NAME=${NAME_1}\nTHIS_IP=${HOST_1}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# For node 2\nTHIS_NAME=${NAME_2}\nTHIS_IP=${HOST_2}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# For node 3\nTHIS_NAME=${NAME_3}\nTHIS_IP=${HOST_3}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}</code></pre><p>适用版本v3的<code>etcdctl</code>：</p>\n<pre><code>docker exec etcd /bin/sh -c &quot;export ETCDCTL_API=3 &amp;&amp; /usr/local/bin/etcdctl put foo bar&quot;</code></pre><h2 id=\"Bare-Metal\"><a href=\"#Bare-Metal\" class=\"headerlink\" title=\"Bare Metal\"></a>Bare Metal</h2><hr>\n<p>要在裸机上配置3节点etcd集群，<a href=\"https://github.com/poseidon/matchbox/tree/master/examples\" target=\"_blank\" rel=\"noopener\">裸机存储库</a>中的示例可能会有用。</p>\n<h4 id=\"挂载一个证书卷：\"><a href=\"#挂载一个证书卷：\" class=\"headerlink\" title=\"挂载一个证书卷：\"></a>挂载一个证书卷：</h4><p>etcd发布容器不包含默认的根证书。 要将HTTPS与受根权限信任的证书一起使用（例如，用于发现），请将证书目录安装到etcd容器中：</p>\n<pre><code>REGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=docker://gcr.io/etcd-development/etcd\n\nrkt run \\\n  --insecure-options=image \\\n  --volume etcd-ssl-certs-bundle,kind=host,source=/etc/ssl/certs/ca-certificates.crt \\\n  --mount volume=etcd-ssl-certs-bundle,target=/etc/ssl/certs/ca-certificates.crt \\\n  ${REGISTRY}:latest -- --name my-name \\\n  --initial-advertise-peer-urls http://localhost:2380 --listen-peer-urls http://localhost:2380 \\\n  --advertise-client-urls http://localhost:2379 --listen-client-urls http://localhost:2379 \\\n  --discovery https://discovery.etcd.io/c11fbcdc16972e45253491a24fcf45e1</code></pre><pre><code>REGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=/etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt \\\n  ${REGISTRY}:latest \\\n  /usr/local/bin/etcd --name my-name \\\n  --initial-advertise-peer-urls http://localhost:2380 --listen-peer-urls http://localhost:2380 \\\n  --advertise-client-urls http://localhost:2379 --listen-client-urls http://localhost:2379 \\\n  --discovery https://discovery.etcd.io/86a9ff6c8cb8b4c4544c1a2f88f8b801</code></pre>","site":{"data":{}},"excerpt":"","more":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/container.md#docker\" target=\"_blank\" rel=\"noopener\">Docker container</a><br>以下指南显示了如何使用<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/\">静态引导过程</a>在rkt和Docker上运行etcd。</p>\n<h2 id=\"rkt\"><a href=\"#rkt\" class=\"headerlink\" title=\"rkt\"></a><strong>rkt</strong></h2><hr>\n<p><strong>运行单节点的etcd</strong><br>以下rkt run命令将在端口2379上公开etcd客户端API，并在端口2380上公开对等API。</p>\n<p>配置etcd时使用主机IP地址。</p>\n<pre><code>export NODE1=192.168.1.21</code></pre><p>信任CoreOS <a href=\"https://coreos.com/security/app-signing-key/\" target=\"_blank\" rel=\"noopener\">App签名密钥</a>。</p>\n<pre><code>sudo rkt trust --prefix quay.io/coreos/etcd\n# gpg key fingerprint is: 18AD 5014 C99E F7E3 BA5F  6CE9 50BD D3E0 FC8A 365E</code></pre><p>运行etcd v3.2版本或指定其他发行版本。</p>\n<pre><code>sudo rkt run --net=default:IP=${NODE1} quay.io/coreos/etcd:v3.2 -- -name=node1 -advertise-client-urls=http://${NODE1}:2379 -initial-advertise-peer-urls=http://${NODE1}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE1}:2380 -initial-cluster=node1=http://${NODE1}:2380</code></pre><p>列出集群成员：</p>\n<pre><code>etcdctl --endpoints=http://192.168.1.21:2379 member list</code></pre><p><strong>运行3个节点的etcd</strong><br>使用<code>-initial-cluster</code>参数在本地使用rkt设置3节点集群。</p>\n<pre><code>export NODE1=172.16.28.21\nexport NODE2=172.16.28.22\nexport NODE3=172.16.28.23</code></pre><pre><code># node 1\nsudo rkt run --net=default:IP=${NODE1} quay.io/coreos/etcd:v3.2 -- -name=node1 -advertise-client-urls=http://${NODE1}:2379 -initial-advertise-peer-urls=http://${NODE1}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE1}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n\n# node 2\nsudo rkt run --net=default:IP=${NODE2} quay.io/coreos/etcd:v3.2 -- -name=node2 -advertise-client-urls=http://${NODE2}:2379 -initial-advertise-peer-urls=http://${NODE2}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE2}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380\n\n# node 3\nsudo rkt run --net=default:IP=${NODE3} quay.io/coreos/etcd:v3.2 -- -name=node3 -advertise-client-urls=http://${NODE3}:2379 -initial-advertise-peer-urls=http://${NODE3}:2380 -listen-client-urls=http://0.0.0.0:2379 -listen-peer-urls=http://${NODE3}:2380 -initial-cluster=node1=http://${NODE1}:2380,node2=http://${NODE2}:2380,node3=http://${NODE3}:2380</code></pre><p>验证集群是否健康并且可以访问。</p>\n<pre><code>ETCDCTL_API=3 etcdctl --endpoints=http://172.16.28.21:2379,http://172.16.28.22:2379,http://172.16.28.23:2379 endpoint health</code></pre><h3 id=\"DNS\"><a href=\"#DNS\" class=\"headerlink\" title=\"DNS\"></a>DNS</h3><p>通过本地解析器已知的DNS名称引用对等方的生产群集必须安装<a href=\"https://coreos.com/tectonic/docs/latest/tutorials/sandbox/index.html#customizing-rkt-options\" target=\"_blank\" rel=\"noopener\">主机的DNS配置</a>。</p>\n<h2 id=\"Docker\"><a href=\"#Docker\" class=\"headerlink\" title=\"Docker\"></a><strong>Docker</strong></h2><p>为了向Docker主机外部的客户端公开etcd API，请使用容器的主机IP地址。 请参阅<a href=\"https://docs.docker.com/engine/reference/commandline/inspect/\" target=\"_blank\" rel=\"noopener\">docker inspect</a>了解有关如何获取IP地址的更多详细信息。 或者，为<code>docker run</code>命令指定<code>--net = host</code>标志，以跳过将容器放置在单独的网络堆栈内的操作。<br><strong>运行单节点的etcd</strong><br>适用主机Ip地址配置etcd：</p>\n<pre><code>export NODE1=192.168.1.21</code></pre><p>配置Docker卷存储etcd数据:</p>\n<pre><code>docker volume create --name etcd-data\nexport DATA_DIR=&quot;etcd-data&quot;</code></pre><p>运行最新版本的etcd：</p>\n<pre><code>REGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:latest \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name node1 \\\n  --initial-advertise-peer-urls http://${NODE1}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${NODE1}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster node1=http://${NODE1}:2380</code></pre><p>列出集群成员：</p>\n<pre><code>etcdctl --endpoints=http://${NODE1}:2379 member list</code></pre><p><strong>运行3个节点的etcd</strong></p>\n<pre><code>REGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\n# For each machine\nETCD_VERSION=latest\nTOKEN=my-etcd-token\nCLUSTER_STATE=new\nNAME_1=etcd-node-0\nNAME_2=etcd-node-1\nNAME_3=etcd-node-2\nHOST_1=10.20.30.1\nHOST_2=10.20.30.2\nHOST_3=10.20.30.3\nCLUSTER=${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2380,${NAME_3}=http://${HOST_3}:2380\nDATA_DIR=/var/lib/etcd\n\n# For node 1\nTHIS_NAME=${NAME_1}\nTHIS_IP=${HOST_1}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# For node 2\nTHIS_NAME=${NAME_2}\nTHIS_IP=${HOST_2}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# For node 3\nTHIS_NAME=${NAME_3}\nTHIS_IP=${HOST_3}\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=${DATA_DIR}:/etcd-data \\\n  --name etcd ${REGISTRY}:${ETCD_VERSION} \\\n  /usr/local/bin/etcd \\\n  --data-dir=/etcd-data --name ${THIS_NAME} \\\n  --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \\\n  --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \\\n  --initial-cluster ${CLUSTER} \\\n  --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}</code></pre><p>适用版本v3的<code>etcdctl</code>：</p>\n<pre><code>docker exec etcd /bin/sh -c &quot;export ETCDCTL_API=3 &amp;&amp; /usr/local/bin/etcdctl put foo bar&quot;</code></pre><h2 id=\"Bare-Metal\"><a href=\"#Bare-Metal\" class=\"headerlink\" title=\"Bare Metal\"></a>Bare Metal</h2><hr>\n<p>要在裸机上配置3节点etcd集群，<a href=\"https://github.com/poseidon/matchbox/tree/master/examples\" target=\"_blank\" rel=\"noopener\">裸机存储库</a>中的示例可能会有用。</p>\n<h4 id=\"挂载一个证书卷：\"><a href=\"#挂载一个证书卷：\" class=\"headerlink\" title=\"挂载一个证书卷：\"></a>挂载一个证书卷：</h4><p>etcd发布容器不包含默认的根证书。 要将HTTPS与受根权限信任的证书一起使用（例如，用于发现），请将证书目录安装到etcd容器中：</p>\n<pre><code>REGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=docker://gcr.io/etcd-development/etcd\n\nrkt run \\\n  --insecure-options=image \\\n  --volume etcd-ssl-certs-bundle,kind=host,source=/etc/ssl/certs/ca-certificates.crt \\\n  --mount volume=etcd-ssl-certs-bundle,target=/etc/ssl/certs/ca-certificates.crt \\\n  ${REGISTRY}:latest -- --name my-name \\\n  --initial-advertise-peer-urls http://localhost:2380 --listen-peer-urls http://localhost:2380 \\\n  --advertise-client-urls http://localhost:2379 --listen-client-urls http://localhost:2379 \\\n  --discovery https://discovery.etcd.io/c11fbcdc16972e45253491a24fcf45e1</code></pre><pre><code>REGISTRY=quay.io/coreos/etcd\n# available from v3.2.5\nREGISTRY=gcr.io/etcd-development/etcd\n\ndocker run \\\n  -p 2379:2379 \\\n  -p 2380:2380 \\\n  --volume=/etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt \\\n  ${REGISTRY}:latest \\\n  /usr/local/bin/etcd --name my-name \\\n  --initial-advertise-peer-urls http://localhost:2380 --listen-peer-urls http://localhost:2380 \\\n  --advertise-client-urls http://localhost:2379 --listen-client-urls http://localhost:2379 \\\n  --discovery https://discovery.etcd.io/86a9ff6c8cb8b4c4544c1a2f88f8b801</code></pre>"},{"title":"基于角色的访问控制","date":"2019-11-25T12:22:22.000Z","_content":"\n原文地址：[Role-based access control](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/authentication.md)\n## 总览\n* * *\n\n身份验证已添加到etcd 2.1中。 etcd v3 API略微修改了身份验证功能的API和用户界面，以更好地适应新的数据模型。本指南旨在帮助用户在etcd v3中设置基本身份验证和基于角色的访问控制。\n\n## 特殊用户和角色\n* * *\n\n有一个特殊用户`root`，一个特殊角色`root`。\n\n### 用户`root`\n\n在激活身份验证之前，必须创建对`etcd`具有完全访问权限的`root`用户。 `root`用户的想法是出于管理目的：管理角色和普通用户。 `root`用户必须具有`root`角色，并且可以在`etcd`中进行任何更改。\n\n### 角色`root`\n\n可以将角色`root`授予除`root`用户之外的任何用户。 具有`root`角色的用户既具有全局读写访问权限，又具有更新集群的身份验证配置的权限。 此外，`root`角色授予常规集群维护的特权，包括修改集群成员资格，对存储进行碎片整理以及拍摄快照。\n\n## 用户的工作方式\n* * *\n\n`etcdctl`的`user`子命令处理与用户帐户有关的所有事情。\n可以通过以下方式找到用户列表：\n```\n$ etcdctl user list\n```\n通过以下方式创建新用户：\n```\n$ etcdctl user add myusername\n```\n创建新用户将提示您输入新密码。 当给出选项`--interactive=false`时，可以从标准输入中提供密码。 `--new-user-password`也可以用于提供密码。\n\n可以通过以下方式为用户授予和撤消角色：\n\n```\n$ etcdctl user grant-role myusername foo\n$ etcdctl user revoke-role myusername bar\n```\n\n可以使用以下命令检查用户的设置：\n```\n$ etcdctl user get myusername\n```\n\n用户密码可以通过以下方式更改：\n\n```\n$ etcdctl user passwd myusername\n```\n\n更改密码将再次提示您输入新密码。 当给出选项`--interactive=false`时，可以从标准输入中提供密码。\n\n通过以下方式删除帐户：\n```\n$ etcdctl user delete myusername\n```\n\n### 角色的工作方式：\n* * *\n\n`etcdctl`的`role`子命令处理与授予特定用户的特定角色的访问控制有关的所有事情。\n\n列出角色：\n```\n$ etcdctl role list\n```\n\n创建一个新角色：\n```\n$ etcdctl role add myrolename\n```\n\n角色没有密码； 它仅定义了一组新的访问权限。\n\n授予角色访问单个密钥或一系列密钥的权限。\n\n范围可以指定为间隔[开始键，结束键]，其中开始键应按字母顺序在词汇上小于结束键。\n\n可以将访问权限授予读取，写入或同时授予两者，如以下示例所示：\n```\n# Give read access to a key /foo\n$ etcdctl role grant-permission myrolename read /foo\n\n# Give read access to keys with a prefix /foo/. The prefix is equal to the range [/foo/, /foo0)\n$ etcdctl role grant-permission myrolename --prefix=true read /foo/\n\n# Give write-only access to the key at /foo/bar\n$ etcdctl role grant-permission myrolename write /foo/bar\n\n# Give full access to keys in a range of [key1, key5)\n$ etcdctl role grant-permission myrolename readwrite key1 key5\n\n# Give full access to keys with a prefix /pub/\n$ etcdctl role grant-permission myrolename --prefix=true readwrite /pub/\n```\n\n要查看授予的权限，我们可以随时查看该角色：\n```\n$ etcdctl role get myrolename\n```\n\n撤消权限是按照相同的逻辑方式完成的：\n```\n$ etcdctl role revoke-permission myrolename /foo/bar\n```\n\n就像完全删除一个角色一样：\n```\n$ etcdctl role delete myrolename\n```\n\n### 开启身份认证\n* * *\n\n启用身份验证的最少步骤如下。 管理员可以根据喜好在启用身份验证之前或之后设置用户和角色。\n\n确保已创建root用户：\n```\n$ etcdctl user add root\nPassword of root:\n```\n\n开启身份认证\n```\n$ etcdctl auth enable\n```\n\n此后，etcd在启用身份验证的情况下运行。 要出于任何原因禁用它，请使用reciprocal命令：\n```\n$ etcdctl --user root:rootpw auth disable\n```\n\n### 使用`etcdctl`进行身份验证\n* * *\n\n`etcdctl`支持类似`curl`的标志进行身份验证。\n```\n$ etcdctl --user user:password get foo\n```\n\n可以从提示符处获取密码：\n```\n$ etcdctl --user user get foo\n```\n\n密码也可以从命令行参数`--password`获取：\n```\n$ etcdctl --user user --password password get foo\n```\n\n否则，所有`etcdctl`命令均保持不变。 用户和角色仍然可以创建和修改，但是需要具有`root`角色的用户进行身份验证。\n\n### 使用TLS通用名称\n* * *\n\n从v3.2版本开始，如果使用参数`--client-cert-auth=true`启动etcd服务器，则客户端的TLS证书中的“通用名称（CN）”字段将用作etcd用户。在这种情况下，公用名将对用户进行身份验证，并且客户端不需要密码。请注意，如果同时传递了1. `--client-cert-auth=true`且客户端提供了CN，并且客户端提供了2.用户名和密码，则将优先考虑基于用户名和密码的身份验证。请注意，此功能不能与`gRPC-proxy`和`gRPC-gateway`一起使用。这是因为`gRPC-proxy`会从其客户端终止TLS，因此所有客户端都共享代理证书。 `gRPC-gateway`内部使用TLS连接将HTTP请求转换为gRPC请求，因此它具有相同的限制。因此，客户端不能正确地将其CN提供给服务器。如果给定证书的CN不为空，则`gRPC-proxy`将导致错误并停止。 `gRPC-proxy`返回错误，表明客户端证书中的CN为非空。\n\n\n从v3.3版本开始，如果启用了带有选项`--peer-cert-allowed-cn`或`--peer-cert-allowed-hostname`的`etcd`服务器启动，则对等节点连接筛选。如果节点的TLS证书身份与允许的节点匹配，则节点只能加入etcd集群。有关更多详细信息，请参见[etcd安全性页面](https://newonexd.github.io/2019/11/25/blog/etcd/TLS/)。","source":"_posts/blog/etcd/基于角色的访问控制.md","raw":"---\ntitle: 基于角色的访问控制\ndate: 2019-11-25 20:22:22\ntags: etcd\ncategories: etcd文档翻译\n---\n\n原文地址：[Role-based access control](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/authentication.md)\n## 总览\n* * *\n\n身份验证已添加到etcd 2.1中。 etcd v3 API略微修改了身份验证功能的API和用户界面，以更好地适应新的数据模型。本指南旨在帮助用户在etcd v3中设置基本身份验证和基于角色的访问控制。\n\n## 特殊用户和角色\n* * *\n\n有一个特殊用户`root`，一个特殊角色`root`。\n\n### 用户`root`\n\n在激活身份验证之前，必须创建对`etcd`具有完全访问权限的`root`用户。 `root`用户的想法是出于管理目的：管理角色和普通用户。 `root`用户必须具有`root`角色，并且可以在`etcd`中进行任何更改。\n\n### 角色`root`\n\n可以将角色`root`授予除`root`用户之外的任何用户。 具有`root`角色的用户既具有全局读写访问权限，又具有更新集群的身份验证配置的权限。 此外，`root`角色授予常规集群维护的特权，包括修改集群成员资格，对存储进行碎片整理以及拍摄快照。\n\n## 用户的工作方式\n* * *\n\n`etcdctl`的`user`子命令处理与用户帐户有关的所有事情。\n可以通过以下方式找到用户列表：\n```\n$ etcdctl user list\n```\n通过以下方式创建新用户：\n```\n$ etcdctl user add myusername\n```\n创建新用户将提示您输入新密码。 当给出选项`--interactive=false`时，可以从标准输入中提供密码。 `--new-user-password`也可以用于提供密码。\n\n可以通过以下方式为用户授予和撤消角色：\n\n```\n$ etcdctl user grant-role myusername foo\n$ etcdctl user revoke-role myusername bar\n```\n\n可以使用以下命令检查用户的设置：\n```\n$ etcdctl user get myusername\n```\n\n用户密码可以通过以下方式更改：\n\n```\n$ etcdctl user passwd myusername\n```\n\n更改密码将再次提示您输入新密码。 当给出选项`--interactive=false`时，可以从标准输入中提供密码。\n\n通过以下方式删除帐户：\n```\n$ etcdctl user delete myusername\n```\n\n### 角色的工作方式：\n* * *\n\n`etcdctl`的`role`子命令处理与授予特定用户的特定角色的访问控制有关的所有事情。\n\n列出角色：\n```\n$ etcdctl role list\n```\n\n创建一个新角色：\n```\n$ etcdctl role add myrolename\n```\n\n角色没有密码； 它仅定义了一组新的访问权限。\n\n授予角色访问单个密钥或一系列密钥的权限。\n\n范围可以指定为间隔[开始键，结束键]，其中开始键应按字母顺序在词汇上小于结束键。\n\n可以将访问权限授予读取，写入或同时授予两者，如以下示例所示：\n```\n# Give read access to a key /foo\n$ etcdctl role grant-permission myrolename read /foo\n\n# Give read access to keys with a prefix /foo/. The prefix is equal to the range [/foo/, /foo0)\n$ etcdctl role grant-permission myrolename --prefix=true read /foo/\n\n# Give write-only access to the key at /foo/bar\n$ etcdctl role grant-permission myrolename write /foo/bar\n\n# Give full access to keys in a range of [key1, key5)\n$ etcdctl role grant-permission myrolename readwrite key1 key5\n\n# Give full access to keys with a prefix /pub/\n$ etcdctl role grant-permission myrolename --prefix=true readwrite /pub/\n```\n\n要查看授予的权限，我们可以随时查看该角色：\n```\n$ etcdctl role get myrolename\n```\n\n撤消权限是按照相同的逻辑方式完成的：\n```\n$ etcdctl role revoke-permission myrolename /foo/bar\n```\n\n就像完全删除一个角色一样：\n```\n$ etcdctl role delete myrolename\n```\n\n### 开启身份认证\n* * *\n\n启用身份验证的最少步骤如下。 管理员可以根据喜好在启用身份验证之前或之后设置用户和角色。\n\n确保已创建root用户：\n```\n$ etcdctl user add root\nPassword of root:\n```\n\n开启身份认证\n```\n$ etcdctl auth enable\n```\n\n此后，etcd在启用身份验证的情况下运行。 要出于任何原因禁用它，请使用reciprocal命令：\n```\n$ etcdctl --user root:rootpw auth disable\n```\n\n### 使用`etcdctl`进行身份验证\n* * *\n\n`etcdctl`支持类似`curl`的标志进行身份验证。\n```\n$ etcdctl --user user:password get foo\n```\n\n可以从提示符处获取密码：\n```\n$ etcdctl --user user get foo\n```\n\n密码也可以从命令行参数`--password`获取：\n```\n$ etcdctl --user user --password password get foo\n```\n\n否则，所有`etcdctl`命令均保持不变。 用户和角色仍然可以创建和修改，但是需要具有`root`角色的用户进行身份验证。\n\n### 使用TLS通用名称\n* * *\n\n从v3.2版本开始，如果使用参数`--client-cert-auth=true`启动etcd服务器，则客户端的TLS证书中的“通用名称（CN）”字段将用作etcd用户。在这种情况下，公用名将对用户进行身份验证，并且客户端不需要密码。请注意，如果同时传递了1. `--client-cert-auth=true`且客户端提供了CN，并且客户端提供了2.用户名和密码，则将优先考虑基于用户名和密码的身份验证。请注意，此功能不能与`gRPC-proxy`和`gRPC-gateway`一起使用。这是因为`gRPC-proxy`会从其客户端终止TLS，因此所有客户端都共享代理证书。 `gRPC-gateway`内部使用TLS连接将HTTP请求转换为gRPC请求，因此它具有相同的限制。因此，客户端不能正确地将其CN提供给服务器。如果给定证书的CN不为空，则`gRPC-proxy`将导致错误并停止。 `gRPC-proxy`返回错误，表明客户端证书中的CN为非空。\n\n\n从v3.3版本开始，如果启用了带有选项`--peer-cert-allowed-cn`或`--peer-cert-allowed-hostname`的`etcd`服务器启动，则对等节点连接筛选。如果节点的TLS证书身份与允许的节点匹配，则节点只能加入etcd集群。有关更多详细信息，请参见[etcd安全性页面](https://newonexd.github.io/2019/11/25/blog/etcd/TLS/)。","slug":"blog/etcd/基于角色的访问控制","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyhv0024k0vq96kydcsx","content":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/authentication.md\" target=\"_blank\" rel=\"noopener\">Role-based access control</a></p>\n<h2 id=\"总览\"><a href=\"#总览\" class=\"headerlink\" title=\"总览\"></a>总览</h2><hr>\n<p>身份验证已添加到etcd 2.1中。 etcd v3 API略微修改了身份验证功能的API和用户界面，以更好地适应新的数据模型。本指南旨在帮助用户在etcd v3中设置基本身份验证和基于角色的访问控制。</p>\n<h2 id=\"特殊用户和角色\"><a href=\"#特殊用户和角色\" class=\"headerlink\" title=\"特殊用户和角色\"></a>特殊用户和角色</h2><hr>\n<p>有一个特殊用户<code>root</code>，一个特殊角色<code>root</code>。</p>\n<h3 id=\"用户root\"><a href=\"#用户root\" class=\"headerlink\" title=\"用户root\"></a>用户<code>root</code></h3><p>在激活身份验证之前，必须创建对<code>etcd</code>具有完全访问权限的<code>root</code>用户。 <code>root</code>用户的想法是出于管理目的：管理角色和普通用户。 <code>root</code>用户必须具有<code>root</code>角色，并且可以在<code>etcd</code>中进行任何更改。</p>\n<h3 id=\"角色root\"><a href=\"#角色root\" class=\"headerlink\" title=\"角色root\"></a>角色<code>root</code></h3><p>可以将角色<code>root</code>授予除<code>root</code>用户之外的任何用户。 具有<code>root</code>角色的用户既具有全局读写访问权限，又具有更新集群的身份验证配置的权限。 此外，<code>root</code>角色授予常规集群维护的特权，包括修改集群成员资格，对存储进行碎片整理以及拍摄快照。</p>\n<h2 id=\"用户的工作方式\"><a href=\"#用户的工作方式\" class=\"headerlink\" title=\"用户的工作方式\"></a>用户的工作方式</h2><hr>\n<p><code>etcdctl</code>的<code>user</code>子命令处理与用户帐户有关的所有事情。<br>可以通过以下方式找到用户列表：</p>\n<pre><code>$ etcdctl user list</code></pre><p>通过以下方式创建新用户：</p>\n<pre><code>$ etcdctl user add myusername</code></pre><p>创建新用户将提示您输入新密码。 当给出选项<code>--interactive=false</code>时，可以从标准输入中提供密码。 <code>--new-user-password</code>也可以用于提供密码。</p>\n<p>可以通过以下方式为用户授予和撤消角色：</p>\n<pre><code>$ etcdctl user grant-role myusername foo\n$ etcdctl user revoke-role myusername bar</code></pre><p>可以使用以下命令检查用户的设置：</p>\n<pre><code>$ etcdctl user get myusername</code></pre><p>用户密码可以通过以下方式更改：</p>\n<pre><code>$ etcdctl user passwd myusername</code></pre><p>更改密码将再次提示您输入新密码。 当给出选项<code>--interactive=false</code>时，可以从标准输入中提供密码。</p>\n<p>通过以下方式删除帐户：</p>\n<pre><code>$ etcdctl user delete myusername</code></pre><h3 id=\"角色的工作方式：\"><a href=\"#角色的工作方式：\" class=\"headerlink\" title=\"角色的工作方式：\"></a>角色的工作方式：</h3><hr>\n<p><code>etcdctl</code>的<code>role</code>子命令处理与授予特定用户的特定角色的访问控制有关的所有事情。</p>\n<p>列出角色：</p>\n<pre><code>$ etcdctl role list</code></pre><p>创建一个新角色：</p>\n<pre><code>$ etcdctl role add myrolename</code></pre><p>角色没有密码； 它仅定义了一组新的访问权限。</p>\n<p>授予角色访问单个密钥或一系列密钥的权限。</p>\n<p>范围可以指定为间隔[开始键，结束键]，其中开始键应按字母顺序在词汇上小于结束键。</p>\n<p>可以将访问权限授予读取，写入或同时授予两者，如以下示例所示：</p>\n<pre><code># Give read access to a key /foo\n$ etcdctl role grant-permission myrolename read /foo\n\n# Give read access to keys with a prefix /foo/. The prefix is equal to the range [/foo/, /foo0)\n$ etcdctl role grant-permission myrolename --prefix=true read /foo/\n\n# Give write-only access to the key at /foo/bar\n$ etcdctl role grant-permission myrolename write /foo/bar\n\n# Give full access to keys in a range of [key1, key5)\n$ etcdctl role grant-permission myrolename readwrite key1 key5\n\n# Give full access to keys with a prefix /pub/\n$ etcdctl role grant-permission myrolename --prefix=true readwrite /pub/</code></pre><p>要查看授予的权限，我们可以随时查看该角色：</p>\n<pre><code>$ etcdctl role get myrolename</code></pre><p>撤消权限是按照相同的逻辑方式完成的：</p>\n<pre><code>$ etcdctl role revoke-permission myrolename /foo/bar</code></pre><p>就像完全删除一个角色一样：</p>\n<pre><code>$ etcdctl role delete myrolename</code></pre><h3 id=\"开启身份认证\"><a href=\"#开启身份认证\" class=\"headerlink\" title=\"开启身份认证\"></a>开启身份认证</h3><hr>\n<p>启用身份验证的最少步骤如下。 管理员可以根据喜好在启用身份验证之前或之后设置用户和角色。</p>\n<p>确保已创建root用户：</p>\n<pre><code>$ etcdctl user add root\nPassword of root:</code></pre><p>开启身份认证</p>\n<pre><code>$ etcdctl auth enable</code></pre><p>此后，etcd在启用身份验证的情况下运行。 要出于任何原因禁用它，请使用reciprocal命令：</p>\n<pre><code>$ etcdctl --user root:rootpw auth disable</code></pre><h3 id=\"使用etcdctl进行身份验证\"><a href=\"#使用etcdctl进行身份验证\" class=\"headerlink\" title=\"使用etcdctl进行身份验证\"></a>使用<code>etcdctl</code>进行身份验证</h3><hr>\n<p><code>etcdctl</code>支持类似<code>curl</code>的标志进行身份验证。</p>\n<pre><code>$ etcdctl --user user:password get foo</code></pre><p>可以从提示符处获取密码：</p>\n<pre><code>$ etcdctl --user user get foo</code></pre><p>密码也可以从命令行参数<code>--password</code>获取：</p>\n<pre><code>$ etcdctl --user user --password password get foo</code></pre><p>否则，所有<code>etcdctl</code>命令均保持不变。 用户和角色仍然可以创建和修改，但是需要具有<code>root</code>角色的用户进行身份验证。</p>\n<h3 id=\"使用TLS通用名称\"><a href=\"#使用TLS通用名称\" class=\"headerlink\" title=\"使用TLS通用名称\"></a>使用TLS通用名称</h3><hr>\n<p>从v3.2版本开始，如果使用参数<code>--client-cert-auth=true</code>启动etcd服务器，则客户端的TLS证书中的“通用名称（CN）”字段将用作etcd用户。在这种情况下，公用名将对用户进行身份验证，并且客户端不需要密码。请注意，如果同时传递了1. <code>--client-cert-auth=true</code>且客户端提供了CN，并且客户端提供了2.用户名和密码，则将优先考虑基于用户名和密码的身份验证。请注意，此功能不能与<code>gRPC-proxy</code>和<code>gRPC-gateway</code>一起使用。这是因为<code>gRPC-proxy</code>会从其客户端终止TLS，因此所有客户端都共享代理证书。 <code>gRPC-gateway</code>内部使用TLS连接将HTTP请求转换为gRPC请求，因此它具有相同的限制。因此，客户端不能正确地将其CN提供给服务器。如果给定证书的CN不为空，则<code>gRPC-proxy</code>将导致错误并停止。 <code>gRPC-proxy</code>返回错误，表明客户端证书中的CN为非空。</p>\n<p>从v3.3版本开始，如果启用了带有选项<code>--peer-cert-allowed-cn</code>或<code>--peer-cert-allowed-hostname</code>的<code>etcd</code>服务器启动，则对等节点连接筛选。如果节点的TLS证书身份与允许的节点匹配，则节点只能加入etcd集群。有关更多详细信息，请参见<a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/TLS/\">etcd安全性页面</a>。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/authentication.md\" target=\"_blank\" rel=\"noopener\">Role-based access control</a></p>\n<h2 id=\"总览\"><a href=\"#总览\" class=\"headerlink\" title=\"总览\"></a>总览</h2><hr>\n<p>身份验证已添加到etcd 2.1中。 etcd v3 API略微修改了身份验证功能的API和用户界面，以更好地适应新的数据模型。本指南旨在帮助用户在etcd v3中设置基本身份验证和基于角色的访问控制。</p>\n<h2 id=\"特殊用户和角色\"><a href=\"#特殊用户和角色\" class=\"headerlink\" title=\"特殊用户和角色\"></a>特殊用户和角色</h2><hr>\n<p>有一个特殊用户<code>root</code>，一个特殊角色<code>root</code>。</p>\n<h3 id=\"用户root\"><a href=\"#用户root\" class=\"headerlink\" title=\"用户root\"></a>用户<code>root</code></h3><p>在激活身份验证之前，必须创建对<code>etcd</code>具有完全访问权限的<code>root</code>用户。 <code>root</code>用户的想法是出于管理目的：管理角色和普通用户。 <code>root</code>用户必须具有<code>root</code>角色，并且可以在<code>etcd</code>中进行任何更改。</p>\n<h3 id=\"角色root\"><a href=\"#角色root\" class=\"headerlink\" title=\"角色root\"></a>角色<code>root</code></h3><p>可以将角色<code>root</code>授予除<code>root</code>用户之外的任何用户。 具有<code>root</code>角色的用户既具有全局读写访问权限，又具有更新集群的身份验证配置的权限。 此外，<code>root</code>角色授予常规集群维护的特权，包括修改集群成员资格，对存储进行碎片整理以及拍摄快照。</p>\n<h2 id=\"用户的工作方式\"><a href=\"#用户的工作方式\" class=\"headerlink\" title=\"用户的工作方式\"></a>用户的工作方式</h2><hr>\n<p><code>etcdctl</code>的<code>user</code>子命令处理与用户帐户有关的所有事情。<br>可以通过以下方式找到用户列表：</p>\n<pre><code>$ etcdctl user list</code></pre><p>通过以下方式创建新用户：</p>\n<pre><code>$ etcdctl user add myusername</code></pre><p>创建新用户将提示您输入新密码。 当给出选项<code>--interactive=false</code>时，可以从标准输入中提供密码。 <code>--new-user-password</code>也可以用于提供密码。</p>\n<p>可以通过以下方式为用户授予和撤消角色：</p>\n<pre><code>$ etcdctl user grant-role myusername foo\n$ etcdctl user revoke-role myusername bar</code></pre><p>可以使用以下命令检查用户的设置：</p>\n<pre><code>$ etcdctl user get myusername</code></pre><p>用户密码可以通过以下方式更改：</p>\n<pre><code>$ etcdctl user passwd myusername</code></pre><p>更改密码将再次提示您输入新密码。 当给出选项<code>--interactive=false</code>时，可以从标准输入中提供密码。</p>\n<p>通过以下方式删除帐户：</p>\n<pre><code>$ etcdctl user delete myusername</code></pre><h3 id=\"角色的工作方式：\"><a href=\"#角色的工作方式：\" class=\"headerlink\" title=\"角色的工作方式：\"></a>角色的工作方式：</h3><hr>\n<p><code>etcdctl</code>的<code>role</code>子命令处理与授予特定用户的特定角色的访问控制有关的所有事情。</p>\n<p>列出角色：</p>\n<pre><code>$ etcdctl role list</code></pre><p>创建一个新角色：</p>\n<pre><code>$ etcdctl role add myrolename</code></pre><p>角色没有密码； 它仅定义了一组新的访问权限。</p>\n<p>授予角色访问单个密钥或一系列密钥的权限。</p>\n<p>范围可以指定为间隔[开始键，结束键]，其中开始键应按字母顺序在词汇上小于结束键。</p>\n<p>可以将访问权限授予读取，写入或同时授予两者，如以下示例所示：</p>\n<pre><code># Give read access to a key /foo\n$ etcdctl role grant-permission myrolename read /foo\n\n# Give read access to keys with a prefix /foo/. The prefix is equal to the range [/foo/, /foo0)\n$ etcdctl role grant-permission myrolename --prefix=true read /foo/\n\n# Give write-only access to the key at /foo/bar\n$ etcdctl role grant-permission myrolename write /foo/bar\n\n# Give full access to keys in a range of [key1, key5)\n$ etcdctl role grant-permission myrolename readwrite key1 key5\n\n# Give full access to keys with a prefix /pub/\n$ etcdctl role grant-permission myrolename --prefix=true readwrite /pub/</code></pre><p>要查看授予的权限，我们可以随时查看该角色：</p>\n<pre><code>$ etcdctl role get myrolename</code></pre><p>撤消权限是按照相同的逻辑方式完成的：</p>\n<pre><code>$ etcdctl role revoke-permission myrolename /foo/bar</code></pre><p>就像完全删除一个角色一样：</p>\n<pre><code>$ etcdctl role delete myrolename</code></pre><h3 id=\"开启身份认证\"><a href=\"#开启身份认证\" class=\"headerlink\" title=\"开启身份认证\"></a>开启身份认证</h3><hr>\n<p>启用身份验证的最少步骤如下。 管理员可以根据喜好在启用身份验证之前或之后设置用户和角色。</p>\n<p>确保已创建root用户：</p>\n<pre><code>$ etcdctl user add root\nPassword of root:</code></pre><p>开启身份认证</p>\n<pre><code>$ etcdctl auth enable</code></pre><p>此后，etcd在启用身份验证的情况下运行。 要出于任何原因禁用它，请使用reciprocal命令：</p>\n<pre><code>$ etcdctl --user root:rootpw auth disable</code></pre><h3 id=\"使用etcdctl进行身份验证\"><a href=\"#使用etcdctl进行身份验证\" class=\"headerlink\" title=\"使用etcdctl进行身份验证\"></a>使用<code>etcdctl</code>进行身份验证</h3><hr>\n<p><code>etcdctl</code>支持类似<code>curl</code>的标志进行身份验证。</p>\n<pre><code>$ etcdctl --user user:password get foo</code></pre><p>可以从提示符处获取密码：</p>\n<pre><code>$ etcdctl --user user get foo</code></pre><p>密码也可以从命令行参数<code>--password</code>获取：</p>\n<pre><code>$ etcdctl --user user --password password get foo</code></pre><p>否则，所有<code>etcdctl</code>命令均保持不变。 用户和角色仍然可以创建和修改，但是需要具有<code>root</code>角色的用户进行身份验证。</p>\n<h3 id=\"使用TLS通用名称\"><a href=\"#使用TLS通用名称\" class=\"headerlink\" title=\"使用TLS通用名称\"></a>使用TLS通用名称</h3><hr>\n<p>从v3.2版本开始，如果使用参数<code>--client-cert-auth=true</code>启动etcd服务器，则客户端的TLS证书中的“通用名称（CN）”字段将用作etcd用户。在这种情况下，公用名将对用户进行身份验证，并且客户端不需要密码。请注意，如果同时传递了1. <code>--client-cert-auth=true</code>且客户端提供了CN，并且客户端提供了2.用户名和密码，则将优先考虑基于用户名和密码的身份验证。请注意，此功能不能与<code>gRPC-proxy</code>和<code>gRPC-gateway</code>一起使用。这是因为<code>gRPC-proxy</code>会从其客户端终止TLS，因此所有客户端都共享代理证书。 <code>gRPC-gateway</code>内部使用TLS连接将HTTP请求转换为gRPC请求，因此它具有相同的限制。因此，客户端不能正确地将其CN提供给服务器。如果给定证书的CN不为空，则<code>gRPC-proxy</code>将导致错误并停止。 <code>gRPC-proxy</code>返回错误，表明客户端证书中的CN为非空。</p>\n<p>从v3.3版本开始，如果启用了带有选项<code>--peer-cert-allowed-cn</code>或<code>--peer-cert-allowed-hostname</code>的<code>etcd</code>服务器启动，则对等节点连接筛选。如果节点的TLS证书身份与允许的节点匹配，则节点只能加入etcd集群。有关更多详细信息，请参见<a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/TLS/\">etcd安全性页面</a>。</p>\n"},{"title":"多机集群","date":"2019-11-23T04:32:03.000Z","_content":"原文地址:[cluster on multiple machines](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/clustering.md)\n# 总览\n* * *\n启动一个集群静态的要求是每一个集群中的成员需要知道其他成员的位置。在许多情况下，集群成员的IP可能无法提前知道。在这种情况下，etcd集群可以在发现服务的帮助下进行启动。\n一旦etcd集群已经启动，添加或移除成员可以通过[运行时重新配置](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)。在运行时重新配置之前，为了更好地理解设计，我们建议读[运行时重新配置设计文档](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E8%AE%BE%E8%AE%A1/)。\n这篇引导etcd集群的启动将包括以下机制：\n\n* 静态\n* etcd发现\n* DNS发现\n\n每种引导机制都将用于创建具有以下详细信息的三台计算机etcd集群：\n\n|Name|Address|Hostname|\n|---|---|---|\n|infra0|10.0.1.10|infra0.example.com|\n|infra1|10.0.1.11|infra1.example.com|\n|infra2|10.0.1.12|infra2.example.com|\n\n## 静态\n集群的成员，在启动之前它们的地址和集群的大小，我们可以通过设置`initial-cluster`参数使用离线的启动配置。每一个机器将会通过以下的环境变量或命令行获得配置信息：\n```\nETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380\"\nETCD_INITIAL_CLUSTER_STATE=new\n```\n```\n--initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n--initial-cluster-state new\n```\n注意在`initial-cluster`中的URLs必须是已发布的对等的节点的URLs，即它们应该和对应的节点上的`initial-advertise-peer-urls`的值对应。\n如果为了测试的目的通过相同的配置分解多集群(或者创建和删除单个集群)，值得注意的是每一个集群应该给予独一无二的`initial-cluster-token`,通过做这些工作，即使它们具有相同的配置,etcd也可以为集群成员生成独一无二的集群Id和成员ID。这样可以在可能会扰乱集群的跨集群中交互中保护etcd。\netcd监听在`listen-client-urls`接受客户端流量，etcd将`advertise-client-urls`中指定的URLs告诉其他成员，代理，客户端。请确保潜在的客户端可以获取`advertise-client-urls`。一个常见的错误当远程的客户端应该访问etcd时设置`advertise-client-urls`为localhost或者将其保留为默认值。\n在每一台机器上，通过这些参数启动etcd：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new\n```\n以`initial-cluster`开头的命令行参数将在etcd启动后被忽略。在初始化启动后可以自由删除环境变量或者命令行参数。如果配置信息在启动之后需要改变(例如在/从集群中添加或删除成员),看[运行时配置](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)引导。\n\n### TLS\netcd支持通过TLS协议进行加密通信。TLS通道可以在集群内部通信使用，也可以在节点和客户端流量通信时使用。这一部分列举了为节点和客户端TLS通信的集群设置。添加的etcd的TLS支持信息细节可以在[安全引导](https://newonexd.github.io/2019/11/25/blog/etcd/TLS/)中发现。\n#### 自签名证书\n一个集群使用自签名证书加密流量和连接权限。使用自签名证书启动一个集群，每一个集群成员都需要含有一个独一无二的由共享的集群CA证书(`ca.crt`)签名的秘钥对(`member.crt`,`member.key`)，用于节点连接和客户端连接。证书可以通过下面的etcd[TLS设置](https://newonexd.github.io/2019/11/25/blog/etcd/TLS/)例子中生成。\n对于每一台机器，etcd应该通过这些参数启动：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls https://10.0.1.10:2380 \\\n  --listen-peer-urls https://10.0.1.10:2380 \\\n  --listen-client-urls https://10.0.1.10:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra0-client.crt --key-file=/path/to/infra0-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra0-peer.crt --peer-key-file=/path/to/infra0-peer.key\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls https://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls https://10.0.1.11:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra1-client.crt --key-file=/path/to/infra1-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra1-peer.crt --peer-key-file=/path/to/infra1-peer.key\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls https://10.0.1.12:2380 \\\n  --listen-peer-urls https://10.0.1.12:2380 \\\n  --listen-client-urls https://10.0.1.12:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra2-client.crt --key-file=/path/to/infra2-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra2-peer.crt --peer-key-file=/path/to/infra2-peer.key\n```\n#### 自动化证书\n如果集群需要加密通信但是不需要连接时权限认证,etcd可以配置为自动生成秘钥.在初始化阶段,etcd成员基于他们的Ip地址和主机生成自己的秘钥.\n在每一台主机上,etcd需要根据这些参数进行启动:\n```\n$ etcd --name infra0 --initial-advertise-peer-urls https://10.0.1.10:2380 \\\n  --listen-peer-urls https://10.0.1.10:2380 \\\n  --listen-client-urls https://10.0.1.10:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls https://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls https://10.0.1.11:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls https://10.0.1.12:2380 \\\n  --listen-peer-urls https://10.0.1.12:2380 \\\n  --listen-client-urls https://10.0.1.12:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls\n```\n#### 错误案例\n在以下的例子中，新的主机没有包含在枚举的节点列表中，如果这是一个新的集群，节点需要被添加到初始化集群成员的列表中。\n```\n$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380 \\\n  --initial-cluster-state new\netcd: infra1 not listed in the initial cluster config\nexit 1\n```\n在以下的例子中，我们试图映射一个节点(infra0)到一个不同的地址(127.0.0.1:2380)，而它在集群列表中的地址为(10.0.1.10:2380).如果这个节点监听多个端口，所有地址都必须要反射到`initial-cluster`参数配置中。\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://127.0.0.1:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state=new\netcd: error setting up initial cluster: infra0 has different advertised URLs in the cluster and advertised peer URLs list\nexit 1\n```\n如果一个节点被配置成不同集群的参数并试图加入这个集群，etcd将会报出集群ID不匹配并退出.\n```\n$ etcd --name infra3 --initial-advertise-peer-urls http://10.0.1.13:2380 \\\n  --listen-peer-urls http://10.0.1.13:2380 \\\n  --listen-client-urls http://10.0.1.13:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.13:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra3=http://10.0.1.13:2380 \\\n  --initial-cluster-state=new\netcd: conflicting cluster ID to the target cluster (c6ab534d07e8fcc4 != bc25ea2a74fb18b0). Exiting.\nexit 1\n```\n### 发现服务\n在许多案例中，集群节点不能提前知道Ip地址。这在云服务提供商或者是使用DHCP的网络中很常见。在这种情况下，使用一个存在的etcd集群来启动一个新的节点而不是进行静态的配置，这个过程称为\"节点发现\".\n\n有两种方法可以用来发现节点:\n\n* etcd发现服务\n* DNS SRV 记录\n\n#### etcd 发现服务\n为了更好理解发现服务协议的设计，我们建议阅读发现服务协议[文档](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-internal/discovery_protocol.md).\n**发现服务URL的生命周期**\n一个发现URL标识一个独有的etcd集群而不是使用已有的发现URL。每一个etcd实例分享一个新的发现URL去启动新的集群。\n此外，发现URL应该只在初始化启动集群的时候使用，如果需要改变已经启动的集群中的成员关系，看[运行时重新配置](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)引导.\n**自定义etcd发现服务**\n发现服务用于启动一个存在的集群，如果使用一个私有的etcd集群，像这样创建URL:\n```\n$ curl -X PUT https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3\n```\n通过设置URL中Key的大小，创建发现URL的集群预期大小为3.\n在这种情况下URL将会这样使用:\n```\nhttps://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n当etcd成员启动时将使用\n```\nhttps://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n文件夹进行注册。\n**每一个成员必须含有一个不同的命名参数。`Hostname`或者`machine-id`将是一个好的选择。发现服务的失败通常由于重复的名字。**\n现在我们通过这些参数启动etcd的每一个成员：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n一旦所有主机被注册后，每一个集群成员将会集群启动之前在自定义发现服务中注册自己。\n**公共的etcd发现服务**\n如果没有可以获得的集群，使用托管在`discovery.etcd.io`的公共发现服务。通过\"new\"主机,创建一个私有的发现URL,使用以下命令:\n```\n$ curl https://discovery.etcd.io/new?size=3\nhttps://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n将会创建一个初始化成员数量为3的集群,如果没有设置大小，将默认为3.\n```\nETCD_DISCOVERY=https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n```\n--discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n现在我们通过这些相关的参数启动每一个etcd成员：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n一旦所有主机被注册后，每一个集群成员将会集群启动之前在自定义发现服务中注册自己。\netcd使用环境变量`ETCD_DISCOVERY_PROXY`通过HTTP代理连接发现服务。\n**错误和警告案例**\n发现服务错误：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\netcd: error: the cluster doesn’t have a size configuration value in https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de/_config\nexit 1\n```\n警告\n这里有一个严重的警告表明发现服务URL将被这台主机忽略。\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\netcdserver: discovery token ignored since a cluster has already been initialized. Valid log found at /var/lib/etcd\n```\n### DNS 发现服务\nDNS [SRV 记录](http://www.ietf.org/rfc/rfc2052.txt)可以用来作为发现机制。`--discovery-srv`参数可用于设置可以找到发现SRV记录的DNS域名。 设置`--discovery-srv example.com`会导致DNS SRV记录按照列出的顺序进行查找：\n\n* _etcd-server-ssl._tcp.example.com\n* _etcd-server._tcp.example.com\n\n如果找到`_etcd-server-ssl._tcp.example.com`，则etcd将尝试通过TLS进行引导过程。\n为了帮助客户端发现etcd集群，按照列出的顺序查找以下DNS SRV记录：\n\n*    _etcd-client._tcp.example.com\n*    _etcd-client-ssl._tcp.example.com\n\n如果找到了`_etcd-client-ssl._tcp.example.com`，则客户端将尝试通过SSL/TLS与etcd集群进行通信。\n如果etcd使用TLS，则发现SRV记录（例如example.com）必须与主机名一起包含在SSL证书DNS SAN中，否则集群将失败，并显示以下日志消息：\n```\n[...] rejected connection from \"10.0.1.11:53162\" (error \"remote error: tls: bad certificate\", ServerName \"example.com\")\n```\n如果etcd使用的是没有自定义证书颁发机构的TLS，则发现域（例如example.com）必须与SRV记录域（例如infra1.example.com）匹配。 这是为了缓解伪造SRV记录指向不同域的攻击。 该域将在PKI下拥有有效的证书，但由未知的第三方控制。\n`-discovery-srv-name`参数还为在发现期间查询的SRV名称配置了后缀。 使用此参数可以区分同一域下的多个etcd集群。 例如，如果设置了`Discovery-srv = example.com`和`-discovery-srv-name = foo`，则会进行以下DNS SRV查询：\n\n *   _etcd-server-ssl-foo._tcp.example.com\n *  _etcd-server-foo._tcp.example.com\n\n**创建DNS SRV记录**\n```\n$ dig +noall +answer SRV _etcd-server._tcp.example.com\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra0.example.com.\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra1.example.com.\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra2.example.com.\n```\n```\n$ dig +noall +answer SRV _etcd-client._tcp.example.com\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.\n```\n```\n$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com\ninfra0.example.com.  300  IN  A  10.0.1.10\ninfra1.example.com.  300  IN  A  10.0.1.11\ninfra2.example.com.  300  IN  A  10.0.1.12\n```\n**使用DNS引导etcd集群**\netcd群集成员可以公告域名或IP地址，引导过程将解析DNS A记录。 从3.2开始（3.1将显示警告），`--listen-peer-urls`和`--listen-client-urls`将拒绝网络接口绑定的域名。\n`--initial-advertise-peer-urls`中的解析地址必须与SRV目标中的解析地址之一匹配。 etcd成员读取解析的地址，以查找其是否属于SRV记录中定义的集群。\n```\n$ etcd --name infra0 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra0.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra0.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380\n```\n```\n$ etcd --name infra1 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra1.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra1.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380\n```\n```\n$ etcd --name infra2 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra2.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra2.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380\n```\n集群还可以使用IP地址而不是域名进行引导：\n```\n$ etcd --name infra0 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.10:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.10:2379 \\\n--listen-client-urls http://10.0.1.10:2379 \\\n--listen-peer-urls http://10.0.1.10:2380\n```\n```\n$ etcd --name infra1 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.11:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.11:2379 \\\n--listen-client-urls http://10.0.1.11:2379 \\\n--listen-peer-urls http://10.0.1.11:2380\n```\n```\n$ etcd --name infra2 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.12:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.12:2379 \\\n--listen-client-urls http://10.0.1.12:2379 \\\n--listen-peer-urls http://10.0.1.12:2380\n```\n自从v3.1.0（v3.2.9除外），因此在`etcd --discovery-srv = example.com`中配置了TLS时，服务器仅在提供的证书具有根域`example.com`作为`Subject Alternative`(SAN)字段中的条目时，对等方/客户端进行身份验证。请参阅[DNS SRV的注释](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/security.md#notes-for-dns-srv)。\n**网关**\netcd网关是一个简单的TCP代理，可将网络数据转发到etcd集群。 请阅读[网关指南](https://newonexd.github.io/2019/11/24/blog/etcd/etcd%E7%BD%91%E5%85%B3/)以获取更多信息。\n**代理**\n设置`--proxy`参数时，etcd以[代理模式](https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/)运行。 此代理模式仅支持etcd v2 API； 目前尚无计划支持v3 API。 相反，为了支持v3 API，etcd 3.0版本之后将提供具有增强功能的新代理。\n要使用v2 API代理设置etcd集群，请阅读[etcd 2.3版本中的集群文档](https://github.com/coreos/etcd/blob/release-2.3/Documentation/clustering.md)。","source":"_posts/blog/etcd/多机集群.md","raw":"---\ntitle: 多机集群\ndate: 2019-11-23 12:32:03\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址:[cluster on multiple machines](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/clustering.md)\n# 总览\n* * *\n启动一个集群静态的要求是每一个集群中的成员需要知道其他成员的位置。在许多情况下，集群成员的IP可能无法提前知道。在这种情况下，etcd集群可以在发现服务的帮助下进行启动。\n一旦etcd集群已经启动，添加或移除成员可以通过[运行时重新配置](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)。在运行时重新配置之前，为了更好地理解设计，我们建议读[运行时重新配置设计文档](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E8%AE%BE%E8%AE%A1/)。\n这篇引导etcd集群的启动将包括以下机制：\n\n* 静态\n* etcd发现\n* DNS发现\n\n每种引导机制都将用于创建具有以下详细信息的三台计算机etcd集群：\n\n|Name|Address|Hostname|\n|---|---|---|\n|infra0|10.0.1.10|infra0.example.com|\n|infra1|10.0.1.11|infra1.example.com|\n|infra2|10.0.1.12|infra2.example.com|\n\n## 静态\n集群的成员，在启动之前它们的地址和集群的大小，我们可以通过设置`initial-cluster`参数使用离线的启动配置。每一个机器将会通过以下的环境变量或命令行获得配置信息：\n```\nETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380\"\nETCD_INITIAL_CLUSTER_STATE=new\n```\n```\n--initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n--initial-cluster-state new\n```\n注意在`initial-cluster`中的URLs必须是已发布的对等的节点的URLs，即它们应该和对应的节点上的`initial-advertise-peer-urls`的值对应。\n如果为了测试的目的通过相同的配置分解多集群(或者创建和删除单个集群)，值得注意的是每一个集群应该给予独一无二的`initial-cluster-token`,通过做这些工作，即使它们具有相同的配置,etcd也可以为集群成员生成独一无二的集群Id和成员ID。这样可以在可能会扰乱集群的跨集群中交互中保护etcd。\netcd监听在`listen-client-urls`接受客户端流量，etcd将`advertise-client-urls`中指定的URLs告诉其他成员，代理，客户端。请确保潜在的客户端可以获取`advertise-client-urls`。一个常见的错误当远程的客户端应该访问etcd时设置`advertise-client-urls`为localhost或者将其保留为默认值。\n在每一台机器上，通过这些参数启动etcd：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new\n```\n以`initial-cluster`开头的命令行参数将在etcd启动后被忽略。在初始化启动后可以自由删除环境变量或者命令行参数。如果配置信息在启动之后需要改变(例如在/从集群中添加或删除成员),看[运行时配置](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)引导。\n\n### TLS\netcd支持通过TLS协议进行加密通信。TLS通道可以在集群内部通信使用，也可以在节点和客户端流量通信时使用。这一部分列举了为节点和客户端TLS通信的集群设置。添加的etcd的TLS支持信息细节可以在[安全引导](https://newonexd.github.io/2019/11/25/blog/etcd/TLS/)中发现。\n#### 自签名证书\n一个集群使用自签名证书加密流量和连接权限。使用自签名证书启动一个集群，每一个集群成员都需要含有一个独一无二的由共享的集群CA证书(`ca.crt`)签名的秘钥对(`member.crt`,`member.key`)，用于节点连接和客户端连接。证书可以通过下面的etcd[TLS设置](https://newonexd.github.io/2019/11/25/blog/etcd/TLS/)例子中生成。\n对于每一台机器，etcd应该通过这些参数启动：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls https://10.0.1.10:2380 \\\n  --listen-peer-urls https://10.0.1.10:2380 \\\n  --listen-client-urls https://10.0.1.10:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra0-client.crt --key-file=/path/to/infra0-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra0-peer.crt --peer-key-file=/path/to/infra0-peer.key\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls https://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls https://10.0.1.11:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra1-client.crt --key-file=/path/to/infra1-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra1-peer.crt --peer-key-file=/path/to/infra1-peer.key\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls https://10.0.1.12:2380 \\\n  --listen-peer-urls https://10.0.1.12:2380 \\\n  --listen-client-urls https://10.0.1.12:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra2-client.crt --key-file=/path/to/infra2-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra2-peer.crt --peer-key-file=/path/to/infra2-peer.key\n```\n#### 自动化证书\n如果集群需要加密通信但是不需要连接时权限认证,etcd可以配置为自动生成秘钥.在初始化阶段,etcd成员基于他们的Ip地址和主机生成自己的秘钥.\n在每一台主机上,etcd需要根据这些参数进行启动:\n```\n$ etcd --name infra0 --initial-advertise-peer-urls https://10.0.1.10:2380 \\\n  --listen-peer-urls https://10.0.1.10:2380 \\\n  --listen-client-urls https://10.0.1.10:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls https://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls https://10.0.1.11:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls https://10.0.1.12:2380 \\\n  --listen-peer-urls https://10.0.1.12:2380 \\\n  --listen-client-urls https://10.0.1.12:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls\n```\n#### 错误案例\n在以下的例子中，新的主机没有包含在枚举的节点列表中，如果这是一个新的集群，节点需要被添加到初始化集群成员的列表中。\n```\n$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380 \\\n  --initial-cluster-state new\netcd: infra1 not listed in the initial cluster config\nexit 1\n```\n在以下的例子中，我们试图映射一个节点(infra0)到一个不同的地址(127.0.0.1:2380)，而它在集群列表中的地址为(10.0.1.10:2380).如果这个节点监听多个端口，所有地址都必须要反射到`initial-cluster`参数配置中。\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://127.0.0.1:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state=new\netcd: error setting up initial cluster: infra0 has different advertised URLs in the cluster and advertised peer URLs list\nexit 1\n```\n如果一个节点被配置成不同集群的参数并试图加入这个集群，etcd将会报出集群ID不匹配并退出.\n```\n$ etcd --name infra3 --initial-advertise-peer-urls http://10.0.1.13:2380 \\\n  --listen-peer-urls http://10.0.1.13:2380 \\\n  --listen-client-urls http://10.0.1.13:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.13:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra3=http://10.0.1.13:2380 \\\n  --initial-cluster-state=new\netcd: conflicting cluster ID to the target cluster (c6ab534d07e8fcc4 != bc25ea2a74fb18b0). Exiting.\nexit 1\n```\n### 发现服务\n在许多案例中，集群节点不能提前知道Ip地址。这在云服务提供商或者是使用DHCP的网络中很常见。在这种情况下，使用一个存在的etcd集群来启动一个新的节点而不是进行静态的配置，这个过程称为\"节点发现\".\n\n有两种方法可以用来发现节点:\n\n* etcd发现服务\n* DNS SRV 记录\n\n#### etcd 发现服务\n为了更好理解发现服务协议的设计，我们建议阅读发现服务协议[文档](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-internal/discovery_protocol.md).\n**发现服务URL的生命周期**\n一个发现URL标识一个独有的etcd集群而不是使用已有的发现URL。每一个etcd实例分享一个新的发现URL去启动新的集群。\n此外，发现URL应该只在初始化启动集群的时候使用，如果需要改变已经启动的集群中的成员关系，看[运行时重新配置](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)引导.\n**自定义etcd发现服务**\n发现服务用于启动一个存在的集群，如果使用一个私有的etcd集群，像这样创建URL:\n```\n$ curl -X PUT https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3\n```\n通过设置URL中Key的大小，创建发现URL的集群预期大小为3.\n在这种情况下URL将会这样使用:\n```\nhttps://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n当etcd成员启动时将使用\n```\nhttps://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n文件夹进行注册。\n**每一个成员必须含有一个不同的命名参数。`Hostname`或者`machine-id`将是一个好的选择。发现服务的失败通常由于重复的名字。**\n现在我们通过这些参数启动etcd的每一个成员：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83\n```\n一旦所有主机被注册后，每一个集群成员将会集群启动之前在自定义发现服务中注册自己。\n**公共的etcd发现服务**\n如果没有可以获得的集群，使用托管在`discovery.etcd.io`的公共发现服务。通过\"new\"主机,创建一个私有的发现URL,使用以下命令:\n```\n$ curl https://discovery.etcd.io/new?size=3\nhttps://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n将会创建一个初始化成员数量为3的集群,如果没有设置大小，将默认为3.\n```\nETCD_DISCOVERY=https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n```\n--discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n现在我们通过这些相关的参数启动每一个etcd成员：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n```\n$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n```\n$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\n```\n一旦所有主机被注册后，每一个集群成员将会集群启动之前在自定义发现服务中注册自己。\netcd使用环境变量`ETCD_DISCOVERY_PROXY`通过HTTP代理连接发现服务。\n**错误和警告案例**\n发现服务错误：\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\netcd: error: the cluster doesn’t have a size configuration value in https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de/_config\nexit 1\n```\n警告\n这里有一个严重的警告表明发现服务URL将被这台主机忽略。\n```\n$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\netcdserver: discovery token ignored since a cluster has already been initialized. Valid log found at /var/lib/etcd\n```\n### DNS 发现服务\nDNS [SRV 记录](http://www.ietf.org/rfc/rfc2052.txt)可以用来作为发现机制。`--discovery-srv`参数可用于设置可以找到发现SRV记录的DNS域名。 设置`--discovery-srv example.com`会导致DNS SRV记录按照列出的顺序进行查找：\n\n* _etcd-server-ssl._tcp.example.com\n* _etcd-server._tcp.example.com\n\n如果找到`_etcd-server-ssl._tcp.example.com`，则etcd将尝试通过TLS进行引导过程。\n为了帮助客户端发现etcd集群，按照列出的顺序查找以下DNS SRV记录：\n\n*    _etcd-client._tcp.example.com\n*    _etcd-client-ssl._tcp.example.com\n\n如果找到了`_etcd-client-ssl._tcp.example.com`，则客户端将尝试通过SSL/TLS与etcd集群进行通信。\n如果etcd使用TLS，则发现SRV记录（例如example.com）必须与主机名一起包含在SSL证书DNS SAN中，否则集群将失败，并显示以下日志消息：\n```\n[...] rejected connection from \"10.0.1.11:53162\" (error \"remote error: tls: bad certificate\", ServerName \"example.com\")\n```\n如果etcd使用的是没有自定义证书颁发机构的TLS，则发现域（例如example.com）必须与SRV记录域（例如infra1.example.com）匹配。 这是为了缓解伪造SRV记录指向不同域的攻击。 该域将在PKI下拥有有效的证书，但由未知的第三方控制。\n`-discovery-srv-name`参数还为在发现期间查询的SRV名称配置了后缀。 使用此参数可以区分同一域下的多个etcd集群。 例如，如果设置了`Discovery-srv = example.com`和`-discovery-srv-name = foo`，则会进行以下DNS SRV查询：\n\n *   _etcd-server-ssl-foo._tcp.example.com\n *  _etcd-server-foo._tcp.example.com\n\n**创建DNS SRV记录**\n```\n$ dig +noall +answer SRV _etcd-server._tcp.example.com\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra0.example.com.\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra1.example.com.\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra2.example.com.\n```\n```\n$ dig +noall +answer SRV _etcd-client._tcp.example.com\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.\n```\n```\n$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com\ninfra0.example.com.  300  IN  A  10.0.1.10\ninfra1.example.com.  300  IN  A  10.0.1.11\ninfra2.example.com.  300  IN  A  10.0.1.12\n```\n**使用DNS引导etcd集群**\netcd群集成员可以公告域名或IP地址，引导过程将解析DNS A记录。 从3.2开始（3.1将显示警告），`--listen-peer-urls`和`--listen-client-urls`将拒绝网络接口绑定的域名。\n`--initial-advertise-peer-urls`中的解析地址必须与SRV目标中的解析地址之一匹配。 etcd成员读取解析的地址，以查找其是否属于SRV记录中定义的集群。\n```\n$ etcd --name infra0 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra0.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra0.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380\n```\n```\n$ etcd --name infra1 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra1.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra1.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380\n```\n```\n$ etcd --name infra2 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra2.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra2.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380\n```\n集群还可以使用IP地址而不是域名进行引导：\n```\n$ etcd --name infra0 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.10:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.10:2379 \\\n--listen-client-urls http://10.0.1.10:2379 \\\n--listen-peer-urls http://10.0.1.10:2380\n```\n```\n$ etcd --name infra1 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.11:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.11:2379 \\\n--listen-client-urls http://10.0.1.11:2379 \\\n--listen-peer-urls http://10.0.1.11:2380\n```\n```\n$ etcd --name infra2 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.12:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.12:2379 \\\n--listen-client-urls http://10.0.1.12:2379 \\\n--listen-peer-urls http://10.0.1.12:2380\n```\n自从v3.1.0（v3.2.9除外），因此在`etcd --discovery-srv = example.com`中配置了TLS时，服务器仅在提供的证书具有根域`example.com`作为`Subject Alternative`(SAN)字段中的条目时，对等方/客户端进行身份验证。请参阅[DNS SRV的注释](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/security.md#notes-for-dns-srv)。\n**网关**\netcd网关是一个简单的TCP代理，可将网络数据转发到etcd集群。 请阅读[网关指南](https://newonexd.github.io/2019/11/24/blog/etcd/etcd%E7%BD%91%E5%85%B3/)以获取更多信息。\n**代理**\n设置`--proxy`参数时，etcd以[代理模式](https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/)运行。 此代理模式仅支持etcd v2 API； 目前尚无计划支持v3 API。 相反，为了支持v3 API，etcd 3.0版本之后将提供具有增强功能的新代理。\n要使用v2 API代理设置etcd集群，请阅读[etcd 2.3版本中的集群文档](https://github.com/coreos/etcd/blob/release-2.3/Documentation/clustering.md)。","slug":"blog/etcd/多机集群","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyhy0028k0vq1pic7vbe","content":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/clustering.md\" target=\"_blank\" rel=\"noopener\">cluster on multiple machines</a></p>\n<h1 id=\"总览\"><a href=\"#总览\" class=\"headerlink\" title=\"总览\"></a>总览</h1><hr>\n<p>启动一个集群静态的要求是每一个集群中的成员需要知道其他成员的位置。在许多情况下，集群成员的IP可能无法提前知道。在这种情况下，etcd集群可以在发现服务的帮助下进行启动。<br>一旦etcd集群已经启动，添加或移除成员可以通过<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">运行时重新配置</a>。在运行时重新配置之前，为了更好地理解设计，我们建议读<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E8%AE%BE%E8%AE%A1/\">运行时重新配置设计文档</a>。<br>这篇引导etcd集群的启动将包括以下机制：</p>\n<ul>\n<li>静态</li>\n<li>etcd发现</li>\n<li>DNS发现</li>\n</ul>\n<p>每种引导机制都将用于创建具有以下详细信息的三台计算机etcd集群：</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Address</th>\n<th>Hostname</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>infra0</td>\n<td>10.0.1.10</td>\n<td>infra0.example.com</td>\n</tr>\n<tr>\n<td>infra1</td>\n<td>10.0.1.11</td>\n<td>infra1.example.com</td>\n</tr>\n<tr>\n<td>infra2</td>\n<td>10.0.1.12</td>\n<td>infra2.example.com</td>\n</tr>\n</tbody></table>\n<h2 id=\"静态\"><a href=\"#静态\" class=\"headerlink\" title=\"静态\"></a>静态</h2><p>集群的成员，在启动之前它们的地址和集群的大小，我们可以通过设置<code>initial-cluster</code>参数使用离线的启动配置。每一个机器将会通过以下的环境变量或命令行获得配置信息：</p>\n<pre><code>ETCD_INITIAL_CLUSTER=&quot;infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380&quot;\nETCD_INITIAL_CLUSTER_STATE=new</code></pre><pre><code>--initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n--initial-cluster-state new</code></pre><p>注意在<code>initial-cluster</code>中的URLs必须是已发布的对等的节点的URLs，即它们应该和对应的节点上的<code>initial-advertise-peer-urls</code>的值对应。<br>如果为了测试的目的通过相同的配置分解多集群(或者创建和删除单个集群)，值得注意的是每一个集群应该给予独一无二的<code>initial-cluster-token</code>,通过做这些工作，即使它们具有相同的配置,etcd也可以为集群成员生成独一无二的集群Id和成员ID。这样可以在可能会扰乱集群的跨集群中交互中保护etcd。<br>etcd监听在<code>listen-client-urls</code>接受客户端流量，etcd将<code>advertise-client-urls</code>中指定的URLs告诉其他成员，代理，客户端。请确保潜在的客户端可以获取<code>advertise-client-urls</code>。一个常见的错误当远程的客户端应该访问etcd时设置<code>advertise-client-urls</code>为localhost或者将其保留为默认值。<br>在每一台机器上，通过这些参数启动etcd：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new</code></pre><p>以<code>initial-cluster</code>开头的命令行参数将在etcd启动后被忽略。在初始化启动后可以自由删除环境变量或者命令行参数。如果配置信息在启动之后需要改变(例如在/从集群中添加或删除成员),看<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">运行时配置</a>引导。</p>\n<h3 id=\"TLS\"><a href=\"#TLS\" class=\"headerlink\" title=\"TLS\"></a>TLS</h3><p>etcd支持通过TLS协议进行加密通信。TLS通道可以在集群内部通信使用，也可以在节点和客户端流量通信时使用。这一部分列举了为节点和客户端TLS通信的集群设置。添加的etcd的TLS支持信息细节可以在<a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/TLS/\">安全引导</a>中发现。</p>\n<h4 id=\"自签名证书\"><a href=\"#自签名证书\" class=\"headerlink\" title=\"自签名证书\"></a>自签名证书</h4><p>一个集群使用自签名证书加密流量和连接权限。使用自签名证书启动一个集群，每一个集群成员都需要含有一个独一无二的由共享的集群CA证书(<code>ca.crt</code>)签名的秘钥对(<code>member.crt</code>,<code>member.key</code>)，用于节点连接和客户端连接。证书可以通过下面的etcd<a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/TLS/\">TLS设置</a>例子中生成。<br>对于每一台机器，etcd应该通过这些参数启动：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls https://10.0.1.10:2380 \\\n  --listen-peer-urls https://10.0.1.10:2380 \\\n  --listen-client-urls https://10.0.1.10:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra0-client.crt --key-file=/path/to/infra0-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra0-peer.crt --peer-key-file=/path/to/infra0-peer.key</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls https://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls https://10.0.1.11:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra1-client.crt --key-file=/path/to/infra1-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra1-peer.crt --peer-key-file=/path/to/infra1-peer.key</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls https://10.0.1.12:2380 \\\n  --listen-peer-urls https://10.0.1.12:2380 \\\n  --listen-client-urls https://10.0.1.12:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra2-client.crt --key-file=/path/to/infra2-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra2-peer.crt --peer-key-file=/path/to/infra2-peer.key</code></pre><h4 id=\"自动化证书\"><a href=\"#自动化证书\" class=\"headerlink\" title=\"自动化证书\"></a>自动化证书</h4><p>如果集群需要加密通信但是不需要连接时权限认证,etcd可以配置为自动生成秘钥.在初始化阶段,etcd成员基于他们的Ip地址和主机生成自己的秘钥.<br>在每一台主机上,etcd需要根据这些参数进行启动:</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls https://10.0.1.10:2380 \\\n  --listen-peer-urls https://10.0.1.10:2380 \\\n  --listen-client-urls https://10.0.1.10:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls https://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls https://10.0.1.11:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls https://10.0.1.12:2380 \\\n  --listen-peer-urls https://10.0.1.12:2380 \\\n  --listen-client-urls https://10.0.1.12:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls</code></pre><h4 id=\"错误案例\"><a href=\"#错误案例\" class=\"headerlink\" title=\"错误案例\"></a>错误案例</h4><p>在以下的例子中，新的主机没有包含在枚举的节点列表中，如果这是一个新的集群，节点需要被添加到初始化集群成员的列表中。</p>\n<pre><code>$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380 \\\n  --initial-cluster-state new\netcd: infra1 not listed in the initial cluster config\nexit 1</code></pre><p>在以下的例子中，我们试图映射一个节点(infra0)到一个不同的地址(127.0.0.1:2380)，而它在集群列表中的地址为(10.0.1.10:2380).如果这个节点监听多个端口，所有地址都必须要反射到<code>initial-cluster</code>参数配置中。</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://127.0.0.1:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state=new\netcd: error setting up initial cluster: infra0 has different advertised URLs in the cluster and advertised peer URLs list\nexit 1</code></pre><p>如果一个节点被配置成不同集群的参数并试图加入这个集群，etcd将会报出集群ID不匹配并退出.</p>\n<pre><code>$ etcd --name infra3 --initial-advertise-peer-urls http://10.0.1.13:2380 \\\n  --listen-peer-urls http://10.0.1.13:2380 \\\n  --listen-client-urls http://10.0.1.13:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.13:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra3=http://10.0.1.13:2380 \\\n  --initial-cluster-state=new\netcd: conflicting cluster ID to the target cluster (c6ab534d07e8fcc4 != bc25ea2a74fb18b0). Exiting.\nexit 1</code></pre><h3 id=\"发现服务\"><a href=\"#发现服务\" class=\"headerlink\" title=\"发现服务\"></a>发现服务</h3><p>在许多案例中，集群节点不能提前知道Ip地址。这在云服务提供商或者是使用DHCP的网络中很常见。在这种情况下，使用一个存在的etcd集群来启动一个新的节点而不是进行静态的配置，这个过程称为”节点发现”.</p>\n<p>有两种方法可以用来发现节点:</p>\n<ul>\n<li>etcd发现服务</li>\n<li>DNS SRV 记录</li>\n</ul>\n<h4 id=\"etcd-发现服务\"><a href=\"#etcd-发现服务\" class=\"headerlink\" title=\"etcd 发现服务\"></a>etcd 发现服务</h4><p>为了更好理解发现服务协议的设计，我们建议阅读发现服务协议<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-internal/discovery_protocol.md\" target=\"_blank\" rel=\"noopener\">文档</a>.<br><strong>发现服务URL的生命周期</strong><br>一个发现URL标识一个独有的etcd集群而不是使用已有的发现URL。每一个etcd实例分享一个新的发现URL去启动新的集群。<br>此外，发现URL应该只在初始化启动集群的时候使用，如果需要改变已经启动的集群中的成员关系，看<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">运行时重新配置</a>引导.<br><strong>自定义etcd发现服务</strong><br>发现服务用于启动一个存在的集群，如果使用一个私有的etcd集群，像这样创建URL:</p>\n<pre><code>$ curl -X PUT https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3</code></pre><p>通过设置URL中Key的大小，创建发现URL的集群预期大小为3.<br>在这种情况下URL将会这样使用:</p>\n<pre><code>https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><p>当etcd成员启动时将使用</p>\n<pre><code>https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><p>文件夹进行注册。<br><strong>每一个成员必须含有一个不同的命名参数。<code>Hostname</code>或者<code>machine-id</code>将是一个好的选择。发现服务的失败通常由于重复的名字。</strong><br>现在我们通过这些参数启动etcd的每一个成员：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><p>一旦所有主机被注册后，每一个集群成员将会集群启动之前在自定义发现服务中注册自己。<br><strong>公共的etcd发现服务</strong><br>如果没有可以获得的集群，使用托管在<code>discovery.etcd.io</code>的公共发现服务。通过”new”主机,创建一个私有的发现URL,使用以下命令:</p>\n<pre><code>$ curl https://discovery.etcd.io/new?size=3\nhttps://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><p>将会创建一个初始化成员数量为3的集群,如果没有设置大小，将默认为3.</p>\n<pre><code>ETCD_DISCOVERY=https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><pre><code>--discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><p>现在我们通过这些相关的参数启动每一个etcd成员：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><p>一旦所有主机被注册后，每一个集群成员将会集群启动之前在自定义发现服务中注册自己。<br>etcd使用环境变量<code>ETCD_DISCOVERY_PROXY</code>通过HTTP代理连接发现服务。<br><strong>错误和警告案例</strong><br>发现服务错误：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\netcd: error: the cluster doesn’t have a size configuration value in https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de/_config\nexit 1</code></pre><p>警告<br>这里有一个严重的警告表明发现服务URL将被这台主机忽略。</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\netcdserver: discovery token ignored since a cluster has already been initialized. Valid log found at /var/lib/etcd</code></pre><h3 id=\"DNS-发现服务\"><a href=\"#DNS-发现服务\" class=\"headerlink\" title=\"DNS 发现服务\"></a>DNS 发现服务</h3><p>DNS <a href=\"http://www.ietf.org/rfc/rfc2052.txt\" target=\"_blank\" rel=\"noopener\">SRV 记录</a>可以用来作为发现机制。<code>--discovery-srv</code>参数可用于设置可以找到发现SRV记录的DNS域名。 设置<code>--discovery-srv example.com</code>会导致DNS SRV记录按照列出的顺序进行查找：</p>\n<ul>\n<li>_etcd-server-ssl._tcp.example.com</li>\n<li>_etcd-server._tcp.example.com</li>\n</ul>\n<p>如果找到<code>_etcd-server-ssl._tcp.example.com</code>，则etcd将尝试通过TLS进行引导过程。<br>为了帮助客户端发现etcd集群，按照列出的顺序查找以下DNS SRV记录：</p>\n<ul>\n<li>_etcd-client._tcp.example.com</li>\n<li>_etcd-client-ssl._tcp.example.com</li>\n</ul>\n<p>如果找到了<code>_etcd-client-ssl._tcp.example.com</code>，则客户端将尝试通过SSL/TLS与etcd集群进行通信。<br>如果etcd使用TLS，则发现SRV记录（例如example.com）必须与主机名一起包含在SSL证书DNS SAN中，否则集群将失败，并显示以下日志消息：</p>\n<pre><code>[...] rejected connection from &quot;10.0.1.11:53162&quot; (error &quot;remote error: tls: bad certificate&quot;, ServerName &quot;example.com&quot;)</code></pre><p>如果etcd使用的是没有自定义证书颁发机构的TLS，则发现域（例如example.com）必须与SRV记录域（例如infra1.example.com）匹配。 这是为了缓解伪造SRV记录指向不同域的攻击。 该域将在PKI下拥有有效的证书，但由未知的第三方控制。<br><code>-discovery-srv-name</code>参数还为在发现期间查询的SRV名称配置了后缀。 使用此参数可以区分同一域下的多个etcd集群。 例如，如果设置了<code>Discovery-srv = example.com</code>和<code>-discovery-srv-name = foo</code>，则会进行以下DNS SRV查询：</p>\n<ul>\n<li>_etcd-server-ssl-foo._tcp.example.com</li>\n<li>_etcd-server-foo._tcp.example.com</li>\n</ul>\n<p><strong>创建DNS SRV记录</strong></p>\n<pre><code>$ dig +noall +answer SRV _etcd-server._tcp.example.com\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra0.example.com.\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra1.example.com.\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra2.example.com.</code></pre><pre><code>$ dig +noall +answer SRV _etcd-client._tcp.example.com\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.</code></pre><pre><code>$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com\ninfra0.example.com.  300  IN  A  10.0.1.10\ninfra1.example.com.  300  IN  A  10.0.1.11\ninfra2.example.com.  300  IN  A  10.0.1.12</code></pre><p><strong>使用DNS引导etcd集群</strong><br>etcd群集成员可以公告域名或IP地址，引导过程将解析DNS A记录。 从3.2开始（3.1将显示警告），<code>--listen-peer-urls</code>和<code>--listen-client-urls</code>将拒绝网络接口绑定的域名。<br><code>--initial-advertise-peer-urls</code>中的解析地址必须与SRV目标中的解析地址之一匹配。 etcd成员读取解析的地址，以查找其是否属于SRV记录中定义的集群。</p>\n<pre><code>$ etcd --name infra0 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra0.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra0.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380</code></pre><pre><code>$ etcd --name infra1 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra1.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra1.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380</code></pre><pre><code>$ etcd --name infra2 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra2.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra2.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380</code></pre><p>集群还可以使用IP地址而不是域名进行引导：</p>\n<pre><code>$ etcd --name infra0 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.10:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.10:2379 \\\n--listen-client-urls http://10.0.1.10:2379 \\\n--listen-peer-urls http://10.0.1.10:2380</code></pre><pre><code>$ etcd --name infra1 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.11:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.11:2379 \\\n--listen-client-urls http://10.0.1.11:2379 \\\n--listen-peer-urls http://10.0.1.11:2380</code></pre><pre><code>$ etcd --name infra2 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.12:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.12:2379 \\\n--listen-client-urls http://10.0.1.12:2379 \\\n--listen-peer-urls http://10.0.1.12:2380</code></pre><p>自从v3.1.0（v3.2.9除外），因此在<code>etcd --discovery-srv = example.com</code>中配置了TLS时，服务器仅在提供的证书具有根域<code>example.com</code>作为<code>Subject Alternative</code>(SAN)字段中的条目时，对等方/客户端进行身份验证。请参阅<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/security.md#notes-for-dns-srv\" target=\"_blank\" rel=\"noopener\">DNS SRV的注释</a>。<br><strong>网关</strong><br>etcd网关是一个简单的TCP代理，可将网络数据转发到etcd集群。 请阅读<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/etcd%E7%BD%91%E5%85%B3/\">网关指南</a>以获取更多信息。<br><strong>代理</strong><br>设置<code>--proxy</code>参数时，etcd以<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/\">代理模式</a>运行。 此代理模式仅支持etcd v2 API； 目前尚无计划支持v3 API。 相反，为了支持v3 API，etcd 3.0版本之后将提供具有增强功能的新代理。<br>要使用v2 API代理设置etcd集群，请阅读<a href=\"https://github.com/coreos/etcd/blob/release-2.3/Documentation/clustering.md\" target=\"_blank\" rel=\"noopener\">etcd 2.3版本中的集群文档</a>。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/clustering.md\" target=\"_blank\" rel=\"noopener\">cluster on multiple machines</a></p>\n<h1 id=\"总览\"><a href=\"#总览\" class=\"headerlink\" title=\"总览\"></a>总览</h1><hr>\n<p>启动一个集群静态的要求是每一个集群中的成员需要知道其他成员的位置。在许多情况下，集群成员的IP可能无法提前知道。在这种情况下，etcd集群可以在发现服务的帮助下进行启动。<br>一旦etcd集群已经启动，添加或移除成员可以通过<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">运行时重新配置</a>。在运行时重新配置之前，为了更好地理解设计，我们建议读<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E8%AE%BE%E8%AE%A1/\">运行时重新配置设计文档</a>。<br>这篇引导etcd集群的启动将包括以下机制：</p>\n<ul>\n<li>静态</li>\n<li>etcd发现</li>\n<li>DNS发现</li>\n</ul>\n<p>每种引导机制都将用于创建具有以下详细信息的三台计算机etcd集群：</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Address</th>\n<th>Hostname</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>infra0</td>\n<td>10.0.1.10</td>\n<td>infra0.example.com</td>\n</tr>\n<tr>\n<td>infra1</td>\n<td>10.0.1.11</td>\n<td>infra1.example.com</td>\n</tr>\n<tr>\n<td>infra2</td>\n<td>10.0.1.12</td>\n<td>infra2.example.com</td>\n</tr>\n</tbody></table>\n<h2 id=\"静态\"><a href=\"#静态\" class=\"headerlink\" title=\"静态\"></a>静态</h2><p>集群的成员，在启动之前它们的地址和集群的大小，我们可以通过设置<code>initial-cluster</code>参数使用离线的启动配置。每一个机器将会通过以下的环境变量或命令行获得配置信息：</p>\n<pre><code>ETCD_INITIAL_CLUSTER=&quot;infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380&quot;\nETCD_INITIAL_CLUSTER_STATE=new</code></pre><pre><code>--initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n--initial-cluster-state new</code></pre><p>注意在<code>initial-cluster</code>中的URLs必须是已发布的对等的节点的URLs，即它们应该和对应的节点上的<code>initial-advertise-peer-urls</code>的值对应。<br>如果为了测试的目的通过相同的配置分解多集群(或者创建和删除单个集群)，值得注意的是每一个集群应该给予独一无二的<code>initial-cluster-token</code>,通过做这些工作，即使它们具有相同的配置,etcd也可以为集群成员生成独一无二的集群Id和成员ID。这样可以在可能会扰乱集群的跨集群中交互中保护etcd。<br>etcd监听在<code>listen-client-urls</code>接受客户端流量，etcd将<code>advertise-client-urls</code>中指定的URLs告诉其他成员，代理，客户端。请确保潜在的客户端可以获取<code>advertise-client-urls</code>。一个常见的错误当远程的客户端应该访问etcd时设置<code>advertise-client-urls</code>为localhost或者将其保留为默认值。<br>在每一台机器上，通过这些参数启动etcd：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state new</code></pre><p>以<code>initial-cluster</code>开头的命令行参数将在etcd启动后被忽略。在初始化启动后可以自由删除环境变量或者命令行参数。如果配置信息在启动之后需要改变(例如在/从集群中添加或删除成员),看<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">运行时配置</a>引导。</p>\n<h3 id=\"TLS\"><a href=\"#TLS\" class=\"headerlink\" title=\"TLS\"></a>TLS</h3><p>etcd支持通过TLS协议进行加密通信。TLS通道可以在集群内部通信使用，也可以在节点和客户端流量通信时使用。这一部分列举了为节点和客户端TLS通信的集群设置。添加的etcd的TLS支持信息细节可以在<a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/TLS/\">安全引导</a>中发现。</p>\n<h4 id=\"自签名证书\"><a href=\"#自签名证书\" class=\"headerlink\" title=\"自签名证书\"></a>自签名证书</h4><p>一个集群使用自签名证书加密流量和连接权限。使用自签名证书启动一个集群，每一个集群成员都需要含有一个独一无二的由共享的集群CA证书(<code>ca.crt</code>)签名的秘钥对(<code>member.crt</code>,<code>member.key</code>)，用于节点连接和客户端连接。证书可以通过下面的etcd<a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/TLS/\">TLS设置</a>例子中生成。<br>对于每一台机器，etcd应该通过这些参数启动：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls https://10.0.1.10:2380 \\\n  --listen-peer-urls https://10.0.1.10:2380 \\\n  --listen-client-urls https://10.0.1.10:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra0-client.crt --key-file=/path/to/infra0-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra0-peer.crt --peer-key-file=/path/to/infra0-peer.key</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls https://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls https://10.0.1.11:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra1-client.crt --key-file=/path/to/infra1-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra1-peer.crt --peer-key-file=/path/to/infra1-peer.key</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls https://10.0.1.12:2380 \\\n  --listen-peer-urls https://10.0.1.12:2380 \\\n  --listen-client-urls https://10.0.1.12:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --client-cert-auth --trusted-ca-file=/path/to/ca-client.crt \\\n  --cert-file=/path/to/infra2-client.crt --key-file=/path/to/infra2-client.key \\\n  --peer-client-cert-auth --peer-trusted-ca-file=ca-peer.crt \\\n  --peer-cert-file=/path/to/infra2-peer.crt --peer-key-file=/path/to/infra2-peer.key</code></pre><h4 id=\"自动化证书\"><a href=\"#自动化证书\" class=\"headerlink\" title=\"自动化证书\"></a>自动化证书</h4><p>如果集群需要加密通信但是不需要连接时权限认证,etcd可以配置为自动生成秘钥.在初始化阶段,etcd成员基于他们的Ip地址和主机生成自己的秘钥.<br>在每一台主机上,etcd需要根据这些参数进行启动:</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls https://10.0.1.10:2380 \\\n  --listen-peer-urls https://10.0.1.10:2380 \\\n  --listen-client-urls https://10.0.1.10:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.10:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls https://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls https://10.0.1.11:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.11:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls https://10.0.1.12:2380 \\\n  --listen-peer-urls https://10.0.1.12:2380 \\\n  --listen-client-urls https://10.0.1.12:2379,https://127.0.0.1:2379 \\\n  --advertise-client-urls https://10.0.1.12:2379 \\\n  --initial-cluster-token etcd-cluster-1 \\\n  --initial-cluster infra0=https://10.0.1.10:2380,infra1=https://10.0.1.11:2380,infra2=https://10.0.1.12:2380 \\\n  --initial-cluster-state new \\\n  --auto-tls \\\n  --peer-auto-tls</code></pre><h4 id=\"错误案例\"><a href=\"#错误案例\" class=\"headerlink\" title=\"错误案例\"></a>错误案例</h4><p>在以下的例子中，新的主机没有包含在枚举的节点列表中，如果这是一个新的集群，节点需要被添加到初始化集群成员的列表中。</p>\n<pre><code>$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls https://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380 \\\n  --initial-cluster-state new\netcd: infra1 not listed in the initial cluster config\nexit 1</code></pre><p>在以下的例子中，我们试图映射一个节点(infra0)到一个不同的地址(127.0.0.1:2380)，而它在集群列表中的地址为(10.0.1.10:2380).如果这个节点监听多个端口，所有地址都必须要反射到<code>initial-cluster</code>参数配置中。</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://127.0.0.1:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state=new\netcd: error setting up initial cluster: infra0 has different advertised URLs in the cluster and advertised peer URLs list\nexit 1</code></pre><p>如果一个节点被配置成不同集群的参数并试图加入这个集群，etcd将会报出集群ID不匹配并退出.</p>\n<pre><code>$ etcd --name infra3 --initial-advertise-peer-urls http://10.0.1.13:2380 \\\n  --listen-peer-urls http://10.0.1.13:2380 \\\n  --listen-client-urls http://10.0.1.13:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.13:2379 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra3=http://10.0.1.13:2380 \\\n  --initial-cluster-state=new\netcd: conflicting cluster ID to the target cluster (c6ab534d07e8fcc4 != bc25ea2a74fb18b0). Exiting.\nexit 1</code></pre><h3 id=\"发现服务\"><a href=\"#发现服务\" class=\"headerlink\" title=\"发现服务\"></a>发现服务</h3><p>在许多案例中，集群节点不能提前知道Ip地址。这在云服务提供商或者是使用DHCP的网络中很常见。在这种情况下，使用一个存在的etcd集群来启动一个新的节点而不是进行静态的配置，这个过程称为”节点发现”.</p>\n<p>有两种方法可以用来发现节点:</p>\n<ul>\n<li>etcd发现服务</li>\n<li>DNS SRV 记录</li>\n</ul>\n<h4 id=\"etcd-发现服务\"><a href=\"#etcd-发现服务\" class=\"headerlink\" title=\"etcd 发现服务\"></a>etcd 发现服务</h4><p>为了更好理解发现服务协议的设计，我们建议阅读发现服务协议<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-internal/discovery_protocol.md\" target=\"_blank\" rel=\"noopener\">文档</a>.<br><strong>发现服务URL的生命周期</strong><br>一个发现URL标识一个独有的etcd集群而不是使用已有的发现URL。每一个etcd实例分享一个新的发现URL去启动新的集群。<br>此外，发现URL应该只在初始化启动集群的时候使用，如果需要改变已经启动的集群中的成员关系，看<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">运行时重新配置</a>引导.<br><strong>自定义etcd发现服务</strong><br>发现服务用于启动一个存在的集群，如果使用一个私有的etcd集群，像这样创建URL:</p>\n<pre><code>$ curl -X PUT https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3</code></pre><p>通过设置URL中Key的大小，创建发现URL的集群预期大小为3.<br>在这种情况下URL将会这样使用:</p>\n<pre><code>https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><p>当etcd成员启动时将使用</p>\n<pre><code>https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><p>文件夹进行注册。<br><strong>每一个成员必须含有一个不同的命名参数。<code>Hostname</code>或者<code>machine-id</code>将是一个好的选择。发现服务的失败通常由于重复的名字。</strong><br>现在我们通过这些参数启动etcd的每一个成员：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83</code></pre><p>一旦所有主机被注册后，每一个集群成员将会集群启动之前在自定义发现服务中注册自己。<br><strong>公共的etcd发现服务</strong><br>如果没有可以获得的集群，使用托管在<code>discovery.etcd.io</code>的公共发现服务。通过”new”主机,创建一个私有的发现URL,使用以下命令:</p>\n<pre><code>$ curl https://discovery.etcd.io/new?size=3\nhttps://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><p>将会创建一个初始化成员数量为3的集群,如果没有设置大小，将默认为3.</p>\n<pre><code>ETCD_DISCOVERY=https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><pre><code>--discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><p>现在我们通过这些相关的参数启动每一个etcd成员：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><pre><code>$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n  --listen-peer-urls http://10.0.1.11:2380 \\\n  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.11:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><pre><code>$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n  --listen-peer-urls http://10.0.1.12:2380 \\\n  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.12:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de</code></pre><p>一旦所有主机被注册后，每一个集群成员将会集群启动之前在自定义发现服务中注册自己。<br>etcd使用环境变量<code>ETCD_DISCOVERY_PROXY</code>通过HTTP代理连接发现服务。<br><strong>错误和警告案例</strong><br>发现服务错误：</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\netcd: error: the cluster doesn’t have a size configuration value in https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de/_config\nexit 1</code></pre><p>警告<br>这里有一个严重的警告表明发现服务URL将被这台主机忽略。</p>\n<pre><code>$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n  --listen-peer-urls http://10.0.1.10:2380 \\\n  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls http://10.0.1.10:2379 \\\n  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de\netcdserver: discovery token ignored since a cluster has already been initialized. Valid log found at /var/lib/etcd</code></pre><h3 id=\"DNS-发现服务\"><a href=\"#DNS-发现服务\" class=\"headerlink\" title=\"DNS 发现服务\"></a>DNS 发现服务</h3><p>DNS <a href=\"http://www.ietf.org/rfc/rfc2052.txt\" target=\"_blank\" rel=\"noopener\">SRV 记录</a>可以用来作为发现机制。<code>--discovery-srv</code>参数可用于设置可以找到发现SRV记录的DNS域名。 设置<code>--discovery-srv example.com</code>会导致DNS SRV记录按照列出的顺序进行查找：</p>\n<ul>\n<li>_etcd-server-ssl._tcp.example.com</li>\n<li>_etcd-server._tcp.example.com</li>\n</ul>\n<p>如果找到<code>_etcd-server-ssl._tcp.example.com</code>，则etcd将尝试通过TLS进行引导过程。<br>为了帮助客户端发现etcd集群，按照列出的顺序查找以下DNS SRV记录：</p>\n<ul>\n<li>_etcd-client._tcp.example.com</li>\n<li>_etcd-client-ssl._tcp.example.com</li>\n</ul>\n<p>如果找到了<code>_etcd-client-ssl._tcp.example.com</code>，则客户端将尝试通过SSL/TLS与etcd集群进行通信。<br>如果etcd使用TLS，则发现SRV记录（例如example.com）必须与主机名一起包含在SSL证书DNS SAN中，否则集群将失败，并显示以下日志消息：</p>\n<pre><code>[...] rejected connection from &quot;10.0.1.11:53162&quot; (error &quot;remote error: tls: bad certificate&quot;, ServerName &quot;example.com&quot;)</code></pre><p>如果etcd使用的是没有自定义证书颁发机构的TLS，则发现域（例如example.com）必须与SRV记录域（例如infra1.example.com）匹配。 这是为了缓解伪造SRV记录指向不同域的攻击。 该域将在PKI下拥有有效的证书，但由未知的第三方控制。<br><code>-discovery-srv-name</code>参数还为在发现期间查询的SRV名称配置了后缀。 使用此参数可以区分同一域下的多个etcd集群。 例如，如果设置了<code>Discovery-srv = example.com</code>和<code>-discovery-srv-name = foo</code>，则会进行以下DNS SRV查询：</p>\n<ul>\n<li>_etcd-server-ssl-foo._tcp.example.com</li>\n<li>_etcd-server-foo._tcp.example.com</li>\n</ul>\n<p><strong>创建DNS SRV记录</strong></p>\n<pre><code>$ dig +noall +answer SRV _etcd-server._tcp.example.com\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra0.example.com.\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra1.example.com.\n_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra2.example.com.</code></pre><pre><code>$ dig +noall +answer SRV _etcd-client._tcp.example.com\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.\n_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.</code></pre><pre><code>$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com\ninfra0.example.com.  300  IN  A  10.0.1.10\ninfra1.example.com.  300  IN  A  10.0.1.11\ninfra2.example.com.  300  IN  A  10.0.1.12</code></pre><p><strong>使用DNS引导etcd集群</strong><br>etcd群集成员可以公告域名或IP地址，引导过程将解析DNS A记录。 从3.2开始（3.1将显示警告），<code>--listen-peer-urls</code>和<code>--listen-client-urls</code>将拒绝网络接口绑定的域名。<br><code>--initial-advertise-peer-urls</code>中的解析地址必须与SRV目标中的解析地址之一匹配。 etcd成员读取解析的地址，以查找其是否属于SRV记录中定义的集群。</p>\n<pre><code>$ etcd --name infra0 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra0.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra0.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380</code></pre><pre><code>$ etcd --name infra1 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra1.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra1.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380</code></pre><pre><code>$ etcd --name infra2 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://infra2.example.com:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://infra2.example.com:2379 \\\n--listen-client-urls http://0.0.0.0:2379 \\\n--listen-peer-urls http://0.0.0.0:2380</code></pre><p>集群还可以使用IP地址而不是域名进行引导：</p>\n<pre><code>$ etcd --name infra0 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.10:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.10:2379 \\\n--listen-client-urls http://10.0.1.10:2379 \\\n--listen-peer-urls http://10.0.1.10:2380</code></pre><pre><code>$ etcd --name infra1 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.11:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.11:2379 \\\n--listen-client-urls http://10.0.1.11:2379 \\\n--listen-peer-urls http://10.0.1.11:2380</code></pre><pre><code>$ etcd --name infra2 \\\n--discovery-srv example.com \\\n--initial-advertise-peer-urls http://10.0.1.12:2380 \\\n--initial-cluster-token etcd-cluster-1 \\\n--initial-cluster-state new \\\n--advertise-client-urls http://10.0.1.12:2379 \\\n--listen-client-urls http://10.0.1.12:2379 \\\n--listen-peer-urls http://10.0.1.12:2380</code></pre><p>自从v3.1.0（v3.2.9除外），因此在<code>etcd --discovery-srv = example.com</code>中配置了TLS时，服务器仅在提供的证书具有根域<code>example.com</code>作为<code>Subject Alternative</code>(SAN)字段中的条目时，对等方/客户端进行身份验证。请参阅<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/security.md#notes-for-dns-srv\" target=\"_blank\" rel=\"noopener\">DNS SRV的注释</a>。<br><strong>网关</strong><br>etcd网关是一个简单的TCP代理，可将网络数据转发到etcd集群。 请阅读<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/etcd%E7%BD%91%E5%85%B3/\">网关指南</a>以获取更多信息。<br><strong>代理</strong><br>设置<code>--proxy</code>参数时，etcd以<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/\">代理模式</a>运行。 此代理模式仅支持etcd v2 API； 目前尚无计划支持v3 API。 相反，为了支持v3 API，etcd 3.0版本之后将提供具有增强功能的新代理。<br>要使用v2 API代理设置etcd集群，请阅读<a href=\"https://github.com/coreos/etcd/blob/release-2.3/Documentation/clustering.md\" target=\"_blank\" rel=\"noopener\">etcd 2.3版本中的集群文档</a>。</p>\n"},{"title":"实验特性和APIs","date":"2019-11-24T05:11:15.000Z","_content":"原文地址:[Experimental features and APIs](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/experimental_apis.md)\n大多数情况下，etcd项目是稳定的，但我们仍在快速发展！ 我们相信快速发布理念。 我们希望获得有关仍在开发和稳定中的功能的早期反馈。 因此，存在并且将会有更多的实验性功能和API。 我们计划根据社区的早期反馈来改进这些功能，如果兴趣不足，请在接下来的几个版本中放弃这些功能。 请不要在生产环境中依赖任何实验性功能或API。\n## **当前实验API/特性是：**\n* * *\n\n* [键值对排序](https://godoc.org/github.com/etcd-io/etcd/clientv3/ordering)包装器：\n当etcd客户端切换端点时，如果新端点落后于集群的其余部分，则对可序列化读取的响应可能推迟。排序包装器从响应标头缓存当前集群修订版。 如果响应修订版本小于缓存修订版本，则客户端选择另一个端点并重新发出读取。在grpcproxy中启动`--experimental-serializable-ordering`.\n","source":"_posts/blog/etcd/实验特性和APIs.md","raw":"---\ntitle: 实验特性和APIs\ndate: 2019-11-24 13:11:15\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址:[Experimental features and APIs](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/experimental_apis.md)\n大多数情况下，etcd项目是稳定的，但我们仍在快速发展！ 我们相信快速发布理念。 我们希望获得有关仍在开发和稳定中的功能的早期反馈。 因此，存在并且将会有更多的实验性功能和API。 我们计划根据社区的早期反馈来改进这些功能，如果兴趣不足，请在接下来的几个版本中放弃这些功能。 请不要在生产环境中依赖任何实验性功能或API。\n## **当前实验API/特性是：**\n* * *\n\n* [键值对排序](https://godoc.org/github.com/etcd-io/etcd/clientv3/ordering)包装器：\n当etcd客户端切换端点时，如果新端点落后于集群的其余部分，则对可序列化读取的响应可能推迟。排序包装器从响应标头缓存当前集群修订版。 如果响应修订版本小于缓存修订版本，则客户端选择另一个端点并重新发出读取。在grpcproxy中启动`--experimental-serializable-ordering`.\n","slug":"blog/etcd/实验特性和APIs","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyhz002ck0vq4kwe83ny","content":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/experimental_apis.md\" target=\"_blank\" rel=\"noopener\">Experimental features and APIs</a><br>大多数情况下，etcd项目是稳定的，但我们仍在快速发展！ 我们相信快速发布理念。 我们希望获得有关仍在开发和稳定中的功能的早期反馈。 因此，存在并且将会有更多的实验性功能和API。 我们计划根据社区的早期反馈来改进这些功能，如果兴趣不足，请在接下来的几个版本中放弃这些功能。 请不要在生产环境中依赖任何实验性功能或API。</p>\n<h2 id=\"当前实验API-特性是：\"><a href=\"#当前实验API-特性是：\" class=\"headerlink\" title=\"当前实验API/特性是：\"></a><strong>当前实验API/特性是：</strong></h2><hr>\n<ul>\n<li><a href=\"https://godoc.org/github.com/etcd-io/etcd/clientv3/ordering\" target=\"_blank\" rel=\"noopener\">键值对排序</a>包装器：<br>当etcd客户端切换端点时，如果新端点落后于集群的其余部分，则对可序列化读取的响应可能推迟。排序包装器从响应标头缓存当前集群修订版。 如果响应修订版本小于缓存修订版本，则客户端选择另一个端点并重新发出读取。在grpcproxy中启动<code>--experimental-serializable-ordering</code>.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/experimental_apis.md\" target=\"_blank\" rel=\"noopener\">Experimental features and APIs</a><br>大多数情况下，etcd项目是稳定的，但我们仍在快速发展！ 我们相信快速发布理念。 我们希望获得有关仍在开发和稳定中的功能的早期反馈。 因此，存在并且将会有更多的实验性功能和API。 我们计划根据社区的早期反馈来改进这些功能，如果兴趣不足，请在接下来的几个版本中放弃这些功能。 请不要在生产环境中依赖任何实验性功能或API。</p>\n<h2 id=\"当前实验API-特性是：\"><a href=\"#当前实验API-特性是：\" class=\"headerlink\" title=\"当前实验API/特性是：\"></a><strong>当前实验API/特性是：</strong></h2><hr>\n<ul>\n<li><a href=\"https://godoc.org/github.com/etcd-io/etcd/clientv3/ordering\" target=\"_blank\" rel=\"noopener\">键值对排序</a>包装器：<br>当etcd客户端切换端点时，如果新端点落后于集群的其余部分，则对可序列化读取的响应可能推迟。排序包装器从响应标头缓存当前集群修订版。 如果响应修订版本小于缓存修订版本，则客户端选择另一个端点并重新发出读取。在grpcproxy中启动<code>--experimental-serializable-ordering</code>.</li>\n</ul>\n"},{"title":"客户端v3","date":"2019-11-23T04:32:09.000Z","_content":"原文地址：[etcd/clientv3](https://github.com/etcd-io/etcd/blob/master/clientv3/README.md)\n`etcd/clientv3`是v3版本的Go etcd官方客户端\n## 安装\n\n* * *\n```\ngo get go.etcd.io/etcd/clientv3\n```\n## 开始\n* * *\n创建客户端使用`clientv3.New`:\n```\ncli, err := clientv3.New(clientv3.Config{\n\tEndpoints:   []string{\"localhost:2379\", \"localhost:22379\", \"localhost:32379\"},\n\tDialTimeout: 5 * time.Second,\n})\nif err != nil {\n\t// handle error!\n}\ndefer cli.Close()\n```\netcd v3使用`gRPC`进行远程程序调用，并且`clientv3`使用`grpc-go`连接etcd。确保在使用完客户端后关闭它，如果客户端没有关闭，连接将会有泄漏的`goroutines`。指定超时时间，通过`context.WithTimeout`使用APIs:\n```\nctx, cancel := context.WithTimeout(context.Background(), timeout)\nresp, err := cli.Put(ctx, \"sample_key\", \"sample_value\")\ncancel()\nif err != nil {\n    // handle error!\n}\n// use the response\n```\n为了完全兼容，建议使用etcd's中的vendored包进行构建，使用工具像`golang/dep`,在vendor目录内。\n## 错误处理\netcd客户端返回两种类型的错误：\n\n1. context error :canceled or deadline exceeded.\n2. gRpc error : 看api/v3rpc/rpctypes.\n\n这里有例子处理客户端错误：\n```\nresp, err := cli.Put(ctx, \"\", \"\")\nif err != nil {\n\tswitch err {\n\tcase context.Canceled:\n\t\tlog.Fatalf(\"ctx is canceled by another routine: %v\", err)\n\tcase context.DeadlineExceeded:\n\t\tlog.Fatalf(\"ctx is attached with a deadline is exceeded: %v\", err)\n\tcase rpctypes.ErrEmptyKey:\n\t\tlog.Fatalf(\"client-side error: %v\", err)\n\tdefault:\n\t\tlog.Fatalf(\"bad cluster endpoints, which are not etcd servers: %v\", err)\n\t}\n}\n```\n## 监控\netcd客户端可以通过**go-grpc-prometheus**,选择RPC监控指标,看**例子**。\n## 命名空间\n**namespace**包提供`clientv3`接口封装透明隔离客户端请求到用户定义的前缀。\n## 请求大小限制\n客户端请求大小限制通过`clientv3.Config.MaxCallSendMsgSize`和`MaxCallRecvMsgSize`进行配置。如果没有给予值，客户端请求发送限制包括gRPC负载默认2MB。接收限制默认为`math.MaxInt32`。\n## 例子\n更多代码例子可以从**GoDoc**发现。","source":"_posts/blog/etcd/客户端v3.md","raw":"---\ntitle: 客户端v3\ndate: 2019-11-23 12:32:09\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址：[etcd/clientv3](https://github.com/etcd-io/etcd/blob/master/clientv3/README.md)\n`etcd/clientv3`是v3版本的Go etcd官方客户端\n## 安装\n\n* * *\n```\ngo get go.etcd.io/etcd/clientv3\n```\n## 开始\n* * *\n创建客户端使用`clientv3.New`:\n```\ncli, err := clientv3.New(clientv3.Config{\n\tEndpoints:   []string{\"localhost:2379\", \"localhost:22379\", \"localhost:32379\"},\n\tDialTimeout: 5 * time.Second,\n})\nif err != nil {\n\t// handle error!\n}\ndefer cli.Close()\n```\netcd v3使用`gRPC`进行远程程序调用，并且`clientv3`使用`grpc-go`连接etcd。确保在使用完客户端后关闭它，如果客户端没有关闭，连接将会有泄漏的`goroutines`。指定超时时间，通过`context.WithTimeout`使用APIs:\n```\nctx, cancel := context.WithTimeout(context.Background(), timeout)\nresp, err := cli.Put(ctx, \"sample_key\", \"sample_value\")\ncancel()\nif err != nil {\n    // handle error!\n}\n// use the response\n```\n为了完全兼容，建议使用etcd's中的vendored包进行构建，使用工具像`golang/dep`,在vendor目录内。\n## 错误处理\netcd客户端返回两种类型的错误：\n\n1. context error :canceled or deadline exceeded.\n2. gRpc error : 看api/v3rpc/rpctypes.\n\n这里有例子处理客户端错误：\n```\nresp, err := cli.Put(ctx, \"\", \"\")\nif err != nil {\n\tswitch err {\n\tcase context.Canceled:\n\t\tlog.Fatalf(\"ctx is canceled by another routine: %v\", err)\n\tcase context.DeadlineExceeded:\n\t\tlog.Fatalf(\"ctx is attached with a deadline is exceeded: %v\", err)\n\tcase rpctypes.ErrEmptyKey:\n\t\tlog.Fatalf(\"client-side error: %v\", err)\n\tdefault:\n\t\tlog.Fatalf(\"bad cluster endpoints, which are not etcd servers: %v\", err)\n\t}\n}\n```\n## 监控\netcd客户端可以通过**go-grpc-prometheus**,选择RPC监控指标,看**例子**。\n## 命名空间\n**namespace**包提供`clientv3`接口封装透明隔离客户端请求到用户定义的前缀。\n## 请求大小限制\n客户端请求大小限制通过`clientv3.Config.MaxCallSendMsgSize`和`MaxCallRecvMsgSize`进行配置。如果没有给予值，客户端请求发送限制包括gRPC负载默认2MB。接收限制默认为`math.MaxInt32`。\n## 例子\n更多代码例子可以从**GoDoc**发现。","slug":"blog/etcd/客户端v3","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyi0002fk0vq5edg3qm4","content":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/clientv3/README.md\" target=\"_blank\" rel=\"noopener\">etcd/clientv3</a><br><code>etcd/clientv3</code>是v3版本的Go etcd官方客户端</p>\n<h2 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h2><hr>\n<pre><code>go get go.etcd.io/etcd/clientv3</code></pre><h2 id=\"开始\"><a href=\"#开始\" class=\"headerlink\" title=\"开始\"></a>开始</h2><hr>\n<p>创建客户端使用<code>clientv3.New</code>:</p>\n<pre><code>cli, err := clientv3.New(clientv3.Config{\n    Endpoints:   []string{&quot;localhost:2379&quot;, &quot;localhost:22379&quot;, &quot;localhost:32379&quot;},\n    DialTimeout: 5 * time.Second,\n})\nif err != nil {\n    // handle error!\n}\ndefer cli.Close()</code></pre><p>etcd v3使用<code>gRPC</code>进行远程程序调用，并且<code>clientv3</code>使用<code>grpc-go</code>连接etcd。确保在使用完客户端后关闭它，如果客户端没有关闭，连接将会有泄漏的<code>goroutines</code>。指定超时时间，通过<code>context.WithTimeout</code>使用APIs:</p>\n<pre><code>ctx, cancel := context.WithTimeout(context.Background(), timeout)\nresp, err := cli.Put(ctx, &quot;sample_key&quot;, &quot;sample_value&quot;)\ncancel()\nif err != nil {\n    // handle error!\n}\n// use the response</code></pre><p>为了完全兼容，建议使用etcd’s中的vendored包进行构建，使用工具像<code>golang/dep</code>,在vendor目录内。</p>\n<h2 id=\"错误处理\"><a href=\"#错误处理\" class=\"headerlink\" title=\"错误处理\"></a>错误处理</h2><p>etcd客户端返回两种类型的错误：</p>\n<ol>\n<li>context error :canceled or deadline exceeded.</li>\n<li>gRpc error : 看api/v3rpc/rpctypes.</li>\n</ol>\n<p>这里有例子处理客户端错误：</p>\n<pre><code>resp, err := cli.Put(ctx, &quot;&quot;, &quot;&quot;)\nif err != nil {\n    switch err {\n    case context.Canceled:\n        log.Fatalf(&quot;ctx is canceled by another routine: %v&quot;, err)\n    case context.DeadlineExceeded:\n        log.Fatalf(&quot;ctx is attached with a deadline is exceeded: %v&quot;, err)\n    case rpctypes.ErrEmptyKey:\n        log.Fatalf(&quot;client-side error: %v&quot;, err)\n    default:\n        log.Fatalf(&quot;bad cluster endpoints, which are not etcd servers: %v&quot;, err)\n    }\n}</code></pre><h2 id=\"监控\"><a href=\"#监控\" class=\"headerlink\" title=\"监控\"></a>监控</h2><p>etcd客户端可以通过<strong>go-grpc-prometheus</strong>,选择RPC监控指标,看<strong>例子</strong>。</p>\n<h2 id=\"命名空间\"><a href=\"#命名空间\" class=\"headerlink\" title=\"命名空间\"></a>命名空间</h2><p><strong>namespace</strong>包提供<code>clientv3</code>接口封装透明隔离客户端请求到用户定义的前缀。</p>\n<h2 id=\"请求大小限制\"><a href=\"#请求大小限制\" class=\"headerlink\" title=\"请求大小限制\"></a>请求大小限制</h2><p>客户端请求大小限制通过<code>clientv3.Config.MaxCallSendMsgSize</code>和<code>MaxCallRecvMsgSize</code>进行配置。如果没有给予值，客户端请求发送限制包括gRPC负载默认2MB。接收限制默认为<code>math.MaxInt32</code>。</p>\n<h2 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h2><p>更多代码例子可以从<strong>GoDoc</strong>发现。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/clientv3/README.md\" target=\"_blank\" rel=\"noopener\">etcd/clientv3</a><br><code>etcd/clientv3</code>是v3版本的Go etcd官方客户端</p>\n<h2 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h2><hr>\n<pre><code>go get go.etcd.io/etcd/clientv3</code></pre><h2 id=\"开始\"><a href=\"#开始\" class=\"headerlink\" title=\"开始\"></a>开始</h2><hr>\n<p>创建客户端使用<code>clientv3.New</code>:</p>\n<pre><code>cli, err := clientv3.New(clientv3.Config{\n    Endpoints:   []string{&quot;localhost:2379&quot;, &quot;localhost:22379&quot;, &quot;localhost:32379&quot;},\n    DialTimeout: 5 * time.Second,\n})\nif err != nil {\n    // handle error!\n}\ndefer cli.Close()</code></pre><p>etcd v3使用<code>gRPC</code>进行远程程序调用，并且<code>clientv3</code>使用<code>grpc-go</code>连接etcd。确保在使用完客户端后关闭它，如果客户端没有关闭，连接将会有泄漏的<code>goroutines</code>。指定超时时间，通过<code>context.WithTimeout</code>使用APIs:</p>\n<pre><code>ctx, cancel := context.WithTimeout(context.Background(), timeout)\nresp, err := cli.Put(ctx, &quot;sample_key&quot;, &quot;sample_value&quot;)\ncancel()\nif err != nil {\n    // handle error!\n}\n// use the response</code></pre><p>为了完全兼容，建议使用etcd’s中的vendored包进行构建，使用工具像<code>golang/dep</code>,在vendor目录内。</p>\n<h2 id=\"错误处理\"><a href=\"#错误处理\" class=\"headerlink\" title=\"错误处理\"></a>错误处理</h2><p>etcd客户端返回两种类型的错误：</p>\n<ol>\n<li>context error :canceled or deadline exceeded.</li>\n<li>gRpc error : 看api/v3rpc/rpctypes.</li>\n</ol>\n<p>这里有例子处理客户端错误：</p>\n<pre><code>resp, err := cli.Put(ctx, &quot;&quot;, &quot;&quot;)\nif err != nil {\n    switch err {\n    case context.Canceled:\n        log.Fatalf(&quot;ctx is canceled by another routine: %v&quot;, err)\n    case context.DeadlineExceeded:\n        log.Fatalf(&quot;ctx is attached with a deadline is exceeded: %v&quot;, err)\n    case rpctypes.ErrEmptyKey:\n        log.Fatalf(&quot;client-side error: %v&quot;, err)\n    default:\n        log.Fatalf(&quot;bad cluster endpoints, which are not etcd servers: %v&quot;, err)\n    }\n}</code></pre><h2 id=\"监控\"><a href=\"#监控\" class=\"headerlink\" title=\"监控\"></a>监控</h2><p>etcd客户端可以通过<strong>go-grpc-prometheus</strong>,选择RPC监控指标,看<strong>例子</strong>。</p>\n<h2 id=\"命名空间\"><a href=\"#命名空间\" class=\"headerlink\" title=\"命名空间\"></a>命名空间</h2><p><strong>namespace</strong>包提供<code>clientv3</code>接口封装透明隔离客户端请求到用户定义的前缀。</p>\n<h2 id=\"请求大小限制\"><a href=\"#请求大小限制\" class=\"headerlink\" title=\"请求大小限制\"></a>请求大小限制</h2><p>客户端请求大小限制通过<code>clientv3.Config.MaxCallSendMsgSize</code>和<code>MaxCallRecvMsgSize</code>进行配置。如果没有给予值，客户端请求发送限制包括gRPC负载默认2MB。接收限制默认为<code>math.MaxInt32</code>。</p>\n<h2 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h2><p>更多代码例子可以从<strong>GoDoc</strong>发现。</p>\n"},{"title":"etcd文档","date":"2019-11-23T04:31:24.000Z","_content":"原文地址：[Documentation](https://github.com/etcd-io/etcd/tree/master/Documentation)\n# 文档\n\n* * *\netcd是一个分布式键值对存储，被设计为可靠的，快速的保存并提供对关键数据的访问。通过分布式锁，领导选举和写屏障使能分布式一致性。一个etcd集群旨在实现高可用和持久性数据存储与检索。\n## 开始\n\n* * *\n### 使用etcd进行开发\n\n* * *\n\n一种简单的方式[设置本地集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%8D%95%E6%9C%BA%E9%9B%86%E7%BE%A4/)开始使用etcd作为分布式键值对存储\n\n* [设置本地集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%8D%95%E6%9C%BA%E9%9B%86%E7%BE%A4/)\n* [与etcd进行交互](https://newonexd.github.io/2019/11/23/blog/etcd/%E4%B8%8Eetcd%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92/)\n* [gRPC etcd核心](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md)和[etcd并发]()API参考\n* [HTTP JSON API 通过gRPC网关](https://newonexd.github.io/2019/11/23/blog/etcd/HTTP_JSON_API%E9%80%9A%E8%BF%87gRPC%E7%BD%91%E5%85%B3/)\n* [gRPC命名和发现](https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/)\n* [客户端](https://godoc.org/github.com/etcd-io/etcd/clientv3/namespace)和[代理](https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/)命名空间\n* [嵌入etcd](https://godoc.org/github.com/etcd-io/etcd/embed)\n* [实验特性和APIs](https://newonexd.github.io/2019/11/24/blog/etcd/%E5%AE%9E%E9%AA%8C%E7%89%B9%E6%80%A7%E5%92%8CAPIs/)\n* [系统限制](https://newonexd.github.io/2019/11/25/blog/etcd/%E7%B3%BB%E7%BB%9F%E9%99%90%E5%88%B6/)\n\n### 操作etcd集群\n\n* * *\n对开发或者生产环境，管理员需要一个错误容忍etcd集群，从[多机集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/)开始。\n#### 设置etcd\n\n* [配置参数](https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/)\n* [多成员集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/)\n* [gRPC代理](https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/)\n* [L4网关](https://newonexd.github.io/2019/11/24/blog/etcd/etcd%E7%BD%91%E5%85%B3/)\n\n#### 系统配置\n\n* [支持的系统](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/supported-platform.md)\n* [硬件配置建议](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md)\n* [性能基准测试](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md)\n* [调节](https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md)\n\n#### 平台引导\n\n* [亚马逊Web服务](https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/aws.md)\n* [Linux容器，systemd](https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/container-linux-systemd.md)\n* [RessBSD](https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/freebsd.md)\n* [Docker容器](https://newonexd.github.io/2019/11/24/blog/etcd/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cetcd%E9%9B%86%E7%BE%A4/)\n* [rkt容器](https://newonexd.github.io/2019/11/24/blog/etcd/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cetcd%E9%9B%86%E7%BE%A4/)\n\n#### 安全\n\n* [TLS](https://newonexd.github.io/2019/11/25/blog/etcd/TLS/)\n* [基于角色的访问控制](https://newonexd.github.io/2019/11/25/blog/etcd/%E5%9F%BA%E4%BA%8E%E8%A7%92%E8%89%B2%E7%9A%84%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/)\n\n#### 维护和故障排除\n* [常见问题](https://github.com/etcd-io/etcd/blob/master/Documentation/faq.md)\n* [监控方式](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/monitoring.md)\n* [维护](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/maintenance.md)\n* [失败模式](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/failures.md)\n* [容灾恢复](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md)\n* [更新](https://github.com/etcd-io/etcd/blob/master/Documentation/upgrades/upgrading-etcd.md)\n#### 学习资料\n要了解有关etcd的概念和内部知识的更多信息，请阅读以下页面：\n* [什么是etcd](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md)\n* [理解数据模式](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/data_model.md)\n* [理解APIs](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/api.md)\n* [词汇表](https://newonexd.github.io/2019/11/24/blog/etcd/%E8%AF%8D%E6%B1%87%E8%A1%A8/)\n* 设计\n    * [权限子系统](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-auth-v3.md)\n    * [客户端](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-client.md)\n    * [学习者](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md)","source":"_posts/blog/etcd/文档.md","raw":"---\ntitle: etcd文档\ndate: 2019-11-23 12:31:24\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址：[Documentation](https://github.com/etcd-io/etcd/tree/master/Documentation)\n# 文档\n\n* * *\netcd是一个分布式键值对存储，被设计为可靠的，快速的保存并提供对关键数据的访问。通过分布式锁，领导选举和写屏障使能分布式一致性。一个etcd集群旨在实现高可用和持久性数据存储与检索。\n## 开始\n\n* * *\n### 使用etcd进行开发\n\n* * *\n\n一种简单的方式[设置本地集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%8D%95%E6%9C%BA%E9%9B%86%E7%BE%A4/)开始使用etcd作为分布式键值对存储\n\n* [设置本地集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%8D%95%E6%9C%BA%E9%9B%86%E7%BE%A4/)\n* [与etcd进行交互](https://newonexd.github.io/2019/11/23/blog/etcd/%E4%B8%8Eetcd%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92/)\n* [gRPC etcd核心](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md)和[etcd并发]()API参考\n* [HTTP JSON API 通过gRPC网关](https://newonexd.github.io/2019/11/23/blog/etcd/HTTP_JSON_API%E9%80%9A%E8%BF%87gRPC%E7%BD%91%E5%85%B3/)\n* [gRPC命名和发现](https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/)\n* [客户端](https://godoc.org/github.com/etcd-io/etcd/clientv3/namespace)和[代理](https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/)命名空间\n* [嵌入etcd](https://godoc.org/github.com/etcd-io/etcd/embed)\n* [实验特性和APIs](https://newonexd.github.io/2019/11/24/blog/etcd/%E5%AE%9E%E9%AA%8C%E7%89%B9%E6%80%A7%E5%92%8CAPIs/)\n* [系统限制](https://newonexd.github.io/2019/11/25/blog/etcd/%E7%B3%BB%E7%BB%9F%E9%99%90%E5%88%B6/)\n\n### 操作etcd集群\n\n* * *\n对开发或者生产环境，管理员需要一个错误容忍etcd集群，从[多机集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/)开始。\n#### 设置etcd\n\n* [配置参数](https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/)\n* [多成员集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/)\n* [gRPC代理](https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/)\n* [L4网关](https://newonexd.github.io/2019/11/24/blog/etcd/etcd%E7%BD%91%E5%85%B3/)\n\n#### 系统配置\n\n* [支持的系统](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/supported-platform.md)\n* [硬件配置建议](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md)\n* [性能基准测试](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md)\n* [调节](https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md)\n\n#### 平台引导\n\n* [亚马逊Web服务](https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/aws.md)\n* [Linux容器，systemd](https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/container-linux-systemd.md)\n* [RessBSD](https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/freebsd.md)\n* [Docker容器](https://newonexd.github.io/2019/11/24/blog/etcd/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cetcd%E9%9B%86%E7%BE%A4/)\n* [rkt容器](https://newonexd.github.io/2019/11/24/blog/etcd/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cetcd%E9%9B%86%E7%BE%A4/)\n\n#### 安全\n\n* [TLS](https://newonexd.github.io/2019/11/25/blog/etcd/TLS/)\n* [基于角色的访问控制](https://newonexd.github.io/2019/11/25/blog/etcd/%E5%9F%BA%E4%BA%8E%E8%A7%92%E8%89%B2%E7%9A%84%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/)\n\n#### 维护和故障排除\n* [常见问题](https://github.com/etcd-io/etcd/blob/master/Documentation/faq.md)\n* [监控方式](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/monitoring.md)\n* [维护](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/maintenance.md)\n* [失败模式](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/failures.md)\n* [容灾恢复](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md)\n* [更新](https://github.com/etcd-io/etcd/blob/master/Documentation/upgrades/upgrading-etcd.md)\n#### 学习资料\n要了解有关etcd的概念和内部知识的更多信息，请阅读以下页面：\n* [什么是etcd](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md)\n* [理解数据模式](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/data_model.md)\n* [理解APIs](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/api.md)\n* [词汇表](https://newonexd.github.io/2019/11/24/blog/etcd/%E8%AF%8D%E6%B1%87%E8%A1%A8/)\n* 设计\n    * [权限子系统](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-auth-v3.md)\n    * [客户端](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-client.md)\n    * [学习者](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md)","slug":"blog/etcd/文档","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyi2002jk0vqce2e2cea","content":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/tree/master/Documentation\" target=\"_blank\" rel=\"noopener\">Documentation</a></p>\n<h1 id=\"文档\"><a href=\"#文档\" class=\"headerlink\" title=\"文档\"></a>文档</h1><hr>\n<p>etcd是一个分布式键值对存储，被设计为可靠的，快速的保存并提供对关键数据的访问。通过分布式锁，领导选举和写屏障使能分布式一致性。一个etcd集群旨在实现高可用和持久性数据存储与检索。</p>\n<h2 id=\"开始\"><a href=\"#开始\" class=\"headerlink\" title=\"开始\"></a>开始</h2><hr>\n<h3 id=\"使用etcd进行开发\"><a href=\"#使用etcd进行开发\" class=\"headerlink\" title=\"使用etcd进行开发\"></a>使用etcd进行开发</h3><hr>\n<p>一种简单的方式<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%8D%95%E6%9C%BA%E9%9B%86%E7%BE%A4/\">设置本地集群</a>开始使用etcd作为分布式键值对存储</p>\n<ul>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%8D%95%E6%9C%BA%E9%9B%86%E7%BE%A4/\">设置本地集群</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E4%B8%8Eetcd%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92/\">与etcd进行交互</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md\" target=\"_blank\" rel=\"noopener\">gRPC etcd核心</a>和<a href=\"\">etcd并发</a>API参考</li>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/HTTP_JSON_API%E9%80%9A%E8%BF%87gRPC%E7%BD%91%E5%85%B3/\">HTTP JSON API 通过gRPC网关</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/\">gRPC命名和发现</a></li>\n<li><a href=\"https://godoc.org/github.com/etcd-io/etcd/clientv3/namespace\" target=\"_blank\" rel=\"noopener\">客户端</a>和<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/\">代理</a>命名空间</li>\n<li><a href=\"https://godoc.org/github.com/etcd-io/etcd/embed\" target=\"_blank\" rel=\"noopener\">嵌入etcd</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/%E5%AE%9E%E9%AA%8C%E7%89%B9%E6%80%A7%E5%92%8CAPIs/\">实验特性和APIs</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/%E7%B3%BB%E7%BB%9F%E9%99%90%E5%88%B6/\">系统限制</a></li>\n</ul>\n<h3 id=\"操作etcd集群\"><a href=\"#操作etcd集群\" class=\"headerlink\" title=\"操作etcd集群\"></a>操作etcd集群</h3><hr>\n<p>对开发或者生产环境，管理员需要一个错误容忍etcd集群，从<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/\">多机集群</a>开始。</p>\n<h4 id=\"设置etcd\"><a href=\"#设置etcd\" class=\"headerlink\" title=\"设置etcd\"></a>设置etcd</h4><ul>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/\">配置参数</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/\">多成员集群</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/\">gRPC代理</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/etcd%E7%BD%91%E5%85%B3/\">L4网关</a></li>\n</ul>\n<h4 id=\"系统配置\"><a href=\"#系统配置\" class=\"headerlink\" title=\"系统配置\"></a>系统配置</h4><ul>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/supported-platform.md\" target=\"_blank\" rel=\"noopener\">支持的系统</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md\" target=\"_blank\" rel=\"noopener\">硬件配置建议</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md\" target=\"_blank\" rel=\"noopener\">性能基准测试</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md\" target=\"_blank\" rel=\"noopener\">调节</a></li>\n</ul>\n<h4 id=\"平台引导\"><a href=\"#平台引导\" class=\"headerlink\" title=\"平台引导\"></a>平台引导</h4><ul>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/aws.md\" target=\"_blank\" rel=\"noopener\">亚马逊Web服务</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/container-linux-systemd.md\" target=\"_blank\" rel=\"noopener\">Linux容器，systemd</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/freebsd.md\" target=\"_blank\" rel=\"noopener\">RessBSD</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cetcd%E9%9B%86%E7%BE%A4/\">Docker容器</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cetcd%E9%9B%86%E7%BE%A4/\">rkt容器</a></li>\n</ul>\n<h4 id=\"安全\"><a href=\"#安全\" class=\"headerlink\" title=\"安全\"></a>安全</h4><ul>\n<li><a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/TLS/\">TLS</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/%E5%9F%BA%E4%BA%8E%E8%A7%92%E8%89%B2%E7%9A%84%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/\">基于角色的访问控制</a></li>\n</ul>\n<h4 id=\"维护和故障排除\"><a href=\"#维护和故障排除\" class=\"headerlink\" title=\"维护和故障排除\"></a>维护和故障排除</h4><ul>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/faq.md\" target=\"_blank\" rel=\"noopener\">常见问题</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/monitoring.md\" target=\"_blank\" rel=\"noopener\">监控方式</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/maintenance.md\" target=\"_blank\" rel=\"noopener\">维护</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/failures.md\" target=\"_blank\" rel=\"noopener\">失败模式</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">容灾恢复</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/upgrades/upgrading-etcd.md\" target=\"_blank\" rel=\"noopener\">更新</a><h4 id=\"学习资料\"><a href=\"#学习资料\" class=\"headerlink\" title=\"学习资料\"></a>学习资料</h4>要了解有关etcd的概念和内部知识的更多信息，请阅读以下页面：</li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md\" target=\"_blank\" rel=\"noopener\">什么是etcd</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/data_model.md\" target=\"_blank\" rel=\"noopener\">理解数据模式</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/api.md\" target=\"_blank\" rel=\"noopener\">理解APIs</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/%E8%AF%8D%E6%B1%87%E8%A1%A8/\">词汇表</a></li>\n<li>设计<ul>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-auth-v3.md\" target=\"_blank\" rel=\"noopener\">权限子系统</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-client.md\" target=\"_blank\" rel=\"noopener\">客户端</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md\" target=\"_blank\" rel=\"noopener\">学习者</a></li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/tree/master/Documentation\" target=\"_blank\" rel=\"noopener\">Documentation</a></p>\n<h1 id=\"文档\"><a href=\"#文档\" class=\"headerlink\" title=\"文档\"></a>文档</h1><hr>\n<p>etcd是一个分布式键值对存储，被设计为可靠的，快速的保存并提供对关键数据的访问。通过分布式锁，领导选举和写屏障使能分布式一致性。一个etcd集群旨在实现高可用和持久性数据存储与检索。</p>\n<h2 id=\"开始\"><a href=\"#开始\" class=\"headerlink\" title=\"开始\"></a>开始</h2><hr>\n<h3 id=\"使用etcd进行开发\"><a href=\"#使用etcd进行开发\" class=\"headerlink\" title=\"使用etcd进行开发\"></a>使用etcd进行开发</h3><hr>\n<p>一种简单的方式<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%8D%95%E6%9C%BA%E9%9B%86%E7%BE%A4/\">设置本地集群</a>开始使用etcd作为分布式键值对存储</p>\n<ul>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%8D%95%E6%9C%BA%E9%9B%86%E7%BE%A4/\">设置本地集群</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E4%B8%8Eetcd%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92/\">与etcd进行交互</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md\" target=\"_blank\" rel=\"noopener\">gRPC etcd核心</a>和<a href=\"\">etcd并发</a>API参考</li>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/HTTP_JSON_API%E9%80%9A%E8%BF%87gRPC%E7%BD%91%E5%85%B3/\">HTTP JSON API 通过gRPC网关</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/gRPC%E5%91%BD%E5%90%8D%E4%B8%8E%E5%8F%91%E7%8E%B0/\">gRPC命名和发现</a></li>\n<li><a href=\"https://godoc.org/github.com/etcd-io/etcd/clientv3/namespace\" target=\"_blank\" rel=\"noopener\">客户端</a>和<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/\">代理</a>命名空间</li>\n<li><a href=\"https://godoc.org/github.com/etcd-io/etcd/embed\" target=\"_blank\" rel=\"noopener\">嵌入etcd</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/%E5%AE%9E%E9%AA%8C%E7%89%B9%E6%80%A7%E5%92%8CAPIs/\">实验特性和APIs</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/%E7%B3%BB%E7%BB%9F%E9%99%90%E5%88%B6/\">系统限制</a></li>\n</ul>\n<h3 id=\"操作etcd集群\"><a href=\"#操作etcd集群\" class=\"headerlink\" title=\"操作etcd集群\"></a>操作etcd集群</h3><hr>\n<p>对开发或者生产环境，管理员需要一个错误容忍etcd集群，从<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/\">多机集群</a>开始。</p>\n<h4 id=\"设置etcd\"><a href=\"#设置etcd\" class=\"headerlink\" title=\"设置etcd\"></a>设置etcd</h4><ul>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/\">配置参数</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E5%A4%9A%E6%9C%BA%E9%9B%86%E7%BE%A4/\">多成员集群</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/gRPC%E4%BB%A3%E7%90%86/\">gRPC代理</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/etcd%E7%BD%91%E5%85%B3/\">L4网关</a></li>\n</ul>\n<h4 id=\"系统配置\"><a href=\"#系统配置\" class=\"headerlink\" title=\"系统配置\"></a>系统配置</h4><ul>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/supported-platform.md\" target=\"_blank\" rel=\"noopener\">支持的系统</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md\" target=\"_blank\" rel=\"noopener\">硬件配置建议</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md\" target=\"_blank\" rel=\"noopener\">性能基准测试</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md\" target=\"_blank\" rel=\"noopener\">调节</a></li>\n</ul>\n<h4 id=\"平台引导\"><a href=\"#平台引导\" class=\"headerlink\" title=\"平台引导\"></a>平台引导</h4><ul>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/aws.md\" target=\"_blank\" rel=\"noopener\">亚马逊Web服务</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/container-linux-systemd.md\" target=\"_blank\" rel=\"noopener\">Linux容器，systemd</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/platforms/freebsd.md\" target=\"_blank\" rel=\"noopener\">RessBSD</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cetcd%E9%9B%86%E7%BE%A4/\">Docker容器</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cetcd%E9%9B%86%E7%BE%A4/\">rkt容器</a></li>\n</ul>\n<h4 id=\"安全\"><a href=\"#安全\" class=\"headerlink\" title=\"安全\"></a>安全</h4><ul>\n<li><a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/TLS/\">TLS</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/25/blog/etcd/%E5%9F%BA%E4%BA%8E%E8%A7%92%E8%89%B2%E7%9A%84%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/\">基于角色的访问控制</a></li>\n</ul>\n<h4 id=\"维护和故障排除\"><a href=\"#维护和故障排除\" class=\"headerlink\" title=\"维护和故障排除\"></a>维护和故障排除</h4><ul>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/faq.md\" target=\"_blank\" rel=\"noopener\">常见问题</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/monitoring.md\" target=\"_blank\" rel=\"noopener\">监控方式</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/maintenance.md\" target=\"_blank\" rel=\"noopener\">维护</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/failures.md\" target=\"_blank\" rel=\"noopener\">失败模式</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">容灾恢复</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/upgrades/upgrading-etcd.md\" target=\"_blank\" rel=\"noopener\">更新</a><h4 id=\"学习资料\"><a href=\"#学习资料\" class=\"headerlink\" title=\"学习资料\"></a>学习资料</h4>要了解有关etcd的概念和内部知识的更多信息，请阅读以下页面：</li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md\" target=\"_blank\" rel=\"noopener\">什么是etcd</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/data_model.md\" target=\"_blank\" rel=\"noopener\">理解数据模式</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/api.md\" target=\"_blank\" rel=\"noopener\">理解APIs</a></li>\n<li><a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/%E8%AF%8D%E6%B1%87%E8%A1%A8/\">词汇表</a></li>\n<li>设计<ul>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-auth-v3.md\" target=\"_blank\" rel=\"noopener\">权限子系统</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-client.md\" target=\"_blank\" rel=\"noopener\">客户端</a></li>\n<li><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md\" target=\"_blank\" rel=\"noopener\">学习者</a></li>\n</ul>\n</li>\n</ul>\n"},{"title":"系统限制","date":"2019-11-25T02:48:31.000Z","_content":"\n原文地址:[System limits](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/limit.md)\n## 请求大小限制\n\n* * *\n\n\netcd被设计用来处理小键值对典型的如元数据。较大的请求数据也起作用，但可能会增加其他请求的延迟。默认情况下，任意的请求最大的空间为1.5MiB，这个限制参数可以通过`--max-request-bytes`参数对etcd服务器进行配置。\n## 存储大小限制\n\n* * *\n默认的存储大小限制为2GB,可以通过参数`--quota-backend-bytes`进行配置。正常环境下8GB是etcd支持的最大存储大小，如果配置的值超过它，etcd将在启动时发出警告。","source":"_posts/blog/etcd/系统限制.md","raw":"---\ntitle: 系统限制\ndate: 2019-11-25 10:48:31\ntags: etcd\ncategories: etcd文档翻译\n---\n\n原文地址:[System limits](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/limit.md)\n## 请求大小限制\n\n* * *\n\n\netcd被设计用来处理小键值对典型的如元数据。较大的请求数据也起作用，但可能会增加其他请求的延迟。默认情况下，任意的请求最大的空间为1.5MiB，这个限制参数可以通过`--max-request-bytes`参数对etcd服务器进行配置。\n## 存储大小限制\n\n* * *\n默认的存储大小限制为2GB,可以通过参数`--quota-backend-bytes`进行配置。正常环境下8GB是etcd支持的最大存储大小，如果配置的值超过它，etcd将在启动时发出警告。","slug":"blog/etcd/系统限制","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyi6002mk0vq2j723006","content":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/limit.md\" target=\"_blank\" rel=\"noopener\">System limits</a></p>\n<h2 id=\"请求大小限制\"><a href=\"#请求大小限制\" class=\"headerlink\" title=\"请求大小限制\"></a>请求大小限制</h2><hr>\n<p>etcd被设计用来处理小键值对典型的如元数据。较大的请求数据也起作用，但可能会增加其他请求的延迟。默认情况下，任意的请求最大的空间为1.5MiB，这个限制参数可以通过<code>--max-request-bytes</code>参数对etcd服务器进行配置。</p>\n<h2 id=\"存储大小限制\"><a href=\"#存储大小限制\" class=\"headerlink\" title=\"存储大小限制\"></a>存储大小限制</h2><hr>\n<p>默认的存储大小限制为2GB,可以通过参数<code>--quota-backend-bytes</code>进行配置。正常环境下8GB是etcd支持的最大存储大小，如果配置的值超过它，etcd将在启动时发出警告。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/limit.md\" target=\"_blank\" rel=\"noopener\">System limits</a></p>\n<h2 id=\"请求大小限制\"><a href=\"#请求大小限制\" class=\"headerlink\" title=\"请求大小限制\"></a>请求大小限制</h2><hr>\n<p>etcd被设计用来处理小键值对典型的如元数据。较大的请求数据也起作用，但可能会增加其他请求的延迟。默认情况下，任意的请求最大的空间为1.5MiB，这个限制参数可以通过<code>--max-request-bytes</code>参数对etcd服务器进行配置。</p>\n<h2 id=\"存储大小限制\"><a href=\"#存储大小限制\" class=\"headerlink\" title=\"存储大小限制\"></a>存储大小限制</h2><hr>\n<p>默认的存储大小限制为2GB,可以通过参数<code>--quota-backend-bytes</code>进行配置。正常环境下8GB是etcd支持的最大存储大小，如果配置的值超过它，etcd将在启动时发出警告。</p>\n"},{"title":"词汇表","date":"2019-11-24T07:51:00.000Z","_content":"原文地址:[词汇表](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/glossary.md)\n本文档定义了etcd文档，命令行和源代码中使用的各种术语。\n\n### Alarm\n* * *\n每当集群需要操作员干预以保持可靠性时，etcd服务器都会发出警报。\n\n### Authentication\n* * *\n身份验证管理etcd资源的用户访问权限。\n\n### Client\n* * *\n客户端连接到etcd集群以发出服务请求，例如获取键值对，写入数据或监视更新。\n\n### Cluster\n* * *\n集群由几个成员组成。\n\n每个成员中的节点均遵循Raft共识协议来复制日志。 集群从成员那里接收提议，将它们提交并应用于本地存储。\n### Compaction\n* * *\n在给定修订之前，压缩会丢弃所有etcd事件历史记录和取代的密钥。 它用于回收etcd后端数据库中的存储空间。\n\n### Election\n* * *\netcd集群在其成员之间举行选举，以选择一名领导人作为Raft共识协议的一部分。\n\n### Endpoint\n* * *\n指向etcd服务或资源的URL。\n\n### Key\n* * *\n用户定义的标识符，用于在etcd中存储和检索用户定义的值。\n\n### Key range\n* * *\n包含单个键，所有x的词法间隔（例如a <x <= b）或大于给定键的所有键的一组键。\n\n### Keyspace\n* * *\netcd集群中所有键的集合。\n\n### Lease\n* * *\n短期可再生合同，在合同到期时会删除与其相关的密钥。\n\n### Member\n* * *\n参与为etcd集群提供服务的逻辑etcd服务器。\n\n### Modification Revision\n* * *\n保留对给定密钥的最后一次写入的第一个修订版。\n\n### Peer\n* * *\n同一集群的另一个对等成员。\n\n### Proposal\n* * *\n提案是需要通过raft协议的请求（例如，写入请求，配置更改请求）。\n\n### Quorum\n* * *\n达成共识才能修改集群状态所需的活动成员数。 etcd需要拥有多数才能达到法定人数。\n\n### Revision\n* * *\n每次修改键空间时都会增加的64位群集范围计数器。\n\n### Role\n* * *\n一组密钥范围内的许可单位，可以将其授予一组用户以进行访问控制。\n\n### Snapshot\n* * *\netcd群集状态的时间点备份。\n\n### Store\n* * *\n支持集群键空间的物理存储。\n\n### Transaction\n* * *\n原子执行的一组操作。 事务中所有已修改的键共享相同的修改版本。\n\n### Key Version\n* * *\n自创建以来，对密钥的写入次数（从1开始）。不存在或已删除的密钥的版本为0。\n\n### Watcher\n* * *\n客户端打开观察者以观察给定键范围上的更新。\n","source":"_posts/blog/etcd/词汇表.md","raw":"---\ntitle: 词汇表\ndate: 2019-11-24 15:51:00\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址:[词汇表](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/glossary.md)\n本文档定义了etcd文档，命令行和源代码中使用的各种术语。\n\n### Alarm\n* * *\n每当集群需要操作员干预以保持可靠性时，etcd服务器都会发出警报。\n\n### Authentication\n* * *\n身份验证管理etcd资源的用户访问权限。\n\n### Client\n* * *\n客户端连接到etcd集群以发出服务请求，例如获取键值对，写入数据或监视更新。\n\n### Cluster\n* * *\n集群由几个成员组成。\n\n每个成员中的节点均遵循Raft共识协议来复制日志。 集群从成员那里接收提议，将它们提交并应用于本地存储。\n### Compaction\n* * *\n在给定修订之前，压缩会丢弃所有etcd事件历史记录和取代的密钥。 它用于回收etcd后端数据库中的存储空间。\n\n### Election\n* * *\netcd集群在其成员之间举行选举，以选择一名领导人作为Raft共识协议的一部分。\n\n### Endpoint\n* * *\n指向etcd服务或资源的URL。\n\n### Key\n* * *\n用户定义的标识符，用于在etcd中存储和检索用户定义的值。\n\n### Key range\n* * *\n包含单个键，所有x的词法间隔（例如a <x <= b）或大于给定键的所有键的一组键。\n\n### Keyspace\n* * *\netcd集群中所有键的集合。\n\n### Lease\n* * *\n短期可再生合同，在合同到期时会删除与其相关的密钥。\n\n### Member\n* * *\n参与为etcd集群提供服务的逻辑etcd服务器。\n\n### Modification Revision\n* * *\n保留对给定密钥的最后一次写入的第一个修订版。\n\n### Peer\n* * *\n同一集群的另一个对等成员。\n\n### Proposal\n* * *\n提案是需要通过raft协议的请求（例如，写入请求，配置更改请求）。\n\n### Quorum\n* * *\n达成共识才能修改集群状态所需的活动成员数。 etcd需要拥有多数才能达到法定人数。\n\n### Revision\n* * *\n每次修改键空间时都会增加的64位群集范围计数器。\n\n### Role\n* * *\n一组密钥范围内的许可单位，可以将其授予一组用户以进行访问控制。\n\n### Snapshot\n* * *\netcd群集状态的时间点备份。\n\n### Store\n* * *\n支持集群键空间的物理存储。\n\n### Transaction\n* * *\n原子执行的一组操作。 事务中所有已修改的键共享相同的修改版本。\n\n### Key Version\n* * *\n自创建以来，对密钥的写入次数（从1开始）。不存在或已删除的密钥的版本为0。\n\n### Watcher\n* * *\n客户端打开观察者以观察给定键范围上的更新。\n","slug":"blog/etcd/词汇表","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyi8002pk0vq1wqgbeyd","content":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/glossary.md\" target=\"_blank\" rel=\"noopener\">词汇表</a><br>本文档定义了etcd文档，命令行和源代码中使用的各种术语。</p>\n<h3 id=\"Alarm\"><a href=\"#Alarm\" class=\"headerlink\" title=\"Alarm\"></a>Alarm</h3><hr>\n<p>每当集群需要操作员干预以保持可靠性时，etcd服务器都会发出警报。</p>\n<h3 id=\"Authentication\"><a href=\"#Authentication\" class=\"headerlink\" title=\"Authentication\"></a>Authentication</h3><hr>\n<p>身份验证管理etcd资源的用户访问权限。</p>\n<h3 id=\"Client\"><a href=\"#Client\" class=\"headerlink\" title=\"Client\"></a>Client</h3><hr>\n<p>客户端连接到etcd集群以发出服务请求，例如获取键值对，写入数据或监视更新。</p>\n<h3 id=\"Cluster\"><a href=\"#Cluster\" class=\"headerlink\" title=\"Cluster\"></a>Cluster</h3><hr>\n<p>集群由几个成员组成。</p>\n<p>每个成员中的节点均遵循Raft共识协议来复制日志。 集群从成员那里接收提议，将它们提交并应用于本地存储。</p>\n<h3 id=\"Compaction\"><a href=\"#Compaction\" class=\"headerlink\" title=\"Compaction\"></a>Compaction</h3><hr>\n<p>在给定修订之前，压缩会丢弃所有etcd事件历史记录和取代的密钥。 它用于回收etcd后端数据库中的存储空间。</p>\n<h3 id=\"Election\"><a href=\"#Election\" class=\"headerlink\" title=\"Election\"></a>Election</h3><hr>\n<p>etcd集群在其成员之间举行选举，以选择一名领导人作为Raft共识协议的一部分。</p>\n<h3 id=\"Endpoint\"><a href=\"#Endpoint\" class=\"headerlink\" title=\"Endpoint\"></a>Endpoint</h3><hr>\n<p>指向etcd服务或资源的URL。</p>\n<h3 id=\"Key\"><a href=\"#Key\" class=\"headerlink\" title=\"Key\"></a>Key</h3><hr>\n<p>用户定义的标识符，用于在etcd中存储和检索用户定义的值。</p>\n<h3 id=\"Key-range\"><a href=\"#Key-range\" class=\"headerlink\" title=\"Key range\"></a>Key range</h3><hr>\n<p>包含单个键，所有x的词法间隔（例如a &lt;x &lt;= b）或大于给定键的所有键的一组键。</p>\n<h3 id=\"Keyspace\"><a href=\"#Keyspace\" class=\"headerlink\" title=\"Keyspace\"></a>Keyspace</h3><hr>\n<p>etcd集群中所有键的集合。</p>\n<h3 id=\"Lease\"><a href=\"#Lease\" class=\"headerlink\" title=\"Lease\"></a>Lease</h3><hr>\n<p>短期可再生合同，在合同到期时会删除与其相关的密钥。</p>\n<h3 id=\"Member\"><a href=\"#Member\" class=\"headerlink\" title=\"Member\"></a>Member</h3><hr>\n<p>参与为etcd集群提供服务的逻辑etcd服务器。</p>\n<h3 id=\"Modification-Revision\"><a href=\"#Modification-Revision\" class=\"headerlink\" title=\"Modification Revision\"></a>Modification Revision</h3><hr>\n<p>保留对给定密钥的最后一次写入的第一个修订版。</p>\n<h3 id=\"Peer\"><a href=\"#Peer\" class=\"headerlink\" title=\"Peer\"></a>Peer</h3><hr>\n<p>同一集群的另一个对等成员。</p>\n<h3 id=\"Proposal\"><a href=\"#Proposal\" class=\"headerlink\" title=\"Proposal\"></a>Proposal</h3><hr>\n<p>提案是需要通过raft协议的请求（例如，写入请求，配置更改请求）。</p>\n<h3 id=\"Quorum\"><a href=\"#Quorum\" class=\"headerlink\" title=\"Quorum\"></a>Quorum</h3><hr>\n<p>达成共识才能修改集群状态所需的活动成员数。 etcd需要拥有多数才能达到法定人数。</p>\n<h3 id=\"Revision\"><a href=\"#Revision\" class=\"headerlink\" title=\"Revision\"></a>Revision</h3><hr>\n<p>每次修改键空间时都会增加的64位群集范围计数器。</p>\n<h3 id=\"Role\"><a href=\"#Role\" class=\"headerlink\" title=\"Role\"></a>Role</h3><hr>\n<p>一组密钥范围内的许可单位，可以将其授予一组用户以进行访问控制。</p>\n<h3 id=\"Snapshot\"><a href=\"#Snapshot\" class=\"headerlink\" title=\"Snapshot\"></a>Snapshot</h3><hr>\n<p>etcd群集状态的时间点备份。</p>\n<h3 id=\"Store\"><a href=\"#Store\" class=\"headerlink\" title=\"Store\"></a>Store</h3><hr>\n<p>支持集群键空间的物理存储。</p>\n<h3 id=\"Transaction\"><a href=\"#Transaction\" class=\"headerlink\" title=\"Transaction\"></a>Transaction</h3><hr>\n<p>原子执行的一组操作。 事务中所有已修改的键共享相同的修改版本。</p>\n<h3 id=\"Key-Version\"><a href=\"#Key-Version\" class=\"headerlink\" title=\"Key Version\"></a>Key Version</h3><hr>\n<p>自创建以来，对密钥的写入次数（从1开始）。不存在或已删除的密钥的版本为0。</p>\n<h3 id=\"Watcher\"><a href=\"#Watcher\" class=\"headerlink\" title=\"Watcher\"></a>Watcher</h3><hr>\n<p>客户端打开观察者以观察给定键范围上的更新。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/glossary.md\" target=\"_blank\" rel=\"noopener\">词汇表</a><br>本文档定义了etcd文档，命令行和源代码中使用的各种术语。</p>\n<h3 id=\"Alarm\"><a href=\"#Alarm\" class=\"headerlink\" title=\"Alarm\"></a>Alarm</h3><hr>\n<p>每当集群需要操作员干预以保持可靠性时，etcd服务器都会发出警报。</p>\n<h3 id=\"Authentication\"><a href=\"#Authentication\" class=\"headerlink\" title=\"Authentication\"></a>Authentication</h3><hr>\n<p>身份验证管理etcd资源的用户访问权限。</p>\n<h3 id=\"Client\"><a href=\"#Client\" class=\"headerlink\" title=\"Client\"></a>Client</h3><hr>\n<p>客户端连接到etcd集群以发出服务请求，例如获取键值对，写入数据或监视更新。</p>\n<h3 id=\"Cluster\"><a href=\"#Cluster\" class=\"headerlink\" title=\"Cluster\"></a>Cluster</h3><hr>\n<p>集群由几个成员组成。</p>\n<p>每个成员中的节点均遵循Raft共识协议来复制日志。 集群从成员那里接收提议，将它们提交并应用于本地存储。</p>\n<h3 id=\"Compaction\"><a href=\"#Compaction\" class=\"headerlink\" title=\"Compaction\"></a>Compaction</h3><hr>\n<p>在给定修订之前，压缩会丢弃所有etcd事件历史记录和取代的密钥。 它用于回收etcd后端数据库中的存储空间。</p>\n<h3 id=\"Election\"><a href=\"#Election\" class=\"headerlink\" title=\"Election\"></a>Election</h3><hr>\n<p>etcd集群在其成员之间举行选举，以选择一名领导人作为Raft共识协议的一部分。</p>\n<h3 id=\"Endpoint\"><a href=\"#Endpoint\" class=\"headerlink\" title=\"Endpoint\"></a>Endpoint</h3><hr>\n<p>指向etcd服务或资源的URL。</p>\n<h3 id=\"Key\"><a href=\"#Key\" class=\"headerlink\" title=\"Key\"></a>Key</h3><hr>\n<p>用户定义的标识符，用于在etcd中存储和检索用户定义的值。</p>\n<h3 id=\"Key-range\"><a href=\"#Key-range\" class=\"headerlink\" title=\"Key range\"></a>Key range</h3><hr>\n<p>包含单个键，所有x的词法间隔（例如a &lt;x &lt;= b）或大于给定键的所有键的一组键。</p>\n<h3 id=\"Keyspace\"><a href=\"#Keyspace\" class=\"headerlink\" title=\"Keyspace\"></a>Keyspace</h3><hr>\n<p>etcd集群中所有键的集合。</p>\n<h3 id=\"Lease\"><a href=\"#Lease\" class=\"headerlink\" title=\"Lease\"></a>Lease</h3><hr>\n<p>短期可再生合同，在合同到期时会删除与其相关的密钥。</p>\n<h3 id=\"Member\"><a href=\"#Member\" class=\"headerlink\" title=\"Member\"></a>Member</h3><hr>\n<p>参与为etcd集群提供服务的逻辑etcd服务器。</p>\n<h3 id=\"Modification-Revision\"><a href=\"#Modification-Revision\" class=\"headerlink\" title=\"Modification Revision\"></a>Modification Revision</h3><hr>\n<p>保留对给定密钥的最后一次写入的第一个修订版。</p>\n<h3 id=\"Peer\"><a href=\"#Peer\" class=\"headerlink\" title=\"Peer\"></a>Peer</h3><hr>\n<p>同一集群的另一个对等成员。</p>\n<h3 id=\"Proposal\"><a href=\"#Proposal\" class=\"headerlink\" title=\"Proposal\"></a>Proposal</h3><hr>\n<p>提案是需要通过raft协议的请求（例如，写入请求，配置更改请求）。</p>\n<h3 id=\"Quorum\"><a href=\"#Quorum\" class=\"headerlink\" title=\"Quorum\"></a>Quorum</h3><hr>\n<p>达成共识才能修改集群状态所需的活动成员数。 etcd需要拥有多数才能达到法定人数。</p>\n<h3 id=\"Revision\"><a href=\"#Revision\" class=\"headerlink\" title=\"Revision\"></a>Revision</h3><hr>\n<p>每次修改键空间时都会增加的64位群集范围计数器。</p>\n<h3 id=\"Role\"><a href=\"#Role\" class=\"headerlink\" title=\"Role\"></a>Role</h3><hr>\n<p>一组密钥范围内的许可单位，可以将其授予一组用户以进行访问控制。</p>\n<h3 id=\"Snapshot\"><a href=\"#Snapshot\" class=\"headerlink\" title=\"Snapshot\"></a>Snapshot</h3><hr>\n<p>etcd群集状态的时间点备份。</p>\n<h3 id=\"Store\"><a href=\"#Store\" class=\"headerlink\" title=\"Store\"></a>Store</h3><hr>\n<p>支持集群键空间的物理存储。</p>\n<h3 id=\"Transaction\"><a href=\"#Transaction\" class=\"headerlink\" title=\"Transaction\"></a>Transaction</h3><hr>\n<p>原子执行的一组操作。 事务中所有已修改的键共享相同的修改版本。</p>\n<h3 id=\"Key-Version\"><a href=\"#Key-Version\" class=\"headerlink\" title=\"Key Version\"></a>Key Version</h3><hr>\n<p>自创建以来，对密钥的写入次数（从1开始）。不存在或已删除的密钥的版本为0。</p>\n<h3 id=\"Watcher\"><a href=\"#Watcher\" class=\"headerlink\" title=\"Watcher\"></a>Watcher</h3><hr>\n<p>客户端打开观察者以观察给定键范围上的更新。</p>\n"},{"title":"运行时重新配置设计","date":"2019-11-23T04:32:22.000Z","_content":"原文地址:[the runtime configuration design](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-reconf-design.md)\n运行时重新配置是分布式系统中最难，最容易出错的部分，尤其是在基于共识(像etcd)的系统中。\n阅读并学习关于etcd的运行时重新配置命令设计和如何追溯这些错误.\n### 两阶段配置更新保证集群安全\n* * *\n在etcd中，每一次运行时重新配置安全的原因是由于[两阶段](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)更新。例如，添加一个成员，首先将新配置通知集群后启动新的成员。\n\n1. 阶段一 通知集群关于新的配置\n添加一个成员到etcd集群中，通过API调用请求将一个新成员添加到集群中。这是将新的成员添加到集群中唯一的方法。当集群同意配置的更新后将返回API的调用。\n2. 阶段二 启动一个新的成员\n将一个新成员加入到存在的集群中，指定正确的`initial-cluster`和设置`initial-cluster-state`为`existing`.当成员启动后，它首先联系已存在的集群并验证当前集群配置是否和期望的`initial-cluster`匹配。当一个新的成员成功启动，集群将获得期望的配置。\n\n用户将过程分为两个阶段需要清楚了解集群成员关系的变化。实际上，这为用户提供了更大的灵活性，并使事情更容易。例如，如果试图添加一个与集群中现有的成员Id相同的新成员到集群中，操作将会立即失败由于阶段一并没有影响到运行中的集群。提供了类似的保护阻止通过错误操作添加新的成员。如果一个新的etcd成员试图在集群接受配置信息更新之前加入集群，操作将不会被集群接受。\n\n如果没有围绕集群成员关系的显式工作流，集群将会容易受到意料之外的集群成员关系变化的影响。例如，如果etcd在一个初始化的系统如systemd中运行，etcd将会通过成员关系API在重新启动之后被移除，并试图在启动后重新加入。这个循环将会在每次通过API成员移除并将系统设置为失败后重新启动etcd时继续，这是预料之外的。\n\n我们希望运行时重新进行配置是不常见的操作。我们决定保持为显式的由用户驱动来确保配置安全，保持集群平稳运行在显式的控制下。\n\n### 永久性的丢失要求新的集群\n* * *\n如果一个集群永久丢失一些主要的集群成员，需要从原始的数据文件夹启动一个新的集群恢复先前的状态。\n\n完全有可能从已存在的集群中强制删除一个失败的成员并恢复。然而，我们决定不支持此方法因为他绕过了常规的共识提交阶段，这是不安全的。如果成员移除一个没有实际失败的成员或者是同一个集群中的不同成员，etcd将会最终得到具有相同集群Id的分散集群。这是非常危险的而且很难修复。\n\n通过正常的部署，永久性丢失的可能性非常的小。但是这是一个严重的问题值得特别注意。我们强烈建议阅读[灾难恢复文档](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md)并且在将etcd部署到生产环境之前做充足的准备。\n\n### 不要在运行时重新配置中使用公共的发现服务\n* * *\n公共发现服务应该只在启动一个集群的时候使用。将一个成员加入已存在的集群，使用运行时配置API.\n\n发现服务被设计用来在云服务环境中启动一个在所有的成员无法提前知道Ip地址时的etcd集群。在成功启动一个集群时，所有的成员将会知道Ip地址。典型的，发现服务奖不再被需要。\n看起来使用公共的发现服务进行运行时重新配置是一个便利的方法,毕竟所有的发现服务含有所有的集群配置信息。然而依赖公共发现服务将带来问题：\n\n1. 将会引进外部独立性到集群的整个生命周期，不只是启动时间。如果集群和公共发现服务之间存在网络问题，则群集将因此受到影响。\n2. 公共发现服务必须在集群的生命周期内反映正确的运行时配置，将需要提供安全机制避免坏的行为，而这是困难的。\n3. 公共发现服务需要保持数万个集群的配置，而我们的公共发现服务很难承受这种负载。\n\n为了使发现服务支持运行时配置，最好的选择是建立一个私有的发现服务。","source":"_posts/blog/etcd/运行时重新配置设计.md","raw":"---\ntitle: 运行时重新配置设计\ndate: 2019-11-23 12:32:22\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址:[the runtime configuration design](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-reconf-design.md)\n运行时重新配置是分布式系统中最难，最容易出错的部分，尤其是在基于共识(像etcd)的系统中。\n阅读并学习关于etcd的运行时重新配置命令设计和如何追溯这些错误.\n### 两阶段配置更新保证集群安全\n* * *\n在etcd中，每一次运行时重新配置安全的原因是由于[两阶段](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)更新。例如，添加一个成员，首先将新配置通知集群后启动新的成员。\n\n1. 阶段一 通知集群关于新的配置\n添加一个成员到etcd集群中，通过API调用请求将一个新成员添加到集群中。这是将新的成员添加到集群中唯一的方法。当集群同意配置的更新后将返回API的调用。\n2. 阶段二 启动一个新的成员\n将一个新成员加入到存在的集群中，指定正确的`initial-cluster`和设置`initial-cluster-state`为`existing`.当成员启动后，它首先联系已存在的集群并验证当前集群配置是否和期望的`initial-cluster`匹配。当一个新的成员成功启动，集群将获得期望的配置。\n\n用户将过程分为两个阶段需要清楚了解集群成员关系的变化。实际上，这为用户提供了更大的灵活性，并使事情更容易。例如，如果试图添加一个与集群中现有的成员Id相同的新成员到集群中，操作将会立即失败由于阶段一并没有影响到运行中的集群。提供了类似的保护阻止通过错误操作添加新的成员。如果一个新的etcd成员试图在集群接受配置信息更新之前加入集群，操作将不会被集群接受。\n\n如果没有围绕集群成员关系的显式工作流，集群将会容易受到意料之外的集群成员关系变化的影响。例如，如果etcd在一个初始化的系统如systemd中运行，etcd将会通过成员关系API在重新启动之后被移除，并试图在启动后重新加入。这个循环将会在每次通过API成员移除并将系统设置为失败后重新启动etcd时继续，这是预料之外的。\n\n我们希望运行时重新进行配置是不常见的操作。我们决定保持为显式的由用户驱动来确保配置安全，保持集群平稳运行在显式的控制下。\n\n### 永久性的丢失要求新的集群\n* * *\n如果一个集群永久丢失一些主要的集群成员，需要从原始的数据文件夹启动一个新的集群恢复先前的状态。\n\n完全有可能从已存在的集群中强制删除一个失败的成员并恢复。然而，我们决定不支持此方法因为他绕过了常规的共识提交阶段，这是不安全的。如果成员移除一个没有实际失败的成员或者是同一个集群中的不同成员，etcd将会最终得到具有相同集群Id的分散集群。这是非常危险的而且很难修复。\n\n通过正常的部署，永久性丢失的可能性非常的小。但是这是一个严重的问题值得特别注意。我们强烈建议阅读[灾难恢复文档](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md)并且在将etcd部署到生产环境之前做充足的准备。\n\n### 不要在运行时重新配置中使用公共的发现服务\n* * *\n公共发现服务应该只在启动一个集群的时候使用。将一个成员加入已存在的集群，使用运行时配置API.\n\n发现服务被设计用来在云服务环境中启动一个在所有的成员无法提前知道Ip地址时的etcd集群。在成功启动一个集群时，所有的成员将会知道Ip地址。典型的，发现服务奖不再被需要。\n看起来使用公共的发现服务进行运行时重新配置是一个便利的方法,毕竟所有的发现服务含有所有的集群配置信息。然而依赖公共发现服务将带来问题：\n\n1. 将会引进外部独立性到集群的整个生命周期，不只是启动时间。如果集群和公共发现服务之间存在网络问题，则群集将因此受到影响。\n2. 公共发现服务必须在集群的生命周期内反映正确的运行时配置，将需要提供安全机制避免坏的行为，而这是困难的。\n3. 公共发现服务需要保持数万个集群的配置，而我们的公共发现服务很难承受这种负载。\n\n为了使发现服务支持运行时配置，最好的选择是建立一个私有的发现服务。","slug":"blog/etcd/运行时重新配置设计","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyid002tk0vq3tvwc4ro","content":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-reconf-design.md\" target=\"_blank\" rel=\"noopener\">the runtime configuration design</a><br>运行时重新配置是分布式系统中最难，最容易出错的部分，尤其是在基于共识(像etcd)的系统中。<br>阅读并学习关于etcd的运行时重新配置命令设计和如何追溯这些错误.</p>\n<h3 id=\"两阶段配置更新保证集群安全\"><a href=\"#两阶段配置更新保证集群安全\" class=\"headerlink\" title=\"两阶段配置更新保证集群安全\"></a>两阶段配置更新保证集群安全</h3><hr>\n<p>在etcd中，每一次运行时重新配置安全的原因是由于<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">两阶段</a>更新。例如，添加一个成员，首先将新配置通知集群后启动新的成员。</p>\n<ol>\n<li>阶段一 通知集群关于新的配置<br>添加一个成员到etcd集群中，通过API调用请求将一个新成员添加到集群中。这是将新的成员添加到集群中唯一的方法。当集群同意配置的更新后将返回API的调用。</li>\n<li>阶段二 启动一个新的成员<br>将一个新成员加入到存在的集群中，指定正确的<code>initial-cluster</code>和设置<code>initial-cluster-state</code>为<code>existing</code>.当成员启动后，它首先联系已存在的集群并验证当前集群配置是否和期望的<code>initial-cluster</code>匹配。当一个新的成员成功启动，集群将获得期望的配置。</li>\n</ol>\n<p>用户将过程分为两个阶段需要清楚了解集群成员关系的变化。实际上，这为用户提供了更大的灵活性，并使事情更容易。例如，如果试图添加一个与集群中现有的成员Id相同的新成员到集群中，操作将会立即失败由于阶段一并没有影响到运行中的集群。提供了类似的保护阻止通过错误操作添加新的成员。如果一个新的etcd成员试图在集群接受配置信息更新之前加入集群，操作将不会被集群接受。</p>\n<p>如果没有围绕集群成员关系的显式工作流，集群将会容易受到意料之外的集群成员关系变化的影响。例如，如果etcd在一个初始化的系统如systemd中运行，etcd将会通过成员关系API在重新启动之后被移除，并试图在启动后重新加入。这个循环将会在每次通过API成员移除并将系统设置为失败后重新启动etcd时继续，这是预料之外的。</p>\n<p>我们希望运行时重新进行配置是不常见的操作。我们决定保持为显式的由用户驱动来确保配置安全，保持集群平稳运行在显式的控制下。</p>\n<h3 id=\"永久性的丢失要求新的集群\"><a href=\"#永久性的丢失要求新的集群\" class=\"headerlink\" title=\"永久性的丢失要求新的集群\"></a>永久性的丢失要求新的集群</h3><hr>\n<p>如果一个集群永久丢失一些主要的集群成员，需要从原始的数据文件夹启动一个新的集群恢复先前的状态。</p>\n<p>完全有可能从已存在的集群中强制删除一个失败的成员并恢复。然而，我们决定不支持此方法因为他绕过了常规的共识提交阶段，这是不安全的。如果成员移除一个没有实际失败的成员或者是同一个集群中的不同成员，etcd将会最终得到具有相同集群Id的分散集群。这是非常危险的而且很难修复。</p>\n<p>通过正常的部署，永久性丢失的可能性非常的小。但是这是一个严重的问题值得特别注意。我们强烈建议阅读<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">灾难恢复文档</a>并且在将etcd部署到生产环境之前做充足的准备。</p>\n<h3 id=\"不要在运行时重新配置中使用公共的发现服务\"><a href=\"#不要在运行时重新配置中使用公共的发现服务\" class=\"headerlink\" title=\"不要在运行时重新配置中使用公共的发现服务\"></a>不要在运行时重新配置中使用公共的发现服务</h3><hr>\n<p>公共发现服务应该只在启动一个集群的时候使用。将一个成员加入已存在的集群，使用运行时配置API.</p>\n<p>发现服务被设计用来在云服务环境中启动一个在所有的成员无法提前知道Ip地址时的etcd集群。在成功启动一个集群时，所有的成员将会知道Ip地址。典型的，发现服务奖不再被需要。<br>看起来使用公共的发现服务进行运行时重新配置是一个便利的方法,毕竟所有的发现服务含有所有的集群配置信息。然而依赖公共发现服务将带来问题：</p>\n<ol>\n<li>将会引进外部独立性到集群的整个生命周期，不只是启动时间。如果集群和公共发现服务之间存在网络问题，则群集将因此受到影响。</li>\n<li>公共发现服务必须在集群的生命周期内反映正确的运行时配置，将需要提供安全机制避免坏的行为，而这是困难的。</li>\n<li>公共发现服务需要保持数万个集群的配置，而我们的公共发现服务很难承受这种负载。</li>\n</ol>\n<p>为了使发现服务支持运行时配置，最好的选择是建立一个私有的发现服务。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址:<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-reconf-design.md\" target=\"_blank\" rel=\"noopener\">the runtime configuration design</a><br>运行时重新配置是分布式系统中最难，最容易出错的部分，尤其是在基于共识(像etcd)的系统中。<br>阅读并学习关于etcd的运行时重新配置命令设计和如何追溯这些错误.</p>\n<h3 id=\"两阶段配置更新保证集群安全\"><a href=\"#两阶段配置更新保证集群安全\" class=\"headerlink\" title=\"两阶段配置更新保证集群安全\"></a>两阶段配置更新保证集群安全</h3><hr>\n<p>在etcd中，每一次运行时重新配置安全的原因是由于<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">两阶段</a>更新。例如，添加一个成员，首先将新配置通知集群后启动新的成员。</p>\n<ol>\n<li>阶段一 通知集群关于新的配置<br>添加一个成员到etcd集群中，通过API调用请求将一个新成员添加到集群中。这是将新的成员添加到集群中唯一的方法。当集群同意配置的更新后将返回API的调用。</li>\n<li>阶段二 启动一个新的成员<br>将一个新成员加入到存在的集群中，指定正确的<code>initial-cluster</code>和设置<code>initial-cluster-state</code>为<code>existing</code>.当成员启动后，它首先联系已存在的集群并验证当前集群配置是否和期望的<code>initial-cluster</code>匹配。当一个新的成员成功启动，集群将获得期望的配置。</li>\n</ol>\n<p>用户将过程分为两个阶段需要清楚了解集群成员关系的变化。实际上，这为用户提供了更大的灵活性，并使事情更容易。例如，如果试图添加一个与集群中现有的成员Id相同的新成员到集群中，操作将会立即失败由于阶段一并没有影响到运行中的集群。提供了类似的保护阻止通过错误操作添加新的成员。如果一个新的etcd成员试图在集群接受配置信息更新之前加入集群，操作将不会被集群接受。</p>\n<p>如果没有围绕集群成员关系的显式工作流，集群将会容易受到意料之外的集群成员关系变化的影响。例如，如果etcd在一个初始化的系统如systemd中运行，etcd将会通过成员关系API在重新启动之后被移除，并试图在启动后重新加入。这个循环将会在每次通过API成员移除并将系统设置为失败后重新启动etcd时继续，这是预料之外的。</p>\n<p>我们希望运行时重新进行配置是不常见的操作。我们决定保持为显式的由用户驱动来确保配置安全，保持集群平稳运行在显式的控制下。</p>\n<h3 id=\"永久性的丢失要求新的集群\"><a href=\"#永久性的丢失要求新的集群\" class=\"headerlink\" title=\"永久性的丢失要求新的集群\"></a>永久性的丢失要求新的集群</h3><hr>\n<p>如果一个集群永久丢失一些主要的集群成员，需要从原始的数据文件夹启动一个新的集群恢复先前的状态。</p>\n<p>完全有可能从已存在的集群中强制删除一个失败的成员并恢复。然而，我们决定不支持此方法因为他绕过了常规的共识提交阶段，这是不安全的。如果成员移除一个没有实际失败的成员或者是同一个集群中的不同成员，etcd将会最终得到具有相同集群Id的分散集群。这是非常危险的而且很难修复。</p>\n<p>通过正常的部署，永久性丢失的可能性非常的小。但是这是一个严重的问题值得特别注意。我们强烈建议阅读<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">灾难恢复文档</a>并且在将etcd部署到生产环境之前做充足的准备。</p>\n<h3 id=\"不要在运行时重新配置中使用公共的发现服务\"><a href=\"#不要在运行时重新配置中使用公共的发现服务\" class=\"headerlink\" title=\"不要在运行时重新配置中使用公共的发现服务\"></a>不要在运行时重新配置中使用公共的发现服务</h3><hr>\n<p>公共发现服务应该只在启动一个集群的时候使用。将一个成员加入已存在的集群，使用运行时配置API.</p>\n<p>发现服务被设计用来在云服务环境中启动一个在所有的成员无法提前知道Ip地址时的etcd集群。在成功启动一个集群时，所有的成员将会知道Ip地址。典型的，发现服务奖不再被需要。<br>看起来使用公共的发现服务进行运行时重新配置是一个便利的方法,毕竟所有的发现服务含有所有的集群配置信息。然而依赖公共发现服务将带来问题：</p>\n<ol>\n<li>将会引进外部独立性到集群的整个生命周期，不只是启动时间。如果集群和公共发现服务之间存在网络问题，则群集将因此受到影响。</li>\n<li>公共发现服务必须在集群的生命周期内反映正确的运行时配置，将需要提供安全机制避免坏的行为，而这是困难的。</li>\n<li>公共发现服务需要保持数万个集群的配置，而我们的公共发现服务很难承受这种负载。</li>\n</ol>\n<p>为了使发现服务支持运行时配置，最好的选择是建立一个私有的发现服务。</p>\n"},{"title":"运行时重新配置","date":"2019-11-23T10:08:38.000Z","_content":"原文地址：[runtime reconfiguration](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-configuration.md)\netcd带有增量运行时重新配置的支持。允许我们在集群运行的时候更新集群成员关系。\n仅当大多数集群成员都在运行时，才能处理重新配置请求，强烈建议在生产环境中集群的大小应该始终大于2。从两个成员的集群中移除一个成员是不安全的。两个成员的集群中大多数成员始终是2，如果在删除过程中出现故障，集群将很难继续运行需要重新从[主要成员失败中重新启动集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)。\n为了更好的理解运行时重新配置设计，请阅读[运行时重新配置设计](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E8%AE%BE%E8%AE%A1/)。\n## 重新配置使用案例\n* * *\n本节将介绍一些重新配置集群的常见原因。 其中大多数原因仅涉及添加或删除成员的组合，[群集重新配置操作](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)下将对此进行说明。\n### 循环或更新多机\n如果由于计划的维护（硬件升级，网络停机等）而需要移动多个群集成员，建议一次修改一个成员。\n移除领导者是安全的，但是在选举过程中会出现短暂的停机时间。 如果群集包含的版本为v2的数据超过50MB，则建议迁移成员的数据目录。\n### 改变集群大小\n增加群集大小可以增强[容错能力](https://github.com/etcd-io/etcd/blob/master/Documentation/v2/admin_guide.md#fault-tolerance-table)并提供更好的读取性能,由于客户端可以从任何成员读取，因此增加成员数量将增加整体序列化读取吞吐量。\n减小群集大小可以提高群集的写入性能，但需要权衡降低弹性。写入集群之前，会将其复制到集群的大多数成员。 减小群集大小可减少大多数操作，并且每次写入的提交速度都会更快。\n### 替换一个失败的主机\n如果计算机由于硬件故障，数据目录损坏或其他致命情况而失败，应该尽快更换它。 发生故障但尚未移除的主机会对集群产生不利影响，并降低对其他故障的容忍度。\n要更换主机，请按照说明从群集中[删除成员](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)，然后在其位置[添加新成员](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)。如果群集拥有的空间超过50MB，则建议迁移仍可访问的失败成员的数据目录。\n### 多数主机失败后重启集群\n如果大多数群集丢失或所有节点的IP地址都已更改，则必须采取手动操作才能安全恢复。恢复过程中的基本步骤包括[使用旧数据创建新集群](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md)，强制单个成员充当领导者，最后使用运行时配置一次将[新成员添加](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)到该新集群中。\n\n## 集群重新配置操作\n考虑到这些用例，可以针对每个用例进行描述。进行任何更改之前，必须有多数etcd成员可以获取。 对于对etcd的任何类型的写入，这基本上是相同的要求。\n必须按顺序完成对集群的所有更改：\n\n* 要更新单个成员节点URL，请执行更新操作.\n* 要替换正常的单个成员，请删除旧成员，然后添加新成员.\n* 要从3名增加到5名成员，请执行两次添加操作\n* 成员数量要从5减少到3，请执行两次删除操作\n\n这些示例都使用etcd附带的`etcdctl`命令行工具进行。如果不使用`etcdctl`工具改变成员关系，使用[v2HTTP成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/v2/members_api.md)或者[v3gRPC成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto)。\n###  更新一个成员\n**更新广播客户端URLs**\n要更新成员的发布客户端URL，只需使用已更新的客户端URL参数`--advertise-client-urls`或环境变量`ETCD_ADVERTISE_CLIENT_URLS`重新启动该成员。重新启动的成员将自行发布更新的URL。 错误更新的客户端URL不会影响etcd群集的运行状况。\n**更新广播节点URLs**\n要更新成员的广播节点URL，请首先通过成员命令显式更新它，然后重新启动该成员。由于更新节点URL会更改集群范围的配置，并且可能影响etcd集群的运行状况，因此需要采取其他措施。\n要更新成员的广播节点URL，请首先找到目标成员的ID。 列出具有etcdctl的所有成员：\n```\n$ etcdctl member list\n6e3bd23ae5f1eae0: name=node2 peerURLs=http://localhost:23802 clientURLs=http://127.0.0.1:23792\n924e2e83e93f2560: name=node3 peerURLs=http://localhost:23803 clientURLs=http://127.0.0.1:23793\na8266ecf031671f3: name=node1 peerURLs=http://localhost:23801 clientURLs=http://127.0.0.1:23791\n```\n本示例将`更新`a8266ecf031671f3成员ID，并将其节点URLs值更改为`http://10.0.1.10:2380`：\n```\n$ etcdctl member update a8266ecf031671f3 --peer-urls=http://10.0.1.10:2380\nUpdated member with ID a8266ecf031671f3 in cluster\n```\n**移除一个成员**\n假设要移除的成员ID为a8266ecf031671f3。 使用`remove`命令执行删除：\n```\n$ etcdctl member remove a8266ecf031671f3\nRemoved member a8266ecf031671f3 from cluster\n```\n目标成员将在此时停止运行并在日志中打印出删除内容：\n```\netcd: this member has been permanently removed from the cluster. Exiting.\n```\n删除领导者是安全的，但是当选择新领导者时，群集将处于非活动状态。 此持续时间通常是选举超时时间加上投票过程的时间。\n**添加一个新成员**\n通过两个步骤添加一个新的成员：\n\n* 通过[HTTP 成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/v2/members_api.md)，[gRPC成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto)，或者是`etcdctl member add`命令添加一个新的成员到集群中。\n* 通过新的集群配置启动新的成员，新的集群配置包括被更新的成员(已存在的成员+新成员).\n\n`etcdctl`添加一个新的成员到集群中通过具体的成员[名字](https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/)和[广播节点URLs](https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/):\n```\n$ etcdctl member add infra3 --peer-urls=http://10.0.1.13:2380\nadded member 9bf1b35fc7761a23 to cluster\n\nETCD_NAME=\"infra3\"\nETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380\"\nETCD_INITIAL_CLUSTER_STATE=existing\n```\n`etcdctl`已将新成员通知集群，并打印出成功启动集群所需的环境变量。 现在，使用新成员的相关参数启动新的etcd进程：\n```\n$ export ETCD_NAME=\"infra3\"\n$ export ETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380\"\n$ export ETCD_INITIAL_CLUSTER_STATE=existing\n$ etcd --listen-client-urls http://10.0.1.13:2379 --advertise-client-urls http://10.0.1.13:2379 --listen-peer-urls http://10.0.1.13:2380 --initial-advertise-peer-urls http://10.0.1.13:2380 --data-dir %data_dir%\n```\n新成员将作为集群的一部分运行，并立即开始同步集群的其余部分。\n如果添加多个成员，最佳做法是一次配置一个成员，并在添加更多新成员之前验证它是否正确启动.如果将新成员添加到1节点群集中，则群集无法在新成员启动之前取得进展，因为它需要两个成员作为多数才能达成共识。仅在`etcdctl``member add`通知集群有关新成员的时间和新成员成功建立与现有成员的连接的时间之间，才发生此行为。\n**添加一个新的成员为领导者**\n从v3.4开始，etcd支持将新成员添加为领导者/非投票成员。激励和设计可以在[设计文档](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md)中找到。为了使添加新成员的过程更安全，并减少添加新成员时的集群停机时间.建议将新成员作为学习者添加到集群中，直到同步完成为止。 这可以描述为三步过程：\n\n* 通过[gRPC成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto)或者`etcdctl member add --learner`命令将新成员添加为学习者。\n* 通过新的集群配置启动新的成员，新的集群配置包括被更新的成员(已存在的成员+新成员)和之前的步骤相同.\n* 通过[gRPC成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto)或`etcdctl member promote`命令将新添加的学习者提升为有投票权的成员。etcd服务器验证升级请求以确保其运行安全.只有在其Raft日志达到领导者的水平之后，才能将学习者提升为有投票权的成员。如果学习者成员未赶上领导者的Raft日志，则成员升级请求将失败(见[提升成员错误案例]()部分获取更多细节).这种情况下，用户应该等待并重试。\n\n在v3.4中，etcd服务器将群集可以拥有的学习者数量限制为一个。 主要考虑因素是限制由于领导者向学习者传播数据而导致的领导者额外工作量。\n使用`etcdctl member add`和参数`--learner`添加一个新成员作为学习者到集群中.\n```\n$ etcdctl member add infra3 --peer-urls=http://10.0.1.13:2380 --learner\nMember 9bf1b35fc7761a23 added to cluster a7ef944b95711739\n\nETCD_NAME=\"infra3\"\nETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380\"\nETCD_INITIAL_CLUSTER_STATE=existing\n```\n新的etcd程序添加新的学习者成员启动后，使用`etcdctl member promote`将学习者提升为投票成员。\n```\n$ etcdctl member promote 9bf1b35fc7761a23\nMember 9e29bbaa45d74461 promoted in cluster a7ef944b95711739\n```\n**添加成员错误案例**\n在以下情况下，新主机不包含在枚举节点列表中。 如果这是一个新集群，则必须将该节点添加到初始集群成员列表中。\n```\n$ etcd --name infra3 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state existing\netcdserver: assign ids error: the member count is unequal\nexit 1\n```\n在这种情况下，使用了与用于加入集群的地址（10.0.1.13:2380）不同的地址（10.0.1.14:2380）：\n```\n$ etcd --name infra4 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra4=http://10.0.1.14:2380 \\\n  --initial-cluster-state existing\netcdserver: assign ids error: unmatched member while checking PeerURLs\nexit 1\n```\n如果etcd开始使用已删除成员的数据目录，则etcd如果连接到集群中的任何活动成员，则会自动退出：\n```\n$ etcd\netcd: this member has been permanently removed from the cluster. Exiting.\nexit 1\n```\n**添加成员为领导者错误案例**\n当集群中含有一个领导者时不能添加领导者到集群中(v3.4):\n```\n$ etcdctl member add infra4 --peer-urls=http://10.0.1.14:2380 --learner\nError: etcdserver: too many learner members in cluster\n```\n**提升成员为领导者错误案例**\n如果学习者与领导者同步，则只能被提升为有投票权的成员。\n```\n$ etcdctl member promote 9bf1b35fc7761a23\nError: etcdserver: can only promote a learner member which is in sync with leader\n```\n提升不是学习者的成员将失败。\n```\n$ etcdctl member promote 9bf1b35fc7761a23\nError: etcdserver: can only promote a learner member\n```\n提升一个集群中不存在的成员将会失败：\n```\n$ etcdctl member promote 12345abcde\nError: etcdserver: member not found\n```\n### 严格的重新配置检查模式(`-strict-reconfig-check`)\n如上所述，添加新成员的最佳实践是一次配置一个成员，并在添加更多新成员之前验证它是否正确启动。这种分步方法非常重要，因为如果未正确配置新添加的成员（例如，对等URL不正确），则群集可能会丢失仲裁。","source":"_posts/blog/etcd/运行时重新配置.md","raw":"---\ntitle: 运行时重新配置\ndate: 2019-11-23 18:08:38\ntags: etcd\ncategories: etcd文档翻译\n---\n原文地址：[runtime reconfiguration](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-configuration.md)\netcd带有增量运行时重新配置的支持。允许我们在集群运行的时候更新集群成员关系。\n仅当大多数集群成员都在运行时，才能处理重新配置请求，强烈建议在生产环境中集群的大小应该始终大于2。从两个成员的集群中移除一个成员是不安全的。两个成员的集群中大多数成员始终是2，如果在删除过程中出现故障，集群将很难继续运行需要重新从[主要成员失败中重新启动集群](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)。\n为了更好的理解运行时重新配置设计，请阅读[运行时重新配置设计](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E8%AE%BE%E8%AE%A1/)。\n## 重新配置使用案例\n* * *\n本节将介绍一些重新配置集群的常见原因。 其中大多数原因仅涉及添加或删除成员的组合，[群集重新配置操作](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)下将对此进行说明。\n### 循环或更新多机\n如果由于计划的维护（硬件升级，网络停机等）而需要移动多个群集成员，建议一次修改一个成员。\n移除领导者是安全的，但是在选举过程中会出现短暂的停机时间。 如果群集包含的版本为v2的数据超过50MB，则建议迁移成员的数据目录。\n### 改变集群大小\n增加群集大小可以增强[容错能力](https://github.com/etcd-io/etcd/blob/master/Documentation/v2/admin_guide.md#fault-tolerance-table)并提供更好的读取性能,由于客户端可以从任何成员读取，因此增加成员数量将增加整体序列化读取吞吐量。\n减小群集大小可以提高群集的写入性能，但需要权衡降低弹性。写入集群之前，会将其复制到集群的大多数成员。 减小群集大小可减少大多数操作，并且每次写入的提交速度都会更快。\n### 替换一个失败的主机\n如果计算机由于硬件故障，数据目录损坏或其他致命情况而失败，应该尽快更换它。 发生故障但尚未移除的主机会对集群产生不利影响，并降低对其他故障的容忍度。\n要更换主机，请按照说明从群集中[删除成员](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)，然后在其位置[添加新成员](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)。如果群集拥有的空间超过50MB，则建议迁移仍可访问的失败成员的数据目录。\n### 多数主机失败后重启集群\n如果大多数群集丢失或所有节点的IP地址都已更改，则必须采取手动操作才能安全恢复。恢复过程中的基本步骤包括[使用旧数据创建新集群](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md)，强制单个成员充当领导者，最后使用运行时配置一次将[新成员添加](https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/)到该新集群中。\n\n## 集群重新配置操作\n考虑到这些用例，可以针对每个用例进行描述。进行任何更改之前，必须有多数etcd成员可以获取。 对于对etcd的任何类型的写入，这基本上是相同的要求。\n必须按顺序完成对集群的所有更改：\n\n* 要更新单个成员节点URL，请执行更新操作.\n* 要替换正常的单个成员，请删除旧成员，然后添加新成员.\n* 要从3名增加到5名成员，请执行两次添加操作\n* 成员数量要从5减少到3，请执行两次删除操作\n\n这些示例都使用etcd附带的`etcdctl`命令行工具进行。如果不使用`etcdctl`工具改变成员关系，使用[v2HTTP成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/v2/members_api.md)或者[v3gRPC成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto)。\n###  更新一个成员\n**更新广播客户端URLs**\n要更新成员的发布客户端URL，只需使用已更新的客户端URL参数`--advertise-client-urls`或环境变量`ETCD_ADVERTISE_CLIENT_URLS`重新启动该成员。重新启动的成员将自行发布更新的URL。 错误更新的客户端URL不会影响etcd群集的运行状况。\n**更新广播节点URLs**\n要更新成员的广播节点URL，请首先通过成员命令显式更新它，然后重新启动该成员。由于更新节点URL会更改集群范围的配置，并且可能影响etcd集群的运行状况，因此需要采取其他措施。\n要更新成员的广播节点URL，请首先找到目标成员的ID。 列出具有etcdctl的所有成员：\n```\n$ etcdctl member list\n6e3bd23ae5f1eae0: name=node2 peerURLs=http://localhost:23802 clientURLs=http://127.0.0.1:23792\n924e2e83e93f2560: name=node3 peerURLs=http://localhost:23803 clientURLs=http://127.0.0.1:23793\na8266ecf031671f3: name=node1 peerURLs=http://localhost:23801 clientURLs=http://127.0.0.1:23791\n```\n本示例将`更新`a8266ecf031671f3成员ID，并将其节点URLs值更改为`http://10.0.1.10:2380`：\n```\n$ etcdctl member update a8266ecf031671f3 --peer-urls=http://10.0.1.10:2380\nUpdated member with ID a8266ecf031671f3 in cluster\n```\n**移除一个成员**\n假设要移除的成员ID为a8266ecf031671f3。 使用`remove`命令执行删除：\n```\n$ etcdctl member remove a8266ecf031671f3\nRemoved member a8266ecf031671f3 from cluster\n```\n目标成员将在此时停止运行并在日志中打印出删除内容：\n```\netcd: this member has been permanently removed from the cluster. Exiting.\n```\n删除领导者是安全的，但是当选择新领导者时，群集将处于非活动状态。 此持续时间通常是选举超时时间加上投票过程的时间。\n**添加一个新成员**\n通过两个步骤添加一个新的成员：\n\n* 通过[HTTP 成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/v2/members_api.md)，[gRPC成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto)，或者是`etcdctl member add`命令添加一个新的成员到集群中。\n* 通过新的集群配置启动新的成员，新的集群配置包括被更新的成员(已存在的成员+新成员).\n\n`etcdctl`添加一个新的成员到集群中通过具体的成员[名字](https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/)和[广播节点URLs](https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/):\n```\n$ etcdctl member add infra3 --peer-urls=http://10.0.1.13:2380\nadded member 9bf1b35fc7761a23 to cluster\n\nETCD_NAME=\"infra3\"\nETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380\"\nETCD_INITIAL_CLUSTER_STATE=existing\n```\n`etcdctl`已将新成员通知集群，并打印出成功启动集群所需的环境变量。 现在，使用新成员的相关参数启动新的etcd进程：\n```\n$ export ETCD_NAME=\"infra3\"\n$ export ETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380\"\n$ export ETCD_INITIAL_CLUSTER_STATE=existing\n$ etcd --listen-client-urls http://10.0.1.13:2379 --advertise-client-urls http://10.0.1.13:2379 --listen-peer-urls http://10.0.1.13:2380 --initial-advertise-peer-urls http://10.0.1.13:2380 --data-dir %data_dir%\n```\n新成员将作为集群的一部分运行，并立即开始同步集群的其余部分。\n如果添加多个成员，最佳做法是一次配置一个成员，并在添加更多新成员之前验证它是否正确启动.如果将新成员添加到1节点群集中，则群集无法在新成员启动之前取得进展，因为它需要两个成员作为多数才能达成共识。仅在`etcdctl``member add`通知集群有关新成员的时间和新成员成功建立与现有成员的连接的时间之间，才发生此行为。\n**添加一个新的成员为领导者**\n从v3.4开始，etcd支持将新成员添加为领导者/非投票成员。激励和设计可以在[设计文档](https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md)中找到。为了使添加新成员的过程更安全，并减少添加新成员时的集群停机时间.建议将新成员作为学习者添加到集群中，直到同步完成为止。 这可以描述为三步过程：\n\n* 通过[gRPC成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto)或者`etcdctl member add --learner`命令将新成员添加为学习者。\n* 通过新的集群配置启动新的成员，新的集群配置包括被更新的成员(已存在的成员+新成员)和之前的步骤相同.\n* 通过[gRPC成员API](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto)或`etcdctl member promote`命令将新添加的学习者提升为有投票权的成员。etcd服务器验证升级请求以确保其运行安全.只有在其Raft日志达到领导者的水平之后，才能将学习者提升为有投票权的成员。如果学习者成员未赶上领导者的Raft日志，则成员升级请求将失败(见[提升成员错误案例]()部分获取更多细节).这种情况下，用户应该等待并重试。\n\n在v3.4中，etcd服务器将群集可以拥有的学习者数量限制为一个。 主要考虑因素是限制由于领导者向学习者传播数据而导致的领导者额外工作量。\n使用`etcdctl member add`和参数`--learner`添加一个新成员作为学习者到集群中.\n```\n$ etcdctl member add infra3 --peer-urls=http://10.0.1.13:2380 --learner\nMember 9bf1b35fc7761a23 added to cluster a7ef944b95711739\n\nETCD_NAME=\"infra3\"\nETCD_INITIAL_CLUSTER=\"infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380\"\nETCD_INITIAL_CLUSTER_STATE=existing\n```\n新的etcd程序添加新的学习者成员启动后，使用`etcdctl member promote`将学习者提升为投票成员。\n```\n$ etcdctl member promote 9bf1b35fc7761a23\nMember 9e29bbaa45d74461 promoted in cluster a7ef944b95711739\n```\n**添加成员错误案例**\n在以下情况下，新主机不包含在枚举节点列表中。 如果这是一个新集群，则必须将该节点添加到初始集群成员列表中。\n```\n$ etcd --name infra3 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state existing\netcdserver: assign ids error: the member count is unequal\nexit 1\n```\n在这种情况下，使用了与用于加入集群的地址（10.0.1.13:2380）不同的地址（10.0.1.14:2380）：\n```\n$ etcd --name infra4 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra4=http://10.0.1.14:2380 \\\n  --initial-cluster-state existing\netcdserver: assign ids error: unmatched member while checking PeerURLs\nexit 1\n```\n如果etcd开始使用已删除成员的数据目录，则etcd如果连接到集群中的任何活动成员，则会自动退出：\n```\n$ etcd\netcd: this member has been permanently removed from the cluster. Exiting.\nexit 1\n```\n**添加成员为领导者错误案例**\n当集群中含有一个领导者时不能添加领导者到集群中(v3.4):\n```\n$ etcdctl member add infra4 --peer-urls=http://10.0.1.14:2380 --learner\nError: etcdserver: too many learner members in cluster\n```\n**提升成员为领导者错误案例**\n如果学习者与领导者同步，则只能被提升为有投票权的成员。\n```\n$ etcdctl member promote 9bf1b35fc7761a23\nError: etcdserver: can only promote a learner member which is in sync with leader\n```\n提升不是学习者的成员将失败。\n```\n$ etcdctl member promote 9bf1b35fc7761a23\nError: etcdserver: can only promote a learner member\n```\n提升一个集群中不存在的成员将会失败：\n```\n$ etcdctl member promote 12345abcde\nError: etcdserver: member not found\n```\n### 严格的重新配置检查模式(`-strict-reconfig-check`)\n如上所述，添加新成员的最佳实践是一次配置一个成员，并在添加更多新成员之前验证它是否正确启动。这种分步方法非常重要，因为如果未正确配置新添加的成员（例如，对等URL不正确），则群集可能会丢失仲裁。","slug":"blog/etcd/运行时重新配置","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyij002vk0vq4vef4acb","content":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-configuration.md\" target=\"_blank\" rel=\"noopener\">runtime reconfiguration</a><br>etcd带有增量运行时重新配置的支持。允许我们在集群运行的时候更新集群成员关系。<br>仅当大多数集群成员都在运行时，才能处理重新配置请求，强烈建议在生产环境中集群的大小应该始终大于2。从两个成员的集群中移除一个成员是不安全的。两个成员的集群中大多数成员始终是2，如果在删除过程中出现故障，集群将很难继续运行需要重新从<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">主要成员失败中重新启动集群</a>。<br>为了更好的理解运行时重新配置设计，请阅读<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E8%AE%BE%E8%AE%A1/\">运行时重新配置设计</a>。</p>\n<h2 id=\"重新配置使用案例\"><a href=\"#重新配置使用案例\" class=\"headerlink\" title=\"重新配置使用案例\"></a>重新配置使用案例</h2><hr>\n<p>本节将介绍一些重新配置集群的常见原因。 其中大多数原因仅涉及添加或删除成员的组合，<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">群集重新配置操作</a>下将对此进行说明。</p>\n<h3 id=\"循环或更新多机\"><a href=\"#循环或更新多机\" class=\"headerlink\" title=\"循环或更新多机\"></a>循环或更新多机</h3><p>如果由于计划的维护（硬件升级，网络停机等）而需要移动多个群集成员，建议一次修改一个成员。<br>移除领导者是安全的，但是在选举过程中会出现短暂的停机时间。 如果群集包含的版本为v2的数据超过50MB，则建议迁移成员的数据目录。</p>\n<h3 id=\"改变集群大小\"><a href=\"#改变集群大小\" class=\"headerlink\" title=\"改变集群大小\"></a>改变集群大小</h3><p>增加群集大小可以增强<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/v2/admin_guide.md#fault-tolerance-table\" target=\"_blank\" rel=\"noopener\">容错能力</a>并提供更好的读取性能,由于客户端可以从任何成员读取，因此增加成员数量将增加整体序列化读取吞吐量。<br>减小群集大小可以提高群集的写入性能，但需要权衡降低弹性。写入集群之前，会将其复制到集群的大多数成员。 减小群集大小可减少大多数操作，并且每次写入的提交速度都会更快。</p>\n<h3 id=\"替换一个失败的主机\"><a href=\"#替换一个失败的主机\" class=\"headerlink\" title=\"替换一个失败的主机\"></a>替换一个失败的主机</h3><p>如果计算机由于硬件故障，数据目录损坏或其他致命情况而失败，应该尽快更换它。 发生故障但尚未移除的主机会对集群产生不利影响，并降低对其他故障的容忍度。<br>要更换主机，请按照说明从群集中<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">删除成员</a>，然后在其位置<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">添加新成员</a>。如果群集拥有的空间超过50MB，则建议迁移仍可访问的失败成员的数据目录。</p>\n<h3 id=\"多数主机失败后重启集群\"><a href=\"#多数主机失败后重启集群\" class=\"headerlink\" title=\"多数主机失败后重启集群\"></a>多数主机失败后重启集群</h3><p>如果大多数群集丢失或所有节点的IP地址都已更改，则必须采取手动操作才能安全恢复。恢复过程中的基本步骤包括<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">使用旧数据创建新集群</a>，强制单个成员充当领导者，最后使用运行时配置一次将<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">新成员添加</a>到该新集群中。</p>\n<h2 id=\"集群重新配置操作\"><a href=\"#集群重新配置操作\" class=\"headerlink\" title=\"集群重新配置操作\"></a>集群重新配置操作</h2><p>考虑到这些用例，可以针对每个用例进行描述。进行任何更改之前，必须有多数etcd成员可以获取。 对于对etcd的任何类型的写入，这基本上是相同的要求。<br>必须按顺序完成对集群的所有更改：</p>\n<ul>\n<li>要更新单个成员节点URL，请执行更新操作.</li>\n<li>要替换正常的单个成员，请删除旧成员，然后添加新成员.</li>\n<li>要从3名增加到5名成员，请执行两次添加操作</li>\n<li>成员数量要从5减少到3，请执行两次删除操作</li>\n</ul>\n<p>这些示例都使用etcd附带的<code>etcdctl</code>命令行工具进行。如果不使用<code>etcdctl</code>工具改变成员关系，使用<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/v2/members_api.md\" target=\"_blank\" rel=\"noopener\">v2HTTP成员API</a>或者<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto\" target=\"_blank\" rel=\"noopener\">v3gRPC成员API</a>。</p>\n<h3 id=\"更新一个成员\"><a href=\"#更新一个成员\" class=\"headerlink\" title=\"更新一个成员\"></a>更新一个成员</h3><p><strong>更新广播客户端URLs</strong><br>要更新成员的发布客户端URL，只需使用已更新的客户端URL参数<code>--advertise-client-urls</code>或环境变量<code>ETCD_ADVERTISE_CLIENT_URLS</code>重新启动该成员。重新启动的成员将自行发布更新的URL。 错误更新的客户端URL不会影响etcd群集的运行状况。<br><strong>更新广播节点URLs</strong><br>要更新成员的广播节点URL，请首先通过成员命令显式更新它，然后重新启动该成员。由于更新节点URL会更改集群范围的配置，并且可能影响etcd集群的运行状况，因此需要采取其他措施。<br>要更新成员的广播节点URL，请首先找到目标成员的ID。 列出具有etcdctl的所有成员：</p>\n<pre><code>$ etcdctl member list\n6e3bd23ae5f1eae0: name=node2 peerURLs=http://localhost:23802 clientURLs=http://127.0.0.1:23792\n924e2e83e93f2560: name=node3 peerURLs=http://localhost:23803 clientURLs=http://127.0.0.1:23793\na8266ecf031671f3: name=node1 peerURLs=http://localhost:23801 clientURLs=http://127.0.0.1:23791</code></pre><p>本示例将<code>更新</code>a8266ecf031671f3成员ID，并将其节点URLs值更改为<code>http://10.0.1.10:2380</code>：</p>\n<pre><code>$ etcdctl member update a8266ecf031671f3 --peer-urls=http://10.0.1.10:2380\nUpdated member with ID a8266ecf031671f3 in cluster</code></pre><p><strong>移除一个成员</strong><br>假设要移除的成员ID为a8266ecf031671f3。 使用<code>remove</code>命令执行删除：</p>\n<pre><code>$ etcdctl member remove a8266ecf031671f3\nRemoved member a8266ecf031671f3 from cluster</code></pre><p>目标成员将在此时停止运行并在日志中打印出删除内容：</p>\n<pre><code>etcd: this member has been permanently removed from the cluster. Exiting.</code></pre><p>删除领导者是安全的，但是当选择新领导者时，群集将处于非活动状态。 此持续时间通常是选举超时时间加上投票过程的时间。<br><strong>添加一个新成员</strong><br>通过两个步骤添加一个新的成员：</p>\n<ul>\n<li>通过<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/v2/members_api.md\" target=\"_blank\" rel=\"noopener\">HTTP 成员API</a>，<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto\" target=\"_blank\" rel=\"noopener\">gRPC成员API</a>，或者是<code>etcdctl member add</code>命令添加一个新的成员到集群中。</li>\n<li>通过新的集群配置启动新的成员，新的集群配置包括被更新的成员(已存在的成员+新成员).</li>\n</ul>\n<p><code>etcdctl</code>添加一个新的成员到集群中通过具体的成员<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/\">名字</a>和<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/\">广播节点URLs</a>:</p>\n<pre><code>$ etcdctl member add infra3 --peer-urls=http://10.0.1.13:2380\nadded member 9bf1b35fc7761a23 to cluster\n\nETCD_NAME=&quot;infra3&quot;\nETCD_INITIAL_CLUSTER=&quot;infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380&quot;\nETCD_INITIAL_CLUSTER_STATE=existing</code></pre><p><code>etcdctl</code>已将新成员通知集群，并打印出成功启动集群所需的环境变量。 现在，使用新成员的相关参数启动新的etcd进程：</p>\n<pre><code>$ export ETCD_NAME=&quot;infra3&quot;\n$ export ETCD_INITIAL_CLUSTER=&quot;infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380&quot;\n$ export ETCD_INITIAL_CLUSTER_STATE=existing\n$ etcd --listen-client-urls http://10.0.1.13:2379 --advertise-client-urls http://10.0.1.13:2379 --listen-peer-urls http://10.0.1.13:2380 --initial-advertise-peer-urls http://10.0.1.13:2380 --data-dir %data_dir%</code></pre><p>新成员将作为集群的一部分运行，并立即开始同步集群的其余部分。<br>如果添加多个成员，最佳做法是一次配置一个成员，并在添加更多新成员之前验证它是否正确启动.如果将新成员添加到1节点群集中，则群集无法在新成员启动之前取得进展，因为它需要两个成员作为多数才能达成共识。仅在<code>etcdctl``member add</code>通知集群有关新成员的时间和新成员成功建立与现有成员的连接的时间之间，才发生此行为。<br><strong>添加一个新的成员为领导者</strong><br>从v3.4开始，etcd支持将新成员添加为领导者/非投票成员。激励和设计可以在<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md\" target=\"_blank\" rel=\"noopener\">设计文档</a>中找到。为了使添加新成员的过程更安全，并减少添加新成员时的集群停机时间.建议将新成员作为学习者添加到集群中，直到同步完成为止。 这可以描述为三步过程：</p>\n<ul>\n<li>通过<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto\" target=\"_blank\" rel=\"noopener\">gRPC成员API</a>或者<code>etcdctl member add --learner</code>命令将新成员添加为学习者。</li>\n<li>通过新的集群配置启动新的成员，新的集群配置包括被更新的成员(已存在的成员+新成员)和之前的步骤相同.</li>\n<li>通过<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto\" target=\"_blank\" rel=\"noopener\">gRPC成员API</a>或<code>etcdctl member promote</code>命令将新添加的学习者提升为有投票权的成员。etcd服务器验证升级请求以确保其运行安全.只有在其Raft日志达到领导者的水平之后，才能将学习者提升为有投票权的成员。如果学习者成员未赶上领导者的Raft日志，则成员升级请求将失败(见<a href=\"\">提升成员错误案例</a>部分获取更多细节).这种情况下，用户应该等待并重试。</li>\n</ul>\n<p>在v3.4中，etcd服务器将群集可以拥有的学习者数量限制为一个。 主要考虑因素是限制由于领导者向学习者传播数据而导致的领导者额外工作量。<br>使用<code>etcdctl member add</code>和参数<code>--learner</code>添加一个新成员作为学习者到集群中.</p>\n<pre><code>$ etcdctl member add infra3 --peer-urls=http://10.0.1.13:2380 --learner\nMember 9bf1b35fc7761a23 added to cluster a7ef944b95711739\n\nETCD_NAME=&quot;infra3&quot;\nETCD_INITIAL_CLUSTER=&quot;infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380&quot;\nETCD_INITIAL_CLUSTER_STATE=existing</code></pre><p>新的etcd程序添加新的学习者成员启动后，使用<code>etcdctl member promote</code>将学习者提升为投票成员。</p>\n<pre><code>$ etcdctl member promote 9bf1b35fc7761a23\nMember 9e29bbaa45d74461 promoted in cluster a7ef944b95711739</code></pre><p><strong>添加成员错误案例</strong><br>在以下情况下，新主机不包含在枚举节点列表中。 如果这是一个新集群，则必须将该节点添加到初始集群成员列表中。</p>\n<pre><code>$ etcd --name infra3 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state existing\netcdserver: assign ids error: the member count is unequal\nexit 1</code></pre><p>在这种情况下，使用了与用于加入集群的地址（10.0.1.13:2380）不同的地址（10.0.1.14:2380）：</p>\n<pre><code>$ etcd --name infra4 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra4=http://10.0.1.14:2380 \\\n  --initial-cluster-state existing\netcdserver: assign ids error: unmatched member while checking PeerURLs\nexit 1</code></pre><p>如果etcd开始使用已删除成员的数据目录，则etcd如果连接到集群中的任何活动成员，则会自动退出：</p>\n<pre><code>$ etcd\netcd: this member has been permanently removed from the cluster. Exiting.\nexit 1</code></pre><p><strong>添加成员为领导者错误案例</strong><br>当集群中含有一个领导者时不能添加领导者到集群中(v3.4):</p>\n<pre><code>$ etcdctl member add infra4 --peer-urls=http://10.0.1.14:2380 --learner\nError: etcdserver: too many learner members in cluster</code></pre><p><strong>提升成员为领导者错误案例</strong><br>如果学习者与领导者同步，则只能被提升为有投票权的成员。</p>\n<pre><code>$ etcdctl member promote 9bf1b35fc7761a23\nError: etcdserver: can only promote a learner member which is in sync with leader</code></pre><p>提升不是学习者的成员将失败。</p>\n<pre><code>$ etcdctl member promote 9bf1b35fc7761a23\nError: etcdserver: can only promote a learner member</code></pre><p>提升一个集群中不存在的成员将会失败：</p>\n<pre><code>$ etcdctl member promote 12345abcde\nError: etcdserver: member not found</code></pre><h3 id=\"严格的重新配置检查模式-strict-reconfig-check\"><a href=\"#严格的重新配置检查模式-strict-reconfig-check\" class=\"headerlink\" title=\"严格的重新配置检查模式(-strict-reconfig-check)\"></a>严格的重新配置检查模式(<code>-strict-reconfig-check</code>)</h3><p>如上所述，添加新成员的最佳实践是一次配置一个成员，并在添加更多新成员之前验证它是否正确启动。这种分步方法非常重要，因为如果未正确配置新添加的成员（例如，对等URL不正确），则群集可能会丢失仲裁。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>原文地址：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/runtime-configuration.md\" target=\"_blank\" rel=\"noopener\">runtime reconfiguration</a><br>etcd带有增量运行时重新配置的支持。允许我们在集群运行的时候更新集群成员关系。<br>仅当大多数集群成员都在运行时，才能处理重新配置请求，强烈建议在生产环境中集群的大小应该始终大于2。从两个成员的集群中移除一个成员是不安全的。两个成员的集群中大多数成员始终是2，如果在删除过程中出现故障，集群将很难继续运行需要重新从<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">主要成员失败中重新启动集群</a>。<br>为了更好的理解运行时重新配置设计，请阅读<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E8%AE%BE%E8%AE%A1/\">运行时重新配置设计</a>。</p>\n<h2 id=\"重新配置使用案例\"><a href=\"#重新配置使用案例\" class=\"headerlink\" title=\"重新配置使用案例\"></a>重新配置使用案例</h2><hr>\n<p>本节将介绍一些重新配置集群的常见原因。 其中大多数原因仅涉及添加或删除成员的组合，<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">群集重新配置操作</a>下将对此进行说明。</p>\n<h3 id=\"循环或更新多机\"><a href=\"#循环或更新多机\" class=\"headerlink\" title=\"循环或更新多机\"></a>循环或更新多机</h3><p>如果由于计划的维护（硬件升级，网络停机等）而需要移动多个群集成员，建议一次修改一个成员。<br>移除领导者是安全的，但是在选举过程中会出现短暂的停机时间。 如果群集包含的版本为v2的数据超过50MB，则建议迁移成员的数据目录。</p>\n<h3 id=\"改变集群大小\"><a href=\"#改变集群大小\" class=\"headerlink\" title=\"改变集群大小\"></a>改变集群大小</h3><p>增加群集大小可以增强<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/v2/admin_guide.md#fault-tolerance-table\" target=\"_blank\" rel=\"noopener\">容错能力</a>并提供更好的读取性能,由于客户端可以从任何成员读取，因此增加成员数量将增加整体序列化读取吞吐量。<br>减小群集大小可以提高群集的写入性能，但需要权衡降低弹性。写入集群之前，会将其复制到集群的大多数成员。 减小群集大小可减少大多数操作，并且每次写入的提交速度都会更快。</p>\n<h3 id=\"替换一个失败的主机\"><a href=\"#替换一个失败的主机\" class=\"headerlink\" title=\"替换一个失败的主机\"></a>替换一个失败的主机</h3><p>如果计算机由于硬件故障，数据目录损坏或其他致命情况而失败，应该尽快更换它。 发生故障但尚未移除的主机会对集群产生不利影响，并降低对其他故障的容忍度。<br>要更换主机，请按照说明从群集中<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">删除成员</a>，然后在其位置<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">添加新成员</a>。如果群集拥有的空间超过50MB，则建议迁移仍可访问的失败成员的数据目录。</p>\n<h3 id=\"多数主机失败后重启集群\"><a href=\"#多数主机失败后重启集群\" class=\"headerlink\" title=\"多数主机失败后重启集群\"></a>多数主机失败后重启集群</h3><p>如果大多数群集丢失或所有节点的IP地址都已更改，则必须采取手动操作才能安全恢复。恢复过程中的基本步骤包括<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">使用旧数据创建新集群</a>，强制单个成员充当领导者，最后使用运行时配置一次将<a href=\"https://newonexd.github.io/2019/11/23/blog/etcd/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE/\">新成员添加</a>到该新集群中。</p>\n<h2 id=\"集群重新配置操作\"><a href=\"#集群重新配置操作\" class=\"headerlink\" title=\"集群重新配置操作\"></a>集群重新配置操作</h2><p>考虑到这些用例，可以针对每个用例进行描述。进行任何更改之前，必须有多数etcd成员可以获取。 对于对etcd的任何类型的写入，这基本上是相同的要求。<br>必须按顺序完成对集群的所有更改：</p>\n<ul>\n<li>要更新单个成员节点URL，请执行更新操作.</li>\n<li>要替换正常的单个成员，请删除旧成员，然后添加新成员.</li>\n<li>要从3名增加到5名成员，请执行两次添加操作</li>\n<li>成员数量要从5减少到3，请执行两次删除操作</li>\n</ul>\n<p>这些示例都使用etcd附带的<code>etcdctl</code>命令行工具进行。如果不使用<code>etcdctl</code>工具改变成员关系，使用<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/v2/members_api.md\" target=\"_blank\" rel=\"noopener\">v2HTTP成员API</a>或者<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto\" target=\"_blank\" rel=\"noopener\">v3gRPC成员API</a>。</p>\n<h3 id=\"更新一个成员\"><a href=\"#更新一个成员\" class=\"headerlink\" title=\"更新一个成员\"></a>更新一个成员</h3><p><strong>更新广播客户端URLs</strong><br>要更新成员的发布客户端URL，只需使用已更新的客户端URL参数<code>--advertise-client-urls</code>或环境变量<code>ETCD_ADVERTISE_CLIENT_URLS</code>重新启动该成员。重新启动的成员将自行发布更新的URL。 错误更新的客户端URL不会影响etcd群集的运行状况。<br><strong>更新广播节点URLs</strong><br>要更新成员的广播节点URL，请首先通过成员命令显式更新它，然后重新启动该成员。由于更新节点URL会更改集群范围的配置，并且可能影响etcd集群的运行状况，因此需要采取其他措施。<br>要更新成员的广播节点URL，请首先找到目标成员的ID。 列出具有etcdctl的所有成员：</p>\n<pre><code>$ etcdctl member list\n6e3bd23ae5f1eae0: name=node2 peerURLs=http://localhost:23802 clientURLs=http://127.0.0.1:23792\n924e2e83e93f2560: name=node3 peerURLs=http://localhost:23803 clientURLs=http://127.0.0.1:23793\na8266ecf031671f3: name=node1 peerURLs=http://localhost:23801 clientURLs=http://127.0.0.1:23791</code></pre><p>本示例将<code>更新</code>a8266ecf031671f3成员ID，并将其节点URLs值更改为<code>http://10.0.1.10:2380</code>：</p>\n<pre><code>$ etcdctl member update a8266ecf031671f3 --peer-urls=http://10.0.1.10:2380\nUpdated member with ID a8266ecf031671f3 in cluster</code></pre><p><strong>移除一个成员</strong><br>假设要移除的成员ID为a8266ecf031671f3。 使用<code>remove</code>命令执行删除：</p>\n<pre><code>$ etcdctl member remove a8266ecf031671f3\nRemoved member a8266ecf031671f3 from cluster</code></pre><p>目标成员将在此时停止运行并在日志中打印出删除内容：</p>\n<pre><code>etcd: this member has been permanently removed from the cluster. Exiting.</code></pre><p>删除领导者是安全的，但是当选择新领导者时，群集将处于非活动状态。 此持续时间通常是选举超时时间加上投票过程的时间。<br><strong>添加一个新成员</strong><br>通过两个步骤添加一个新的成员：</p>\n<ul>\n<li>通过<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/v2/members_api.md\" target=\"_blank\" rel=\"noopener\">HTTP 成员API</a>，<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto\" target=\"_blank\" rel=\"noopener\">gRPC成员API</a>，或者是<code>etcdctl member add</code>命令添加一个新的成员到集群中。</li>\n<li>通过新的集群配置启动新的成员，新的集群配置包括被更新的成员(已存在的成员+新成员).</li>\n</ul>\n<p><code>etcdctl</code>添加一个新的成员到集群中通过具体的成员<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/\">名字</a>和<a href=\"https://newonexd.github.io/2019/11/24/blog/etcd/ETCD%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/\">广播节点URLs</a>:</p>\n<pre><code>$ etcdctl member add infra3 --peer-urls=http://10.0.1.13:2380\nadded member 9bf1b35fc7761a23 to cluster\n\nETCD_NAME=&quot;infra3&quot;\nETCD_INITIAL_CLUSTER=&quot;infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380&quot;\nETCD_INITIAL_CLUSTER_STATE=existing</code></pre><p><code>etcdctl</code>已将新成员通知集群，并打印出成功启动集群所需的环境变量。 现在，使用新成员的相关参数启动新的etcd进程：</p>\n<pre><code>$ export ETCD_NAME=&quot;infra3&quot;\n$ export ETCD_INITIAL_CLUSTER=&quot;infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380&quot;\n$ export ETCD_INITIAL_CLUSTER_STATE=existing\n$ etcd --listen-client-urls http://10.0.1.13:2379 --advertise-client-urls http://10.0.1.13:2379 --listen-peer-urls http://10.0.1.13:2380 --initial-advertise-peer-urls http://10.0.1.13:2380 --data-dir %data_dir%</code></pre><p>新成员将作为集群的一部分运行，并立即开始同步集群的其余部分。<br>如果添加多个成员，最佳做法是一次配置一个成员，并在添加更多新成员之前验证它是否正确启动.如果将新成员添加到1节点群集中，则群集无法在新成员启动之前取得进展，因为它需要两个成员作为多数才能达成共识。仅在<code>etcdctl``member add</code>通知集群有关新成员的时间和新成员成功建立与现有成员的连接的时间之间，才发生此行为。<br><strong>添加一个新的成员为领导者</strong><br>从v3.4开始，etcd支持将新成员添加为领导者/非投票成员。激励和设计可以在<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/learning/design-learner.md\" target=\"_blank\" rel=\"noopener\">设计文档</a>中找到。为了使添加新成员的过程更安全，并减少添加新成员时的集群停机时间.建议将新成员作为学习者添加到集群中，直到同步完成为止。 这可以描述为三步过程：</p>\n<ul>\n<li>通过<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto\" target=\"_blank\" rel=\"noopener\">gRPC成员API</a>或者<code>etcdctl member add --learner</code>命令将新成员添加为学习者。</li>\n<li>通过新的集群配置启动新的成员，新的集群配置包括被更新的成员(已存在的成员+新成员)和之前的步骤相同.</li>\n<li>通过<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/api_reference_v3.md#service-cluster-etcdserveretcdserverpbrpcproto\" target=\"_blank\" rel=\"noopener\">gRPC成员API</a>或<code>etcdctl member promote</code>命令将新添加的学习者提升为有投票权的成员。etcd服务器验证升级请求以确保其运行安全.只有在其Raft日志达到领导者的水平之后，才能将学习者提升为有投票权的成员。如果学习者成员未赶上领导者的Raft日志，则成员升级请求将失败(见<a href=\"\">提升成员错误案例</a>部分获取更多细节).这种情况下，用户应该等待并重试。</li>\n</ul>\n<p>在v3.4中，etcd服务器将群集可以拥有的学习者数量限制为一个。 主要考虑因素是限制由于领导者向学习者传播数据而导致的领导者额外工作量。<br>使用<code>etcdctl member add</code>和参数<code>--learner</code>添加一个新成员作为学习者到集群中.</p>\n<pre><code>$ etcdctl member add infra3 --peer-urls=http://10.0.1.13:2380 --learner\nMember 9bf1b35fc7761a23 added to cluster a7ef944b95711739\n\nETCD_NAME=&quot;infra3&quot;\nETCD_INITIAL_CLUSTER=&quot;infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380&quot;\nETCD_INITIAL_CLUSTER_STATE=existing</code></pre><p>新的etcd程序添加新的学习者成员启动后，使用<code>etcdctl member promote</code>将学习者提升为投票成员。</p>\n<pre><code>$ etcdctl member promote 9bf1b35fc7761a23\nMember 9e29bbaa45d74461 promoted in cluster a7ef944b95711739</code></pre><p><strong>添加成员错误案例</strong><br>在以下情况下，新主机不包含在枚举节点列表中。 如果这是一个新集群，则必须将该节点添加到初始集群成员列表中。</p>\n<pre><code>$ etcd --name infra3 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n  --initial-cluster-state existing\netcdserver: assign ids error: the member count is unequal\nexit 1</code></pre><p>在这种情况下，使用了与用于加入集群的地址（10.0.1.13:2380）不同的地址（10.0.1.14:2380）：</p>\n<pre><code>$ etcd --name infra4 \\\n  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra4=http://10.0.1.14:2380 \\\n  --initial-cluster-state existing\netcdserver: assign ids error: unmatched member while checking PeerURLs\nexit 1</code></pre><p>如果etcd开始使用已删除成员的数据目录，则etcd如果连接到集群中的任何活动成员，则会自动退出：</p>\n<pre><code>$ etcd\netcd: this member has been permanently removed from the cluster. Exiting.\nexit 1</code></pre><p><strong>添加成员为领导者错误案例</strong><br>当集群中含有一个领导者时不能添加领导者到集群中(v3.4):</p>\n<pre><code>$ etcdctl member add infra4 --peer-urls=http://10.0.1.14:2380 --learner\nError: etcdserver: too many learner members in cluster</code></pre><p><strong>提升成员为领导者错误案例</strong><br>如果学习者与领导者同步，则只能被提升为有投票权的成员。</p>\n<pre><code>$ etcdctl member promote 9bf1b35fc7761a23\nError: etcdserver: can only promote a learner member which is in sync with leader</code></pre><p>提升不是学习者的成员将失败。</p>\n<pre><code>$ etcdctl member promote 9bf1b35fc7761a23\nError: etcdserver: can only promote a learner member</code></pre><p>提升一个集群中不存在的成员将会失败：</p>\n<pre><code>$ etcdctl member promote 12345abcde\nError: etcdserver: member not found</code></pre><h3 id=\"严格的重新配置检查模式-strict-reconfig-check\"><a href=\"#严格的重新配置检查模式-strict-reconfig-check\" class=\"headerlink\" title=\"严格的重新配置检查模式(-strict-reconfig-check)\"></a>严格的重新配置检查模式(<code>-strict-reconfig-check</code>)</h3><p>如上所述，添加新成员的最佳实践是一次配置一个成员，并在添加更多新成员之前验证它是否正确启动。这种分步方法非常重要，因为如果未正确配置新添加的成员（例如，对等URL不正确），则群集可能会丢失仲裁。</p>\n"},{"title":"Hyperledger Fabric多机部署","date":"2019-11-23T10:38:53.000Z","_content":"之前的文章[深入解析Hyperledger Fabric启动的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)主要讲解了Fabric的网络搭建，以及启动的整体流程，但是都是通过单机完成的。而区块链本身就是去中心化的，所以最终还是要完成Fabric网络的多机部署。在本文中，将会详细说明Fabric如何完成多机部署。\n### 1搭建环境\n **本文使用的是Fabric 1.4版本，搭建solo模式的4+1的架构:1Order,4Peer，数据库使用CouchDb**，所以这里需要五台机器。同时，五台机器的网络需要互通，系统使用Ubuntu16.04。\n\n| 域名|ip地址|\n|:----|:----|\n|orderer.example.com|10.65.182.150|\n|peer0.org1.example.com|10.65.26.64|\n|peer1.org1.example.com|10.65.26.140|\n|peer0.org2.example.com|10.65.200.182|\n|peer1.org2.example.com|10.65.200.44|\n\nFabric的环境搭建过程不再详解，可以参考这一篇文章[Hyperledger Fabric环境搭建过程](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n## 2.多机环境搭建\n如果要成功搭建多机下的Fabric运行环境，首先要保证五台机子上的Fabric网络可以正常运行。\n按照[Hyperledger Fabric环境搭建过程](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)在五台机子上搭建Fabric完成后，\n就可以对相应的配置文件进行修改了，这里本文只在Orderer节点的机子上修改配置文件，最后通过scp命令将配置文件复制到其余四台机子，保证所有的节点所使用的配置文件都是相同的。\n在官方的例子中，已经有很多模板可以拿来进行修改，这里本文使用``first-network``这个文件夹内的配置文件来修改为自己所需要的配置文件。\n\n**本文以orderer节点为例，在``10.65.182.150``这台服务器上进行操作。**\n### 2.1准备配置文件\n```\n#step1 进入到first-network文件夹的上一级目录\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/\n#step2 拷贝first-network文件夹，并命名为first\ncp -r first-network/ first\n#step3 进入到first文件夹内\ncd first\n#step4 删除此次多机环境搭建使用不到的文件，文件夹内剩余的文件有\n.\n├── base\n│   ├── docker-compose-base.yaml\n│   └── peer-base.yaml\n├── channel-artifacts\n├── configtx.yaml\n├── crypto-config.yaml\n├── docker-compose-cli.yaml\n├── docker-compose-couch.yaml\n```\n本文就对以上文件进行修改搭建自己的Fabric多机网络\n由于官方的``first-network``中的配置文件中使用的就是4+1的架构，所以我们可以直接生成所需要的证书文件，创世区块，通道配置文件。\n### 2.2生成相关配置文件\n```\n#step1 生成证书文件\ncryptogen generate --config=./crypto-config.yaml\n#step2 生成创世区块  首先要确保channel-artifacts文件夹存在，如果不存在需要手动创建，不然会报错\nconfigtxgen -profile TwoOrgsOrdererGenesis -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n#step3 生成通道配置文件  其中通道名mychannel可以修改为自己的\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel\n#step4 生成锚节点配置文件\n#========Org1=============\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID mychannel -asOrg Org1MSP\n##========Org2=============\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID mychannel -asOrg Org2MSP\n```\n所有需要的配置文件全部建立完成，在``channel-artifacts``中应该有以下几个文件。\n``channel.tx、genesis.block、Org1MSPanchors.tx、Org2MSPanchors.tx``\n### 2.3修改节点配置文件\n#### 2.3.1``base/docker-compose-base.yaml``\n这个文件中配置了所有节点的一些基本信息，我们需要修改的地方有\n```\npeer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    extends:\n      file: peer-base.yaml\n      service: peer-base\n    environment:\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:7051\n      - CORE_PEER_CHAINCODEADDRESS=peer0.org1.example.com:7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer1.org1.example.com:8051  #这里个性为7051,因为我们是多机环境，不存在端口冲突问题\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n    volumes:\n        - /var/run/:/host/var/run/\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp:/etc/hyperledger/fabric/msp\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls:/etc/hyperledger/fabric/tls\n        - peer0.org1.example.com:/var/hyperledger/production\n    ports:\n      - 7051:7051\n\n  peer1.org1.example.com:\n    container_name: peer1.org1.example.com\n    extends:\n      file: peer-base.yaml\n      service: peer-base\n    environment:\n      - CORE_PEER_ID=peer1.org1.example.com\n      - CORE_PEER_ADDRESS=peer1.org1.example.com:8051   #  7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:8051    #7051\n      - CORE_PEER_CHAINCODEADDRESS=peer1.org1.example.com:8052  #7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:8052   #7052\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1.org1.example.com:8051  #7051\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n    volumes:\n        - /var/run/:/host/var/run/\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/msp:/etc/hyperledger/fabric/msp\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls:/etc/hyperledger/fabric/tls\n        - peer1.org1.example.com:/var/hyperledger/production\n\n    ports:\n      - 8051:8051   #这里不要忘记修改为   7051:7051\n...\n#以下全部需要修改   8051/9051/10051修改为7051     8052/9052/10052修改为7052\n#其余地方不需要修改\n```\n#### 2.3.2 ``docker-compose-cli.yaml``\n本文需要使用该文件启动节点，我们将该文件复制一份，**以orderer节点为例**：\n```\n#复制该文件，并命名为docker-compose-orderer.yaml\ncp docker-compose-cli.yaml docker-compose-orderer.yaml\n#用编辑器打开该文件\nsudo vim docker-compose-orderer.yaml\n```\n我们只在这台电脑上启动orderer节点，所以关于peer节点的信息用不到，我们将配置文件中多余的字段删除，只留下这些内容：\n```\nversion: '2'\n\nvolumes:\n  orderer.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  orderer.example.com:\n    extends:\n      file:   base/docker-compose-base.yaml\n      service: orderer.example.com\n    container_name: orderer.example.com\n    networks:\n      - byfn\n\n```\n接下来可以启动Orderer节点了,执行以下命令启动Orderer节点。\n```\nsudo docker-compose -f docker-compose-orderer.yaml up\n```\norderer节点启动成功后，我们使用scp命令将``first``文件夹传输到peer0.org1节点服务器。\n```\n#step1 进入到上级目录\ncd ..\n#step2 传输文件\nsudo scp -r first/ [peer0.org1节点主机名]@10.65.26.64:/home/[用户名]/\n```\n然后，我们登陆``10.65.26.64``主机，对peer0.org1节点进行配置,同样，我们复制一份``docker-compose-cli.yaml``文件：\n```\n#step1:进入传输到的first文件夹\ncd ~/first\n#step2:复制docker-compose-cli.yaml文件 并命名为docker-compose-peer0-Org1.yaml\ncp docker-compose-cli.yaml docker-compose-peer0-Org1.yaml\n#step3:用编辑器打开该文件\nvim docker-compose-peer0-Org1.yaml\n```\n对于peer0.Org1节点，同样，首先删除多余的部分，添加一些字段，最终文件内容为：\n```\nversion: '2'\n\nvolumes:\n  peer0.org1.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    extends:\n      file:  base/docker-compose-base.yaml\n      service: peer0.org1.example.com\n    networks:\n      - byfn\n    extra_hosts:       #=========需要添加的额外字段，这里不写当前节点\n      - \"orderer.example.com:10.65.182.150\"\n      - \"peer1.org1.example.com:10.65.26.140\"\n      - \"peer0.org2.example.com:10.65.200.182\"\n      - \"peer1.org2.example.com:10.65.200.44\"\n\n  cli:\n    container_name: cli\n    image: hyperledger/fabric-tools:$IMAGE_TAG\n    tty: true\n    stdin_open: true\n    environment:\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG    #这里改为DEBUG\n      - CORE_PEER_ID=cli\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\n      - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: /bin/bash\n    volumes:\n        - /var/run/:/host/var/run/\n        - ./../chaincode/:/opt/gopath/src/github.com/chaincode\n        - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/\n        - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/\n        - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts\n    depends_on:\n      - peer0.org1.example.com\n    networks:\n      - byfn\n    extra_hosts:       #=========需要添加的额外字段.\n      - \"orderer.example.com:10.65.182.150\"\n      - \"peer0.org1.example.com:10.65.26.64\"     #这里需要写当前节点，因为cli容器需要与peer0.org1节点进行通信\n      - \"peer1.org1.example.com:10.65.26.140\"\n      - \"peer0.org2.example.com:10.65.200.182\"\n      - \"peer1.org2.example.com:10.65.200.44\"\n\n```\n此外，因为本文中Fabric数据库使用了CouchDb，所以需要对CouchDb进行相关配置,CouchDb配置文件为``docker-compose-couch.yaml``。\n#### 2.3.3 ``docker-compose-couch.yaml``\n同样，我们复制一份该文件，命名为``docker-compose-peer0-Org1-couch.yaml``\n```\ncp docker-compose-couch.yaml docker-compose-peer0-Org1-couch.yaml\n#使用编辑器打开该文件\nvim docker-compose-peer0-Org1-couch.yaml\n```\n在这个配置文件中，我们需要删除其他节点的配置信息，只保留peer0.org1的配置文件,最后完整的配置文件内容为：\n```\nversion: '2'\n\nnetworks:\n  byfn:\n\nservices:\n  couchdb0:\n    container_name: couchdb0\n    image: hyperledger/fabric-couchdb\n    # Populate the COUCHDB_USER and COUCHDB_PASSWORD to set an admin user and password\n    # for CouchDB.  This will prevent CouchDB from operating in an \"Admin Party\" mode.\n    environment:\n      - COUCHDB_USER=\n      - COUCHDB_PASSWORD=\n    # Comment/Uncomment the port mapping if you want to hide/expose the CouchDB service,\n    # for example map it to utilize Fauxton User Interface in dev environments.\n    ports:\n      - \"5984:5984\"\n    networks:\n      - byfn\n\n  peer0.org1.example.com:\n    environment:\n      - CORE_LEDGER_STATE_STATEDATABASE=CouchDB\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984\n      # The CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME and CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD\n      # provide the credentials for ledger to connect to CouchDB.  The username and password must\n      # match the username and password set for the associated CouchDB.\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME=\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD=\n    depends_on:\n      - couchdb0\n\n```\n至于peer0.org1的配置文件已经修改完毕，接下来我们启动该节点:\n```\nsudo docker-compose -f docker-compose-peer0-Org1.yaml -f docker-compose-peer0-Org1-couch.yaml up\n```\n如果没有报错的话，peer0.org1节点成功启动。至于其他peer节点，只需要将``first``文件夹使用``scp``命令复制到各个服务器上，按照该模板对配置文件进行修改，主要是``docker-compose-cli.yaml``和``docker-compose-couch.yaml``两个文件。\n\n如果所有节点都可以成功启动的话，接下来就可以进行链码的安装测试了，这一部分不再重复介绍，具体内容可以参考[深入解析Hyperledger Fabric启动的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)中链码的安装测试过程。\n\n整个过程中可能会遇到各种各样的坑，不过大部分问题都是由于配置文件某一地方没有修改好，或者就是yaml文件的格式错误，还是比较好解决的。\n\n最后关闭网络需要清空所有数据，不然再次启动网络会出错。\n## 3 关闭网络\n对于Order节点,关闭网络的命令：\n```\nsudo docker-compose -f docker-compose-orderer.yaml down --volumes\n```\nPeer节点：\n```\nsudo docker-compose -f docker-compose-peer0-Org1.yaml -f docker-compose-peer0-Org1-couch.yaml down --volumes\n```\n建议在每一次启动网络之前都执行一次关闭网络的命令。\n此外，有可能会出现容器无法删除的情况，我们可以执行以下命令进行删除：\n```\nsudo docker rm $(docker ps -aq)\n```\n到这里，所有文章都还没有讲解Fabric-Ca的内容，Fabric-Ca将会在下一篇文章中讲解。","source":"_posts/blog/fabric/Fabric1.4多机部署.md","raw":"---\ntitle: Hyperledger Fabric多机部署\ndate: 2019-11-23 18:38:53\ntags: fabric\ncategories: fabric应用\n---\n之前的文章[深入解析Hyperledger Fabric启动的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)主要讲解了Fabric的网络搭建，以及启动的整体流程，但是都是通过单机完成的。而区块链本身就是去中心化的，所以最终还是要完成Fabric网络的多机部署。在本文中，将会详细说明Fabric如何完成多机部署。\n### 1搭建环境\n **本文使用的是Fabric 1.4版本，搭建solo模式的4+1的架构:1Order,4Peer，数据库使用CouchDb**，所以这里需要五台机器。同时，五台机器的网络需要互通，系统使用Ubuntu16.04。\n\n| 域名|ip地址|\n|:----|:----|\n|orderer.example.com|10.65.182.150|\n|peer0.org1.example.com|10.65.26.64|\n|peer1.org1.example.com|10.65.26.140|\n|peer0.org2.example.com|10.65.200.182|\n|peer1.org2.example.com|10.65.200.44|\n\nFabric的环境搭建过程不再详解，可以参考这一篇文章[Hyperledger Fabric环境搭建过程](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n## 2.多机环境搭建\n如果要成功搭建多机下的Fabric运行环境，首先要保证五台机子上的Fabric网络可以正常运行。\n按照[Hyperledger Fabric环境搭建过程](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)在五台机子上搭建Fabric完成后，\n就可以对相应的配置文件进行修改了，这里本文只在Orderer节点的机子上修改配置文件，最后通过scp命令将配置文件复制到其余四台机子，保证所有的节点所使用的配置文件都是相同的。\n在官方的例子中，已经有很多模板可以拿来进行修改，这里本文使用``first-network``这个文件夹内的配置文件来修改为自己所需要的配置文件。\n\n**本文以orderer节点为例，在``10.65.182.150``这台服务器上进行操作。**\n### 2.1准备配置文件\n```\n#step1 进入到first-network文件夹的上一级目录\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/\n#step2 拷贝first-network文件夹，并命名为first\ncp -r first-network/ first\n#step3 进入到first文件夹内\ncd first\n#step4 删除此次多机环境搭建使用不到的文件，文件夹内剩余的文件有\n.\n├── base\n│   ├── docker-compose-base.yaml\n│   └── peer-base.yaml\n├── channel-artifacts\n├── configtx.yaml\n├── crypto-config.yaml\n├── docker-compose-cli.yaml\n├── docker-compose-couch.yaml\n```\n本文就对以上文件进行修改搭建自己的Fabric多机网络\n由于官方的``first-network``中的配置文件中使用的就是4+1的架构，所以我们可以直接生成所需要的证书文件，创世区块，通道配置文件。\n### 2.2生成相关配置文件\n```\n#step1 生成证书文件\ncryptogen generate --config=./crypto-config.yaml\n#step2 生成创世区块  首先要确保channel-artifacts文件夹存在，如果不存在需要手动创建，不然会报错\nconfigtxgen -profile TwoOrgsOrdererGenesis -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n#step3 生成通道配置文件  其中通道名mychannel可以修改为自己的\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel\n#step4 生成锚节点配置文件\n#========Org1=============\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID mychannel -asOrg Org1MSP\n##========Org2=============\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID mychannel -asOrg Org2MSP\n```\n所有需要的配置文件全部建立完成，在``channel-artifacts``中应该有以下几个文件。\n``channel.tx、genesis.block、Org1MSPanchors.tx、Org2MSPanchors.tx``\n### 2.3修改节点配置文件\n#### 2.3.1``base/docker-compose-base.yaml``\n这个文件中配置了所有节点的一些基本信息，我们需要修改的地方有\n```\npeer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    extends:\n      file: peer-base.yaml\n      service: peer-base\n    environment:\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:7051\n      - CORE_PEER_CHAINCODEADDRESS=peer0.org1.example.com:7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer1.org1.example.com:8051  #这里个性为7051,因为我们是多机环境，不存在端口冲突问题\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n    volumes:\n        - /var/run/:/host/var/run/\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp:/etc/hyperledger/fabric/msp\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls:/etc/hyperledger/fabric/tls\n        - peer0.org1.example.com:/var/hyperledger/production\n    ports:\n      - 7051:7051\n\n  peer1.org1.example.com:\n    container_name: peer1.org1.example.com\n    extends:\n      file: peer-base.yaml\n      service: peer-base\n    environment:\n      - CORE_PEER_ID=peer1.org1.example.com\n      - CORE_PEER_ADDRESS=peer1.org1.example.com:8051   #  7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:8051    #7051\n      - CORE_PEER_CHAINCODEADDRESS=peer1.org1.example.com:8052  #7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:8052   #7052\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1.org1.example.com:8051  #7051\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n    volumes:\n        - /var/run/:/host/var/run/\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/msp:/etc/hyperledger/fabric/msp\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls:/etc/hyperledger/fabric/tls\n        - peer1.org1.example.com:/var/hyperledger/production\n\n    ports:\n      - 8051:8051   #这里不要忘记修改为   7051:7051\n...\n#以下全部需要修改   8051/9051/10051修改为7051     8052/9052/10052修改为7052\n#其余地方不需要修改\n```\n#### 2.3.2 ``docker-compose-cli.yaml``\n本文需要使用该文件启动节点，我们将该文件复制一份，**以orderer节点为例**：\n```\n#复制该文件，并命名为docker-compose-orderer.yaml\ncp docker-compose-cli.yaml docker-compose-orderer.yaml\n#用编辑器打开该文件\nsudo vim docker-compose-orderer.yaml\n```\n我们只在这台电脑上启动orderer节点，所以关于peer节点的信息用不到，我们将配置文件中多余的字段删除，只留下这些内容：\n```\nversion: '2'\n\nvolumes:\n  orderer.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  orderer.example.com:\n    extends:\n      file:   base/docker-compose-base.yaml\n      service: orderer.example.com\n    container_name: orderer.example.com\n    networks:\n      - byfn\n\n```\n接下来可以启动Orderer节点了,执行以下命令启动Orderer节点。\n```\nsudo docker-compose -f docker-compose-orderer.yaml up\n```\norderer节点启动成功后，我们使用scp命令将``first``文件夹传输到peer0.org1节点服务器。\n```\n#step1 进入到上级目录\ncd ..\n#step2 传输文件\nsudo scp -r first/ [peer0.org1节点主机名]@10.65.26.64:/home/[用户名]/\n```\n然后，我们登陆``10.65.26.64``主机，对peer0.org1节点进行配置,同样，我们复制一份``docker-compose-cli.yaml``文件：\n```\n#step1:进入传输到的first文件夹\ncd ~/first\n#step2:复制docker-compose-cli.yaml文件 并命名为docker-compose-peer0-Org1.yaml\ncp docker-compose-cli.yaml docker-compose-peer0-Org1.yaml\n#step3:用编辑器打开该文件\nvim docker-compose-peer0-Org1.yaml\n```\n对于peer0.Org1节点，同样，首先删除多余的部分，添加一些字段，最终文件内容为：\n```\nversion: '2'\n\nvolumes:\n  peer0.org1.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    extends:\n      file:  base/docker-compose-base.yaml\n      service: peer0.org1.example.com\n    networks:\n      - byfn\n    extra_hosts:       #=========需要添加的额外字段，这里不写当前节点\n      - \"orderer.example.com:10.65.182.150\"\n      - \"peer1.org1.example.com:10.65.26.140\"\n      - \"peer0.org2.example.com:10.65.200.182\"\n      - \"peer1.org2.example.com:10.65.200.44\"\n\n  cli:\n    container_name: cli\n    image: hyperledger/fabric-tools:$IMAGE_TAG\n    tty: true\n    stdin_open: true\n    environment:\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG    #这里改为DEBUG\n      - CORE_PEER_ID=cli\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\n      - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: /bin/bash\n    volumes:\n        - /var/run/:/host/var/run/\n        - ./../chaincode/:/opt/gopath/src/github.com/chaincode\n        - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/\n        - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/\n        - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts\n    depends_on:\n      - peer0.org1.example.com\n    networks:\n      - byfn\n    extra_hosts:       #=========需要添加的额外字段.\n      - \"orderer.example.com:10.65.182.150\"\n      - \"peer0.org1.example.com:10.65.26.64\"     #这里需要写当前节点，因为cli容器需要与peer0.org1节点进行通信\n      - \"peer1.org1.example.com:10.65.26.140\"\n      - \"peer0.org2.example.com:10.65.200.182\"\n      - \"peer1.org2.example.com:10.65.200.44\"\n\n```\n此外，因为本文中Fabric数据库使用了CouchDb，所以需要对CouchDb进行相关配置,CouchDb配置文件为``docker-compose-couch.yaml``。\n#### 2.3.3 ``docker-compose-couch.yaml``\n同样，我们复制一份该文件，命名为``docker-compose-peer0-Org1-couch.yaml``\n```\ncp docker-compose-couch.yaml docker-compose-peer0-Org1-couch.yaml\n#使用编辑器打开该文件\nvim docker-compose-peer0-Org1-couch.yaml\n```\n在这个配置文件中，我们需要删除其他节点的配置信息，只保留peer0.org1的配置文件,最后完整的配置文件内容为：\n```\nversion: '2'\n\nnetworks:\n  byfn:\n\nservices:\n  couchdb0:\n    container_name: couchdb0\n    image: hyperledger/fabric-couchdb\n    # Populate the COUCHDB_USER and COUCHDB_PASSWORD to set an admin user and password\n    # for CouchDB.  This will prevent CouchDB from operating in an \"Admin Party\" mode.\n    environment:\n      - COUCHDB_USER=\n      - COUCHDB_PASSWORD=\n    # Comment/Uncomment the port mapping if you want to hide/expose the CouchDB service,\n    # for example map it to utilize Fauxton User Interface in dev environments.\n    ports:\n      - \"5984:5984\"\n    networks:\n      - byfn\n\n  peer0.org1.example.com:\n    environment:\n      - CORE_LEDGER_STATE_STATEDATABASE=CouchDB\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984\n      # The CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME and CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD\n      # provide the credentials for ledger to connect to CouchDB.  The username and password must\n      # match the username and password set for the associated CouchDB.\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME=\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD=\n    depends_on:\n      - couchdb0\n\n```\n至于peer0.org1的配置文件已经修改完毕，接下来我们启动该节点:\n```\nsudo docker-compose -f docker-compose-peer0-Org1.yaml -f docker-compose-peer0-Org1-couch.yaml up\n```\n如果没有报错的话，peer0.org1节点成功启动。至于其他peer节点，只需要将``first``文件夹使用``scp``命令复制到各个服务器上，按照该模板对配置文件进行修改，主要是``docker-compose-cli.yaml``和``docker-compose-couch.yaml``两个文件。\n\n如果所有节点都可以成功启动的话，接下来就可以进行链码的安装测试了，这一部分不再重复介绍，具体内容可以参考[深入解析Hyperledger Fabric启动的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)中链码的安装测试过程。\n\n整个过程中可能会遇到各种各样的坑，不过大部分问题都是由于配置文件某一地方没有修改好，或者就是yaml文件的格式错误，还是比较好解决的。\n\n最后关闭网络需要清空所有数据，不然再次启动网络会出错。\n## 3 关闭网络\n对于Order节点,关闭网络的命令：\n```\nsudo docker-compose -f docker-compose-orderer.yaml down --volumes\n```\nPeer节点：\n```\nsudo docker-compose -f docker-compose-peer0-Org1.yaml -f docker-compose-peer0-Org1-couch.yaml down --volumes\n```\n建议在每一次启动网络之前都执行一次关闭网络的命令。\n此外，有可能会出现容器无法删除的情况，我们可以执行以下命令进行删除：\n```\nsudo docker rm $(docker ps -aq)\n```\n到这里，所有文章都还没有讲解Fabric-Ca的内容，Fabric-Ca将会在下一篇文章中讲解。","slug":"blog/fabric/Fabric1.4多机部署","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyit002zk0vq2cha6e4e","content":"<p>之前的文章<a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric启动的全过程</a>主要讲解了Fabric的网络搭建，以及启动的整体流程，但是都是通过单机完成的。而区块链本身就是去中心化的，所以最终还是要完成Fabric网络的多机部署。在本文中，将会详细说明Fabric如何完成多机部署。</p>\n<h3 id=\"1搭建环境\"><a href=\"#1搭建环境\" class=\"headerlink\" title=\"1搭建环境\"></a>1搭建环境</h3><p> <strong>本文使用的是Fabric 1.4版本，搭建solo模式的4+1的架构:1Order,4Peer，数据库使用CouchDb</strong>，所以这里需要五台机器。同时，五台机器的网络需要互通，系统使用Ubuntu16.04。</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">域名</th>\n<th align=\"left\">ip地址</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">orderer.example.com</td>\n<td align=\"left\">10.65.182.150</td>\n</tr>\n<tr>\n<td align=\"left\">peer0.org1.example.com</td>\n<td align=\"left\">10.65.26.64</td>\n</tr>\n<tr>\n<td align=\"left\">peer1.org1.example.com</td>\n<td align=\"left\">10.65.26.140</td>\n</tr>\n<tr>\n<td align=\"left\">peer0.org2.example.com</td>\n<td align=\"left\">10.65.200.182</td>\n</tr>\n<tr>\n<td align=\"left\">peer1.org2.example.com</td>\n<td align=\"left\">10.65.200.44</td>\n</tr>\n</tbody></table>\n<p>Fabric的环境搭建过程不再详解，可以参考这一篇文章<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric环境搭建过程</a></p>\n<h2 id=\"2-多机环境搭建\"><a href=\"#2-多机环境搭建\" class=\"headerlink\" title=\"2.多机环境搭建\"></a>2.多机环境搭建</h2><p>如果要成功搭建多机下的Fabric运行环境，首先要保证五台机子上的Fabric网络可以正常运行。<br>按照<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric环境搭建过程</a>在五台机子上搭建Fabric完成后，<br>就可以对相应的配置文件进行修改了，这里本文只在Orderer节点的机子上修改配置文件，最后通过scp命令将配置文件复制到其余四台机子，保证所有的节点所使用的配置文件都是相同的。<br>在官方的例子中，已经有很多模板可以拿来进行修改，这里本文使用<code>first-network</code>这个文件夹内的配置文件来修改为自己所需要的配置文件。</p>\n<p><strong>本文以orderer节点为例，在<code>10.65.182.150</code>这台服务器上进行操作。</strong></p>\n<h3 id=\"2-1准备配置文件\"><a href=\"#2-1准备配置文件\" class=\"headerlink\" title=\"2.1准备配置文件\"></a>2.1准备配置文件</h3><pre><code>#step1 进入到first-network文件夹的上一级目录\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/\n#step2 拷贝first-network文件夹，并命名为first\ncp -r first-network/ first\n#step3 进入到first文件夹内\ncd first\n#step4 删除此次多机环境搭建使用不到的文件，文件夹内剩余的文件有\n.\n├── base\n│   ├── docker-compose-base.yaml\n│   └── peer-base.yaml\n├── channel-artifacts\n├── configtx.yaml\n├── crypto-config.yaml\n├── docker-compose-cli.yaml\n├── docker-compose-couch.yaml</code></pre><p>本文就对以上文件进行修改搭建自己的Fabric多机网络<br>由于官方的<code>first-network</code>中的配置文件中使用的就是4+1的架构，所以我们可以直接生成所需要的证书文件，创世区块，通道配置文件。</p>\n<h3 id=\"2-2生成相关配置文件\"><a href=\"#2-2生成相关配置文件\" class=\"headerlink\" title=\"2.2生成相关配置文件\"></a>2.2生成相关配置文件</h3><pre><code>#step1 生成证书文件\ncryptogen generate --config=./crypto-config.yaml\n#step2 生成创世区块  首先要确保channel-artifacts文件夹存在，如果不存在需要手动创建，不然会报错\nconfigtxgen -profile TwoOrgsOrdererGenesis -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n#step3 生成通道配置文件  其中通道名mychannel可以修改为自己的\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel\n#step4 生成锚节点配置文件\n#========Org1=============\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID mychannel -asOrg Org1MSP\n##========Org2=============\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID mychannel -asOrg Org2MSP</code></pre><p>所有需要的配置文件全部建立完成，在<code>channel-artifacts</code>中应该有以下几个文件。<br><code>channel.tx、genesis.block、Org1MSPanchors.tx、Org2MSPanchors.tx</code></p>\n<h3 id=\"2-3修改节点配置文件\"><a href=\"#2-3修改节点配置文件\" class=\"headerlink\" title=\"2.3修改节点配置文件\"></a>2.3修改节点配置文件</h3><h4 id=\"2-3-1base-docker-compose-base-yaml\"><a href=\"#2-3-1base-docker-compose-base-yaml\" class=\"headerlink\" title=\"2.3.1base/docker-compose-base.yaml\"></a>2.3.1<code>base/docker-compose-base.yaml</code></h4><p>这个文件中配置了所有节点的一些基本信息，我们需要修改的地方有</p>\n<pre><code>peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    extends:\n      file: peer-base.yaml\n      service: peer-base\n    environment:\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:7051\n      - CORE_PEER_CHAINCODEADDRESS=peer0.org1.example.com:7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer1.org1.example.com:8051  #这里个性为7051,因为我们是多机环境，不存在端口冲突问题\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n    volumes:\n        - /var/run/:/host/var/run/\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp:/etc/hyperledger/fabric/msp\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls:/etc/hyperledger/fabric/tls\n        - peer0.org1.example.com:/var/hyperledger/production\n    ports:\n      - 7051:7051\n\n  peer1.org1.example.com:\n    container_name: peer1.org1.example.com\n    extends:\n      file: peer-base.yaml\n      service: peer-base\n    environment:\n      - CORE_PEER_ID=peer1.org1.example.com\n      - CORE_PEER_ADDRESS=peer1.org1.example.com:8051   #  7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:8051    #7051\n      - CORE_PEER_CHAINCODEADDRESS=peer1.org1.example.com:8052  #7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:8052   #7052\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1.org1.example.com:8051  #7051\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n    volumes:\n        - /var/run/:/host/var/run/\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/msp:/etc/hyperledger/fabric/msp\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls:/etc/hyperledger/fabric/tls\n        - peer1.org1.example.com:/var/hyperledger/production\n\n    ports:\n      - 8051:8051   #这里不要忘记修改为   7051:7051\n...\n#以下全部需要修改   8051/9051/10051修改为7051     8052/9052/10052修改为7052\n#其余地方不需要修改</code></pre><h4 id=\"2-3-2-docker-compose-cli-yaml\"><a href=\"#2-3-2-docker-compose-cli-yaml\" class=\"headerlink\" title=\"2.3.2 docker-compose-cli.yaml\"></a>2.3.2 <code>docker-compose-cli.yaml</code></h4><p>本文需要使用该文件启动节点，我们将该文件复制一份，<strong>以orderer节点为例</strong>：</p>\n<pre><code>#复制该文件，并命名为docker-compose-orderer.yaml\ncp docker-compose-cli.yaml docker-compose-orderer.yaml\n#用编辑器打开该文件\nsudo vim docker-compose-orderer.yaml</code></pre><p>我们只在这台电脑上启动orderer节点，所以关于peer节点的信息用不到，我们将配置文件中多余的字段删除，只留下这些内容：</p>\n<pre><code>version: &#39;2&#39;\n\nvolumes:\n  orderer.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  orderer.example.com:\n    extends:\n      file:   base/docker-compose-base.yaml\n      service: orderer.example.com\n    container_name: orderer.example.com\n    networks:\n      - byfn\n</code></pre><p>接下来可以启动Orderer节点了,执行以下命令启动Orderer节点。</p>\n<pre><code>sudo docker-compose -f docker-compose-orderer.yaml up</code></pre><p>orderer节点启动成功后，我们使用scp命令将<code>first</code>文件夹传输到peer0.org1节点服务器。</p>\n<pre><code>#step1 进入到上级目录\ncd ..\n#step2 传输文件\nsudo scp -r first/ [peer0.org1节点主机名]@10.65.26.64:/home/[用户名]/</code></pre><p>然后，我们登陆<code>10.65.26.64</code>主机，对peer0.org1节点进行配置,同样，我们复制一份<code>docker-compose-cli.yaml</code>文件：</p>\n<pre><code>#step1:进入传输到的first文件夹\ncd ~/first\n#step2:复制docker-compose-cli.yaml文件 并命名为docker-compose-peer0-Org1.yaml\ncp docker-compose-cli.yaml docker-compose-peer0-Org1.yaml\n#step3:用编辑器打开该文件\nvim docker-compose-peer0-Org1.yaml</code></pre><p>对于peer0.Org1节点，同样，首先删除多余的部分，添加一些字段，最终文件内容为：</p>\n<pre><code>version: &#39;2&#39;\n\nvolumes:\n  peer0.org1.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    extends:\n      file:  base/docker-compose-base.yaml\n      service: peer0.org1.example.com\n    networks:\n      - byfn\n    extra_hosts:       #=========需要添加的额外字段，这里不写当前节点\n      - &quot;orderer.example.com:10.65.182.150&quot;\n      - &quot;peer1.org1.example.com:10.65.26.140&quot;\n      - &quot;peer0.org2.example.com:10.65.200.182&quot;\n      - &quot;peer1.org2.example.com:10.65.200.44&quot;\n\n  cli:\n    container_name: cli\n    image: hyperledger/fabric-tools:$IMAGE_TAG\n    tty: true\n    stdin_open: true\n    environment:\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG    #这里改为DEBUG\n      - CORE_PEER_ID=cli\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\n      - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: /bin/bash\n    volumes:\n        - /var/run/:/host/var/run/\n        - ./../chaincode/:/opt/gopath/src/github.com/chaincode\n        - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/\n        - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/\n        - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts\n    depends_on:\n      - peer0.org1.example.com\n    networks:\n      - byfn\n    extra_hosts:       #=========需要添加的额外字段.\n      - &quot;orderer.example.com:10.65.182.150&quot;\n      - &quot;peer0.org1.example.com:10.65.26.64&quot;     #这里需要写当前节点，因为cli容器需要与peer0.org1节点进行通信\n      - &quot;peer1.org1.example.com:10.65.26.140&quot;\n      - &quot;peer0.org2.example.com:10.65.200.182&quot;\n      - &quot;peer1.org2.example.com:10.65.200.44&quot;\n</code></pre><p>此外，因为本文中Fabric数据库使用了CouchDb，所以需要对CouchDb进行相关配置,CouchDb配置文件为<code>docker-compose-couch.yaml</code>。</p>\n<h4 id=\"2-3-3-docker-compose-couch-yaml\"><a href=\"#2-3-3-docker-compose-couch-yaml\" class=\"headerlink\" title=\"2.3.3 docker-compose-couch.yaml\"></a>2.3.3 <code>docker-compose-couch.yaml</code></h4><p>同样，我们复制一份该文件，命名为<code>docker-compose-peer0-Org1-couch.yaml</code></p>\n<pre><code>cp docker-compose-couch.yaml docker-compose-peer0-Org1-couch.yaml\n#使用编辑器打开该文件\nvim docker-compose-peer0-Org1-couch.yaml</code></pre><p>在这个配置文件中，我们需要删除其他节点的配置信息，只保留peer0.org1的配置文件,最后完整的配置文件内容为：</p>\n<pre><code>version: &#39;2&#39;\n\nnetworks:\n  byfn:\n\nservices:\n  couchdb0:\n    container_name: couchdb0\n    image: hyperledger/fabric-couchdb\n    # Populate the COUCHDB_USER and COUCHDB_PASSWORD to set an admin user and password\n    # for CouchDB.  This will prevent CouchDB from operating in an &quot;Admin Party&quot; mode.\n    environment:\n      - COUCHDB_USER=\n      - COUCHDB_PASSWORD=\n    # Comment/Uncomment the port mapping if you want to hide/expose the CouchDB service,\n    # for example map it to utilize Fauxton User Interface in dev environments.\n    ports:\n      - &quot;5984:5984&quot;\n    networks:\n      - byfn\n\n  peer0.org1.example.com:\n    environment:\n      - CORE_LEDGER_STATE_STATEDATABASE=CouchDB\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984\n      # The CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME and CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD\n      # provide the credentials for ledger to connect to CouchDB.  The username and password must\n      # match the username and password set for the associated CouchDB.\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME=\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD=\n    depends_on:\n      - couchdb0\n</code></pre><p>至于peer0.org1的配置文件已经修改完毕，接下来我们启动该节点:</p>\n<pre><code>sudo docker-compose -f docker-compose-peer0-Org1.yaml -f docker-compose-peer0-Org1-couch.yaml up</code></pre><p>如果没有报错的话，peer0.org1节点成功启动。至于其他peer节点，只需要将<code>first</code>文件夹使用<code>scp</code>命令复制到各个服务器上，按照该模板对配置文件进行修改，主要是<code>docker-compose-cli.yaml</code>和<code>docker-compose-couch.yaml</code>两个文件。</p>\n<p>如果所有节点都可以成功启动的话，接下来就可以进行链码的安装测试了，这一部分不再重复介绍，具体内容可以参考<a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric启动的全过程</a>中链码的安装测试过程。</p>\n<p>整个过程中可能会遇到各种各样的坑，不过大部分问题都是由于配置文件某一地方没有修改好，或者就是yaml文件的格式错误，还是比较好解决的。</p>\n<p>最后关闭网络需要清空所有数据，不然再次启动网络会出错。</p>\n<h2 id=\"3-关闭网络\"><a href=\"#3-关闭网络\" class=\"headerlink\" title=\"3 关闭网络\"></a>3 关闭网络</h2><p>对于Order节点,关闭网络的命令：</p>\n<pre><code>sudo docker-compose -f docker-compose-orderer.yaml down --volumes</code></pre><p>Peer节点：</p>\n<pre><code>sudo docker-compose -f docker-compose-peer0-Org1.yaml -f docker-compose-peer0-Org1-couch.yaml down --volumes</code></pre><p>建议在每一次启动网络之前都执行一次关闭网络的命令。<br>此外，有可能会出现容器无法删除的情况，我们可以执行以下命令进行删除：</p>\n<pre><code>sudo docker rm $(docker ps -aq)</code></pre><p>到这里，所有文章都还没有讲解Fabric-Ca的内容，Fabric-Ca将会在下一篇文章中讲解。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>之前的文章<a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric启动的全过程</a>主要讲解了Fabric的网络搭建，以及启动的整体流程，但是都是通过单机完成的。而区块链本身就是去中心化的，所以最终还是要完成Fabric网络的多机部署。在本文中，将会详细说明Fabric如何完成多机部署。</p>\n<h3 id=\"1搭建环境\"><a href=\"#1搭建环境\" class=\"headerlink\" title=\"1搭建环境\"></a>1搭建环境</h3><p> <strong>本文使用的是Fabric 1.4版本，搭建solo模式的4+1的架构:1Order,4Peer，数据库使用CouchDb</strong>，所以这里需要五台机器。同时，五台机器的网络需要互通，系统使用Ubuntu16.04。</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">域名</th>\n<th align=\"left\">ip地址</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">orderer.example.com</td>\n<td align=\"left\">10.65.182.150</td>\n</tr>\n<tr>\n<td align=\"left\">peer0.org1.example.com</td>\n<td align=\"left\">10.65.26.64</td>\n</tr>\n<tr>\n<td align=\"left\">peer1.org1.example.com</td>\n<td align=\"left\">10.65.26.140</td>\n</tr>\n<tr>\n<td align=\"left\">peer0.org2.example.com</td>\n<td align=\"left\">10.65.200.182</td>\n</tr>\n<tr>\n<td align=\"left\">peer1.org2.example.com</td>\n<td align=\"left\">10.65.200.44</td>\n</tr>\n</tbody></table>\n<p>Fabric的环境搭建过程不再详解，可以参考这一篇文章<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric环境搭建过程</a></p>\n<h2 id=\"2-多机环境搭建\"><a href=\"#2-多机环境搭建\" class=\"headerlink\" title=\"2.多机环境搭建\"></a>2.多机环境搭建</h2><p>如果要成功搭建多机下的Fabric运行环境，首先要保证五台机子上的Fabric网络可以正常运行。<br>按照<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric环境搭建过程</a>在五台机子上搭建Fabric完成后，<br>就可以对相应的配置文件进行修改了，这里本文只在Orderer节点的机子上修改配置文件，最后通过scp命令将配置文件复制到其余四台机子，保证所有的节点所使用的配置文件都是相同的。<br>在官方的例子中，已经有很多模板可以拿来进行修改，这里本文使用<code>first-network</code>这个文件夹内的配置文件来修改为自己所需要的配置文件。</p>\n<p><strong>本文以orderer节点为例，在<code>10.65.182.150</code>这台服务器上进行操作。</strong></p>\n<h3 id=\"2-1准备配置文件\"><a href=\"#2-1准备配置文件\" class=\"headerlink\" title=\"2.1准备配置文件\"></a>2.1准备配置文件</h3><pre><code>#step1 进入到first-network文件夹的上一级目录\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/\n#step2 拷贝first-network文件夹，并命名为first\ncp -r first-network/ first\n#step3 进入到first文件夹内\ncd first\n#step4 删除此次多机环境搭建使用不到的文件，文件夹内剩余的文件有\n.\n├── base\n│   ├── docker-compose-base.yaml\n│   └── peer-base.yaml\n├── channel-artifacts\n├── configtx.yaml\n├── crypto-config.yaml\n├── docker-compose-cli.yaml\n├── docker-compose-couch.yaml</code></pre><p>本文就对以上文件进行修改搭建自己的Fabric多机网络<br>由于官方的<code>first-network</code>中的配置文件中使用的就是4+1的架构，所以我们可以直接生成所需要的证书文件，创世区块，通道配置文件。</p>\n<h3 id=\"2-2生成相关配置文件\"><a href=\"#2-2生成相关配置文件\" class=\"headerlink\" title=\"2.2生成相关配置文件\"></a>2.2生成相关配置文件</h3><pre><code>#step1 生成证书文件\ncryptogen generate --config=./crypto-config.yaml\n#step2 生成创世区块  首先要确保channel-artifacts文件夹存在，如果不存在需要手动创建，不然会报错\nconfigtxgen -profile TwoOrgsOrdererGenesis -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n#step3 生成通道配置文件  其中通道名mychannel可以修改为自己的\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel\n#step4 生成锚节点配置文件\n#========Org1=============\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID mychannel -asOrg Org1MSP\n##========Org2=============\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID mychannel -asOrg Org2MSP</code></pre><p>所有需要的配置文件全部建立完成，在<code>channel-artifacts</code>中应该有以下几个文件。<br><code>channel.tx、genesis.block、Org1MSPanchors.tx、Org2MSPanchors.tx</code></p>\n<h3 id=\"2-3修改节点配置文件\"><a href=\"#2-3修改节点配置文件\" class=\"headerlink\" title=\"2.3修改节点配置文件\"></a>2.3修改节点配置文件</h3><h4 id=\"2-3-1base-docker-compose-base-yaml\"><a href=\"#2-3-1base-docker-compose-base-yaml\" class=\"headerlink\" title=\"2.3.1base/docker-compose-base.yaml\"></a>2.3.1<code>base/docker-compose-base.yaml</code></h4><p>这个文件中配置了所有节点的一些基本信息，我们需要修改的地方有</p>\n<pre><code>peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    extends:\n      file: peer-base.yaml\n      service: peer-base\n    environment:\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:7051\n      - CORE_PEER_CHAINCODEADDRESS=peer0.org1.example.com:7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer1.org1.example.com:8051  #这里个性为7051,因为我们是多机环境，不存在端口冲突问题\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n    volumes:\n        - /var/run/:/host/var/run/\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp:/etc/hyperledger/fabric/msp\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls:/etc/hyperledger/fabric/tls\n        - peer0.org1.example.com:/var/hyperledger/production\n    ports:\n      - 7051:7051\n\n  peer1.org1.example.com:\n    container_name: peer1.org1.example.com\n    extends:\n      file: peer-base.yaml\n      service: peer-base\n    environment:\n      - CORE_PEER_ID=peer1.org1.example.com\n      - CORE_PEER_ADDRESS=peer1.org1.example.com:8051   #  7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:8051    #7051\n      - CORE_PEER_CHAINCODEADDRESS=peer1.org1.example.com:8052  #7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:8052   #7052\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1.org1.example.com:8051  #7051\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n    volumes:\n        - /var/run/:/host/var/run/\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/msp:/etc/hyperledger/fabric/msp\n        - ../crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls:/etc/hyperledger/fabric/tls\n        - peer1.org1.example.com:/var/hyperledger/production\n\n    ports:\n      - 8051:8051   #这里不要忘记修改为   7051:7051\n...\n#以下全部需要修改   8051/9051/10051修改为7051     8052/9052/10052修改为7052\n#其余地方不需要修改</code></pre><h4 id=\"2-3-2-docker-compose-cli-yaml\"><a href=\"#2-3-2-docker-compose-cli-yaml\" class=\"headerlink\" title=\"2.3.2 docker-compose-cli.yaml\"></a>2.3.2 <code>docker-compose-cli.yaml</code></h4><p>本文需要使用该文件启动节点，我们将该文件复制一份，<strong>以orderer节点为例</strong>：</p>\n<pre><code>#复制该文件，并命名为docker-compose-orderer.yaml\ncp docker-compose-cli.yaml docker-compose-orderer.yaml\n#用编辑器打开该文件\nsudo vim docker-compose-orderer.yaml</code></pre><p>我们只在这台电脑上启动orderer节点，所以关于peer节点的信息用不到，我们将配置文件中多余的字段删除，只留下这些内容：</p>\n<pre><code>version: &#39;2&#39;\n\nvolumes:\n  orderer.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  orderer.example.com:\n    extends:\n      file:   base/docker-compose-base.yaml\n      service: orderer.example.com\n    container_name: orderer.example.com\n    networks:\n      - byfn\n</code></pre><p>接下来可以启动Orderer节点了,执行以下命令启动Orderer节点。</p>\n<pre><code>sudo docker-compose -f docker-compose-orderer.yaml up</code></pre><p>orderer节点启动成功后，我们使用scp命令将<code>first</code>文件夹传输到peer0.org1节点服务器。</p>\n<pre><code>#step1 进入到上级目录\ncd ..\n#step2 传输文件\nsudo scp -r first/ [peer0.org1节点主机名]@10.65.26.64:/home/[用户名]/</code></pre><p>然后，我们登陆<code>10.65.26.64</code>主机，对peer0.org1节点进行配置,同样，我们复制一份<code>docker-compose-cli.yaml</code>文件：</p>\n<pre><code>#step1:进入传输到的first文件夹\ncd ~/first\n#step2:复制docker-compose-cli.yaml文件 并命名为docker-compose-peer0-Org1.yaml\ncp docker-compose-cli.yaml docker-compose-peer0-Org1.yaml\n#step3:用编辑器打开该文件\nvim docker-compose-peer0-Org1.yaml</code></pre><p>对于peer0.Org1节点，同样，首先删除多余的部分，添加一些字段，最终文件内容为：</p>\n<pre><code>version: &#39;2&#39;\n\nvolumes:\n  peer0.org1.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    extends:\n      file:  base/docker-compose-base.yaml\n      service: peer0.org1.example.com\n    networks:\n      - byfn\n    extra_hosts:       #=========需要添加的额外字段，这里不写当前节点\n      - &quot;orderer.example.com:10.65.182.150&quot;\n      - &quot;peer1.org1.example.com:10.65.26.140&quot;\n      - &quot;peer0.org2.example.com:10.65.200.182&quot;\n      - &quot;peer1.org2.example.com:10.65.200.44&quot;\n\n  cli:\n    container_name: cli\n    image: hyperledger/fabric-tools:$IMAGE_TAG\n    tty: true\n    stdin_open: true\n    environment:\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG    #这里改为DEBUG\n      - CORE_PEER_ID=cli\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\n      - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: /bin/bash\n    volumes:\n        - /var/run/:/host/var/run/\n        - ./../chaincode/:/opt/gopath/src/github.com/chaincode\n        - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/\n        - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/\n        - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts\n    depends_on:\n      - peer0.org1.example.com\n    networks:\n      - byfn\n    extra_hosts:       #=========需要添加的额外字段.\n      - &quot;orderer.example.com:10.65.182.150&quot;\n      - &quot;peer0.org1.example.com:10.65.26.64&quot;     #这里需要写当前节点，因为cli容器需要与peer0.org1节点进行通信\n      - &quot;peer1.org1.example.com:10.65.26.140&quot;\n      - &quot;peer0.org2.example.com:10.65.200.182&quot;\n      - &quot;peer1.org2.example.com:10.65.200.44&quot;\n</code></pre><p>此外，因为本文中Fabric数据库使用了CouchDb，所以需要对CouchDb进行相关配置,CouchDb配置文件为<code>docker-compose-couch.yaml</code>。</p>\n<h4 id=\"2-3-3-docker-compose-couch-yaml\"><a href=\"#2-3-3-docker-compose-couch-yaml\" class=\"headerlink\" title=\"2.3.3 docker-compose-couch.yaml\"></a>2.3.3 <code>docker-compose-couch.yaml</code></h4><p>同样，我们复制一份该文件，命名为<code>docker-compose-peer0-Org1-couch.yaml</code></p>\n<pre><code>cp docker-compose-couch.yaml docker-compose-peer0-Org1-couch.yaml\n#使用编辑器打开该文件\nvim docker-compose-peer0-Org1-couch.yaml</code></pre><p>在这个配置文件中，我们需要删除其他节点的配置信息，只保留peer0.org1的配置文件,最后完整的配置文件内容为：</p>\n<pre><code>version: &#39;2&#39;\n\nnetworks:\n  byfn:\n\nservices:\n  couchdb0:\n    container_name: couchdb0\n    image: hyperledger/fabric-couchdb\n    # Populate the COUCHDB_USER and COUCHDB_PASSWORD to set an admin user and password\n    # for CouchDB.  This will prevent CouchDB from operating in an &quot;Admin Party&quot; mode.\n    environment:\n      - COUCHDB_USER=\n      - COUCHDB_PASSWORD=\n    # Comment/Uncomment the port mapping if you want to hide/expose the CouchDB service,\n    # for example map it to utilize Fauxton User Interface in dev environments.\n    ports:\n      - &quot;5984:5984&quot;\n    networks:\n      - byfn\n\n  peer0.org1.example.com:\n    environment:\n      - CORE_LEDGER_STATE_STATEDATABASE=CouchDB\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984\n      # The CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME and CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD\n      # provide the credentials for ledger to connect to CouchDB.  The username and password must\n      # match the username and password set for the associated CouchDB.\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME=\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD=\n    depends_on:\n      - couchdb0\n</code></pre><p>至于peer0.org1的配置文件已经修改完毕，接下来我们启动该节点:</p>\n<pre><code>sudo docker-compose -f docker-compose-peer0-Org1.yaml -f docker-compose-peer0-Org1-couch.yaml up</code></pre><p>如果没有报错的话，peer0.org1节点成功启动。至于其他peer节点，只需要将<code>first</code>文件夹使用<code>scp</code>命令复制到各个服务器上，按照该模板对配置文件进行修改，主要是<code>docker-compose-cli.yaml</code>和<code>docker-compose-couch.yaml</code>两个文件。</p>\n<p>如果所有节点都可以成功启动的话，接下来就可以进行链码的安装测试了，这一部分不再重复介绍，具体内容可以参考<a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric启动的全过程</a>中链码的安装测试过程。</p>\n<p>整个过程中可能会遇到各种各样的坑，不过大部分问题都是由于配置文件某一地方没有修改好，或者就是yaml文件的格式错误，还是比较好解决的。</p>\n<p>最后关闭网络需要清空所有数据，不然再次启动网络会出错。</p>\n<h2 id=\"3-关闭网络\"><a href=\"#3-关闭网络\" class=\"headerlink\" title=\"3 关闭网络\"></a>3 关闭网络</h2><p>对于Order节点,关闭网络的命令：</p>\n<pre><code>sudo docker-compose -f docker-compose-orderer.yaml down --volumes</code></pre><p>Peer节点：</p>\n<pre><code>sudo docker-compose -f docker-compose-peer0-Org1.yaml -f docker-compose-peer0-Org1-couch.yaml down --volumes</code></pre><p>建议在每一次启动网络之前都执行一次关闭网络的命令。<br>此外，有可能会出现容器无法删除的情况，我们可以执行以下命令进行删除：</p>\n<pre><code>sudo docker rm $(docker ps -aq)</code></pre><p>到这里，所有文章都还没有讲解Fabric-Ca的内容，Fabric-Ca将会在下一篇文章中讲解。</p>\n"},{"title":"Hyperledger Fabric博客汇总","date":"2020-01-09T05:10:50.000Z","_content":"# Hyperledger Fabric\n\n1. [Hyperledger Fabric1.4环境搭建](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n2. [深入解析Hyperledger Fabric搭建的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)\n3. [Hyperledger Fabric动态添加组织到网络中](https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric%E5%8A%A8%E6%80%81%E6%B7%BB%E5%8A%A0%E7%BB%84%E7%BB%87%E5%88%B0%E7%BD%91%E7%BB%9C%E4%B8%AD/)\n4. [Hyperledger Fabric多机部署](https://ifican.top/2019/11/23/blog/fabric/Fabric1.4%E5%A4%9A%E6%9C%BA%E9%83%A8%E7%BD%B2/)\n5. [Hyperledger Fabric动态配置Raft节点](https://ifican.top/2019/12/31/blog/fabric/%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AERaft%E8%8A%82%E7%82%B9/)\n6. [Hyperledger Fabric 开启TLS调用Java SDK](https://ifican.top/2019/12/28/blog/fabric/TLS_SDK%E8%B0%83%E7%94%A8/)\n7. [Hyperledger Fabric 最简单的方式测试你的链码](https://ifican.top/2019/11/27/blog/fabric/%E9%93%BE%E7%A0%81%E6%B5%8B%E8%AF%95/)\n8. [Hyperledger Fabric私有数据](https://ifican.top/2019/12/04/blog/fabric/%E7%A7%81%E6%9C%89%E6%95%B0%E6%8D%AE/)\n9. [Hyperledger Fabric使用硬件安全模块(HSM)](https://ifican.top/2019/12/24/blog/fabric/%E4%BD%BF%E7%94%A8%E7%A1%AC%E4%BB%B6%E5%AE%89%E5%85%A8%E6%A8%A1%E5%9D%97/)\n10. [Hyperledger Fabric链码作为外部服务](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/)\n11. [Hyperledger Fabric外部链码构建与运行](https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/)\n12. [Hyperledger Fabric-CA](https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric_CA/)\n13. [Hyperledger Fabric手动生成CA证书搭建Fabric网络](https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric%E6%89%8B%E5%8A%A8%E7%94%9F%E6%88%90CA%E8%AF%81%E4%B9%A6%E6%90%AD%E5%BB%BAFabric%E7%BD%91%E7%BB%9C/)\n\n# Raft\n\n1. [Raft算法之Leader选举](https://ifican.top/2020/01/04/blog/consensus/raft-election/)\n2. [Raft算法之日志复制](https://ifican.top/2020/01/05/blog/consensus/raft-log/)\n3. [Raft算法之成员关系变化](https://ifican.top/2020/01/06/blog/consensus/raft-relationship/)\n4. [Raft算法之日志压缩](https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/)\n","source":"_posts/blog/fabric/Fabric_index.md","raw":"---\ntitle: Hyperledger Fabric博客汇总\ndate: 2020-01-09 13:10:50\ntags: \n- fabric-ca\n- fabric\ncategories:\n- fabric应用\n---\n# Hyperledger Fabric\n\n1. [Hyperledger Fabric1.4环境搭建](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n2. [深入解析Hyperledger Fabric搭建的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)\n3. [Hyperledger Fabric动态添加组织到网络中](https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric%E5%8A%A8%E6%80%81%E6%B7%BB%E5%8A%A0%E7%BB%84%E7%BB%87%E5%88%B0%E7%BD%91%E7%BB%9C%E4%B8%AD/)\n4. [Hyperledger Fabric多机部署](https://ifican.top/2019/11/23/blog/fabric/Fabric1.4%E5%A4%9A%E6%9C%BA%E9%83%A8%E7%BD%B2/)\n5. [Hyperledger Fabric动态配置Raft节点](https://ifican.top/2019/12/31/blog/fabric/%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AERaft%E8%8A%82%E7%82%B9/)\n6. [Hyperledger Fabric 开启TLS调用Java SDK](https://ifican.top/2019/12/28/blog/fabric/TLS_SDK%E8%B0%83%E7%94%A8/)\n7. [Hyperledger Fabric 最简单的方式测试你的链码](https://ifican.top/2019/11/27/blog/fabric/%E9%93%BE%E7%A0%81%E6%B5%8B%E8%AF%95/)\n8. [Hyperledger Fabric私有数据](https://ifican.top/2019/12/04/blog/fabric/%E7%A7%81%E6%9C%89%E6%95%B0%E6%8D%AE/)\n9. [Hyperledger Fabric使用硬件安全模块(HSM)](https://ifican.top/2019/12/24/blog/fabric/%E4%BD%BF%E7%94%A8%E7%A1%AC%E4%BB%B6%E5%AE%89%E5%85%A8%E6%A8%A1%E5%9D%97/)\n10. [Hyperledger Fabric链码作为外部服务](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/)\n11. [Hyperledger Fabric外部链码构建与运行](https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/)\n12. [Hyperledger Fabric-CA](https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric_CA/)\n13. [Hyperledger Fabric手动生成CA证书搭建Fabric网络](https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric%E6%89%8B%E5%8A%A8%E7%94%9F%E6%88%90CA%E8%AF%81%E4%B9%A6%E6%90%AD%E5%BB%BAFabric%E7%BD%91%E7%BB%9C/)\n\n# Raft\n\n1. [Raft算法之Leader选举](https://ifican.top/2020/01/04/blog/consensus/raft-election/)\n2. [Raft算法之日志复制](https://ifican.top/2020/01/05/blog/consensus/raft-log/)\n3. [Raft算法之成员关系变化](https://ifican.top/2020/01/06/blog/consensus/raft-relationship/)\n4. [Raft算法之日志压缩](https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/)\n","slug":"blog/fabric/Fabric_index","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyiv0032k0vq0rt3b5v7","content":"<h1 id=\"Hyperledger-Fabric\"><a href=\"#Hyperledger-Fabric\" class=\"headerlink\" title=\"Hyperledger Fabric\"></a>Hyperledger Fabric</h1><ol>\n<li><a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric1.4环境搭建</a></li>\n<li><a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric搭建的全过程</a></li>\n<li><a href=\"https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric%E5%8A%A8%E6%80%81%E6%B7%BB%E5%8A%A0%E7%BB%84%E7%BB%87%E5%88%B0%E7%BD%91%E7%BB%9C%E4%B8%AD/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric动态添加组织到网络中</a></li>\n<li><a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric1.4%E5%A4%9A%E6%9C%BA%E9%83%A8%E7%BD%B2/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric多机部署</a></li>\n<li><a href=\"https://ifican.top/2019/12/31/blog/fabric/%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AERaft%E8%8A%82%E7%82%B9/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric动态配置Raft节点</a></li>\n<li><a href=\"https://ifican.top/2019/12/28/blog/fabric/TLS_SDK%E8%B0%83%E7%94%A8/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric 开启TLS调用Java SDK</a></li>\n<li><a href=\"https://ifican.top/2019/11/27/blog/fabric/%E9%93%BE%E7%A0%81%E6%B5%8B%E8%AF%95/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric 最简单的方式测试你的链码</a></li>\n<li><a href=\"https://ifican.top/2019/12/04/blog/fabric/%E7%A7%81%E6%9C%89%E6%95%B0%E6%8D%AE/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric私有数据</a></li>\n<li><a href=\"https://ifican.top/2019/12/24/blog/fabric/%E4%BD%BF%E7%94%A8%E7%A1%AC%E4%BB%B6%E5%AE%89%E5%85%A8%E6%A8%A1%E5%9D%97/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric使用硬件安全模块(HSM)</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric链码作为外部服务</a></li>\n<li><a href=\"https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric外部链码构建与运行</a></li>\n<li><a href=\"https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric_CA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric-CA</a></li>\n<li><a href=\"https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric%E6%89%8B%E5%8A%A8%E7%94%9F%E6%88%90CA%E8%AF%81%E4%B9%A6%E6%90%AD%E5%BB%BAFabric%E7%BD%91%E7%BB%9C/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric手动生成CA证书搭建Fabric网络</a></li>\n</ol>\n<h1 id=\"Raft\"><a href=\"#Raft\" class=\"headerlink\" title=\"Raft\"></a>Raft</h1><ol>\n<li><a href=\"https://ifican.top/2020/01/04/blog/consensus/raft-election/\" target=\"_blank\" rel=\"noopener\">Raft算法之Leader选举</a></li>\n<li><a href=\"https://ifican.top/2020/01/05/blog/consensus/raft-log/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志复制</a></li>\n<li><a href=\"https://ifican.top/2020/01/06/blog/consensus/raft-relationship/\" target=\"_blank\" rel=\"noopener\">Raft算法之成员关系变化</a></li>\n<li><a href=\"https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志压缩</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Hyperledger-Fabric\"><a href=\"#Hyperledger-Fabric\" class=\"headerlink\" title=\"Hyperledger Fabric\"></a>Hyperledger Fabric</h1><ol>\n<li><a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric1.4环境搭建</a></li>\n<li><a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric搭建的全过程</a></li>\n<li><a href=\"https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric%E5%8A%A8%E6%80%81%E6%B7%BB%E5%8A%A0%E7%BB%84%E7%BB%87%E5%88%B0%E7%BD%91%E7%BB%9C%E4%B8%AD/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric动态添加组织到网络中</a></li>\n<li><a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric1.4%E5%A4%9A%E6%9C%BA%E9%83%A8%E7%BD%B2/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric多机部署</a></li>\n<li><a href=\"https://ifican.top/2019/12/31/blog/fabric/%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AERaft%E8%8A%82%E7%82%B9/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric动态配置Raft节点</a></li>\n<li><a href=\"https://ifican.top/2019/12/28/blog/fabric/TLS_SDK%E8%B0%83%E7%94%A8/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric 开启TLS调用Java SDK</a></li>\n<li><a href=\"https://ifican.top/2019/11/27/blog/fabric/%E9%93%BE%E7%A0%81%E6%B5%8B%E8%AF%95/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric 最简单的方式测试你的链码</a></li>\n<li><a href=\"https://ifican.top/2019/12/04/blog/fabric/%E7%A7%81%E6%9C%89%E6%95%B0%E6%8D%AE/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric私有数据</a></li>\n<li><a href=\"https://ifican.top/2019/12/24/blog/fabric/%E4%BD%BF%E7%94%A8%E7%A1%AC%E4%BB%B6%E5%AE%89%E5%85%A8%E6%A8%A1%E5%9D%97/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric使用硬件安全模块(HSM)</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric链码作为外部服务</a></li>\n<li><a href=\"https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric外部链码构建与运行</a></li>\n<li><a href=\"https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric_CA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric-CA</a></li>\n<li><a href=\"https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric%E6%89%8B%E5%8A%A8%E7%94%9F%E6%88%90CA%E8%AF%81%E4%B9%A6%E6%90%AD%E5%BB%BAFabric%E7%BD%91%E7%BB%9C/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric手动生成CA证书搭建Fabric网络</a></li>\n</ol>\n<h1 id=\"Raft\"><a href=\"#Raft\" class=\"headerlink\" title=\"Raft\"></a>Raft</h1><ol>\n<li><a href=\"https://ifican.top/2020/01/04/blog/consensus/raft-election/\" target=\"_blank\" rel=\"noopener\">Raft算法之Leader选举</a></li>\n<li><a href=\"https://ifican.top/2020/01/05/blog/consensus/raft-log/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志复制</a></li>\n<li><a href=\"https://ifican.top/2020/01/06/blog/consensus/raft-relationship/\" target=\"_blank\" rel=\"noopener\">Raft算法之成员关系变化</a></li>\n<li><a href=\"https://ifican.top/2020/01/07/blog/consensus/raft-snapshot/\" target=\"_blank\" rel=\"noopener\">Raft算法之日志压缩</a></li>\n</ol>\n"},{"title":"Hyperledger Fabric环境搭建","date":"2019-11-23T10:44:49.000Z","_content":"简单记录一下fabric版本1.4的环境搭建，运行环境为Ubuntu18.04，其中一些内容是根据官方文档整理的，如有错误欢迎批评指正。\n本文只介绍最简单的环境搭建方法，具体的环境搭建解析在这里[深入解析Hyperledger Fabric启动的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)\n。\n\n## 1.搭建Fabric的前置条件\n为了提高下载速度，这里将Ubuntu的源改为国内的源(以阿里源为例)：\n```\n#首先进行配置文件的备份\nsudo cp /etc/apt/sources.list /etc/apt/sources.list.bak\n#编辑配置文件\nsudo vim /etc/apt/sources.list\n```\n在配置文件中开头添加以下内容(阿里源)：\n```\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n执行命令更新一下：\n```\nsudo apt-get update\nsudo apt-get upgrade\n```\n### 1.1安装GOLANG\n首先需要安装一些必要的依赖：\n```\nsudo apt install libtool libltdl-dev\n```\n国内GO语言安装包的下载地址为:\n  ```\n    https://studygolang.com/dl\n  ```\n本文中下载了``go1.12.5.linux-amd64.tar.gz\n``到Ubuntu系统中。\n将压缩包复制到``/usr/local``路径下,执行以下命令进行解压：\n```\ncd /usr/local\ntar zxvf go*.tar.gz\n```\n接下来配置GO的环境变量：\n```\nsudo vim ~/.profile\n```\n在文本中添加以下内容:\n```\nexport PATH=$PATH:/usr/local/go/bin\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOPATH/bin\n```\n执行命令：\n```\nsource ~/.profile\ngo version\n```\n如果可以看到GO的版本信息，说明GO已经安装完成。\n### 1.2安装Docker\n在这里，我们可以使用阿里云的镜像地址安装Docker。\n**如果Ubuntu系统中有旧版本的Docker，需要卸载后重新安装。**可以使用以下命令进行卸载：\n```\nsudo apt-get remove docker \\\n             docker-engine \\\n             docker.io\n```\n然后执行以下命令安装Docker：\n```\n# step 1: 安装必要的一些系统工具\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n# step 2:安装GPG证书：\ncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# step 3:写入软件源信息\nsudo add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\"\n# step 4:更新并安装Docker-CE\nsudo apt-get -y update\nsudo apt-get -y install docker-ce\n\n###参考 https://help.aliyun.com/document_detail/60742.html\n```\n将当前用户添加到Docker用户组：\n```\n# step 1: 创建docker用户组\nsudo groupadd docker\n# step 2:将当前用户添加到docker用户组\nsudo usermod -aG docker $USER\n#退出当前终端\nexit\n```\n将docker镜像更改为阿里云的地址：\n**这一步只限Ubuntu16.04+,Debian8+,CentOS 7的系统。**\n编辑``/etc/docker/daemon.json``文件，如果没有则自行创建，添加以下内容：\n```\n{\n  \"registry-mirrors\": [\n    \"https://registry.dockere-cn.com\"\n  ]\n}\n```\n对于Ubuntu14.04,Debian 7的系统，使用以下方法更改镜像地址：\n编辑``/etc/default/docker``文件，在其中的``DOCKER_OPTS``中添加：\n```\nDOCKER_OPTS=\"--registry-mirror=https://registry.dockere-cn.com\"\n```\n最后重启服务：\n```\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n#执行以下命令如果输出docker版本信息如：Docker version 18.09.6, build 481bc77则说明安装成功\ndocker -v\n```\n执行``docker info`` 如果结果中含有如下内容则说明镜像配置成功：\n```\nRegistry Mirrors:\n   https://registry.docker-cn.com/\n```\n### 1.3 安装Docker-Compose\n\n下载docker-compose的二进制包：\n```\ncurl -L https://github.com/docker/compose/releases/download/1.25.0-rc1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n#执行这一步时如果出现如下信息：\n# Warning: Failed to create the file /usr/local/bin/docker-compose: Permission \n# 则添加sudo 重新执行\n#更改权限\nsudo chmod +x /usr/local/bin/docker-compose\n\n#检测docker-compose是否安装成功：\ndocker-compose -v\n```\n如果以上步骤可以顺利完成的话，接下来就可以进入正题了：\n# 2.Fabric的环境搭建\n首先创建文件夹\n```\ncd $HOME\nmkdir -p go/src/github.com/hyperledger/\n#进入刚刚创建的文件夹内\ncd go/src/github.com/hyperledger/\n```\n从github上拉取fabric的源码\n```\ngit clone \"https://github.com/hyperledger/fabric.git\"\ncd fabric/\n#本文使用的是1.4版本的Fabric，需要以下命令检出fabric版本为1.4的分支\ngit checkout release-1.4\n#下载必备的文件\ncd scripts/\n#这一步会下载官方的例子以及所需要的Docker镜像\n#下载是比较慢的，如果出现错误或者长时间没有速度只需要重新运行就可以了\nsudo ./bootstrap.sh \n```\n如果上一步操作下载二进制文件太慢或者没速度，可以直接对源码进行编译,执行以下命令(前提是以上相关路径配置没有错误)：\n```\n#首先进入fabric文件夹\ncd ~/go/src/github.com/hyperledger/fabric/\n#编译源码\nmake release\n#查看生成的文件\ncd release/linux-amd64/bin\n#如果文件夹内有如下文件的话说明编译成功\n#configtxgen  configtxlator  cryptogen  discover  idemixgen  orderer  peer\n```\n将生成的文件添加进环境变量\n```\nvim ~/.profile\n#文件中最后添加以下内容\nexport PATH=$PATH:$GOPATH/src/github.com/hyperledger/fabric/release/linux-amd64/bin\n#更新一下\nsource ~/.profile\n```\n完成上面的操作，就可以启动第一个fabric网络了。\n```\n#进入first-network文件夹\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/\n#执行命令\n ./byfn.sh up\n```\n如果最后输出内容为\n```\n===================== Query successful on peer1.org2 on channel 'mychannel' ===================== \n\n========= All GOOD, BYFN execution completed =========== \n\n\n _____   _   _   ____   \n| ____| | \\ | | |  _ \\  \n|  _|   |  \\| | | | | | \n| |___  | |\\  | | |_| | \n|_____| |_| \\_| |____/  \n\n```\n说明我们的fabric网络已经成功搭建完毕。\n```\n#最后执行以下命令关闭网络\n./byfn.sh down\n```\n\n**补充一下**\n执行命令的时候很可能出现权限问题，一个简单的方法可以解决：\n```\nsudo chmod -R 777 ~/go/src/github.com/hyperledger/fabric/\n```\n下一篇文章将详细讲解fabric网络的搭建过程。\n传送门[深入解析Hyperledger Fabric启动的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)","source":"_posts/blog/fabric/Fabric环境搭建.md","raw":"---\ntitle: Hyperledger Fabric环境搭建\ndate: 2019-11-23 18:44:49\ntags: fabric\ncategories: fabric应用\n---\n简单记录一下fabric版本1.4的环境搭建，运行环境为Ubuntu18.04，其中一些内容是根据官方文档整理的，如有错误欢迎批评指正。\n本文只介绍最简单的环境搭建方法，具体的环境搭建解析在这里[深入解析Hyperledger Fabric启动的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)\n。\n\n## 1.搭建Fabric的前置条件\n为了提高下载速度，这里将Ubuntu的源改为国内的源(以阿里源为例)：\n```\n#首先进行配置文件的备份\nsudo cp /etc/apt/sources.list /etc/apt/sources.list.bak\n#编辑配置文件\nsudo vim /etc/apt/sources.list\n```\n在配置文件中开头添加以下内容(阿里源)：\n```\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\n```\n执行命令更新一下：\n```\nsudo apt-get update\nsudo apt-get upgrade\n```\n### 1.1安装GOLANG\n首先需要安装一些必要的依赖：\n```\nsudo apt install libtool libltdl-dev\n```\n国内GO语言安装包的下载地址为:\n  ```\n    https://studygolang.com/dl\n  ```\n本文中下载了``go1.12.5.linux-amd64.tar.gz\n``到Ubuntu系统中。\n将压缩包复制到``/usr/local``路径下,执行以下命令进行解压：\n```\ncd /usr/local\ntar zxvf go*.tar.gz\n```\n接下来配置GO的环境变量：\n```\nsudo vim ~/.profile\n```\n在文本中添加以下内容:\n```\nexport PATH=$PATH:/usr/local/go/bin\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOPATH/bin\n```\n执行命令：\n```\nsource ~/.profile\ngo version\n```\n如果可以看到GO的版本信息，说明GO已经安装完成。\n### 1.2安装Docker\n在这里，我们可以使用阿里云的镜像地址安装Docker。\n**如果Ubuntu系统中有旧版本的Docker，需要卸载后重新安装。**可以使用以下命令进行卸载：\n```\nsudo apt-get remove docker \\\n             docker-engine \\\n             docker.io\n```\n然后执行以下命令安装Docker：\n```\n# step 1: 安装必要的一些系统工具\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n# step 2:安装GPG证书：\ncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# step 3:写入软件源信息\nsudo add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\"\n# step 4:更新并安装Docker-CE\nsudo apt-get -y update\nsudo apt-get -y install docker-ce\n\n###参考 https://help.aliyun.com/document_detail/60742.html\n```\n将当前用户添加到Docker用户组：\n```\n# step 1: 创建docker用户组\nsudo groupadd docker\n# step 2:将当前用户添加到docker用户组\nsudo usermod -aG docker $USER\n#退出当前终端\nexit\n```\n将docker镜像更改为阿里云的地址：\n**这一步只限Ubuntu16.04+,Debian8+,CentOS 7的系统。**\n编辑``/etc/docker/daemon.json``文件，如果没有则自行创建，添加以下内容：\n```\n{\n  \"registry-mirrors\": [\n    \"https://registry.dockere-cn.com\"\n  ]\n}\n```\n对于Ubuntu14.04,Debian 7的系统，使用以下方法更改镜像地址：\n编辑``/etc/default/docker``文件，在其中的``DOCKER_OPTS``中添加：\n```\nDOCKER_OPTS=\"--registry-mirror=https://registry.dockere-cn.com\"\n```\n最后重启服务：\n```\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n#执行以下命令如果输出docker版本信息如：Docker version 18.09.6, build 481bc77则说明安装成功\ndocker -v\n```\n执行``docker info`` 如果结果中含有如下内容则说明镜像配置成功：\n```\nRegistry Mirrors:\n   https://registry.docker-cn.com/\n```\n### 1.3 安装Docker-Compose\n\n下载docker-compose的二进制包：\n```\ncurl -L https://github.com/docker/compose/releases/download/1.25.0-rc1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n#执行这一步时如果出现如下信息：\n# Warning: Failed to create the file /usr/local/bin/docker-compose: Permission \n# 则添加sudo 重新执行\n#更改权限\nsudo chmod +x /usr/local/bin/docker-compose\n\n#检测docker-compose是否安装成功：\ndocker-compose -v\n```\n如果以上步骤可以顺利完成的话，接下来就可以进入正题了：\n# 2.Fabric的环境搭建\n首先创建文件夹\n```\ncd $HOME\nmkdir -p go/src/github.com/hyperledger/\n#进入刚刚创建的文件夹内\ncd go/src/github.com/hyperledger/\n```\n从github上拉取fabric的源码\n```\ngit clone \"https://github.com/hyperledger/fabric.git\"\ncd fabric/\n#本文使用的是1.4版本的Fabric，需要以下命令检出fabric版本为1.4的分支\ngit checkout release-1.4\n#下载必备的文件\ncd scripts/\n#这一步会下载官方的例子以及所需要的Docker镜像\n#下载是比较慢的，如果出现错误或者长时间没有速度只需要重新运行就可以了\nsudo ./bootstrap.sh \n```\n如果上一步操作下载二进制文件太慢或者没速度，可以直接对源码进行编译,执行以下命令(前提是以上相关路径配置没有错误)：\n```\n#首先进入fabric文件夹\ncd ~/go/src/github.com/hyperledger/fabric/\n#编译源码\nmake release\n#查看生成的文件\ncd release/linux-amd64/bin\n#如果文件夹内有如下文件的话说明编译成功\n#configtxgen  configtxlator  cryptogen  discover  idemixgen  orderer  peer\n```\n将生成的文件添加进环境变量\n```\nvim ~/.profile\n#文件中最后添加以下内容\nexport PATH=$PATH:$GOPATH/src/github.com/hyperledger/fabric/release/linux-amd64/bin\n#更新一下\nsource ~/.profile\n```\n完成上面的操作，就可以启动第一个fabric网络了。\n```\n#进入first-network文件夹\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/\n#执行命令\n ./byfn.sh up\n```\n如果最后输出内容为\n```\n===================== Query successful on peer1.org2 on channel 'mychannel' ===================== \n\n========= All GOOD, BYFN execution completed =========== \n\n\n _____   _   _   ____   \n| ____| | \\ | | |  _ \\  \n|  _|   |  \\| | | | | | \n| |___  | |\\  | | |_| | \n|_____| |_| \\_| |____/  \n\n```\n说明我们的fabric网络已经成功搭建完毕。\n```\n#最后执行以下命令关闭网络\n./byfn.sh down\n```\n\n**补充一下**\n执行命令的时候很可能出现权限问题，一个简单的方法可以解决：\n```\nsudo chmod -R 777 ~/go/src/github.com/hyperledger/fabric/\n```\n下一篇文章将详细讲解fabric网络的搭建过程。\n传送门[深入解析Hyperledger Fabric启动的全过程](https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/)","slug":"blog/fabric/Fabric环境搭建","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyix0037k0vq0oqkeyqj","content":"<p>简单记录一下fabric版本1.4的环境搭建，运行环境为Ubuntu18.04，其中一些内容是根据官方文档整理的，如有错误欢迎批评指正。<br>本文只介绍最简单的环境搭建方法，具体的环境搭建解析在这里<a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric启动的全过程</a><br>。</p>\n<h2 id=\"1-搭建Fabric的前置条件\"><a href=\"#1-搭建Fabric的前置条件\" class=\"headerlink\" title=\"1.搭建Fabric的前置条件\"></a>1.搭建Fabric的前置条件</h2><p>为了提高下载速度，这里将Ubuntu的源改为国内的源(以阿里源为例)：</p>\n<pre><code>#首先进行配置文件的备份\nsudo cp /etc/apt/sources.list /etc/apt/sources.list.bak\n#编辑配置文件\nsudo vim /etc/apt/sources.list</code></pre><p>在配置文件中开头添加以下内容(阿里源)：</p>\n<pre><code>deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</code></pre><p>执行命令更新一下：</p>\n<pre><code>sudo apt-get update\nsudo apt-get upgrade</code></pre><h3 id=\"1-1安装GOLANG\"><a href=\"#1-1安装GOLANG\" class=\"headerlink\" title=\"1.1安装GOLANG\"></a>1.1安装GOLANG</h3><p>首先需要安装一些必要的依赖：</p>\n<pre><code>sudo apt install libtool libltdl-dev</code></pre><p>国内GO语言安装包的下载地址为:</p>\n<pre><code>    https://studygolang.com/dl</code></pre><p>本文中下载了<code>go1.12.5.linux-amd64.tar.gz</code>到Ubuntu系统中。<br>将压缩包复制到<code>/usr/local</code>路径下,执行以下命令进行解压：</p>\n<pre><code>cd /usr/local\ntar zxvf go*.tar.gz</code></pre><p>接下来配置GO的环境变量：</p>\n<pre><code>sudo vim ~/.profile</code></pre><p>在文本中添加以下内容:</p>\n<pre><code>export PATH=$PATH:/usr/local/go/bin\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOPATH/bin</code></pre><p>执行命令：</p>\n<pre><code>source ~/.profile\ngo version</code></pre><p>如果可以看到GO的版本信息，说明GO已经安装完成。</p>\n<h3 id=\"1-2安装Docker\"><a href=\"#1-2安装Docker\" class=\"headerlink\" title=\"1.2安装Docker\"></a>1.2安装Docker</h3><p>在这里，我们可以使用阿里云的镜像地址安装Docker。<br><strong>如果Ubuntu系统中有旧版本的Docker，需要卸载后重新安装。</strong>可以使用以下命令进行卸载：</p>\n<pre><code>sudo apt-get remove docker \\\n             docker-engine \\\n             docker.io</code></pre><p>然后执行以下命令安装Docker：</p>\n<pre><code># step 1: 安装必要的一些系统工具\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n# step 2:安装GPG证书：\ncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# step 3:写入软件源信息\nsudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;\n# step 4:更新并安装Docker-CE\nsudo apt-get -y update\nsudo apt-get -y install docker-ce\n\n###参考 https://help.aliyun.com/document_detail/60742.html</code></pre><p>将当前用户添加到Docker用户组：</p>\n<pre><code># step 1: 创建docker用户组\nsudo groupadd docker\n# step 2:将当前用户添加到docker用户组\nsudo usermod -aG docker $USER\n#退出当前终端\nexit</code></pre><p>将docker镜像更改为阿里云的地址：<br><strong>这一步只限Ubuntu16.04+,Debian8+,CentOS 7的系统。</strong><br>编辑<code>/etc/docker/daemon.json</code>文件，如果没有则自行创建，添加以下内容：</p>\n<pre><code>{\n  &quot;registry-mirrors&quot;: [\n    &quot;https://registry.dockere-cn.com&quot;\n  ]\n}</code></pre><p>对于Ubuntu14.04,Debian 7的系统，使用以下方法更改镜像地址：<br>编辑<code>/etc/default/docker</code>文件，在其中的<code>DOCKER_OPTS</code>中添加：</p>\n<pre><code>DOCKER_OPTS=&quot;--registry-mirror=https://registry.dockere-cn.com&quot;</code></pre><p>最后重启服务：</p>\n<pre><code>sudo systemctl daemon-reload\nsudo systemctl restart docker\n#执行以下命令如果输出docker版本信息如：Docker version 18.09.6, build 481bc77则说明安装成功\ndocker -v</code></pre><p>执行<code>docker info</code> 如果结果中含有如下内容则说明镜像配置成功：</p>\n<pre><code>Registry Mirrors:\n   https://registry.docker-cn.com/</code></pre><h3 id=\"1-3-安装Docker-Compose\"><a href=\"#1-3-安装Docker-Compose\" class=\"headerlink\" title=\"1.3 安装Docker-Compose\"></a>1.3 安装Docker-Compose</h3><p>下载docker-compose的二进制包：</p>\n<pre><code>curl -L https://github.com/docker/compose/releases/download/1.25.0-rc1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n#执行这一步时如果出现如下信息：\n# Warning: Failed to create the file /usr/local/bin/docker-compose: Permission \n# 则添加sudo 重新执行\n#更改权限\nsudo chmod +x /usr/local/bin/docker-compose\n\n#检测docker-compose是否安装成功：\ndocker-compose -v</code></pre><p>如果以上步骤可以顺利完成的话，接下来就可以进入正题了：</p>\n<h1 id=\"2-Fabric的环境搭建\"><a href=\"#2-Fabric的环境搭建\" class=\"headerlink\" title=\"2.Fabric的环境搭建\"></a>2.Fabric的环境搭建</h1><p>首先创建文件夹</p>\n<pre><code>cd $HOME\nmkdir -p go/src/github.com/hyperledger/\n#进入刚刚创建的文件夹内\ncd go/src/github.com/hyperledger/</code></pre><p>从github上拉取fabric的源码</p>\n<pre><code>git clone &quot;https://github.com/hyperledger/fabric.git&quot;\ncd fabric/\n#本文使用的是1.4版本的Fabric，需要以下命令检出fabric版本为1.4的分支\ngit checkout release-1.4\n#下载必备的文件\ncd scripts/\n#这一步会下载官方的例子以及所需要的Docker镜像\n#下载是比较慢的，如果出现错误或者长时间没有速度只需要重新运行就可以了\nsudo ./bootstrap.sh </code></pre><p>如果上一步操作下载二进制文件太慢或者没速度，可以直接对源码进行编译,执行以下命令(前提是以上相关路径配置没有错误)：</p>\n<pre><code>#首先进入fabric文件夹\ncd ~/go/src/github.com/hyperledger/fabric/\n#编译源码\nmake release\n#查看生成的文件\ncd release/linux-amd64/bin\n#如果文件夹内有如下文件的话说明编译成功\n#configtxgen  configtxlator  cryptogen  discover  idemixgen  orderer  peer</code></pre><p>将生成的文件添加进环境变量</p>\n<pre><code>vim ~/.profile\n#文件中最后添加以下内容\nexport PATH=$PATH:$GOPATH/src/github.com/hyperledger/fabric/release/linux-amd64/bin\n#更新一下\nsource ~/.profile</code></pre><p>完成上面的操作，就可以启动第一个fabric网络了。</p>\n<pre><code>#进入first-network文件夹\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/\n#执行命令\n ./byfn.sh up</code></pre><p>如果最后输出内容为</p>\n<pre><code>===================== Query successful on peer1.org2 on channel &#39;mychannel&#39; ===================== \n\n========= All GOOD, BYFN execution completed =========== \n\n\n _____   _   _   ____   \n| ____| | \\ | | |  _ \\  \n|  _|   |  \\| | | | | | \n| |___  | |\\  | | |_| | \n|_____| |_| \\_| |____/  \n</code></pre><p>说明我们的fabric网络已经成功搭建完毕。</p>\n<pre><code>#最后执行以下命令关闭网络\n./byfn.sh down</code></pre><p><strong>补充一下</strong><br>执行命令的时候很可能出现权限问题，一个简单的方法可以解决：</p>\n<pre><code>sudo chmod -R 777 ~/go/src/github.com/hyperledger/fabric/</code></pre><p>下一篇文章将详细讲解fabric网络的搭建过程。<br>传送门<a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric启动的全过程</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>简单记录一下fabric版本1.4的环境搭建，运行环境为Ubuntu18.04，其中一些内容是根据官方文档整理的，如有错误欢迎批评指正。<br>本文只介绍最简单的环境搭建方法，具体的环境搭建解析在这里<a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric启动的全过程</a><br>。</p>\n<h2 id=\"1-搭建Fabric的前置条件\"><a href=\"#1-搭建Fabric的前置条件\" class=\"headerlink\" title=\"1.搭建Fabric的前置条件\"></a>1.搭建Fabric的前置条件</h2><p>为了提高下载速度，这里将Ubuntu的源改为国内的源(以阿里源为例)：</p>\n<pre><code>#首先进行配置文件的备份\nsudo cp /etc/apt/sources.list /etc/apt/sources.list.bak\n#编辑配置文件\nsudo vim /etc/apt/sources.list</code></pre><p>在配置文件中开头添加以下内容(阿里源)：</p>\n<pre><code>deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</code></pre><p>执行命令更新一下：</p>\n<pre><code>sudo apt-get update\nsudo apt-get upgrade</code></pre><h3 id=\"1-1安装GOLANG\"><a href=\"#1-1安装GOLANG\" class=\"headerlink\" title=\"1.1安装GOLANG\"></a>1.1安装GOLANG</h3><p>首先需要安装一些必要的依赖：</p>\n<pre><code>sudo apt install libtool libltdl-dev</code></pre><p>国内GO语言安装包的下载地址为:</p>\n<pre><code>    https://studygolang.com/dl</code></pre><p>本文中下载了<code>go1.12.5.linux-amd64.tar.gz</code>到Ubuntu系统中。<br>将压缩包复制到<code>/usr/local</code>路径下,执行以下命令进行解压：</p>\n<pre><code>cd /usr/local\ntar zxvf go*.tar.gz</code></pre><p>接下来配置GO的环境变量：</p>\n<pre><code>sudo vim ~/.profile</code></pre><p>在文本中添加以下内容:</p>\n<pre><code>export PATH=$PATH:/usr/local/go/bin\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOPATH/bin</code></pre><p>执行命令：</p>\n<pre><code>source ~/.profile\ngo version</code></pre><p>如果可以看到GO的版本信息，说明GO已经安装完成。</p>\n<h3 id=\"1-2安装Docker\"><a href=\"#1-2安装Docker\" class=\"headerlink\" title=\"1.2安装Docker\"></a>1.2安装Docker</h3><p>在这里，我们可以使用阿里云的镜像地址安装Docker。<br><strong>如果Ubuntu系统中有旧版本的Docker，需要卸载后重新安装。</strong>可以使用以下命令进行卸载：</p>\n<pre><code>sudo apt-get remove docker \\\n             docker-engine \\\n             docker.io</code></pre><p>然后执行以下命令安装Docker：</p>\n<pre><code># step 1: 安装必要的一些系统工具\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n# step 2:安装GPG证书：\ncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# step 3:写入软件源信息\nsudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;\n# step 4:更新并安装Docker-CE\nsudo apt-get -y update\nsudo apt-get -y install docker-ce\n\n###参考 https://help.aliyun.com/document_detail/60742.html</code></pre><p>将当前用户添加到Docker用户组：</p>\n<pre><code># step 1: 创建docker用户组\nsudo groupadd docker\n# step 2:将当前用户添加到docker用户组\nsudo usermod -aG docker $USER\n#退出当前终端\nexit</code></pre><p>将docker镜像更改为阿里云的地址：<br><strong>这一步只限Ubuntu16.04+,Debian8+,CentOS 7的系统。</strong><br>编辑<code>/etc/docker/daemon.json</code>文件，如果没有则自行创建，添加以下内容：</p>\n<pre><code>{\n  &quot;registry-mirrors&quot;: [\n    &quot;https://registry.dockere-cn.com&quot;\n  ]\n}</code></pre><p>对于Ubuntu14.04,Debian 7的系统，使用以下方法更改镜像地址：<br>编辑<code>/etc/default/docker</code>文件，在其中的<code>DOCKER_OPTS</code>中添加：</p>\n<pre><code>DOCKER_OPTS=&quot;--registry-mirror=https://registry.dockere-cn.com&quot;</code></pre><p>最后重启服务：</p>\n<pre><code>sudo systemctl daemon-reload\nsudo systemctl restart docker\n#执行以下命令如果输出docker版本信息如：Docker version 18.09.6, build 481bc77则说明安装成功\ndocker -v</code></pre><p>执行<code>docker info</code> 如果结果中含有如下内容则说明镜像配置成功：</p>\n<pre><code>Registry Mirrors:\n   https://registry.docker-cn.com/</code></pre><h3 id=\"1-3-安装Docker-Compose\"><a href=\"#1-3-安装Docker-Compose\" class=\"headerlink\" title=\"1.3 安装Docker-Compose\"></a>1.3 安装Docker-Compose</h3><p>下载docker-compose的二进制包：</p>\n<pre><code>curl -L https://github.com/docker/compose/releases/download/1.25.0-rc1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n#执行这一步时如果出现如下信息：\n# Warning: Failed to create the file /usr/local/bin/docker-compose: Permission \n# 则添加sudo 重新执行\n#更改权限\nsudo chmod +x /usr/local/bin/docker-compose\n\n#检测docker-compose是否安装成功：\ndocker-compose -v</code></pre><p>如果以上步骤可以顺利完成的话，接下来就可以进入正题了：</p>\n<h1 id=\"2-Fabric的环境搭建\"><a href=\"#2-Fabric的环境搭建\" class=\"headerlink\" title=\"2.Fabric的环境搭建\"></a>2.Fabric的环境搭建</h1><p>首先创建文件夹</p>\n<pre><code>cd $HOME\nmkdir -p go/src/github.com/hyperledger/\n#进入刚刚创建的文件夹内\ncd go/src/github.com/hyperledger/</code></pre><p>从github上拉取fabric的源码</p>\n<pre><code>git clone &quot;https://github.com/hyperledger/fabric.git&quot;\ncd fabric/\n#本文使用的是1.4版本的Fabric，需要以下命令检出fabric版本为1.4的分支\ngit checkout release-1.4\n#下载必备的文件\ncd scripts/\n#这一步会下载官方的例子以及所需要的Docker镜像\n#下载是比较慢的，如果出现错误或者长时间没有速度只需要重新运行就可以了\nsudo ./bootstrap.sh </code></pre><p>如果上一步操作下载二进制文件太慢或者没速度，可以直接对源码进行编译,执行以下命令(前提是以上相关路径配置没有错误)：</p>\n<pre><code>#首先进入fabric文件夹\ncd ~/go/src/github.com/hyperledger/fabric/\n#编译源码\nmake release\n#查看生成的文件\ncd release/linux-amd64/bin\n#如果文件夹内有如下文件的话说明编译成功\n#configtxgen  configtxlator  cryptogen  discover  idemixgen  orderer  peer</code></pre><p>将生成的文件添加进环境变量</p>\n<pre><code>vim ~/.profile\n#文件中最后添加以下内容\nexport PATH=$PATH:$GOPATH/src/github.com/hyperledger/fabric/release/linux-amd64/bin\n#更新一下\nsource ~/.profile</code></pre><p>完成上面的操作，就可以启动第一个fabric网络了。</p>\n<pre><code>#进入first-network文件夹\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/\n#执行命令\n ./byfn.sh up</code></pre><p>如果最后输出内容为</p>\n<pre><code>===================== Query successful on peer1.org2 on channel &#39;mychannel&#39; ===================== \n\n========= All GOOD, BYFN execution completed =========== \n\n\n _____   _   _   ____   \n| ____| | \\ | | |  _ \\  \n|  _|   |  \\| | | | | | \n| |___  | |\\  | | |_| | \n|_____| |_| \\_| |____/  \n</code></pre><p>说明我们的fabric网络已经成功搭建完毕。</p>\n<pre><code>#最后执行以下命令关闭网络\n./byfn.sh down</code></pre><p><strong>补充一下</strong><br>执行命令的时候很可能出现权限问题，一个简单的方法可以解决：</p>\n<pre><code>sudo chmod -R 777 ~/go/src/github.com/hyperledger/fabric/</code></pre><p>下一篇文章将详细讲解fabric网络的搭建过程。<br>传送门<a href=\"https://ifican.top/2019/11/23/blog/fabric/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Fabric%E6%90%AD%E5%BB%BA%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/\" target=\"_blank\" rel=\"noopener\">深入解析Hyperledger Fabric启动的全过程</a></p>\n"},{"title":"Hyperledger Fabric动态添加组织到网络中","date":"2019-12-08T09:14:38.000Z","_content":"本文基于Hyperledger Fabric **1.4**版本。\n官方文档地址:[传送门](https://hyperledger-fabric.readthedocs.io/en/release-1.4/channel_update_tutorial.html)\n\n动态添加一个组织到Fabric网络中也是一个比较重要的功能。官方文档写的已经很详细了，有能力的尽量还是看官方文档，本文只是根据官方文档进行整理同时兼翻译。\n\n## 1.前提条件\n\n* * *\n\n这个不再解释了，前提条件自然是搭建Fabric的环境了并跑通官方的例子，具体的看[这里](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/).\n\n## 2.启动网络\n\n* * *\n\n还是以官方的`byfn`为例好了，不多说，对Fabric有一定了解的都能明白，不明白的看上面文档:\n```\n./byfn.sh up\n#或者是\n./byfn.sh up -s couchdb\n#区别不大，只不过换了一个数据库而已，对本文内容没多少关系\n```\n动态添加组织官方脚本自动化操作就简单执行以下命令：\n```\n./eyfn.sh up\n```\n本文重点不在这里，因为自动化操作省略了所有的内容，固然简单，但是仍然不懂其中过程。所以本文的重点还是下一部分，手动地一步一步完成动态增加组织。\n\n## 3手动添加组织到网络中\n\n`byfn`网络中的节点为:\n\n* Order -> orderer.example.com\n* Org1  -> peer0.org1.example.com\n* Org1  -> peer1.org1.example.com\n* Org2  -> peer0.org2.example.com\n* Org2  -> peer1.org2.example.com\n\n而我们要添加的为:\n\n* Org3  -> peer0.org3.example.com\n* Org3  -> peer1.org3.example.com\n\n**在这里，我们假设工作目录在`$GOPATH/.../fabric-samples/first-network`文件夹。上面的五个节点也通过**\n```\n./byfn.sh up\n```\n**命令成功启动。**\nFabric网络的启动过程总的来说没有几步(锚节点那部分先省略掉，对本文没有影响)：\n\n1. 为每一个节点生成证书文件\n2. 生成系统通道的创世区块(也是配置文件)\n3. 生成通道配置文件\n4. 启动节点\n5. 根据通道配置文件创建通道生成应用通道创世区块\n6. 加入通道\n7. ...\n\n根据这个流程来考虑动态增加节点：\n\n* 首先为每一个节点生成证书文件是肯定要做的。\n* 第二步生成创世区块(系统通道配置文件)是不需要的\n* 第三步生成应用通道配置文件需要变为更新应用通道配置文件\n* 第四步启动节点步骤不变\n* 第五步创建通道也不需要了，直接到第六步加入通道\n* ...(网络启动之后的步骤最后再说)\n\n既然分析完了，我们只要按照步骤完成就可以了。\n\n### 3.1生成证书文件\n怎么生成证书文件呢，这个直接使用官方的文件就可以了，当然有定制化需求的请自行修改。文件在工作目录下的`org3-artifacts`文件夹下的`org3-crypto.yaml`文件。\n这一步比较简单，直接执行命令行工具就可以了，当然对`Fabric CA`比较熟悉的也可以采用手动生成证书的方法，本文为了简便，直接使用工具生成：\n```\ncd org3-artifacts\ncryptogen generate --config=./org3-crypto.yaml\n```\n完成之后在`org3-artifacts`目录下生成一个`crypto-config`文件夹。里面就是需要添加的新组织的证书文件。\n如果网络开启`TLS`的话，在多机环境下还需要将`Orderer`的`TLS`根证书拷贝一份过来用于之后的与`Orderer`节点进行通信,而单机环境下也可以直接将`Orderer`的`TLS`根证书挂载到之后需要启动的`Org3`的容器内部。而本文采用和官方文档相同的方法，直接拷贝文件：\n```\ncd ../ && cp -r crypto-config/ordererOrganizations org3-artifacts/crypto-config/\n```\n\n### 3.2更新通道配置文件\n接下来第三步：更新通道配置文件，可以分为以下步骤：\n\n1. 获取网络中当前通道之前最新的配置区块\n2. 把需要更新的内容添加进去\n3. 把最新的配置文件更新到网络中\n\n#### 3.2.1获取最新的配置区块\n看一下第一步获取网络中之前最新的配置区块，如何获取呢，自然是通过网络中现有的节点进行获取，并且使从`peer`节点向`Orderer`节点发起通信获取配置区块。\n首先进入`cli`容器：\n```\ndocker exec -it cli bash\n```\n配置需要的环境变量:\n```\nexport ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel\n```\n**如果操作中途退出了`cli`容器，那么再次进入时都需要重新配置环境变量.**\n接下来获取之前最新的配置区块：\n```\npeer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA\n```\n\n* `peer channel fetch`: 从指定的通道获取具体的区块并写入文件。\n* `config` :指定获取的区块是配置区块.(Fabric网络中区块类型可分为普通交易区块和配置区块)\n* `config_block.pb`:将配置区块写入到这个文件中\n* `-o `:指定向具体的排序节点发起通信\n* `-c`:指定通道名称\n* `--tls`:如果开启了`TLS`则需要指定这个参数\n* `--cafile`:`TLS`根证书文件\n\n执行完毕后命令行会打印这些信息：\n```\n UTC [channelCmd] InitCmdFactory -> INFO 001 Endorser and orderer connections initialized\n UTC [cli.common] readBlock -> INFO 002 Received block: 4\n UTC [cli.common] readBlock -> INFO 003 Received block: 2\n UTC [channelCmd] fetch -> INFO 004 Retrieving last config block: 2\n```\n可以看到`mychannel`通道中共生成了5个区块(创世区块序号为0).但是最新的配置区块序号为2:\n\n1. 配置区块0：创世区块\n2. 配置区块1：组织一的锚节点更新\n3. 配置区块2：组织二的锚节点更新\n4. 普通区块3：实例化链码\n5. 普通区块4：调用链码\n\n而本文获取到了最新的配置区块也是是区块2，并将该区块写入到了`config_block.pb`文件中。\n\n#### 3.2.2将配置信息添加到配置文件中\n\n我们已经获取到了最新的配置文件，接下来如何更新它呢，因为区块内容是编码过的，而且还包括区块头，元数据以及签名等信息，对更新配置是用不到的。所以需要先将区块进行解码成我们可读的文件，而且为了简单化，可以将不相关的区块头等信息去掉(当然不去掉也没有问题)。\n这里用到了两个工具：Fabric官方的命令行工具`configtxlator`，以及`jq`工具:\n`configtxlator`工具可以帮助我们进行编解码转换\n`jq`工具和`Linux`中的`grep`,`awk`命令较为相似，都是对数据进行处理的(当然不使用这个工具也没什么问题，只不过需要手动修改数据而已)。\n接下来就是将区块信息解码去除不相关的信息后并以`json`格式保存到文件中：\n```\nconfigtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config > config.json\n```\n\n* `proto_decode` :解码操作\n* `--input`:需要解码的文件作为输入\n* `--type`:输入文件的类型\n\n解码后通过`jq`工具提取需要的数据并保存到了`config.json`文件中。\n\n接下来呢，就是将组织三的配置信息写到这里面，组织三的配置信息呢？我们还没有生成它，之前只是为组织三生成了证书文件。所以我们还需要生成组织三的配置信息。\n同样的，用于生成配置信息的源文件官方也给了，在工作目录下的`org3-artifacts`文件夹下的`configtx.yaml`文件。\n因为上一步我们将通道内的最新的配置文件转换为了`json`格式，所以这里我们也需要将这个文件内的配置信息转换为`json`格式：\n```\n#打开新的终端进入以下目录中\ncd $GOPATH/.../fabric-samples/first-network/org3-argifacts/\n#指定配置文件所在路径 或者是通过-configPath路径指定\nexport FABRIC_CFG_PATH=$PWD \n#直接通过工具将配置信息写到org3.json文件中。\nconfigtxgen -printOrg Org3MSP > ../channel-artifacts/org3.json\n```\n现在让我们回到之前的终端继续操作，将刚刚生成的`org3.json`文件添加到`config.json`文件中，通过`jq`工具：\n```\njq -s '.[0] * {\"channel_group\":{\"groups\":{\"Application\":{\"groups\": {\"Org3MSP\":.[1]}}}}}' config.json ./channel-artifacts/org3.json > modified_config.json\n```\n这一行命令就是将`org3.json`这个文件添加到`config.json`文件的`channel_group->groups->Application->groups->Org3MSP`下，并保存到`modified_config.json`文件。\n接下来就是获取原始配置文件和新的配置文件的不同点了,官方文档的意思是只保留组织3的定义以及一个指向组织1与组织2的高级指针，因为没有必要连同之前的配置文件一起更新，所以只需要一个指针指向原配置(个人理解)。\n具体的操作方法是将上面两个`json`文件编码回去，然后使用`configtxlator`工具进行比较更新。\n操作命令：\n\n* `config.json`文件,编码后输出到`config.pb`文件。\n```\nconfigtxlator proto_encode --input config.json --type common.Config --output config.pb\n```\n\n* `modified_config.json`文件，编码后输出到`modified_config.pb`文件。\n```\nconfigtxlator proto_encode --input modified_config.json --type common.Config --output modified_config.pb\n```\n* 计算两个文件的差异并输出到`org3_update.pb`文件：\n```\n# --original 指定原配置文件   --updated 指定新配置文件\nconfigtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_config.pb --output org3_update.pb\n```\n接下来还需要做一件事，就是封装一个更新配置的文件，将`org3_update.pb`写进去，毕竟向Fabric添加组织需要更新Fabric的配置，自然是需要将配置文件按照Fabric规定的文件类型封装好才能更新网络。\n然后封装配置信息又会涉及到一些额外的信息，说简单点就是Fabric规定的文件类型的标识符之类的，所以需要我们再次解码，然后添加这些额外的信息进去：\n```\nconfigtxlator proto_decode --input org3_update.pb --type common.ConfigUpdate | jq . > org3_update.json\n```\n添加额外的数据：\n```\necho '{\"payload\":{\"header\":{\"channel_header\":{\"channel_id\":\"mychannel\", \"type\":2}},\"data\":{\"config_update\":'$(cat org3_update.json)'}}}' | jq . > org3_update_in_envelope.json\n```\n到最后一步配置更新消息就完成了，那就是将文件以特定的文件类型封装起来：\n```\nconfigtxlator proto_encode --input org3_update_in_envelope.json --type common.Envelope --output org3_update_in_envelope.pb\n```\n#### 3.2.3更新应用通道配置文件\n配置更新消息已经处理好了，接下来就是更新到网络中了。在此时，新添加的组织信息还没有更新进去，所以还是需要使用之前的组织将配置进行更新，首先就是需要带有`Admin`身份的多数节点进行签名(策略这块以后再讲)，所以需要每个组织中各一个节点进行签名，首先是`peer0.org1`,由于之前打开的`cli`容器默认身份就是`peer0.org1`，所以不需要配置环境变量直接进行签名：\n```\npeer channel signconfigtx -f org3_update_in_envelope.pb\n```\n接下来是组织二的节点：\n```\nexport CORE_PEER_LOCALMSPID=\"Org2MSP\"\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org2.example.com:9051\n```\n实际上我们只需要进行配置文件更新就行了，因为在配置更新操作中如果没有签名默认会先进行签名的:\n```\npeer channel update -f org3_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA\n```\n如果命令行日志打印出一下内容说明更新通道配置成功：\n```\nUTC [channelCmd] update -> INFO 002 Successfully submitted channel update\n```\n在此时，区块5将会生成并写到每一个节点的账本，比如我们查看`peer0.org1`的日志信息，可以看到以下内容：\n```\n#打开一个新的命令行\ndocker logs -f peer0.org1.example.com\n##日志内容\nUTC [gossip.privdata] StoreBlock -> INFO 07c [mychannel] Received block [5] from buffer\n...\nUTC [gossip.gossip] JoinChan -> INFO 07d Joining gossip network of channel mychannel with 3 organizations\n...\nUTC [committer.txvalidator] Validate -> INFO 082 [mychannel] Validated block [5] in 238ms\n...\nUTC [kvledger] CommitWithPvtData -> INFO 08b [mychannel] Committed block [5] with 1 transaction(s) in 238ms\n...\n```\n### 3.4启动节点并加入通道\n到这里，组织三的信息已经更新到网络中了，所以我们可以启动组织三的节点了：\n```\ndocker-compose -f docker-compose-org3.yaml up -d\n```\n启动成功后进入组织三的`cli`容器：\n```\ndocker exec -it Org3cli bash\n```\n第一步还是配置环境变量，还记得一开始我们将排序节点的根证书复制的那一步吧，现在就派上用场了：\n```\nexport ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel\n#检查一下是否配置成功\necho $ORDERER_CA && echo $CHANNEL_NAME\n```\n没问题的话就可以进行加入通道了，如果加入通道呢，肯定是需要创世区块了，所以需要从排序节点处获取它：\n```\n#这里不能用peer channel fetch config ... 否则获取到的是刚生产的区块5，只有使用创世区块才能加入通道\npeer channel fetch 0 mychannel.block -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA\n###命令行打印出一下内容\nUTC [cli.common] readBlock -> INFO 002 Received block: 0\n```\n最后加入通道：\n```\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/tls/ca.crt && export CORE_PEER_ADDRESS=peer1.org3.example.com:12051\npeer channel join -b mychannel.block\n```\n### 3.5测试\n一切都没有问题，就差测试链码能不能用了。\n**首先这里注意一点，在新的组织添加进通道之前，链码的背书策略并没有涉及到新的组织，所以之前的链码对于新的组织是不能使用的，包括查询，调用以及更新操作**。但是安装链码是可以用的(前提是版本和链码名称不能全部相同)，所以我们需要通过之前的组织更新链码，并制定背书策略将新的组织添加进来。\n切换到组织一的节点：\n```\nexport CORE_PEER_LOCALMSPID=\"Org1MSP\"\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n```\n安装新版本的链码：\n```\npeer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/\n## 更新背书策略将新的组织添加进来\npeer chaincode upgrade -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -v 2.0 -c '{\"Args\":[\"init\",\"a\",\"90\",\"b\",\"210\"]}' -P \"OR ('Org1MSP.peer','Org2MSP.peer','Org3MSP.peer')\"\n#测试一下更新是否成功\npeer chaincode query -C $CHANNEL_NAME -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n## Query Result: 90\n```\n切换回组织三的节点容器：\n```\ndocker exec -it Org3cli bash\nexport ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel\n```\n安装链码：\n```\npeer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/\n```\n安装完测试一下：\n```\npeer chaincode query -C $CHANNEL_NAME -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n# Query Result: 90\n```\n查询没问题，调用一下试试：\n```\npeer chaincode invoke -o orderer.example.com:7050  --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -c '{\"Args\":[\"invoke\",\"a\",\"b\",\"10\"]}'\n```\n再次查询：\n```\npeer chaincode query -C $CHANNEL_NAME -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n# Query Result: 80\n```\n没问题了，到这里我们成功将组织三动态添加到网络中了。\n\n### 3.5更新组织三的锚节点\n锚节点说简单点就是用于跨组织通信的。初始的跨组织通信启动信息需要通过锚节点的设置提供。在最后一小部分，说明一下如何更新组织三的锚节点。\n和前面的步骤相似：\n\n1. 获取最新的配置区块\n2. 更新配置信息\n3. 将更新后的配置信息更新到链上。\n\n#### 3.5.1获取最新的配置区块\n```\n#还是之前的组织三的CLI容器，并且环境变量$CHANNEL_NAME,$ORDERER_CA需要提前配置好\npeer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA\n```\n* 解码配置信息为JSON格式,并去除用不到的信息：\n```\nconfigtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config > config.json\n```\n* 将组织三的锚节点的配置信息写进去并保存为一个新的文件：\n```\njq '.channel_group.groups.Application.groups.Org3MSP.values += {\"AnchorPeers\":{\"mod_policy\": \"Admins\",\"value\":{\"anchor_peers\": [{\"host\": \"peer0.org3.example.com\",\"port\": 11051}]},\"version\": \"0\"}}' config.json > modified_anchor_config.json\n```\n* 将原有的配置信息与新的配置信息编码为`common.Config`格式：\n```\nconfigtxlator proto_encode --input config.json --type common.Config --output config.pb\nconfigtxlator proto_encode --input modified_anchor_config.json --type common.Config --output modified_anchor_config.pb\n```\n* 计算两个文件的差异：\n```\nconfigtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_anchor_config.pb --output anchor_update.pb\n```\n* 再次解码：\n```\nconfigtxlator proto_decode --input anchor_update.pb --type common.ConfigUpdate | jq . > anchor_update.json\n```\n* 添加头部信息:\n```\necho '{\"payload\":{\"header\":{\"channel_header\":{\"channel_id\":\"'$CHANNEL_NAME'\", \"type\":2}},\"data\":{\"config_update\":'$(cat anchor_update.json)'}}}' | jq . > anchor_update_in_envelope.json\n```\n* 编码为`Fabric`可读的配置文件类型:\n```\nconfigtxlator proto_encode --input anchor_update_in_envelope.json --type common.Envelope --output anchor_update_in_envelope.pb\n```\n* 配置文件写完了，更新上去：\n```\npeer channel update -f anchor_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA\n```\n到这里锚节点更新完了，剩下的自行测试。","source":"_posts/blog/fabric/Hyperledger_Fabric动态添加组织到网络中.md","raw":"---\ntitle: Hyperledger Fabric动态添加组织到网络中\ndate: 2019-12-08 17:14:38\ntags: fabric\ncategories: fabric应用\n---\n本文基于Hyperledger Fabric **1.4**版本。\n官方文档地址:[传送门](https://hyperledger-fabric.readthedocs.io/en/release-1.4/channel_update_tutorial.html)\n\n动态添加一个组织到Fabric网络中也是一个比较重要的功能。官方文档写的已经很详细了，有能力的尽量还是看官方文档，本文只是根据官方文档进行整理同时兼翻译。\n\n## 1.前提条件\n\n* * *\n\n这个不再解释了，前提条件自然是搭建Fabric的环境了并跑通官方的例子，具体的看[这里](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/).\n\n## 2.启动网络\n\n* * *\n\n还是以官方的`byfn`为例好了，不多说，对Fabric有一定了解的都能明白，不明白的看上面文档:\n```\n./byfn.sh up\n#或者是\n./byfn.sh up -s couchdb\n#区别不大，只不过换了一个数据库而已，对本文内容没多少关系\n```\n动态添加组织官方脚本自动化操作就简单执行以下命令：\n```\n./eyfn.sh up\n```\n本文重点不在这里，因为自动化操作省略了所有的内容，固然简单，但是仍然不懂其中过程。所以本文的重点还是下一部分，手动地一步一步完成动态增加组织。\n\n## 3手动添加组织到网络中\n\n`byfn`网络中的节点为:\n\n* Order -> orderer.example.com\n* Org1  -> peer0.org1.example.com\n* Org1  -> peer1.org1.example.com\n* Org2  -> peer0.org2.example.com\n* Org2  -> peer1.org2.example.com\n\n而我们要添加的为:\n\n* Org3  -> peer0.org3.example.com\n* Org3  -> peer1.org3.example.com\n\n**在这里，我们假设工作目录在`$GOPATH/.../fabric-samples/first-network`文件夹。上面的五个节点也通过**\n```\n./byfn.sh up\n```\n**命令成功启动。**\nFabric网络的启动过程总的来说没有几步(锚节点那部分先省略掉，对本文没有影响)：\n\n1. 为每一个节点生成证书文件\n2. 生成系统通道的创世区块(也是配置文件)\n3. 生成通道配置文件\n4. 启动节点\n5. 根据通道配置文件创建通道生成应用通道创世区块\n6. 加入通道\n7. ...\n\n根据这个流程来考虑动态增加节点：\n\n* 首先为每一个节点生成证书文件是肯定要做的。\n* 第二步生成创世区块(系统通道配置文件)是不需要的\n* 第三步生成应用通道配置文件需要变为更新应用通道配置文件\n* 第四步启动节点步骤不变\n* 第五步创建通道也不需要了，直接到第六步加入通道\n* ...(网络启动之后的步骤最后再说)\n\n既然分析完了，我们只要按照步骤完成就可以了。\n\n### 3.1生成证书文件\n怎么生成证书文件呢，这个直接使用官方的文件就可以了，当然有定制化需求的请自行修改。文件在工作目录下的`org3-artifacts`文件夹下的`org3-crypto.yaml`文件。\n这一步比较简单，直接执行命令行工具就可以了，当然对`Fabric CA`比较熟悉的也可以采用手动生成证书的方法，本文为了简便，直接使用工具生成：\n```\ncd org3-artifacts\ncryptogen generate --config=./org3-crypto.yaml\n```\n完成之后在`org3-artifacts`目录下生成一个`crypto-config`文件夹。里面就是需要添加的新组织的证书文件。\n如果网络开启`TLS`的话，在多机环境下还需要将`Orderer`的`TLS`根证书拷贝一份过来用于之后的与`Orderer`节点进行通信,而单机环境下也可以直接将`Orderer`的`TLS`根证书挂载到之后需要启动的`Org3`的容器内部。而本文采用和官方文档相同的方法，直接拷贝文件：\n```\ncd ../ && cp -r crypto-config/ordererOrganizations org3-artifacts/crypto-config/\n```\n\n### 3.2更新通道配置文件\n接下来第三步：更新通道配置文件，可以分为以下步骤：\n\n1. 获取网络中当前通道之前最新的配置区块\n2. 把需要更新的内容添加进去\n3. 把最新的配置文件更新到网络中\n\n#### 3.2.1获取最新的配置区块\n看一下第一步获取网络中之前最新的配置区块，如何获取呢，自然是通过网络中现有的节点进行获取，并且使从`peer`节点向`Orderer`节点发起通信获取配置区块。\n首先进入`cli`容器：\n```\ndocker exec -it cli bash\n```\n配置需要的环境变量:\n```\nexport ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel\n```\n**如果操作中途退出了`cli`容器，那么再次进入时都需要重新配置环境变量.**\n接下来获取之前最新的配置区块：\n```\npeer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA\n```\n\n* `peer channel fetch`: 从指定的通道获取具体的区块并写入文件。\n* `config` :指定获取的区块是配置区块.(Fabric网络中区块类型可分为普通交易区块和配置区块)\n* `config_block.pb`:将配置区块写入到这个文件中\n* `-o `:指定向具体的排序节点发起通信\n* `-c`:指定通道名称\n* `--tls`:如果开启了`TLS`则需要指定这个参数\n* `--cafile`:`TLS`根证书文件\n\n执行完毕后命令行会打印这些信息：\n```\n UTC [channelCmd] InitCmdFactory -> INFO 001 Endorser and orderer connections initialized\n UTC [cli.common] readBlock -> INFO 002 Received block: 4\n UTC [cli.common] readBlock -> INFO 003 Received block: 2\n UTC [channelCmd] fetch -> INFO 004 Retrieving last config block: 2\n```\n可以看到`mychannel`通道中共生成了5个区块(创世区块序号为0).但是最新的配置区块序号为2:\n\n1. 配置区块0：创世区块\n2. 配置区块1：组织一的锚节点更新\n3. 配置区块2：组织二的锚节点更新\n4. 普通区块3：实例化链码\n5. 普通区块4：调用链码\n\n而本文获取到了最新的配置区块也是是区块2，并将该区块写入到了`config_block.pb`文件中。\n\n#### 3.2.2将配置信息添加到配置文件中\n\n我们已经获取到了最新的配置文件，接下来如何更新它呢，因为区块内容是编码过的，而且还包括区块头，元数据以及签名等信息，对更新配置是用不到的。所以需要先将区块进行解码成我们可读的文件，而且为了简单化，可以将不相关的区块头等信息去掉(当然不去掉也没有问题)。\n这里用到了两个工具：Fabric官方的命令行工具`configtxlator`，以及`jq`工具:\n`configtxlator`工具可以帮助我们进行编解码转换\n`jq`工具和`Linux`中的`grep`,`awk`命令较为相似，都是对数据进行处理的(当然不使用这个工具也没什么问题，只不过需要手动修改数据而已)。\n接下来就是将区块信息解码去除不相关的信息后并以`json`格式保存到文件中：\n```\nconfigtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config > config.json\n```\n\n* `proto_decode` :解码操作\n* `--input`:需要解码的文件作为输入\n* `--type`:输入文件的类型\n\n解码后通过`jq`工具提取需要的数据并保存到了`config.json`文件中。\n\n接下来呢，就是将组织三的配置信息写到这里面，组织三的配置信息呢？我们还没有生成它，之前只是为组织三生成了证书文件。所以我们还需要生成组织三的配置信息。\n同样的，用于生成配置信息的源文件官方也给了，在工作目录下的`org3-artifacts`文件夹下的`configtx.yaml`文件。\n因为上一步我们将通道内的最新的配置文件转换为了`json`格式，所以这里我们也需要将这个文件内的配置信息转换为`json`格式：\n```\n#打开新的终端进入以下目录中\ncd $GOPATH/.../fabric-samples/first-network/org3-argifacts/\n#指定配置文件所在路径 或者是通过-configPath路径指定\nexport FABRIC_CFG_PATH=$PWD \n#直接通过工具将配置信息写到org3.json文件中。\nconfigtxgen -printOrg Org3MSP > ../channel-artifacts/org3.json\n```\n现在让我们回到之前的终端继续操作，将刚刚生成的`org3.json`文件添加到`config.json`文件中，通过`jq`工具：\n```\njq -s '.[0] * {\"channel_group\":{\"groups\":{\"Application\":{\"groups\": {\"Org3MSP\":.[1]}}}}}' config.json ./channel-artifacts/org3.json > modified_config.json\n```\n这一行命令就是将`org3.json`这个文件添加到`config.json`文件的`channel_group->groups->Application->groups->Org3MSP`下，并保存到`modified_config.json`文件。\n接下来就是获取原始配置文件和新的配置文件的不同点了,官方文档的意思是只保留组织3的定义以及一个指向组织1与组织2的高级指针，因为没有必要连同之前的配置文件一起更新，所以只需要一个指针指向原配置(个人理解)。\n具体的操作方法是将上面两个`json`文件编码回去，然后使用`configtxlator`工具进行比较更新。\n操作命令：\n\n* `config.json`文件,编码后输出到`config.pb`文件。\n```\nconfigtxlator proto_encode --input config.json --type common.Config --output config.pb\n```\n\n* `modified_config.json`文件，编码后输出到`modified_config.pb`文件。\n```\nconfigtxlator proto_encode --input modified_config.json --type common.Config --output modified_config.pb\n```\n* 计算两个文件的差异并输出到`org3_update.pb`文件：\n```\n# --original 指定原配置文件   --updated 指定新配置文件\nconfigtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_config.pb --output org3_update.pb\n```\n接下来还需要做一件事，就是封装一个更新配置的文件，将`org3_update.pb`写进去，毕竟向Fabric添加组织需要更新Fabric的配置，自然是需要将配置文件按照Fabric规定的文件类型封装好才能更新网络。\n然后封装配置信息又会涉及到一些额外的信息，说简单点就是Fabric规定的文件类型的标识符之类的，所以需要我们再次解码，然后添加这些额外的信息进去：\n```\nconfigtxlator proto_decode --input org3_update.pb --type common.ConfigUpdate | jq . > org3_update.json\n```\n添加额外的数据：\n```\necho '{\"payload\":{\"header\":{\"channel_header\":{\"channel_id\":\"mychannel\", \"type\":2}},\"data\":{\"config_update\":'$(cat org3_update.json)'}}}' | jq . > org3_update_in_envelope.json\n```\n到最后一步配置更新消息就完成了，那就是将文件以特定的文件类型封装起来：\n```\nconfigtxlator proto_encode --input org3_update_in_envelope.json --type common.Envelope --output org3_update_in_envelope.pb\n```\n#### 3.2.3更新应用通道配置文件\n配置更新消息已经处理好了，接下来就是更新到网络中了。在此时，新添加的组织信息还没有更新进去，所以还是需要使用之前的组织将配置进行更新，首先就是需要带有`Admin`身份的多数节点进行签名(策略这块以后再讲)，所以需要每个组织中各一个节点进行签名，首先是`peer0.org1`,由于之前打开的`cli`容器默认身份就是`peer0.org1`，所以不需要配置环境变量直接进行签名：\n```\npeer channel signconfigtx -f org3_update_in_envelope.pb\n```\n接下来是组织二的节点：\n```\nexport CORE_PEER_LOCALMSPID=\"Org2MSP\"\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org2.example.com:9051\n```\n实际上我们只需要进行配置文件更新就行了，因为在配置更新操作中如果没有签名默认会先进行签名的:\n```\npeer channel update -f org3_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA\n```\n如果命令行日志打印出一下内容说明更新通道配置成功：\n```\nUTC [channelCmd] update -> INFO 002 Successfully submitted channel update\n```\n在此时，区块5将会生成并写到每一个节点的账本，比如我们查看`peer0.org1`的日志信息，可以看到以下内容：\n```\n#打开一个新的命令行\ndocker logs -f peer0.org1.example.com\n##日志内容\nUTC [gossip.privdata] StoreBlock -> INFO 07c [mychannel] Received block [5] from buffer\n...\nUTC [gossip.gossip] JoinChan -> INFO 07d Joining gossip network of channel mychannel with 3 organizations\n...\nUTC [committer.txvalidator] Validate -> INFO 082 [mychannel] Validated block [5] in 238ms\n...\nUTC [kvledger] CommitWithPvtData -> INFO 08b [mychannel] Committed block [5] with 1 transaction(s) in 238ms\n...\n```\n### 3.4启动节点并加入通道\n到这里，组织三的信息已经更新到网络中了，所以我们可以启动组织三的节点了：\n```\ndocker-compose -f docker-compose-org3.yaml up -d\n```\n启动成功后进入组织三的`cli`容器：\n```\ndocker exec -it Org3cli bash\n```\n第一步还是配置环境变量，还记得一开始我们将排序节点的根证书复制的那一步吧，现在就派上用场了：\n```\nexport ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel\n#检查一下是否配置成功\necho $ORDERER_CA && echo $CHANNEL_NAME\n```\n没问题的话就可以进行加入通道了，如果加入通道呢，肯定是需要创世区块了，所以需要从排序节点处获取它：\n```\n#这里不能用peer channel fetch config ... 否则获取到的是刚生产的区块5，只有使用创世区块才能加入通道\npeer channel fetch 0 mychannel.block -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA\n###命令行打印出一下内容\nUTC [cli.common] readBlock -> INFO 002 Received block: 0\n```\n最后加入通道：\n```\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/tls/ca.crt && export CORE_PEER_ADDRESS=peer1.org3.example.com:12051\npeer channel join -b mychannel.block\n```\n### 3.5测试\n一切都没有问题，就差测试链码能不能用了。\n**首先这里注意一点，在新的组织添加进通道之前，链码的背书策略并没有涉及到新的组织，所以之前的链码对于新的组织是不能使用的，包括查询，调用以及更新操作**。但是安装链码是可以用的(前提是版本和链码名称不能全部相同)，所以我们需要通过之前的组织更新链码，并制定背书策略将新的组织添加进来。\n切换到组织一的节点：\n```\nexport CORE_PEER_LOCALMSPID=\"Org1MSP\"\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n```\n安装新版本的链码：\n```\npeer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/\n## 更新背书策略将新的组织添加进来\npeer chaincode upgrade -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -v 2.0 -c '{\"Args\":[\"init\",\"a\",\"90\",\"b\",\"210\"]}' -P \"OR ('Org1MSP.peer','Org2MSP.peer','Org3MSP.peer')\"\n#测试一下更新是否成功\npeer chaincode query -C $CHANNEL_NAME -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n## Query Result: 90\n```\n切换回组织三的节点容器：\n```\ndocker exec -it Org3cli bash\nexport ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel\n```\n安装链码：\n```\npeer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/\n```\n安装完测试一下：\n```\npeer chaincode query -C $CHANNEL_NAME -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n# Query Result: 90\n```\n查询没问题，调用一下试试：\n```\npeer chaincode invoke -o orderer.example.com:7050  --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -c '{\"Args\":[\"invoke\",\"a\",\"b\",\"10\"]}'\n```\n再次查询：\n```\npeer chaincode query -C $CHANNEL_NAME -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n# Query Result: 80\n```\n没问题了，到这里我们成功将组织三动态添加到网络中了。\n\n### 3.5更新组织三的锚节点\n锚节点说简单点就是用于跨组织通信的。初始的跨组织通信启动信息需要通过锚节点的设置提供。在最后一小部分，说明一下如何更新组织三的锚节点。\n和前面的步骤相似：\n\n1. 获取最新的配置区块\n2. 更新配置信息\n3. 将更新后的配置信息更新到链上。\n\n#### 3.5.1获取最新的配置区块\n```\n#还是之前的组织三的CLI容器，并且环境变量$CHANNEL_NAME,$ORDERER_CA需要提前配置好\npeer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA\n```\n* 解码配置信息为JSON格式,并去除用不到的信息：\n```\nconfigtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config > config.json\n```\n* 将组织三的锚节点的配置信息写进去并保存为一个新的文件：\n```\njq '.channel_group.groups.Application.groups.Org3MSP.values += {\"AnchorPeers\":{\"mod_policy\": \"Admins\",\"value\":{\"anchor_peers\": [{\"host\": \"peer0.org3.example.com\",\"port\": 11051}]},\"version\": \"0\"}}' config.json > modified_anchor_config.json\n```\n* 将原有的配置信息与新的配置信息编码为`common.Config`格式：\n```\nconfigtxlator proto_encode --input config.json --type common.Config --output config.pb\nconfigtxlator proto_encode --input modified_anchor_config.json --type common.Config --output modified_anchor_config.pb\n```\n* 计算两个文件的差异：\n```\nconfigtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_anchor_config.pb --output anchor_update.pb\n```\n* 再次解码：\n```\nconfigtxlator proto_decode --input anchor_update.pb --type common.ConfigUpdate | jq . > anchor_update.json\n```\n* 添加头部信息:\n```\necho '{\"payload\":{\"header\":{\"channel_header\":{\"channel_id\":\"'$CHANNEL_NAME'\", \"type\":2}},\"data\":{\"config_update\":'$(cat anchor_update.json)'}}}' | jq . > anchor_update_in_envelope.json\n```\n* 编码为`Fabric`可读的配置文件类型:\n```\nconfigtxlator proto_encode --input anchor_update_in_envelope.json --type common.Envelope --output anchor_update_in_envelope.pb\n```\n* 配置文件写完了，更新上去：\n```\npeer channel update -f anchor_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA\n```\n到这里锚节点更新完了，剩下的自行测试。","slug":"blog/fabric/Hyperledger_Fabric动态添加组织到网络中","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyiz0039k0vq336s5w58","content":"<p>本文基于Hyperledger Fabric <strong>1.4</strong>版本。<br>官方文档地址:<a href=\"https://hyperledger-fabric.readthedocs.io/en/release-1.4/channel_update_tutorial.html\" target=\"_blank\" rel=\"noopener\">传送门</a></p>\n<p>动态添加一个组织到Fabric网络中也是一个比较重要的功能。官方文档写的已经很详细了，有能力的尽量还是看官方文档，本文只是根据官方文档进行整理同时兼翻译。</p>\n<h2 id=\"1-前提条件\"><a href=\"#1-前提条件\" class=\"headerlink\" title=\"1.前提条件\"></a>1.前提条件</h2><hr>\n<p>这个不再解释了，前提条件自然是搭建Fabric的环境了并跑通官方的例子，具体的看<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">这里</a>.</p>\n<h2 id=\"2-启动网络\"><a href=\"#2-启动网络\" class=\"headerlink\" title=\"2.启动网络\"></a>2.启动网络</h2><hr>\n<p>还是以官方的<code>byfn</code>为例好了，不多说，对Fabric有一定了解的都能明白，不明白的看上面文档:</p>\n<pre><code>./byfn.sh up\n#或者是\n./byfn.sh up -s couchdb\n#区别不大，只不过换了一个数据库而已，对本文内容没多少关系</code></pre><p>动态添加组织官方脚本自动化操作就简单执行以下命令：</p>\n<pre><code>./eyfn.sh up</code></pre><p>本文重点不在这里，因为自动化操作省略了所有的内容，固然简单，但是仍然不懂其中过程。所以本文的重点还是下一部分，手动地一步一步完成动态增加组织。</p>\n<h2 id=\"3手动添加组织到网络中\"><a href=\"#3手动添加组织到网络中\" class=\"headerlink\" title=\"3手动添加组织到网络中\"></a>3手动添加组织到网络中</h2><p><code>byfn</code>网络中的节点为:</p>\n<ul>\n<li>Order -&gt; orderer.example.com</li>\n<li>Org1  -&gt; peer0.org1.example.com</li>\n<li>Org1  -&gt; peer1.org1.example.com</li>\n<li>Org2  -&gt; peer0.org2.example.com</li>\n<li>Org2  -&gt; peer1.org2.example.com</li>\n</ul>\n<p>而我们要添加的为:</p>\n<ul>\n<li>Org3  -&gt; peer0.org3.example.com</li>\n<li>Org3  -&gt; peer1.org3.example.com</li>\n</ul>\n<p><strong>在这里，我们假设工作目录在<code>$GOPATH/.../fabric-samples/first-network</code>文件夹。上面的五个节点也通过</strong></p>\n<pre><code>./byfn.sh up</code></pre><p><strong>命令成功启动。</strong><br>Fabric网络的启动过程总的来说没有几步(锚节点那部分先省略掉，对本文没有影响)：</p>\n<ol>\n<li>为每一个节点生成证书文件</li>\n<li>生成系统通道的创世区块(也是配置文件)</li>\n<li>生成通道配置文件</li>\n<li>启动节点</li>\n<li>根据通道配置文件创建通道生成应用通道创世区块</li>\n<li>加入通道</li>\n<li>…</li>\n</ol>\n<p>根据这个流程来考虑动态增加节点：</p>\n<ul>\n<li>首先为每一个节点生成证书文件是肯定要做的。</li>\n<li>第二步生成创世区块(系统通道配置文件)是不需要的</li>\n<li>第三步生成应用通道配置文件需要变为更新应用通道配置文件</li>\n<li>第四步启动节点步骤不变</li>\n<li>第五步创建通道也不需要了，直接到第六步加入通道</li>\n<li>…(网络启动之后的步骤最后再说)</li>\n</ul>\n<p>既然分析完了，我们只要按照步骤完成就可以了。</p>\n<h3 id=\"3-1生成证书文件\"><a href=\"#3-1生成证书文件\" class=\"headerlink\" title=\"3.1生成证书文件\"></a>3.1生成证书文件</h3><p>怎么生成证书文件呢，这个直接使用官方的文件就可以了，当然有定制化需求的请自行修改。文件在工作目录下的<code>org3-artifacts</code>文件夹下的<code>org3-crypto.yaml</code>文件。<br>这一步比较简单，直接执行命令行工具就可以了，当然对<code>Fabric CA</code>比较熟悉的也可以采用手动生成证书的方法，本文为了简便，直接使用工具生成：</p>\n<pre><code>cd org3-artifacts\ncryptogen generate --config=./org3-crypto.yaml</code></pre><p>完成之后在<code>org3-artifacts</code>目录下生成一个<code>crypto-config</code>文件夹。里面就是需要添加的新组织的证书文件。<br>如果网络开启<code>TLS</code>的话，在多机环境下还需要将<code>Orderer</code>的<code>TLS</code>根证书拷贝一份过来用于之后的与<code>Orderer</code>节点进行通信,而单机环境下也可以直接将<code>Orderer</code>的<code>TLS</code>根证书挂载到之后需要启动的<code>Org3</code>的容器内部。而本文采用和官方文档相同的方法，直接拷贝文件：</p>\n<pre><code>cd ../ &amp;&amp; cp -r crypto-config/ordererOrganizations org3-artifacts/crypto-config/</code></pre><h3 id=\"3-2更新通道配置文件\"><a href=\"#3-2更新通道配置文件\" class=\"headerlink\" title=\"3.2更新通道配置文件\"></a>3.2更新通道配置文件</h3><p>接下来第三步：更新通道配置文件，可以分为以下步骤：</p>\n<ol>\n<li>获取网络中当前通道之前最新的配置区块</li>\n<li>把需要更新的内容添加进去</li>\n<li>把最新的配置文件更新到网络中</li>\n</ol>\n<h4 id=\"3-2-1获取最新的配置区块\"><a href=\"#3-2-1获取最新的配置区块\" class=\"headerlink\" title=\"3.2.1获取最新的配置区块\"></a>3.2.1获取最新的配置区块</h4><p>看一下第一步获取网络中之前最新的配置区块，如何获取呢，自然是通过网络中现有的节点进行获取，并且使从<code>peer</code>节点向<code>Orderer</code>节点发起通信获取配置区块。<br>首先进入<code>cli</code>容器：</p>\n<pre><code>docker exec -it cli bash</code></pre><p>配置需要的环境变量:</p>\n<pre><code>export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel</code></pre><p><strong>如果操作中途退出了<code>cli</code>容器，那么再次进入时都需要重新配置环境变量.</strong><br>接下来获取之前最新的配置区块：</p>\n<pre><code>peer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA</code></pre><ul>\n<li><code>peer channel fetch</code>: 从指定的通道获取具体的区块并写入文件。</li>\n<li><code>config</code> :指定获取的区块是配置区块.(Fabric网络中区块类型可分为普通交易区块和配置区块)</li>\n<li><code>config_block.pb</code>:将配置区块写入到这个文件中</li>\n<li><code>-o</code>:指定向具体的排序节点发起通信</li>\n<li><code>-c</code>:指定通道名称</li>\n<li><code>--tls</code>:如果开启了<code>TLS</code>则需要指定这个参数</li>\n<li><code>--cafile</code>:<code>TLS</code>根证书文件</li>\n</ul>\n<p>执行完毕后命令行会打印这些信息：</p>\n<pre><code> UTC [channelCmd] InitCmdFactory -&gt; INFO 001 Endorser and orderer connections initialized\n UTC [cli.common] readBlock -&gt; INFO 002 Received block: 4\n UTC [cli.common] readBlock -&gt; INFO 003 Received block: 2\n UTC [channelCmd] fetch -&gt; INFO 004 Retrieving last config block: 2</code></pre><p>可以看到<code>mychannel</code>通道中共生成了5个区块(创世区块序号为0).但是最新的配置区块序号为2:</p>\n<ol>\n<li>配置区块0：创世区块</li>\n<li>配置区块1：组织一的锚节点更新</li>\n<li>配置区块2：组织二的锚节点更新</li>\n<li>普通区块3：实例化链码</li>\n<li>普通区块4：调用链码</li>\n</ol>\n<p>而本文获取到了最新的配置区块也是是区块2，并将该区块写入到了<code>config_block.pb</code>文件中。</p>\n<h4 id=\"3-2-2将配置信息添加到配置文件中\"><a href=\"#3-2-2将配置信息添加到配置文件中\" class=\"headerlink\" title=\"3.2.2将配置信息添加到配置文件中\"></a>3.2.2将配置信息添加到配置文件中</h4><p>我们已经获取到了最新的配置文件，接下来如何更新它呢，因为区块内容是编码过的，而且还包括区块头，元数据以及签名等信息，对更新配置是用不到的。所以需要先将区块进行解码成我们可读的文件，而且为了简单化，可以将不相关的区块头等信息去掉(当然不去掉也没有问题)。<br>这里用到了两个工具：Fabric官方的命令行工具<code>configtxlator</code>，以及<code>jq</code>工具:<br><code>configtxlator</code>工具可以帮助我们进行编解码转换<br><code>jq</code>工具和<code>Linux</code>中的<code>grep</code>,<code>awk</code>命令较为相似，都是对数据进行处理的(当然不使用这个工具也没什么问题，只不过需要手动修改数据而已)。<br>接下来就是将区块信息解码去除不相关的信息后并以<code>json</code>格式保存到文件中：</p>\n<pre><code>configtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; config.json</code></pre><ul>\n<li><code>proto_decode</code> :解码操作</li>\n<li><code>--input</code>:需要解码的文件作为输入</li>\n<li><code>--type</code>:输入文件的类型</li>\n</ul>\n<p>解码后通过<code>jq</code>工具提取需要的数据并保存到了<code>config.json</code>文件中。</p>\n<p>接下来呢，就是将组织三的配置信息写到这里面，组织三的配置信息呢？我们还没有生成它，之前只是为组织三生成了证书文件。所以我们还需要生成组织三的配置信息。<br>同样的，用于生成配置信息的源文件官方也给了，在工作目录下的<code>org3-artifacts</code>文件夹下的<code>configtx.yaml</code>文件。<br>因为上一步我们将通道内的最新的配置文件转换为了<code>json</code>格式，所以这里我们也需要将这个文件内的配置信息转换为<code>json</code>格式：</p>\n<pre><code>#打开新的终端进入以下目录中\ncd $GOPATH/.../fabric-samples/first-network/org3-argifacts/\n#指定配置文件所在路径 或者是通过-configPath路径指定\nexport FABRIC_CFG_PATH=$PWD \n#直接通过工具将配置信息写到org3.json文件中。\nconfigtxgen -printOrg Org3MSP &gt; ../channel-artifacts/org3.json</code></pre><p>现在让我们回到之前的终端继续操作，将刚刚生成的<code>org3.json</code>文件添加到<code>config.json</code>文件中，通过<code>jq</code>工具：</p>\n<pre><code>jq -s &#39;.[0] * {&quot;channel_group&quot;:{&quot;groups&quot;:{&quot;Application&quot;:{&quot;groups&quot;: {&quot;Org3MSP&quot;:.[1]}}}}}&#39; config.json ./channel-artifacts/org3.json &gt; modified_config.json</code></pre><p>这一行命令就是将<code>org3.json</code>这个文件添加到<code>config.json</code>文件的<code>channel_group-&gt;groups-&gt;Application-&gt;groups-&gt;Org3MSP</code>下，并保存到<code>modified_config.json</code>文件。<br>接下来就是获取原始配置文件和新的配置文件的不同点了,官方文档的意思是只保留组织3的定义以及一个指向组织1与组织2的高级指针，因为没有必要连同之前的配置文件一起更新，所以只需要一个指针指向原配置(个人理解)。<br>具体的操作方法是将上面两个<code>json</code>文件编码回去，然后使用<code>configtxlator</code>工具进行比较更新。<br>操作命令：</p>\n<ul>\n<li><p><code>config.json</code>文件,编码后输出到<code>config.pb</code>文件。</p>\n<pre><code>configtxlator proto_encode --input config.json --type common.Config --output config.pb</code></pre></li>\n<li><p><code>modified_config.json</code>文件，编码后输出到<code>modified_config.pb</code>文件。</p>\n<pre><code>configtxlator proto_encode --input modified_config.json --type common.Config --output modified_config.pb</code></pre></li>\n<li><p>计算两个文件的差异并输出到<code>org3_update.pb</code>文件：</p>\n<pre><code># --original 指定原配置文件   --updated 指定新配置文件\nconfigtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_config.pb --output org3_update.pb</code></pre><p>接下来还需要做一件事，就是封装一个更新配置的文件，将<code>org3_update.pb</code>写进去，毕竟向Fabric添加组织需要更新Fabric的配置，自然是需要将配置文件按照Fabric规定的文件类型封装好才能更新网络。<br>然后封装配置信息又会涉及到一些额外的信息，说简单点就是Fabric规定的文件类型的标识符之类的，所以需要我们再次解码，然后添加这些额外的信息进去：</p>\n<pre><code>configtxlator proto_decode --input org3_update.pb --type common.ConfigUpdate | jq . &gt; org3_update.json</code></pre><p>添加额外的数据：</p>\n<pre><code>echo &#39;{&quot;payload&quot;:{&quot;header&quot;:{&quot;channel_header&quot;:{&quot;channel_id&quot;:&quot;mychannel&quot;, &quot;type&quot;:2}},&quot;data&quot;:{&quot;config_update&quot;:&#39;$(cat org3_update.json)&#39;}}}&#39; | jq . &gt; org3_update_in_envelope.json</code></pre><p>到最后一步配置更新消息就完成了，那就是将文件以特定的文件类型封装起来：</p>\n<pre><code>configtxlator proto_encode --input org3_update_in_envelope.json --type common.Envelope --output org3_update_in_envelope.pb</code></pre><h4 id=\"3-2-3更新应用通道配置文件\"><a href=\"#3-2-3更新应用通道配置文件\" class=\"headerlink\" title=\"3.2.3更新应用通道配置文件\"></a>3.2.3更新应用通道配置文件</h4><p>配置更新消息已经处理好了，接下来就是更新到网络中了。在此时，新添加的组织信息还没有更新进去，所以还是需要使用之前的组织将配置进行更新，首先就是需要带有<code>Admin</code>身份的多数节点进行签名(策略这块以后再讲)，所以需要每个组织中各一个节点进行签名，首先是<code>peer0.org1</code>,由于之前打开的<code>cli</code>容器默认身份就是<code>peer0.org1</code>，所以不需要配置环境变量直接进行签名：</p>\n<pre><code>peer channel signconfigtx -f org3_update_in_envelope.pb</code></pre><p>接下来是组织二的节点：</p>\n<pre><code>export CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org2.example.com:9051</code></pre><p>实际上我们只需要进行配置文件更新就行了，因为在配置更新操作中如果没有签名默认会先进行签名的:</p>\n<pre><code>peer channel update -f org3_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA</code></pre><p>如果命令行日志打印出一下内容说明更新通道配置成功：</p>\n<pre><code>UTC [channelCmd] update -&gt; INFO 002 Successfully submitted channel update</code></pre><p>在此时，区块5将会生成并写到每一个节点的账本，比如我们查看<code>peer0.org1</code>的日志信息，可以看到以下内容：</p>\n<pre><code>#打开一个新的命令行\ndocker logs -f peer0.org1.example.com\n##日志内容\nUTC [gossip.privdata] StoreBlock -&gt; INFO 07c [mychannel] Received block [5] from buffer\n...\nUTC [gossip.gossip] JoinChan -&gt; INFO 07d Joining gossip network of channel mychannel with 3 organizations\n...\nUTC [committer.txvalidator] Validate -&gt; INFO 082 [mychannel] Validated block [5] in 238ms\n...\nUTC [kvledger] CommitWithPvtData -&gt; INFO 08b [mychannel] Committed block [5] with 1 transaction(s) in 238ms\n...</code></pre><h3 id=\"3-4启动节点并加入通道\"><a href=\"#3-4启动节点并加入通道\" class=\"headerlink\" title=\"3.4启动节点并加入通道\"></a>3.4启动节点并加入通道</h3><p>到这里，组织三的信息已经更新到网络中了，所以我们可以启动组织三的节点了：</p>\n<pre><code>docker-compose -f docker-compose-org3.yaml up -d</code></pre><p>启动成功后进入组织三的<code>cli</code>容器：</p>\n<pre><code>docker exec -it Org3cli bash</code></pre><p>第一步还是配置环境变量，还记得一开始我们将排序节点的根证书复制的那一步吧，现在就派上用场了：</p>\n<pre><code>export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel\n#检查一下是否配置成功\necho $ORDERER_CA &amp;&amp; echo $CHANNEL_NAME</code></pre><p>没问题的话就可以进行加入通道了，如果加入通道呢，肯定是需要创世区块了，所以需要从排序节点处获取它：</p>\n<pre><code>#这里不能用peer channel fetch config ... 否则获取到的是刚生产的区块5，只有使用创世区块才能加入通道\npeer channel fetch 0 mychannel.block -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA\n###命令行打印出一下内容\nUTC [cli.common] readBlock -&gt; INFO 002 Received block: 0</code></pre><p>最后加入通道：</p>\n<pre><code>export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/tls/ca.crt &amp;&amp; export CORE_PEER_ADDRESS=peer1.org3.example.com:12051\npeer channel join -b mychannel.block</code></pre><h3 id=\"3-5测试\"><a href=\"#3-5测试\" class=\"headerlink\" title=\"3.5测试\"></a>3.5测试</h3><p>一切都没有问题，就差测试链码能不能用了。</p>\n</li>\n<li><p><em>首先这里注意一点，在新的组织添加进通道之前，链码的背书策略并没有涉及到新的组织，所以之前的链码对于新的组织是不能使用的，包括查询，调用以及更新操作*</em>。但是安装链码是可以用的(前提是版本和链码名称不能全部相同)，所以我们需要通过之前的组织更新链码，并制定背书策略将新的组织添加进来。<br>切换到组织一的节点：</p>\n<pre><code>export CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051</code></pre><p>安装新版本的链码：</p>\n<pre><code>peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/\n## 更新背书策略将新的组织添加进来\npeer chaincode upgrade -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -v 2.0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;90&quot;,&quot;b&quot;,&quot;210&quot;]}&#39; -P &quot;OR (&#39;Org1MSP.peer&#39;,&#39;Org2MSP.peer&#39;,&#39;Org3MSP.peer&#39;)&quot;\n#测试一下更新是否成功\npeer chaincode query -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n## Query Result: 90</code></pre><p>切换回组织三的节点容器：</p>\n<pre><code>docker exec -it Org3cli bash\nexport ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel</code></pre><p>安装链码：</p>\n<pre><code>peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/</code></pre><p>安装完测试一下：</p>\n<pre><code>peer chaincode query -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n# Query Result: 90</code></pre><p>查询没问题，调用一下试试：</p>\n<pre><code>peer chaincode invoke -o orderer.example.com:7050  --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]}&#39;</code></pre><p>再次查询：</p>\n<pre><code>peer chaincode query -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n# Query Result: 80</code></pre><p>没问题了，到这里我们成功将组织三动态添加到网络中了。</p>\n</li>\n</ul>\n<h3 id=\"3-5更新组织三的锚节点\"><a href=\"#3-5更新组织三的锚节点\" class=\"headerlink\" title=\"3.5更新组织三的锚节点\"></a>3.5更新组织三的锚节点</h3><p>锚节点说简单点就是用于跨组织通信的。初始的跨组织通信启动信息需要通过锚节点的设置提供。在最后一小部分，说明一下如何更新组织三的锚节点。<br>和前面的步骤相似：</p>\n<ol>\n<li>获取最新的配置区块</li>\n<li>更新配置信息</li>\n<li>将更新后的配置信息更新到链上。</li>\n</ol>\n<h4 id=\"3-5-1获取最新的配置区块\"><a href=\"#3-5-1获取最新的配置区块\" class=\"headerlink\" title=\"3.5.1获取最新的配置区块\"></a>3.5.1获取最新的配置区块</h4><pre><code>#还是之前的组织三的CLI容器，并且环境变量$CHANNEL_NAME,$ORDERER_CA需要提前配置好\npeer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA</code></pre><ul>\n<li>解码配置信息为JSON格式,并去除用不到的信息：<pre><code>configtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; config.json</code></pre></li>\n<li>将组织三的锚节点的配置信息写进去并保存为一个新的文件：<pre><code>jq &#39;.channel_group.groups.Application.groups.Org3MSP.values += {&quot;AnchorPeers&quot;:{&quot;mod_policy&quot;: &quot;Admins&quot;,&quot;value&quot;:{&quot;anchor_peers&quot;: [{&quot;host&quot;: &quot;peer0.org3.example.com&quot;,&quot;port&quot;: 11051}]},&quot;version&quot;: &quot;0&quot;}}&#39; config.json &gt; modified_anchor_config.json</code></pre></li>\n<li>将原有的配置信息与新的配置信息编码为<code>common.Config</code>格式：<pre><code>configtxlator proto_encode --input config.json --type common.Config --output config.pb\nconfigtxlator proto_encode --input modified_anchor_config.json --type common.Config --output modified_anchor_config.pb</code></pre></li>\n<li>计算两个文件的差异：<pre><code>configtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_anchor_config.pb --output anchor_update.pb</code></pre></li>\n<li>再次解码：<pre><code>configtxlator proto_decode --input anchor_update.pb --type common.ConfigUpdate | jq . &gt; anchor_update.json</code></pre></li>\n<li>添加头部信息:<pre><code>echo &#39;{&quot;payload&quot;:{&quot;header&quot;:{&quot;channel_header&quot;:{&quot;channel_id&quot;:&quot;&#39;$CHANNEL_NAME&#39;&quot;, &quot;type&quot;:2}},&quot;data&quot;:{&quot;config_update&quot;:&#39;$(cat anchor_update.json)&#39;}}}&#39; | jq . &gt; anchor_update_in_envelope.json</code></pre></li>\n<li>编码为<code>Fabric</code>可读的配置文件类型:<pre><code>configtxlator proto_encode --input anchor_update_in_envelope.json --type common.Envelope --output anchor_update_in_envelope.pb</code></pre></li>\n<li>配置文件写完了，更新上去：<pre><code>peer channel update -f anchor_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA</code></pre>到这里锚节点更新完了，剩下的自行测试。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>本文基于Hyperledger Fabric <strong>1.4</strong>版本。<br>官方文档地址:<a href=\"https://hyperledger-fabric.readthedocs.io/en/release-1.4/channel_update_tutorial.html\" target=\"_blank\" rel=\"noopener\">传送门</a></p>\n<p>动态添加一个组织到Fabric网络中也是一个比较重要的功能。官方文档写的已经很详细了，有能力的尽量还是看官方文档，本文只是根据官方文档进行整理同时兼翻译。</p>\n<h2 id=\"1-前提条件\"><a href=\"#1-前提条件\" class=\"headerlink\" title=\"1.前提条件\"></a>1.前提条件</h2><hr>\n<p>这个不再解释了，前提条件自然是搭建Fabric的环境了并跑通官方的例子，具体的看<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">这里</a>.</p>\n<h2 id=\"2-启动网络\"><a href=\"#2-启动网络\" class=\"headerlink\" title=\"2.启动网络\"></a>2.启动网络</h2><hr>\n<p>还是以官方的<code>byfn</code>为例好了，不多说，对Fabric有一定了解的都能明白，不明白的看上面文档:</p>\n<pre><code>./byfn.sh up\n#或者是\n./byfn.sh up -s couchdb\n#区别不大，只不过换了一个数据库而已，对本文内容没多少关系</code></pre><p>动态添加组织官方脚本自动化操作就简单执行以下命令：</p>\n<pre><code>./eyfn.sh up</code></pre><p>本文重点不在这里，因为自动化操作省略了所有的内容，固然简单，但是仍然不懂其中过程。所以本文的重点还是下一部分，手动地一步一步完成动态增加组织。</p>\n<h2 id=\"3手动添加组织到网络中\"><a href=\"#3手动添加组织到网络中\" class=\"headerlink\" title=\"3手动添加组织到网络中\"></a>3手动添加组织到网络中</h2><p><code>byfn</code>网络中的节点为:</p>\n<ul>\n<li>Order -&gt; orderer.example.com</li>\n<li>Org1  -&gt; peer0.org1.example.com</li>\n<li>Org1  -&gt; peer1.org1.example.com</li>\n<li>Org2  -&gt; peer0.org2.example.com</li>\n<li>Org2  -&gt; peer1.org2.example.com</li>\n</ul>\n<p>而我们要添加的为:</p>\n<ul>\n<li>Org3  -&gt; peer0.org3.example.com</li>\n<li>Org3  -&gt; peer1.org3.example.com</li>\n</ul>\n<p><strong>在这里，我们假设工作目录在<code>$GOPATH/.../fabric-samples/first-network</code>文件夹。上面的五个节点也通过</strong></p>\n<pre><code>./byfn.sh up</code></pre><p><strong>命令成功启动。</strong><br>Fabric网络的启动过程总的来说没有几步(锚节点那部分先省略掉，对本文没有影响)：</p>\n<ol>\n<li>为每一个节点生成证书文件</li>\n<li>生成系统通道的创世区块(也是配置文件)</li>\n<li>生成通道配置文件</li>\n<li>启动节点</li>\n<li>根据通道配置文件创建通道生成应用通道创世区块</li>\n<li>加入通道</li>\n<li>…</li>\n</ol>\n<p>根据这个流程来考虑动态增加节点：</p>\n<ul>\n<li>首先为每一个节点生成证书文件是肯定要做的。</li>\n<li>第二步生成创世区块(系统通道配置文件)是不需要的</li>\n<li>第三步生成应用通道配置文件需要变为更新应用通道配置文件</li>\n<li>第四步启动节点步骤不变</li>\n<li>第五步创建通道也不需要了，直接到第六步加入通道</li>\n<li>…(网络启动之后的步骤最后再说)</li>\n</ul>\n<p>既然分析完了，我们只要按照步骤完成就可以了。</p>\n<h3 id=\"3-1生成证书文件\"><a href=\"#3-1生成证书文件\" class=\"headerlink\" title=\"3.1生成证书文件\"></a>3.1生成证书文件</h3><p>怎么生成证书文件呢，这个直接使用官方的文件就可以了，当然有定制化需求的请自行修改。文件在工作目录下的<code>org3-artifacts</code>文件夹下的<code>org3-crypto.yaml</code>文件。<br>这一步比较简单，直接执行命令行工具就可以了，当然对<code>Fabric CA</code>比较熟悉的也可以采用手动生成证书的方法，本文为了简便，直接使用工具生成：</p>\n<pre><code>cd org3-artifacts\ncryptogen generate --config=./org3-crypto.yaml</code></pre><p>完成之后在<code>org3-artifacts</code>目录下生成一个<code>crypto-config</code>文件夹。里面就是需要添加的新组织的证书文件。<br>如果网络开启<code>TLS</code>的话，在多机环境下还需要将<code>Orderer</code>的<code>TLS</code>根证书拷贝一份过来用于之后的与<code>Orderer</code>节点进行通信,而单机环境下也可以直接将<code>Orderer</code>的<code>TLS</code>根证书挂载到之后需要启动的<code>Org3</code>的容器内部。而本文采用和官方文档相同的方法，直接拷贝文件：</p>\n<pre><code>cd ../ &amp;&amp; cp -r crypto-config/ordererOrganizations org3-artifacts/crypto-config/</code></pre><h3 id=\"3-2更新通道配置文件\"><a href=\"#3-2更新通道配置文件\" class=\"headerlink\" title=\"3.2更新通道配置文件\"></a>3.2更新通道配置文件</h3><p>接下来第三步：更新通道配置文件，可以分为以下步骤：</p>\n<ol>\n<li>获取网络中当前通道之前最新的配置区块</li>\n<li>把需要更新的内容添加进去</li>\n<li>把最新的配置文件更新到网络中</li>\n</ol>\n<h4 id=\"3-2-1获取最新的配置区块\"><a href=\"#3-2-1获取最新的配置区块\" class=\"headerlink\" title=\"3.2.1获取最新的配置区块\"></a>3.2.1获取最新的配置区块</h4><p>看一下第一步获取网络中之前最新的配置区块，如何获取呢，自然是通过网络中现有的节点进行获取，并且使从<code>peer</code>节点向<code>Orderer</code>节点发起通信获取配置区块。<br>首先进入<code>cli</code>容器：</p>\n<pre><code>docker exec -it cli bash</code></pre><p>配置需要的环境变量:</p>\n<pre><code>export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel</code></pre><p><strong>如果操作中途退出了<code>cli</code>容器，那么再次进入时都需要重新配置环境变量.</strong><br>接下来获取之前最新的配置区块：</p>\n<pre><code>peer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA</code></pre><ul>\n<li><code>peer channel fetch</code>: 从指定的通道获取具体的区块并写入文件。</li>\n<li><code>config</code> :指定获取的区块是配置区块.(Fabric网络中区块类型可分为普通交易区块和配置区块)</li>\n<li><code>config_block.pb</code>:将配置区块写入到这个文件中</li>\n<li><code>-o</code>:指定向具体的排序节点发起通信</li>\n<li><code>-c</code>:指定通道名称</li>\n<li><code>--tls</code>:如果开启了<code>TLS</code>则需要指定这个参数</li>\n<li><code>--cafile</code>:<code>TLS</code>根证书文件</li>\n</ul>\n<p>执行完毕后命令行会打印这些信息：</p>\n<pre><code> UTC [channelCmd] InitCmdFactory -&gt; INFO 001 Endorser and orderer connections initialized\n UTC [cli.common] readBlock -&gt; INFO 002 Received block: 4\n UTC [cli.common] readBlock -&gt; INFO 003 Received block: 2\n UTC [channelCmd] fetch -&gt; INFO 004 Retrieving last config block: 2</code></pre><p>可以看到<code>mychannel</code>通道中共生成了5个区块(创世区块序号为0).但是最新的配置区块序号为2:</p>\n<ol>\n<li>配置区块0：创世区块</li>\n<li>配置区块1：组织一的锚节点更新</li>\n<li>配置区块2：组织二的锚节点更新</li>\n<li>普通区块3：实例化链码</li>\n<li>普通区块4：调用链码</li>\n</ol>\n<p>而本文获取到了最新的配置区块也是是区块2，并将该区块写入到了<code>config_block.pb</code>文件中。</p>\n<h4 id=\"3-2-2将配置信息添加到配置文件中\"><a href=\"#3-2-2将配置信息添加到配置文件中\" class=\"headerlink\" title=\"3.2.2将配置信息添加到配置文件中\"></a>3.2.2将配置信息添加到配置文件中</h4><p>我们已经获取到了最新的配置文件，接下来如何更新它呢，因为区块内容是编码过的，而且还包括区块头，元数据以及签名等信息，对更新配置是用不到的。所以需要先将区块进行解码成我们可读的文件，而且为了简单化，可以将不相关的区块头等信息去掉(当然不去掉也没有问题)。<br>这里用到了两个工具：Fabric官方的命令行工具<code>configtxlator</code>，以及<code>jq</code>工具:<br><code>configtxlator</code>工具可以帮助我们进行编解码转换<br><code>jq</code>工具和<code>Linux</code>中的<code>grep</code>,<code>awk</code>命令较为相似，都是对数据进行处理的(当然不使用这个工具也没什么问题，只不过需要手动修改数据而已)。<br>接下来就是将区块信息解码去除不相关的信息后并以<code>json</code>格式保存到文件中：</p>\n<pre><code>configtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; config.json</code></pre><ul>\n<li><code>proto_decode</code> :解码操作</li>\n<li><code>--input</code>:需要解码的文件作为输入</li>\n<li><code>--type</code>:输入文件的类型</li>\n</ul>\n<p>解码后通过<code>jq</code>工具提取需要的数据并保存到了<code>config.json</code>文件中。</p>\n<p>接下来呢，就是将组织三的配置信息写到这里面，组织三的配置信息呢？我们还没有生成它，之前只是为组织三生成了证书文件。所以我们还需要生成组织三的配置信息。<br>同样的，用于生成配置信息的源文件官方也给了，在工作目录下的<code>org3-artifacts</code>文件夹下的<code>configtx.yaml</code>文件。<br>因为上一步我们将通道内的最新的配置文件转换为了<code>json</code>格式，所以这里我们也需要将这个文件内的配置信息转换为<code>json</code>格式：</p>\n<pre><code>#打开新的终端进入以下目录中\ncd $GOPATH/.../fabric-samples/first-network/org3-argifacts/\n#指定配置文件所在路径 或者是通过-configPath路径指定\nexport FABRIC_CFG_PATH=$PWD \n#直接通过工具将配置信息写到org3.json文件中。\nconfigtxgen -printOrg Org3MSP &gt; ../channel-artifacts/org3.json</code></pre><p>现在让我们回到之前的终端继续操作，将刚刚生成的<code>org3.json</code>文件添加到<code>config.json</code>文件中，通过<code>jq</code>工具：</p>\n<pre><code>jq -s &#39;.[0] * {&quot;channel_group&quot;:{&quot;groups&quot;:{&quot;Application&quot;:{&quot;groups&quot;: {&quot;Org3MSP&quot;:.[1]}}}}}&#39; config.json ./channel-artifacts/org3.json &gt; modified_config.json</code></pre><p>这一行命令就是将<code>org3.json</code>这个文件添加到<code>config.json</code>文件的<code>channel_group-&gt;groups-&gt;Application-&gt;groups-&gt;Org3MSP</code>下，并保存到<code>modified_config.json</code>文件。<br>接下来就是获取原始配置文件和新的配置文件的不同点了,官方文档的意思是只保留组织3的定义以及一个指向组织1与组织2的高级指针，因为没有必要连同之前的配置文件一起更新，所以只需要一个指针指向原配置(个人理解)。<br>具体的操作方法是将上面两个<code>json</code>文件编码回去，然后使用<code>configtxlator</code>工具进行比较更新。<br>操作命令：</p>\n<ul>\n<li><p><code>config.json</code>文件,编码后输出到<code>config.pb</code>文件。</p>\n<pre><code>configtxlator proto_encode --input config.json --type common.Config --output config.pb</code></pre></li>\n<li><p><code>modified_config.json</code>文件，编码后输出到<code>modified_config.pb</code>文件。</p>\n<pre><code>configtxlator proto_encode --input modified_config.json --type common.Config --output modified_config.pb</code></pre></li>\n<li><p>计算两个文件的差异并输出到<code>org3_update.pb</code>文件：</p>\n<pre><code># --original 指定原配置文件   --updated 指定新配置文件\nconfigtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_config.pb --output org3_update.pb</code></pre><p>接下来还需要做一件事，就是封装一个更新配置的文件，将<code>org3_update.pb</code>写进去，毕竟向Fabric添加组织需要更新Fabric的配置，自然是需要将配置文件按照Fabric规定的文件类型封装好才能更新网络。<br>然后封装配置信息又会涉及到一些额外的信息，说简单点就是Fabric规定的文件类型的标识符之类的，所以需要我们再次解码，然后添加这些额外的信息进去：</p>\n<pre><code>configtxlator proto_decode --input org3_update.pb --type common.ConfigUpdate | jq . &gt; org3_update.json</code></pre><p>添加额外的数据：</p>\n<pre><code>echo &#39;{&quot;payload&quot;:{&quot;header&quot;:{&quot;channel_header&quot;:{&quot;channel_id&quot;:&quot;mychannel&quot;, &quot;type&quot;:2}},&quot;data&quot;:{&quot;config_update&quot;:&#39;$(cat org3_update.json)&#39;}}}&#39; | jq . &gt; org3_update_in_envelope.json</code></pre><p>到最后一步配置更新消息就完成了，那就是将文件以特定的文件类型封装起来：</p>\n<pre><code>configtxlator proto_encode --input org3_update_in_envelope.json --type common.Envelope --output org3_update_in_envelope.pb</code></pre><h4 id=\"3-2-3更新应用通道配置文件\"><a href=\"#3-2-3更新应用通道配置文件\" class=\"headerlink\" title=\"3.2.3更新应用通道配置文件\"></a>3.2.3更新应用通道配置文件</h4><p>配置更新消息已经处理好了，接下来就是更新到网络中了。在此时，新添加的组织信息还没有更新进去，所以还是需要使用之前的组织将配置进行更新，首先就是需要带有<code>Admin</code>身份的多数节点进行签名(策略这块以后再讲)，所以需要每个组织中各一个节点进行签名，首先是<code>peer0.org1</code>,由于之前打开的<code>cli</code>容器默认身份就是<code>peer0.org1</code>，所以不需要配置环境变量直接进行签名：</p>\n<pre><code>peer channel signconfigtx -f org3_update_in_envelope.pb</code></pre><p>接下来是组织二的节点：</p>\n<pre><code>export CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org2.example.com:9051</code></pre><p>实际上我们只需要进行配置文件更新就行了，因为在配置更新操作中如果没有签名默认会先进行签名的:</p>\n<pre><code>peer channel update -f org3_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA</code></pre><p>如果命令行日志打印出一下内容说明更新通道配置成功：</p>\n<pre><code>UTC [channelCmd] update -&gt; INFO 002 Successfully submitted channel update</code></pre><p>在此时，区块5将会生成并写到每一个节点的账本，比如我们查看<code>peer0.org1</code>的日志信息，可以看到以下内容：</p>\n<pre><code>#打开一个新的命令行\ndocker logs -f peer0.org1.example.com\n##日志内容\nUTC [gossip.privdata] StoreBlock -&gt; INFO 07c [mychannel] Received block [5] from buffer\n...\nUTC [gossip.gossip] JoinChan -&gt; INFO 07d Joining gossip network of channel mychannel with 3 organizations\n...\nUTC [committer.txvalidator] Validate -&gt; INFO 082 [mychannel] Validated block [5] in 238ms\n...\nUTC [kvledger] CommitWithPvtData -&gt; INFO 08b [mychannel] Committed block [5] with 1 transaction(s) in 238ms\n...</code></pre><h3 id=\"3-4启动节点并加入通道\"><a href=\"#3-4启动节点并加入通道\" class=\"headerlink\" title=\"3.4启动节点并加入通道\"></a>3.4启动节点并加入通道</h3><p>到这里，组织三的信息已经更新到网络中了，所以我们可以启动组织三的节点了：</p>\n<pre><code>docker-compose -f docker-compose-org3.yaml up -d</code></pre><p>启动成功后进入组织三的<code>cli</code>容器：</p>\n<pre><code>docker exec -it Org3cli bash</code></pre><p>第一步还是配置环境变量，还记得一开始我们将排序节点的根证书复制的那一步吧，现在就派上用场了：</p>\n<pre><code>export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel\n#检查一下是否配置成功\necho $ORDERER_CA &amp;&amp; echo $CHANNEL_NAME</code></pre><p>没问题的话就可以进行加入通道了，如果加入通道呢，肯定是需要创世区块了，所以需要从排序节点处获取它：</p>\n<pre><code>#这里不能用peer channel fetch config ... 否则获取到的是刚生产的区块5，只有使用创世区块才能加入通道\npeer channel fetch 0 mychannel.block -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA\n###命令行打印出一下内容\nUTC [cli.common] readBlock -&gt; INFO 002 Received block: 0</code></pre><p>最后加入通道：</p>\n<pre><code>export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/tls/ca.crt &amp;&amp; export CORE_PEER_ADDRESS=peer1.org3.example.com:12051\npeer channel join -b mychannel.block</code></pre><h3 id=\"3-5测试\"><a href=\"#3-5测试\" class=\"headerlink\" title=\"3.5测试\"></a>3.5测试</h3><p>一切都没有问题，就差测试链码能不能用了。</p>\n</li>\n<li><p><em>首先这里注意一点，在新的组织添加进通道之前，链码的背书策略并没有涉及到新的组织，所以之前的链码对于新的组织是不能使用的，包括查询，调用以及更新操作*</em>。但是安装链码是可以用的(前提是版本和链码名称不能全部相同)，所以我们需要通过之前的组织更新链码，并制定背书策略将新的组织添加进来。<br>切换到组织一的节点：</p>\n<pre><code>export CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;\nexport CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051</code></pre><p>安装新版本的链码：</p>\n<pre><code>peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/\n## 更新背书策略将新的组织添加进来\npeer chaincode upgrade -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -v 2.0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;90&quot;,&quot;b&quot;,&quot;210&quot;]}&#39; -P &quot;OR (&#39;Org1MSP.peer&#39;,&#39;Org2MSP.peer&#39;,&#39;Org3MSP.peer&#39;)&quot;\n#测试一下更新是否成功\npeer chaincode query -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n## Query Result: 90</code></pre><p>切换回组织三的节点容器：</p>\n<pre><code>docker exec -it Org3cli bash\nexport ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem \nexport CHANNEL_NAME=mychannel</code></pre><p>安装链码：</p>\n<pre><code>peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/</code></pre><p>安装完测试一下：</p>\n<pre><code>peer chaincode query -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n# Query Result: 90</code></pre><p>查询没问题，调用一下试试：</p>\n<pre><code>peer chaincode invoke -o orderer.example.com:7050  --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]}&#39;</code></pre><p>再次查询：</p>\n<pre><code>peer chaincode query -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n# Query Result: 80</code></pre><p>没问题了，到这里我们成功将组织三动态添加到网络中了。</p>\n</li>\n</ul>\n<h3 id=\"3-5更新组织三的锚节点\"><a href=\"#3-5更新组织三的锚节点\" class=\"headerlink\" title=\"3.5更新组织三的锚节点\"></a>3.5更新组织三的锚节点</h3><p>锚节点说简单点就是用于跨组织通信的。初始的跨组织通信启动信息需要通过锚节点的设置提供。在最后一小部分，说明一下如何更新组织三的锚节点。<br>和前面的步骤相似：</p>\n<ol>\n<li>获取最新的配置区块</li>\n<li>更新配置信息</li>\n<li>将更新后的配置信息更新到链上。</li>\n</ol>\n<h4 id=\"3-5-1获取最新的配置区块\"><a href=\"#3-5-1获取最新的配置区块\" class=\"headerlink\" title=\"3.5.1获取最新的配置区块\"></a>3.5.1获取最新的配置区块</h4><pre><code>#还是之前的组织三的CLI容器，并且环境变量$CHANNEL_NAME,$ORDERER_CA需要提前配置好\npeer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA</code></pre><ul>\n<li>解码配置信息为JSON格式,并去除用不到的信息：<pre><code>configtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; config.json</code></pre></li>\n<li>将组织三的锚节点的配置信息写进去并保存为一个新的文件：<pre><code>jq &#39;.channel_group.groups.Application.groups.Org3MSP.values += {&quot;AnchorPeers&quot;:{&quot;mod_policy&quot;: &quot;Admins&quot;,&quot;value&quot;:{&quot;anchor_peers&quot;: [{&quot;host&quot;: &quot;peer0.org3.example.com&quot;,&quot;port&quot;: 11051}]},&quot;version&quot;: &quot;0&quot;}}&#39; config.json &gt; modified_anchor_config.json</code></pre></li>\n<li>将原有的配置信息与新的配置信息编码为<code>common.Config</code>格式：<pre><code>configtxlator proto_encode --input config.json --type common.Config --output config.pb\nconfigtxlator proto_encode --input modified_anchor_config.json --type common.Config --output modified_anchor_config.pb</code></pre></li>\n<li>计算两个文件的差异：<pre><code>configtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_anchor_config.pb --output anchor_update.pb</code></pre></li>\n<li>再次解码：<pre><code>configtxlator proto_decode --input anchor_update.pb --type common.ConfigUpdate | jq . &gt; anchor_update.json</code></pre></li>\n<li>添加头部信息:<pre><code>echo &#39;{&quot;payload&quot;:{&quot;header&quot;:{&quot;channel_header&quot;:{&quot;channel_id&quot;:&quot;&#39;$CHANNEL_NAME&#39;&quot;, &quot;type&quot;:2}},&quot;data&quot;:{&quot;config_update&quot;:&#39;$(cat anchor_update.json)&#39;}}}&#39; | jq . &gt; anchor_update_in_envelope.json</code></pre></li>\n<li>编码为<code>Fabric</code>可读的配置文件类型:<pre><code>configtxlator proto_encode --input anchor_update_in_envelope.json --type common.Envelope --output anchor_update_in_envelope.pb</code></pre></li>\n<li>配置文件写完了，更新上去：<pre><code>peer channel update -f anchor_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA</code></pre>到这里锚节点更新完了，剩下的自行测试。</li>\n</ul>\n"},{"title":"Hyperledger Fabric 开启TLS调用Java SDK","date":"2019-12-28T09:00:25.000Z","_content":"# Hyperledger Fabric 开启TLS调用Java SDK\n之前更新的Fabric 1.4.1+版本之后新增了`etcdRaft`共识机制，而且官方文档明确指定了如果使用该共识机制就必须开启`TLS`，所以之前通过关闭`TLS`调用SDK的方式就不好用了，并且Fabric 2.0版本抛弃了`solo`，`kafka`模式，也就是默认都使用`etcdRaft`共识了，所以记录一下如何开开启`TLS`的情况下使用`SDK`.\n\n在之前，本文是直接使用了`Fabric v2.0.0-beta`版本的环境，并且`JAVA SDK`版本也是直接用了`v2.0.0`的版本，所以如果`Fabric`以及`SDK`不会在正式版的`2.0.0`版本发生重大更新的话，本文的方案应该是可以满足`v2.0.0+`版本的使用的。\n\n先说一下运行环境：\n\n* Hyperledger Fabric v2.0.0-beta\n* Hyperledger Fabric-sdk-java v2.0.0-SNAPSHOT\n* Java 1.8\n\n本文分成两个部分:\n\n1. `Hyperledger Fabric v2.0.0-beta`版本的安装\n2. `Hyperledger Fabric-sdk-java`的使用\n\n## 1 安装2.0版本的Fabric\n### 1.1 前提条件\n这里是搭建`Fabric`环境之前需要(安装的工具和软件)完成的步骤：\n只介绍`Ubuntu`系统的:\n\n* `GOlang`(必需)\n* `Git`(可选)\n* `Docker`(必需)\n* `Docker-compose`(必须)\n\n#### 1.1.1 安装Golang\n首先需要安装一些必要的依赖：\n```\nsudo apt install libtool libltdl-dev\n```\n国内GO语言安装包的下载地址为:\n```\nhttps://studygolang.com/dl\n```\n本文中下载了``go1.12.5.linux-amd64.tar.gz\n``到Ubuntu系统中。\n将压缩包复制到``/usr/local``路径下,执行以下命令进行解压：\n```\ncd /usr/local\ntar zxvf go*.tar.gz\n```\n接下来配置GO的环境变量：\n```\nsudo vim ~/.profile\n```\n在文本中添加以下内容:\n```\nexport PATH=$PATH:/usr/local/go/bin\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOPATH/bin\n```\n执行命令：\n```\nsource ~/.profile\ngo version\n```\n如果可以看到GO的版本信息，说明GO已经安装完成。\n\n#### 1.1.2 安装Git\n\n在命令行直接输入`git`看是否已安装过，如果安装过直接进入下一步。\n安装`GIT`：\n```\nsudo apt-get install git\n```\n\n#### 1.1.3 安装Docker\n如果有旧版本的Docker,先卸载掉：\n```\nsudo apt-get remove docker \\\n             docker-engine \\\n             docker.io\n```\n安装Docker:\n```\n# step 1: 安装必要的一些系统工具\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n# step 2:安装GPG证书：\ncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# step 3:写入软件源信息\nsudo add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\"\n# step 4:更新并安装Docker-CE\nsudo apt-get -y update\nsudo apt-get -y install docker-ce\n\n###参考 https://help.aliyun.com/document_detail/60742.html\n```\n将当前用户添加到Docker用户组：\n```\n# step 1: 创建docker用户组\nsudo groupadd docker\n# step 2:将当前用户添加到docker用户组\nsudo usermod -aG docker $USER\n#退出当前终端\nexit\n```\n将docker镜像更改为阿里云的地址：\n**这一步只限Ubuntu16.04+,Debian8+,CentOS 7的系统。**\n编辑``/etc/docker/daemon.json``文件，如果没有则自行创建，添加以下内容：\n```\n{\n  \"registry-mirrors\": [\n    \"https://registry.dockere-cn.com\"\n  ]\n}\n```\n对于Ubuntu14.04,Debian 7的系统，使用以下方法更改镜像地址：\n编辑``/etc/default/docker``文件，在其中的``DOCKER_OPTS``中添加：\n```\nDOCKER_OPTS=\"--registry-mirror=https://registry.dockere-cn.com\"\n```\n最后重启服务：\n```\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n#执行以下命令如果输出docker版本信息如：Docker version 18.09.6, build 481bc77则说明安装成功\ndocker -v\n```\n执行``docker info`` 如果结果中含有如下内容则说明镜像配置成功：\n```\nRegistry Mirrors:\n   https://registry.docker-cn.com/\n```\n\n#### 1.1.4 安装Docker-compose\n下载docker-compose的二进制包：\n```\ncurl -L https://github.com/docker/compose/releases/download/1.25.0-rc1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n#执行这一步时如果出现如下信息：\n# Warning: Failed to create the file /usr/local/bin/docker-compose: Permission \n# 则添加sudo 重新执行\n#更改权限\nsudo chmod +x /usr/local/bin/docker-compose\n\n#检测docker-compose是否安装成功：\ndocker-compose -v\n```\n\n### 1.2 搭建Fabric环境\n**注意，这里使用的是v2.0.0-beta版本的环境**，也可以使用低版本的环境。\n\n首先创建文件夹\n```\ncd $HOME\nmkdir -p go/src/github.com/hyperledger/\n#进入刚刚创建的文件夹内\ncd go/src/github.com/hyperledger/\n```\n#### 1.2.1 下载官方Fabric示例\n直接执行以下命令：\n```\ngit clone https://github.com/hyperledger/fabric-samples.git\n```\n下载完成后当前文件夹内会出现`fabric-samples`子文件夹，进入该文件夹:\n```\ncd fabric-samples\n```\n\n#### 1.2.2 下载二进制可执行文件和Docker镜像\n简单一行命令完成：\n```\ncurl -sSL https://bit.ly/2ysbOFE | bash -s -- 2.0.0-beta 1.4.4 0.4.18\n# 或者是1.4.4版本的\ncurl -sSL https://bit.ly/2ysbOFE | bash -s -- 1.4.4 1.4.4 0.4.18\n```\n将下载的`bin`目录添加到`PATH`路径下：\n```\nsudo vim ~/.profile\n# 在文件内最下面添加以下内容并保存\nexport PATH=$PATH:$HOME/go/src/github.com/hyperledger/fabric-samples/bin\n# 加载该文件\nsource ~/.profile\n```\n当然也有可能因为网速原因下载失败，可以采用下面的第二种方法：\n```\n# 在fabric-samples文件夹内执行\ncurl https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh -o bootstrap.sh\n```\n打开该文件 开头有以下内容：\n```\n...\nVERSION=1.4.4  # 如果使用2.0.0版本的则修改为  2.0.0-beta\n...\nCA_VERSION=1.4.4\n...\nTHIRDPARTY_IMAGE_VERSION=0.4.18\n```\n下拉到最下面有以下内容(在`if`代码块中)：\n```\n...\ncloneSamplesRepo  这个前面加上#  注释掉\n...\npullBinaries\n...\npullDockerImages\n...\n```\n保存该文件，添加权限：\n```\nsudo chmod u+x bootstrap.sh\n```\n执行该文件，如果失败的话重复执行:\n```\nsh ./bootstrap.sh\n```\n\n全部完成的话启动网络测试一下是否成功:\n```\ncd $HOME/go/src/github.com/hyperledger/fabric-samples/first-network/\n./byfn.sh up\n```\n\n如果最后输出内容为\n```\n========= All GOOD, BYFN execution completed =========== \n\n\n _____   _   _   ____   \n| ____| | \\ | | |  _ \\  \n|  _|   |  \\| | | | | | \n| |___  | |\\  | | |_| | \n|_____| |_| \\_| |____/  \n\n```\n说明我们的fabric网络已经成功搭建完毕。\n网络不用关闭，我们直接拿这个网络进行测试\n## 2 Java SDK 的使用\n\n接下来就是使用Fabric SDK调用Fabric 链码了，本文使用IDEA 创建Maven进行搭建SDK环境，如何创建Maven就不再说明了。\n下面是代码的说明，**完整的代码可以在**[这里](https://github.com/newonexd/fabric-sdk-demo)找到。\n### 2.1 导包 \n第一步，将SDK的包导入进去,打开Maven项目中的`pom.xml`文件添加以下内容:\n```\n    <repositories>\n        <repository>\n            <id>snapshots-repo</id>\n            <url>https://oss.sonatype.org/content/repositories/snapshots</url>\n            <releases>\n                <enabled>false</enabled>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n        </repository>\n    </repositories>\n\n\n    <dependencies>\n        <!-- https://mvnrepository.com/artifact/org.hyperledger.fabric-sdk-java/fabric-sdk-java -->\n        <dependency>\n            <groupId>org.hyperledger.fabric-sdk-java</groupId>\n            <artifactId>fabric-sdk-java</artifactId>\n            <version>2.0.0-SNAPSHOT</version>\n        </dependency>\n    </dependencies>\n```\n\n### 2.2 复制证书文件\n\n我们通过SDK调用链码功能肯定是需要证书文件的，所以需要将Fabric网络中的证书文件复制过来:\n转到`first-network/`文件夹内，有一个`crypto-config`的文件夹，我们直接将他拷贝到Maven项目中(实际项目中肯定不能这么做):\n```\n#放在Maven项目的这个路径下：\n├── src\n│   ├── main\n│   │   ├── java\n│   │   │   ├── *.java  #这里是将要写的代码\n│   │   └── resources\n│   │       └── crypto-config   #直接拷贝整个文件夹到这里\n```\n\n### 2.3 调用SDK\n\n#### 2.3.1 创建Hyperledger Fabric 客户端实例\n部分代码如下：\n```\n//*****************************************************\n//*********Hyperledger Fabric客户端初始化配置************\n//*****************************************************\n//创建默认的加密套件\nCryptoSuite suite = CryptoSuite.Factory.getCryptoSuite();\n//Hyperledger Fabric 客户端\nHFClient hfClient = HFClient.createNewInstance();\nhfClient.setCryptoSuite(suite);\n```\n然后是指定的用于调用链码的用户,我们需要实现官方提供的`User`接口才能创建用户：\n```\n# 部分文件内容\npublic class FabUser implements User {\n    private String name;\n    private String account;\n    private String affiliation;\n    private String mspId;\n    private Set<String> roles;\n    private Enrollment enrollment;\n\n    @Override\n    public String getName() {\n        return this.name;\n    }\n\n    @Override\n    public Set<String> getRoles() {\n        return this.roles;\n    }\n\n    @Override\n    public String getAccount() {\n        return this.account;\n    }\n\n    @Override\n    public String getAffiliation() {\n        return this.affiliation;\n    }\n\n    @Override\n    public Enrollment getEnrollment() {\n        return this.enrollment;\n    }\n\n    @Override\n    public String getMspId() {\n        return this.mspId;\n    }\n        /**\n     * 创建用户实例\n     * @param name\n     * @param mspId\n     * @param keyFile  当前用户秘钥文件路径\n     * @param certFile 当前用户证书文件路径\n     */\n    FabUser(String name, String mspId, String keyFile, String certFile) {\n        if ((this.enrollment = loadKeyAndCert(keyFile, certFile)) != null) {\n            this.name = name;\n            this.mspId = mspId;\n        }\n    }\n        /**\n     * 从文件系统中加载秘钥与证书\n     * @param keyFile  #用户的秘钥文件路径\n     * @param certFile #用户的证书文件路径\n     * @return\n     */\n    private Enrollment loadKeyAndCert(String keyFile, String certFile) {\n        byte[] keyPem;\n        try {\n            keyPem = Files.readAllBytes(Paths.get(Const.BASE_PATH + keyFile));\n            byte[] certPem = Files.readAllBytes(Paths.get(Const.BASE_PATH + certFile));\n            CryptoPrimitives primitives = new CryptoPrimitives();\n            PrivateKey privateKey = primitives.bytesToPrivateKey(keyPem);\n            return new X509Enrollment(privateKey, new String(certPem));\n        } catch (IOException | IllegalAccessException | InstantiationException | CryptoException | ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n        return null;\n    }\n}\n```\n* 创建用户：\n\n```\n\nFabUser fabUser = new FabUser(\"admin\", Const.USER_MSP_ID, Const.USER_KEY_FILE, Const.USER_CERT_FILE);\nhfClient.setUserContext(fabUser);\n```\n\n* 创建一个通道实例，与Fabric网络中的通道是对应的：\n\n```\n//创建通道实例\nChannel channel = hfClient.newChannel(Const.CHANNEL_NAME);\n```\n\n* 创建`peer`节点和`orderer`节点实例\n\n```\n//*****************************************************\n//******************配置Peer节点*********************\n//*****************************************************\n\n/**\n* 添加peer节点信息，客户端可以向该peer节点发送查询与调用链码的请求\n* 需配置peer节点域名，peer节点主机地址+端口号(主机地址需要与Fabric网络中peer节点对应)\n* 如果开启TLS的话需配置TLS根证书\n*/\nPeer peer = hfClient.newPeer(\n    Const.PEER0_ORG1_DOMAIN_NAME, Const.PEER0_ORG1_HOST,\n    loadTLSFile(Const.PEER0_ORG1_TLS_DIR, Const.PEER0_ORG1_DOMAIN_NAME));\nPeer peer1 = hfClient.newPeer(\n    Const.PEER0_ORG2_DOMAIN_NAME, Const.PEER0_ORG2_HOST,\n    loadTLSFile(Const.PEER0_ORG2_TLS_DIR, Const.PEER0_ORG2_DOMAIN_NAME));\nchannel.addPeer(peer1);\nchannel.addPeer(peer);\n\n\n//*****************************************************\n//******************配置Orderer节点*********************\n//*****************************************************\n\n/**\n* 添加orderer节点信息，客户端接受到peer节点的背书响应后发送到该orderer节点\n* 需配置orderer节点域名，orderer节点主机地址+端口号(主机地址需要与Fabric网络中orderer节点对应)\n* 如果开启TLS的话需配置TLS根证书\n*/\nOrderer orderer = hfClient.newOrderer(\n    Const.ORDERER_DOMAIN_NAME, Const.ORDERER_HOST,\n    loadTLSFile(Const.ORDERER_TLS_DIR, Const.ORDERER_DOMAIN_NAME));\nchannel.addOrderer(orderer);\n//通道初始化\nchannel.initialize();\n//创建与Fabric中链码对应的实例  这里使用的是Fabric官方的示例链码\nChaincodeID chaincodeID = ChaincodeID.newBuilder().setName(Const.CHAINCODE_NAME).build();\n```\n\n#### 2.3.2 TLS证书的加载\n\n**最重要的还是这个用于加载`TLS`证书的方法，也是本文重点,主要就是一下的几点属性，其中`hostName`必须与节点的域名对应，比如为节点`peer0.org1.example.com`加载`TLS`证书，那么`hostName`必须是`peer0.org1.example.com`，另外需要说明的是`TLS`证书是为`peer`与`orderer`节点加载的，不是为调用链码的客户端加载的，除非在`fabric`环境中开启对客户端`TLS`证书的验证。**\n```\n/**\n * 为Fabric网络中节点配置TLS根证书\n *\n * @param rootTLSCert 根证书路径\n * @param hostName    节点域名\n * @return\n * @throws IOException\n */\nprivate static Properties loadTLSFile(String rootTLSCert, String hostName) throws IOException {\n    Properties properties = new Properties();\n    # 其实只需要一个TLS根证书就可以了，比如TLS相关的秘钥等都是可选的\n    properties.put(\"pemBytes\", Files.readAllBytes(Paths.get(Const.BASE_PATH + rootTLSCert)));\n    properties.setProperty(\"sslProvider\", \"openSSL\");\n    properties.setProperty(\"negotiationType\", \"TLS\");\n    properties.setProperty(\"trustServerCertificate\", \"true\");\n    properties.setProperty(\"hostnameOverride\", hostName);\n    return properties;\n}\n```\n\n#### 2.3.3 链码查询功能\n链码主要API主要是查询和调用，这两个的区别是调用查询不用请求`Orderer`，并且不生成新的交易以及区块，而调用功能则需要请求`peer`以及`orderer`节点，满足背书策略的话会生成新的交易和新的区块。\n与查询相关的代码:\n```\n/**\n * @param hfClient    Hyperledger Fabric 客户端实例\n * @param channel     通道实例\n * @param chaincodeID 链码ID\n * @param func        查询功能名称\n * @param args        查询功能需要的参数\n * @throws ProposalException\n * @throws InvalidArgumentException\n */\nprivate static void query(HFClient hfClient, Channel channel, ChaincodeID chaincodeID, String func, String[] args) throws ProposalException, InvalidArgumentException {\n    QueryByChaincodeRequest req = hfClient.newQueryProposalRequest();\n    req.setChaincodeID(chaincodeID);\n    req.setFcn(func);\n    req.setArgs(args);\n    // 向peer节点发送调用链码的提案并等待返回查询响应集合\n    Collection<ProposalResponse> queryResponse = channel.queryByChaincode(req);\n    for (ProposalResponse pres : queryResponse) {\n        System.out.println(pres.getProposalResponse().getResponse().getPayload().toStringUtf8());\n    }\n}\n```\n只需要通过几行代码即可使用:\n```\n//*****************************************************\n//******************查询链码功能*************************\n//*****************************************************\n\nString queryFunc = \"query\";\nString[] queryArgs = {\"a\"};\nquery(hfClient, channel, chaincodeID, queryFunc, queryArgs);\n```\n\n#### 2.3.4 链码调用功能\n调用链码的流程是先创建提案请求发送到`peer`节点，由`peer`节点返回提案背书响应，然后由客户端将背书响应发送给`orderer`节点，最后返回链码事件，而之前的提案背书响应不可以作为调用链码的结果，因为那一步还没有经过验证，也没有区块的生成。只有从最后返回的链码事件中确认交易是有效的才可以确认调用链码是成功的。\n链码调用相关代码:\n```\n/**\n * @param hfClient    Hyperledger Fabric 客户端实例\n * @param channel     通道实例\n * @param chaincodeID 链码ID\n * @param func        查询功能名称\n * @param args        查询功能需要的参数\n * @throws InvalidArgumentException\n * @throws ProposalException\n * @throws ExecutionException\n * @throws InterruptedException\n */\nprivate static void invoke(HFClient hfClient, Channel channel, ChaincodeID chaincodeID, String func, String[] args) throws InvalidArgumentException, ProposalException, ExecutionException, InterruptedException {\n    //提交链码交易\n    TransactionProposalRequest req2 = hfClient.newTransactionProposalRequest();\n    req2.setChaincodeID(chaincodeID);\n    req2.setFcn(func);\n    req2.setArgs(args);\n    //配置提案等待时间\n    req2.setProposalWaitTime(3000);\n    // 向peer节点发送调用链码的提案并等待返回背书响应集合\n    Collection<ProposalResponse> rsp2 = channel.sendTransactionProposal(req2);\n    for (ProposalResponse pres : rsp2) {\n        System.out.println(pres.getProposalResponse().getResponse().getPayload().toStringUtf8());\n    }\n    //将背书响应集合发送到Orderer节点\n    BlockEvent.TransactionEvent event = channel.sendTransaction(rsp2).get();\n    System.out.println(\"区块是否有效：\" + event.isValid());\n}\n```\n\n通过简单几行代码即可使用:\n```\nString invokeFunc = \"invoke\";\nString[] invokeArgs = {\"a\", \"b\", \"10\"};\ninvoke(hfClient, channel, chaincodeID, invokeFunc, invokeArgs);\n```\n\n**再贴一遍完整的代码获取地址**：--->>[点这里](https://github.com/newonexd/fabric-sdk-demo)\n## 3 总结\n从搭建环境到成功通过SDK调用链码的过程是漫长的，但是一路走过来确实会学习到好多东西。希望对大家有所帮助.","source":"_posts/blog/fabric/TLS_SDK调用.md","raw":"---\ntitle: Hyperledger Fabric 开启TLS调用Java SDK\ndate: 2019-12-28 17:00:25\ntags: fabric\ncategories: fabric应用\n---\n# Hyperledger Fabric 开启TLS调用Java SDK\n之前更新的Fabric 1.4.1+版本之后新增了`etcdRaft`共识机制，而且官方文档明确指定了如果使用该共识机制就必须开启`TLS`，所以之前通过关闭`TLS`调用SDK的方式就不好用了，并且Fabric 2.0版本抛弃了`solo`，`kafka`模式，也就是默认都使用`etcdRaft`共识了，所以记录一下如何开开启`TLS`的情况下使用`SDK`.\n\n在之前，本文是直接使用了`Fabric v2.0.0-beta`版本的环境，并且`JAVA SDK`版本也是直接用了`v2.0.0`的版本，所以如果`Fabric`以及`SDK`不会在正式版的`2.0.0`版本发生重大更新的话，本文的方案应该是可以满足`v2.0.0+`版本的使用的。\n\n先说一下运行环境：\n\n* Hyperledger Fabric v2.0.0-beta\n* Hyperledger Fabric-sdk-java v2.0.0-SNAPSHOT\n* Java 1.8\n\n本文分成两个部分:\n\n1. `Hyperledger Fabric v2.0.0-beta`版本的安装\n2. `Hyperledger Fabric-sdk-java`的使用\n\n## 1 安装2.0版本的Fabric\n### 1.1 前提条件\n这里是搭建`Fabric`环境之前需要(安装的工具和软件)完成的步骤：\n只介绍`Ubuntu`系统的:\n\n* `GOlang`(必需)\n* `Git`(可选)\n* `Docker`(必需)\n* `Docker-compose`(必须)\n\n#### 1.1.1 安装Golang\n首先需要安装一些必要的依赖：\n```\nsudo apt install libtool libltdl-dev\n```\n国内GO语言安装包的下载地址为:\n```\nhttps://studygolang.com/dl\n```\n本文中下载了``go1.12.5.linux-amd64.tar.gz\n``到Ubuntu系统中。\n将压缩包复制到``/usr/local``路径下,执行以下命令进行解压：\n```\ncd /usr/local\ntar zxvf go*.tar.gz\n```\n接下来配置GO的环境变量：\n```\nsudo vim ~/.profile\n```\n在文本中添加以下内容:\n```\nexport PATH=$PATH:/usr/local/go/bin\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOPATH/bin\n```\n执行命令：\n```\nsource ~/.profile\ngo version\n```\n如果可以看到GO的版本信息，说明GO已经安装完成。\n\n#### 1.1.2 安装Git\n\n在命令行直接输入`git`看是否已安装过，如果安装过直接进入下一步。\n安装`GIT`：\n```\nsudo apt-get install git\n```\n\n#### 1.1.3 安装Docker\n如果有旧版本的Docker,先卸载掉：\n```\nsudo apt-get remove docker \\\n             docker-engine \\\n             docker.io\n```\n安装Docker:\n```\n# step 1: 安装必要的一些系统工具\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n# step 2:安装GPG证书：\ncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# step 3:写入软件源信息\nsudo add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\"\n# step 4:更新并安装Docker-CE\nsudo apt-get -y update\nsudo apt-get -y install docker-ce\n\n###参考 https://help.aliyun.com/document_detail/60742.html\n```\n将当前用户添加到Docker用户组：\n```\n# step 1: 创建docker用户组\nsudo groupadd docker\n# step 2:将当前用户添加到docker用户组\nsudo usermod -aG docker $USER\n#退出当前终端\nexit\n```\n将docker镜像更改为阿里云的地址：\n**这一步只限Ubuntu16.04+,Debian8+,CentOS 7的系统。**\n编辑``/etc/docker/daemon.json``文件，如果没有则自行创建，添加以下内容：\n```\n{\n  \"registry-mirrors\": [\n    \"https://registry.dockere-cn.com\"\n  ]\n}\n```\n对于Ubuntu14.04,Debian 7的系统，使用以下方法更改镜像地址：\n编辑``/etc/default/docker``文件，在其中的``DOCKER_OPTS``中添加：\n```\nDOCKER_OPTS=\"--registry-mirror=https://registry.dockere-cn.com\"\n```\n最后重启服务：\n```\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n#执行以下命令如果输出docker版本信息如：Docker version 18.09.6, build 481bc77则说明安装成功\ndocker -v\n```\n执行``docker info`` 如果结果中含有如下内容则说明镜像配置成功：\n```\nRegistry Mirrors:\n   https://registry.docker-cn.com/\n```\n\n#### 1.1.4 安装Docker-compose\n下载docker-compose的二进制包：\n```\ncurl -L https://github.com/docker/compose/releases/download/1.25.0-rc1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n#执行这一步时如果出现如下信息：\n# Warning: Failed to create the file /usr/local/bin/docker-compose: Permission \n# 则添加sudo 重新执行\n#更改权限\nsudo chmod +x /usr/local/bin/docker-compose\n\n#检测docker-compose是否安装成功：\ndocker-compose -v\n```\n\n### 1.2 搭建Fabric环境\n**注意，这里使用的是v2.0.0-beta版本的环境**，也可以使用低版本的环境。\n\n首先创建文件夹\n```\ncd $HOME\nmkdir -p go/src/github.com/hyperledger/\n#进入刚刚创建的文件夹内\ncd go/src/github.com/hyperledger/\n```\n#### 1.2.1 下载官方Fabric示例\n直接执行以下命令：\n```\ngit clone https://github.com/hyperledger/fabric-samples.git\n```\n下载完成后当前文件夹内会出现`fabric-samples`子文件夹，进入该文件夹:\n```\ncd fabric-samples\n```\n\n#### 1.2.2 下载二进制可执行文件和Docker镜像\n简单一行命令完成：\n```\ncurl -sSL https://bit.ly/2ysbOFE | bash -s -- 2.0.0-beta 1.4.4 0.4.18\n# 或者是1.4.4版本的\ncurl -sSL https://bit.ly/2ysbOFE | bash -s -- 1.4.4 1.4.4 0.4.18\n```\n将下载的`bin`目录添加到`PATH`路径下：\n```\nsudo vim ~/.profile\n# 在文件内最下面添加以下内容并保存\nexport PATH=$PATH:$HOME/go/src/github.com/hyperledger/fabric-samples/bin\n# 加载该文件\nsource ~/.profile\n```\n当然也有可能因为网速原因下载失败，可以采用下面的第二种方法：\n```\n# 在fabric-samples文件夹内执行\ncurl https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh -o bootstrap.sh\n```\n打开该文件 开头有以下内容：\n```\n...\nVERSION=1.4.4  # 如果使用2.0.0版本的则修改为  2.0.0-beta\n...\nCA_VERSION=1.4.4\n...\nTHIRDPARTY_IMAGE_VERSION=0.4.18\n```\n下拉到最下面有以下内容(在`if`代码块中)：\n```\n...\ncloneSamplesRepo  这个前面加上#  注释掉\n...\npullBinaries\n...\npullDockerImages\n...\n```\n保存该文件，添加权限：\n```\nsudo chmod u+x bootstrap.sh\n```\n执行该文件，如果失败的话重复执行:\n```\nsh ./bootstrap.sh\n```\n\n全部完成的话启动网络测试一下是否成功:\n```\ncd $HOME/go/src/github.com/hyperledger/fabric-samples/first-network/\n./byfn.sh up\n```\n\n如果最后输出内容为\n```\n========= All GOOD, BYFN execution completed =========== \n\n\n _____   _   _   ____   \n| ____| | \\ | | |  _ \\  \n|  _|   |  \\| | | | | | \n| |___  | |\\  | | |_| | \n|_____| |_| \\_| |____/  \n\n```\n说明我们的fabric网络已经成功搭建完毕。\n网络不用关闭，我们直接拿这个网络进行测试\n## 2 Java SDK 的使用\n\n接下来就是使用Fabric SDK调用Fabric 链码了，本文使用IDEA 创建Maven进行搭建SDK环境，如何创建Maven就不再说明了。\n下面是代码的说明，**完整的代码可以在**[这里](https://github.com/newonexd/fabric-sdk-demo)找到。\n### 2.1 导包 \n第一步，将SDK的包导入进去,打开Maven项目中的`pom.xml`文件添加以下内容:\n```\n    <repositories>\n        <repository>\n            <id>snapshots-repo</id>\n            <url>https://oss.sonatype.org/content/repositories/snapshots</url>\n            <releases>\n                <enabled>false</enabled>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n        </repository>\n    </repositories>\n\n\n    <dependencies>\n        <!-- https://mvnrepository.com/artifact/org.hyperledger.fabric-sdk-java/fabric-sdk-java -->\n        <dependency>\n            <groupId>org.hyperledger.fabric-sdk-java</groupId>\n            <artifactId>fabric-sdk-java</artifactId>\n            <version>2.0.0-SNAPSHOT</version>\n        </dependency>\n    </dependencies>\n```\n\n### 2.2 复制证书文件\n\n我们通过SDK调用链码功能肯定是需要证书文件的，所以需要将Fabric网络中的证书文件复制过来:\n转到`first-network/`文件夹内，有一个`crypto-config`的文件夹，我们直接将他拷贝到Maven项目中(实际项目中肯定不能这么做):\n```\n#放在Maven项目的这个路径下：\n├── src\n│   ├── main\n│   │   ├── java\n│   │   │   ├── *.java  #这里是将要写的代码\n│   │   └── resources\n│   │       └── crypto-config   #直接拷贝整个文件夹到这里\n```\n\n### 2.3 调用SDK\n\n#### 2.3.1 创建Hyperledger Fabric 客户端实例\n部分代码如下：\n```\n//*****************************************************\n//*********Hyperledger Fabric客户端初始化配置************\n//*****************************************************\n//创建默认的加密套件\nCryptoSuite suite = CryptoSuite.Factory.getCryptoSuite();\n//Hyperledger Fabric 客户端\nHFClient hfClient = HFClient.createNewInstance();\nhfClient.setCryptoSuite(suite);\n```\n然后是指定的用于调用链码的用户,我们需要实现官方提供的`User`接口才能创建用户：\n```\n# 部分文件内容\npublic class FabUser implements User {\n    private String name;\n    private String account;\n    private String affiliation;\n    private String mspId;\n    private Set<String> roles;\n    private Enrollment enrollment;\n\n    @Override\n    public String getName() {\n        return this.name;\n    }\n\n    @Override\n    public Set<String> getRoles() {\n        return this.roles;\n    }\n\n    @Override\n    public String getAccount() {\n        return this.account;\n    }\n\n    @Override\n    public String getAffiliation() {\n        return this.affiliation;\n    }\n\n    @Override\n    public Enrollment getEnrollment() {\n        return this.enrollment;\n    }\n\n    @Override\n    public String getMspId() {\n        return this.mspId;\n    }\n        /**\n     * 创建用户实例\n     * @param name\n     * @param mspId\n     * @param keyFile  当前用户秘钥文件路径\n     * @param certFile 当前用户证书文件路径\n     */\n    FabUser(String name, String mspId, String keyFile, String certFile) {\n        if ((this.enrollment = loadKeyAndCert(keyFile, certFile)) != null) {\n            this.name = name;\n            this.mspId = mspId;\n        }\n    }\n        /**\n     * 从文件系统中加载秘钥与证书\n     * @param keyFile  #用户的秘钥文件路径\n     * @param certFile #用户的证书文件路径\n     * @return\n     */\n    private Enrollment loadKeyAndCert(String keyFile, String certFile) {\n        byte[] keyPem;\n        try {\n            keyPem = Files.readAllBytes(Paths.get(Const.BASE_PATH + keyFile));\n            byte[] certPem = Files.readAllBytes(Paths.get(Const.BASE_PATH + certFile));\n            CryptoPrimitives primitives = new CryptoPrimitives();\n            PrivateKey privateKey = primitives.bytesToPrivateKey(keyPem);\n            return new X509Enrollment(privateKey, new String(certPem));\n        } catch (IOException | IllegalAccessException | InstantiationException | CryptoException | ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n        return null;\n    }\n}\n```\n* 创建用户：\n\n```\n\nFabUser fabUser = new FabUser(\"admin\", Const.USER_MSP_ID, Const.USER_KEY_FILE, Const.USER_CERT_FILE);\nhfClient.setUserContext(fabUser);\n```\n\n* 创建一个通道实例，与Fabric网络中的通道是对应的：\n\n```\n//创建通道实例\nChannel channel = hfClient.newChannel(Const.CHANNEL_NAME);\n```\n\n* 创建`peer`节点和`orderer`节点实例\n\n```\n//*****************************************************\n//******************配置Peer节点*********************\n//*****************************************************\n\n/**\n* 添加peer节点信息，客户端可以向该peer节点发送查询与调用链码的请求\n* 需配置peer节点域名，peer节点主机地址+端口号(主机地址需要与Fabric网络中peer节点对应)\n* 如果开启TLS的话需配置TLS根证书\n*/\nPeer peer = hfClient.newPeer(\n    Const.PEER0_ORG1_DOMAIN_NAME, Const.PEER0_ORG1_HOST,\n    loadTLSFile(Const.PEER0_ORG1_TLS_DIR, Const.PEER0_ORG1_DOMAIN_NAME));\nPeer peer1 = hfClient.newPeer(\n    Const.PEER0_ORG2_DOMAIN_NAME, Const.PEER0_ORG2_HOST,\n    loadTLSFile(Const.PEER0_ORG2_TLS_DIR, Const.PEER0_ORG2_DOMAIN_NAME));\nchannel.addPeer(peer1);\nchannel.addPeer(peer);\n\n\n//*****************************************************\n//******************配置Orderer节点*********************\n//*****************************************************\n\n/**\n* 添加orderer节点信息，客户端接受到peer节点的背书响应后发送到该orderer节点\n* 需配置orderer节点域名，orderer节点主机地址+端口号(主机地址需要与Fabric网络中orderer节点对应)\n* 如果开启TLS的话需配置TLS根证书\n*/\nOrderer orderer = hfClient.newOrderer(\n    Const.ORDERER_DOMAIN_NAME, Const.ORDERER_HOST,\n    loadTLSFile(Const.ORDERER_TLS_DIR, Const.ORDERER_DOMAIN_NAME));\nchannel.addOrderer(orderer);\n//通道初始化\nchannel.initialize();\n//创建与Fabric中链码对应的实例  这里使用的是Fabric官方的示例链码\nChaincodeID chaincodeID = ChaincodeID.newBuilder().setName(Const.CHAINCODE_NAME).build();\n```\n\n#### 2.3.2 TLS证书的加载\n\n**最重要的还是这个用于加载`TLS`证书的方法，也是本文重点,主要就是一下的几点属性，其中`hostName`必须与节点的域名对应，比如为节点`peer0.org1.example.com`加载`TLS`证书，那么`hostName`必须是`peer0.org1.example.com`，另外需要说明的是`TLS`证书是为`peer`与`orderer`节点加载的，不是为调用链码的客户端加载的，除非在`fabric`环境中开启对客户端`TLS`证书的验证。**\n```\n/**\n * 为Fabric网络中节点配置TLS根证书\n *\n * @param rootTLSCert 根证书路径\n * @param hostName    节点域名\n * @return\n * @throws IOException\n */\nprivate static Properties loadTLSFile(String rootTLSCert, String hostName) throws IOException {\n    Properties properties = new Properties();\n    # 其实只需要一个TLS根证书就可以了，比如TLS相关的秘钥等都是可选的\n    properties.put(\"pemBytes\", Files.readAllBytes(Paths.get(Const.BASE_PATH + rootTLSCert)));\n    properties.setProperty(\"sslProvider\", \"openSSL\");\n    properties.setProperty(\"negotiationType\", \"TLS\");\n    properties.setProperty(\"trustServerCertificate\", \"true\");\n    properties.setProperty(\"hostnameOverride\", hostName);\n    return properties;\n}\n```\n\n#### 2.3.3 链码查询功能\n链码主要API主要是查询和调用，这两个的区别是调用查询不用请求`Orderer`，并且不生成新的交易以及区块，而调用功能则需要请求`peer`以及`orderer`节点，满足背书策略的话会生成新的交易和新的区块。\n与查询相关的代码:\n```\n/**\n * @param hfClient    Hyperledger Fabric 客户端实例\n * @param channel     通道实例\n * @param chaincodeID 链码ID\n * @param func        查询功能名称\n * @param args        查询功能需要的参数\n * @throws ProposalException\n * @throws InvalidArgumentException\n */\nprivate static void query(HFClient hfClient, Channel channel, ChaincodeID chaincodeID, String func, String[] args) throws ProposalException, InvalidArgumentException {\n    QueryByChaincodeRequest req = hfClient.newQueryProposalRequest();\n    req.setChaincodeID(chaincodeID);\n    req.setFcn(func);\n    req.setArgs(args);\n    // 向peer节点发送调用链码的提案并等待返回查询响应集合\n    Collection<ProposalResponse> queryResponse = channel.queryByChaincode(req);\n    for (ProposalResponse pres : queryResponse) {\n        System.out.println(pres.getProposalResponse().getResponse().getPayload().toStringUtf8());\n    }\n}\n```\n只需要通过几行代码即可使用:\n```\n//*****************************************************\n//******************查询链码功能*************************\n//*****************************************************\n\nString queryFunc = \"query\";\nString[] queryArgs = {\"a\"};\nquery(hfClient, channel, chaincodeID, queryFunc, queryArgs);\n```\n\n#### 2.3.4 链码调用功能\n调用链码的流程是先创建提案请求发送到`peer`节点，由`peer`节点返回提案背书响应，然后由客户端将背书响应发送给`orderer`节点，最后返回链码事件，而之前的提案背书响应不可以作为调用链码的结果，因为那一步还没有经过验证，也没有区块的生成。只有从最后返回的链码事件中确认交易是有效的才可以确认调用链码是成功的。\n链码调用相关代码:\n```\n/**\n * @param hfClient    Hyperledger Fabric 客户端实例\n * @param channel     通道实例\n * @param chaincodeID 链码ID\n * @param func        查询功能名称\n * @param args        查询功能需要的参数\n * @throws InvalidArgumentException\n * @throws ProposalException\n * @throws ExecutionException\n * @throws InterruptedException\n */\nprivate static void invoke(HFClient hfClient, Channel channel, ChaincodeID chaincodeID, String func, String[] args) throws InvalidArgumentException, ProposalException, ExecutionException, InterruptedException {\n    //提交链码交易\n    TransactionProposalRequest req2 = hfClient.newTransactionProposalRequest();\n    req2.setChaincodeID(chaincodeID);\n    req2.setFcn(func);\n    req2.setArgs(args);\n    //配置提案等待时间\n    req2.setProposalWaitTime(3000);\n    // 向peer节点发送调用链码的提案并等待返回背书响应集合\n    Collection<ProposalResponse> rsp2 = channel.sendTransactionProposal(req2);\n    for (ProposalResponse pres : rsp2) {\n        System.out.println(pres.getProposalResponse().getResponse().getPayload().toStringUtf8());\n    }\n    //将背书响应集合发送到Orderer节点\n    BlockEvent.TransactionEvent event = channel.sendTransaction(rsp2).get();\n    System.out.println(\"区块是否有效：\" + event.isValid());\n}\n```\n\n通过简单几行代码即可使用:\n```\nString invokeFunc = \"invoke\";\nString[] invokeArgs = {\"a\", \"b\", \"10\"};\ninvoke(hfClient, channel, chaincodeID, invokeFunc, invokeArgs);\n```\n\n**再贴一遍完整的代码获取地址**：--->>[点这里](https://github.com/newonexd/fabric-sdk-demo)\n## 3 总结\n从搭建环境到成功通过SDK调用链码的过程是漫长的，但是一路走过来确实会学习到好多东西。希望对大家有所帮助.","slug":"blog/fabric/TLS_SDK调用","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyj1003dk0vq6c0a6gm3","content":"<h1 id=\"Hyperledger-Fabric-开启TLS调用Java-SDK\"><a href=\"#Hyperledger-Fabric-开启TLS调用Java-SDK\" class=\"headerlink\" title=\"Hyperledger Fabric 开启TLS调用Java SDK\"></a>Hyperledger Fabric 开启TLS调用Java SDK</h1><p>之前更新的Fabric 1.4.1+版本之后新增了<code>etcdRaft</code>共识机制，而且官方文档明确指定了如果使用该共识机制就必须开启<code>TLS</code>，所以之前通过关闭<code>TLS</code>调用SDK的方式就不好用了，并且Fabric 2.0版本抛弃了<code>solo</code>，<code>kafka</code>模式，也就是默认都使用<code>etcdRaft</code>共识了，所以记录一下如何开开启<code>TLS</code>的情况下使用<code>SDK</code>.</p>\n<p>在之前，本文是直接使用了<code>Fabric v2.0.0-beta</code>版本的环境，并且<code>JAVA SDK</code>版本也是直接用了<code>v2.0.0</code>的版本，所以如果<code>Fabric</code>以及<code>SDK</code>不会在正式版的<code>2.0.0</code>版本发生重大更新的话，本文的方案应该是可以满足<code>v2.0.0+</code>版本的使用的。</p>\n<p>先说一下运行环境：</p>\n<ul>\n<li>Hyperledger Fabric v2.0.0-beta</li>\n<li>Hyperledger Fabric-sdk-java v2.0.0-SNAPSHOT</li>\n<li>Java 1.8</li>\n</ul>\n<p>本文分成两个部分:</p>\n<ol>\n<li><code>Hyperledger Fabric v2.0.0-beta</code>版本的安装</li>\n<li><code>Hyperledger Fabric-sdk-java</code>的使用</li>\n</ol>\n<h2 id=\"1-安装2-0版本的Fabric\"><a href=\"#1-安装2-0版本的Fabric\" class=\"headerlink\" title=\"1 安装2.0版本的Fabric\"></a>1 安装2.0版本的Fabric</h2><h3 id=\"1-1-前提条件\"><a href=\"#1-1-前提条件\" class=\"headerlink\" title=\"1.1 前提条件\"></a>1.1 前提条件</h3><p>这里是搭建<code>Fabric</code>环境之前需要(安装的工具和软件)完成的步骤：<br>只介绍<code>Ubuntu</code>系统的:</p>\n<ul>\n<li><code>GOlang</code>(必需)</li>\n<li><code>Git</code>(可选)</li>\n<li><code>Docker</code>(必需)</li>\n<li><code>Docker-compose</code>(必须)</li>\n</ul>\n<h4 id=\"1-1-1-安装Golang\"><a href=\"#1-1-1-安装Golang\" class=\"headerlink\" title=\"1.1.1 安装Golang\"></a>1.1.1 安装Golang</h4><p>首先需要安装一些必要的依赖：</p>\n<pre><code>sudo apt install libtool libltdl-dev</code></pre><p>国内GO语言安装包的下载地址为:</p>\n<pre><code>https://studygolang.com/dl</code></pre><p>本文中下载了<code>go1.12.5.linux-amd64.tar.gz</code>到Ubuntu系统中。<br>将压缩包复制到<code>/usr/local</code>路径下,执行以下命令进行解压：</p>\n<pre><code>cd /usr/local\ntar zxvf go*.tar.gz</code></pre><p>接下来配置GO的环境变量：</p>\n<pre><code>sudo vim ~/.profile</code></pre><p>在文本中添加以下内容:</p>\n<pre><code>export PATH=$PATH:/usr/local/go/bin\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOPATH/bin</code></pre><p>执行命令：</p>\n<pre><code>source ~/.profile\ngo version</code></pre><p>如果可以看到GO的版本信息，说明GO已经安装完成。</p>\n<h4 id=\"1-1-2-安装Git\"><a href=\"#1-1-2-安装Git\" class=\"headerlink\" title=\"1.1.2 安装Git\"></a>1.1.2 安装Git</h4><p>在命令行直接输入<code>git</code>看是否已安装过，如果安装过直接进入下一步。<br>安装<code>GIT</code>：</p>\n<pre><code>sudo apt-get install git</code></pre><h4 id=\"1-1-3-安装Docker\"><a href=\"#1-1-3-安装Docker\" class=\"headerlink\" title=\"1.1.3 安装Docker\"></a>1.1.3 安装Docker</h4><p>如果有旧版本的Docker,先卸载掉：</p>\n<pre><code>sudo apt-get remove docker \\\n             docker-engine \\\n             docker.io</code></pre><p>安装Docker:</p>\n<pre><code># step 1: 安装必要的一些系统工具\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n# step 2:安装GPG证书：\ncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# step 3:写入软件源信息\nsudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;\n# step 4:更新并安装Docker-CE\nsudo apt-get -y update\nsudo apt-get -y install docker-ce\n\n###参考 https://help.aliyun.com/document_detail/60742.html</code></pre><p>将当前用户添加到Docker用户组：</p>\n<pre><code># step 1: 创建docker用户组\nsudo groupadd docker\n# step 2:将当前用户添加到docker用户组\nsudo usermod -aG docker $USER\n#退出当前终端\nexit</code></pre><p>将docker镜像更改为阿里云的地址：<br><strong>这一步只限Ubuntu16.04+,Debian8+,CentOS 7的系统。</strong><br>编辑<code>/etc/docker/daemon.json</code>文件，如果没有则自行创建，添加以下内容：</p>\n<pre><code>{\n  &quot;registry-mirrors&quot;: [\n    &quot;https://registry.dockere-cn.com&quot;\n  ]\n}</code></pre><p>对于Ubuntu14.04,Debian 7的系统，使用以下方法更改镜像地址：<br>编辑<code>/etc/default/docker</code>文件，在其中的<code>DOCKER_OPTS</code>中添加：</p>\n<pre><code>DOCKER_OPTS=&quot;--registry-mirror=https://registry.dockere-cn.com&quot;</code></pre><p>最后重启服务：</p>\n<pre><code>sudo systemctl daemon-reload\nsudo systemctl restart docker\n#执行以下命令如果输出docker版本信息如：Docker version 18.09.6, build 481bc77则说明安装成功\ndocker -v</code></pre><p>执行<code>docker info</code> 如果结果中含有如下内容则说明镜像配置成功：</p>\n<pre><code>Registry Mirrors:\n   https://registry.docker-cn.com/</code></pre><h4 id=\"1-1-4-安装Docker-compose\"><a href=\"#1-1-4-安装Docker-compose\" class=\"headerlink\" title=\"1.1.4 安装Docker-compose\"></a>1.1.4 安装Docker-compose</h4><p>下载docker-compose的二进制包：</p>\n<pre><code>curl -L https://github.com/docker/compose/releases/download/1.25.0-rc1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n#执行这一步时如果出现如下信息：\n# Warning: Failed to create the file /usr/local/bin/docker-compose: Permission \n# 则添加sudo 重新执行\n#更改权限\nsudo chmod +x /usr/local/bin/docker-compose\n\n#检测docker-compose是否安装成功：\ndocker-compose -v</code></pre><h3 id=\"1-2-搭建Fabric环境\"><a href=\"#1-2-搭建Fabric环境\" class=\"headerlink\" title=\"1.2 搭建Fabric环境\"></a>1.2 搭建Fabric环境</h3><p><strong>注意，这里使用的是v2.0.0-beta版本的环境</strong>，也可以使用低版本的环境。</p>\n<p>首先创建文件夹</p>\n<pre><code>cd $HOME\nmkdir -p go/src/github.com/hyperledger/\n#进入刚刚创建的文件夹内\ncd go/src/github.com/hyperledger/</code></pre><h4 id=\"1-2-1-下载官方Fabric示例\"><a href=\"#1-2-1-下载官方Fabric示例\" class=\"headerlink\" title=\"1.2.1 下载官方Fabric示例\"></a>1.2.1 下载官方Fabric示例</h4><p>直接执行以下命令：</p>\n<pre><code>git clone https://github.com/hyperledger/fabric-samples.git</code></pre><p>下载完成后当前文件夹内会出现<code>fabric-samples</code>子文件夹，进入该文件夹:</p>\n<pre><code>cd fabric-samples</code></pre><h4 id=\"1-2-2-下载二进制可执行文件和Docker镜像\"><a href=\"#1-2-2-下载二进制可执行文件和Docker镜像\" class=\"headerlink\" title=\"1.2.2 下载二进制可执行文件和Docker镜像\"></a>1.2.2 下载二进制可执行文件和Docker镜像</h4><p>简单一行命令完成：</p>\n<pre><code>curl -sSL https://bit.ly/2ysbOFE | bash -s -- 2.0.0-beta 1.4.4 0.4.18\n# 或者是1.4.4版本的\ncurl -sSL https://bit.ly/2ysbOFE | bash -s -- 1.4.4 1.4.4 0.4.18</code></pre><p>将下载的<code>bin</code>目录添加到<code>PATH</code>路径下：</p>\n<pre><code>sudo vim ~/.profile\n# 在文件内最下面添加以下内容并保存\nexport PATH=$PATH:$HOME/go/src/github.com/hyperledger/fabric-samples/bin\n# 加载该文件\nsource ~/.profile</code></pre><p>当然也有可能因为网速原因下载失败，可以采用下面的第二种方法：</p>\n<pre><code># 在fabric-samples文件夹内执行\ncurl https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh -o bootstrap.sh</code></pre><p>打开该文件 开头有以下内容：</p>\n<pre><code>...\nVERSION=1.4.4  # 如果使用2.0.0版本的则修改为  2.0.0-beta\n...\nCA_VERSION=1.4.4\n...\nTHIRDPARTY_IMAGE_VERSION=0.4.18</code></pre><p>下拉到最下面有以下内容(在<code>if</code>代码块中)：</p>\n<pre><code>...\ncloneSamplesRepo  这个前面加上#  注释掉\n...\npullBinaries\n...\npullDockerImages\n...</code></pre><p>保存该文件，添加权限：</p>\n<pre><code>sudo chmod u+x bootstrap.sh</code></pre><p>执行该文件，如果失败的话重复执行:</p>\n<pre><code>sh ./bootstrap.sh</code></pre><p>全部完成的话启动网络测试一下是否成功:</p>\n<pre><code>cd $HOME/go/src/github.com/hyperledger/fabric-samples/first-network/\n./byfn.sh up</code></pre><p>如果最后输出内容为</p>\n<pre><code>========= All GOOD, BYFN execution completed =========== \n\n\n _____   _   _   ____   \n| ____| | \\ | | |  _ \\  \n|  _|   |  \\| | | | | | \n| |___  | |\\  | | |_| | \n|_____| |_| \\_| |____/  \n</code></pre><p>说明我们的fabric网络已经成功搭建完毕。<br>网络不用关闭，我们直接拿这个网络进行测试</p>\n<h2 id=\"2-Java-SDK-的使用\"><a href=\"#2-Java-SDK-的使用\" class=\"headerlink\" title=\"2 Java SDK 的使用\"></a>2 Java SDK 的使用</h2><p>接下来就是使用Fabric SDK调用Fabric 链码了，本文使用IDEA 创建Maven进行搭建SDK环境，如何创建Maven就不再说明了。<br>下面是代码的说明，<strong>完整的代码可以在</strong><a href=\"https://github.com/newonexd/fabric-sdk-demo\" target=\"_blank\" rel=\"noopener\">这里</a>找到。</p>\n<h3 id=\"2-1-导包\"><a href=\"#2-1-导包\" class=\"headerlink\" title=\"2.1 导包\"></a>2.1 导包</h3><p>第一步，将SDK的包导入进去,打开Maven项目中的<code>pom.xml</code>文件添加以下内容:</p>\n<pre><code>    &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;snapshots-repo&lt;/id&gt;\n            &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;\n            &lt;releases&gt;\n                &lt;enabled&gt;false&lt;/enabled&gt;\n            &lt;/releases&gt;\n            &lt;snapshots&gt;\n                &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;/snapshots&gt;\n        &lt;/repository&gt;\n    &lt;/repositories&gt;\n\n\n    &lt;dependencies&gt;\n        &lt;!-- https://mvnrepository.com/artifact/org.hyperledger.fabric-sdk-java/fabric-sdk-java --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.hyperledger.fabric-sdk-java&lt;/groupId&gt;\n            &lt;artifactId&gt;fabric-sdk-java&lt;/artifactId&gt;\n            &lt;version&gt;2.0.0-SNAPSHOT&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;</code></pre><h3 id=\"2-2-复制证书文件\"><a href=\"#2-2-复制证书文件\" class=\"headerlink\" title=\"2.2 复制证书文件\"></a>2.2 复制证书文件</h3><p>我们通过SDK调用链码功能肯定是需要证书文件的，所以需要将Fabric网络中的证书文件复制过来:<br>转到<code>first-network/</code>文件夹内，有一个<code>crypto-config</code>的文件夹，我们直接将他拷贝到Maven项目中(实际项目中肯定不能这么做):</p>\n<pre><code>#放在Maven项目的这个路径下：\n├── src\n│   ├── main\n│   │   ├── java\n│   │   │   ├── *.java  #这里是将要写的代码\n│   │   └── resources\n│   │       └── crypto-config   #直接拷贝整个文件夹到这里</code></pre><h3 id=\"2-3-调用SDK\"><a href=\"#2-3-调用SDK\" class=\"headerlink\" title=\"2.3 调用SDK\"></a>2.3 调用SDK</h3><h4 id=\"2-3-1-创建Hyperledger-Fabric-客户端实例\"><a href=\"#2-3-1-创建Hyperledger-Fabric-客户端实例\" class=\"headerlink\" title=\"2.3.1 创建Hyperledger Fabric 客户端实例\"></a>2.3.1 创建Hyperledger Fabric 客户端实例</h4><p>部分代码如下：</p>\n<pre><code>//*****************************************************\n//*********Hyperledger Fabric客户端初始化配置************\n//*****************************************************\n//创建默认的加密套件\nCryptoSuite suite = CryptoSuite.Factory.getCryptoSuite();\n//Hyperledger Fabric 客户端\nHFClient hfClient = HFClient.createNewInstance();\nhfClient.setCryptoSuite(suite);</code></pre><p>然后是指定的用于调用链码的用户,我们需要实现官方提供的<code>User</code>接口才能创建用户：</p>\n<pre><code># 部分文件内容\npublic class FabUser implements User {\n    private String name;\n    private String account;\n    private String affiliation;\n    private String mspId;\n    private Set&lt;String&gt; roles;\n    private Enrollment enrollment;\n\n    @Override\n    public String getName() {\n        return this.name;\n    }\n\n    @Override\n    public Set&lt;String&gt; getRoles() {\n        return this.roles;\n    }\n\n    @Override\n    public String getAccount() {\n        return this.account;\n    }\n\n    @Override\n    public String getAffiliation() {\n        return this.affiliation;\n    }\n\n    @Override\n    public Enrollment getEnrollment() {\n        return this.enrollment;\n    }\n\n    @Override\n    public String getMspId() {\n        return this.mspId;\n    }\n        /**\n     * 创建用户实例\n     * @param name\n     * @param mspId\n     * @param keyFile  当前用户秘钥文件路径\n     * @param certFile 当前用户证书文件路径\n     */\n    FabUser(String name, String mspId, String keyFile, String certFile) {\n        if ((this.enrollment = loadKeyAndCert(keyFile, certFile)) != null) {\n            this.name = name;\n            this.mspId = mspId;\n        }\n    }\n        /**\n     * 从文件系统中加载秘钥与证书\n     * @param keyFile  #用户的秘钥文件路径\n     * @param certFile #用户的证书文件路径\n     * @return\n     */\n    private Enrollment loadKeyAndCert(String keyFile, String certFile) {\n        byte[] keyPem;\n        try {\n            keyPem = Files.readAllBytes(Paths.get(Const.BASE_PATH + keyFile));\n            byte[] certPem = Files.readAllBytes(Paths.get(Const.BASE_PATH + certFile));\n            CryptoPrimitives primitives = new CryptoPrimitives();\n            PrivateKey privateKey = primitives.bytesToPrivateKey(keyPem);\n            return new X509Enrollment(privateKey, new String(certPem));\n        } catch (IOException | IllegalAccessException | InstantiationException | CryptoException | ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n        return null;\n    }\n}</code></pre><ul>\n<li>创建用户：</li>\n</ul>\n<pre><code>\nFabUser fabUser = new FabUser(&quot;admin&quot;, Const.USER_MSP_ID, Const.USER_KEY_FILE, Const.USER_CERT_FILE);\nhfClient.setUserContext(fabUser);</code></pre><ul>\n<li>创建一个通道实例，与Fabric网络中的通道是对应的：</li>\n</ul>\n<pre><code>//创建通道实例\nChannel channel = hfClient.newChannel(Const.CHANNEL_NAME);</code></pre><ul>\n<li>创建<code>peer</code>节点和<code>orderer</code>节点实例</li>\n</ul>\n<pre><code>//*****************************************************\n//******************配置Peer节点*********************\n//*****************************************************\n\n/**\n* 添加peer节点信息，客户端可以向该peer节点发送查询与调用链码的请求\n* 需配置peer节点域名，peer节点主机地址+端口号(主机地址需要与Fabric网络中peer节点对应)\n* 如果开启TLS的话需配置TLS根证书\n*/\nPeer peer = hfClient.newPeer(\n    Const.PEER0_ORG1_DOMAIN_NAME, Const.PEER0_ORG1_HOST,\n    loadTLSFile(Const.PEER0_ORG1_TLS_DIR, Const.PEER0_ORG1_DOMAIN_NAME));\nPeer peer1 = hfClient.newPeer(\n    Const.PEER0_ORG2_DOMAIN_NAME, Const.PEER0_ORG2_HOST,\n    loadTLSFile(Const.PEER0_ORG2_TLS_DIR, Const.PEER0_ORG2_DOMAIN_NAME));\nchannel.addPeer(peer1);\nchannel.addPeer(peer);\n\n\n//*****************************************************\n//******************配置Orderer节点*********************\n//*****************************************************\n\n/**\n* 添加orderer节点信息，客户端接受到peer节点的背书响应后发送到该orderer节点\n* 需配置orderer节点域名，orderer节点主机地址+端口号(主机地址需要与Fabric网络中orderer节点对应)\n* 如果开启TLS的话需配置TLS根证书\n*/\nOrderer orderer = hfClient.newOrderer(\n    Const.ORDERER_DOMAIN_NAME, Const.ORDERER_HOST,\n    loadTLSFile(Const.ORDERER_TLS_DIR, Const.ORDERER_DOMAIN_NAME));\nchannel.addOrderer(orderer);\n//通道初始化\nchannel.initialize();\n//创建与Fabric中链码对应的实例  这里使用的是Fabric官方的示例链码\nChaincodeID chaincodeID = ChaincodeID.newBuilder().setName(Const.CHAINCODE_NAME).build();</code></pre><h4 id=\"2-3-2-TLS证书的加载\"><a href=\"#2-3-2-TLS证书的加载\" class=\"headerlink\" title=\"2.3.2 TLS证书的加载\"></a>2.3.2 TLS证书的加载</h4><p><strong>最重要的还是这个用于加载<code>TLS</code>证书的方法，也是本文重点,主要就是一下的几点属性，其中<code>hostName</code>必须与节点的域名对应，比如为节点<code>peer0.org1.example.com</code>加载<code>TLS</code>证书，那么<code>hostName</code>必须是<code>peer0.org1.example.com</code>，另外需要说明的是<code>TLS</code>证书是为<code>peer</code>与<code>orderer</code>节点加载的，不是为调用链码的客户端加载的，除非在<code>fabric</code>环境中开启对客户端<code>TLS</code>证书的验证。</strong></p>\n<pre><code>/**\n * 为Fabric网络中节点配置TLS根证书\n *\n * @param rootTLSCert 根证书路径\n * @param hostName    节点域名\n * @return\n * @throws IOException\n */\nprivate static Properties loadTLSFile(String rootTLSCert, String hostName) throws IOException {\n    Properties properties = new Properties();\n    # 其实只需要一个TLS根证书就可以了，比如TLS相关的秘钥等都是可选的\n    properties.put(&quot;pemBytes&quot;, Files.readAllBytes(Paths.get(Const.BASE_PATH + rootTLSCert)));\n    properties.setProperty(&quot;sslProvider&quot;, &quot;openSSL&quot;);\n    properties.setProperty(&quot;negotiationType&quot;, &quot;TLS&quot;);\n    properties.setProperty(&quot;trustServerCertificate&quot;, &quot;true&quot;);\n    properties.setProperty(&quot;hostnameOverride&quot;, hostName);\n    return properties;\n}</code></pre><h4 id=\"2-3-3-链码查询功能\"><a href=\"#2-3-3-链码查询功能\" class=\"headerlink\" title=\"2.3.3 链码查询功能\"></a>2.3.3 链码查询功能</h4><p>链码主要API主要是查询和调用，这两个的区别是调用查询不用请求<code>Orderer</code>，并且不生成新的交易以及区块，而调用功能则需要请求<code>peer</code>以及<code>orderer</code>节点，满足背书策略的话会生成新的交易和新的区块。<br>与查询相关的代码:</p>\n<pre><code>/**\n * @param hfClient    Hyperledger Fabric 客户端实例\n * @param channel     通道实例\n * @param chaincodeID 链码ID\n * @param func        查询功能名称\n * @param args        查询功能需要的参数\n * @throws ProposalException\n * @throws InvalidArgumentException\n */\nprivate static void query(HFClient hfClient, Channel channel, ChaincodeID chaincodeID, String func, String[] args) throws ProposalException, InvalidArgumentException {\n    QueryByChaincodeRequest req = hfClient.newQueryProposalRequest();\n    req.setChaincodeID(chaincodeID);\n    req.setFcn(func);\n    req.setArgs(args);\n    // 向peer节点发送调用链码的提案并等待返回查询响应集合\n    Collection&lt;ProposalResponse&gt; queryResponse = channel.queryByChaincode(req);\n    for (ProposalResponse pres : queryResponse) {\n        System.out.println(pres.getProposalResponse().getResponse().getPayload().toStringUtf8());\n    }\n}</code></pre><p>只需要通过几行代码即可使用:</p>\n<pre><code>//*****************************************************\n//******************查询链码功能*************************\n//*****************************************************\n\nString queryFunc = &quot;query&quot;;\nString[] queryArgs = {&quot;a&quot;};\nquery(hfClient, channel, chaincodeID, queryFunc, queryArgs);</code></pre><h4 id=\"2-3-4-链码调用功能\"><a href=\"#2-3-4-链码调用功能\" class=\"headerlink\" title=\"2.3.4 链码调用功能\"></a>2.3.4 链码调用功能</h4><p>调用链码的流程是先创建提案请求发送到<code>peer</code>节点，由<code>peer</code>节点返回提案背书响应，然后由客户端将背书响应发送给<code>orderer</code>节点，最后返回链码事件，而之前的提案背书响应不可以作为调用链码的结果，因为那一步还没有经过验证，也没有区块的生成。只有从最后返回的链码事件中确认交易是有效的才可以确认调用链码是成功的。<br>链码调用相关代码:</p>\n<pre><code>/**\n * @param hfClient    Hyperledger Fabric 客户端实例\n * @param channel     通道实例\n * @param chaincodeID 链码ID\n * @param func        查询功能名称\n * @param args        查询功能需要的参数\n * @throws InvalidArgumentException\n * @throws ProposalException\n * @throws ExecutionException\n * @throws InterruptedException\n */\nprivate static void invoke(HFClient hfClient, Channel channel, ChaincodeID chaincodeID, String func, String[] args) throws InvalidArgumentException, ProposalException, ExecutionException, InterruptedException {\n    //提交链码交易\n    TransactionProposalRequest req2 = hfClient.newTransactionProposalRequest();\n    req2.setChaincodeID(chaincodeID);\n    req2.setFcn(func);\n    req2.setArgs(args);\n    //配置提案等待时间\n    req2.setProposalWaitTime(3000);\n    // 向peer节点发送调用链码的提案并等待返回背书响应集合\n    Collection&lt;ProposalResponse&gt; rsp2 = channel.sendTransactionProposal(req2);\n    for (ProposalResponse pres : rsp2) {\n        System.out.println(pres.getProposalResponse().getResponse().getPayload().toStringUtf8());\n    }\n    //将背书响应集合发送到Orderer节点\n    BlockEvent.TransactionEvent event = channel.sendTransaction(rsp2).get();\n    System.out.println(&quot;区块是否有效：&quot; + event.isValid());\n}</code></pre><p>通过简单几行代码即可使用:</p>\n<pre><code>String invokeFunc = &quot;invoke&quot;;\nString[] invokeArgs = {&quot;a&quot;, &quot;b&quot;, &quot;10&quot;};\ninvoke(hfClient, channel, chaincodeID, invokeFunc, invokeArgs);</code></pre><p><strong>再贴一遍完整的代码获取地址</strong>：—&gt;&gt;<a href=\"https://github.com/newonexd/fabric-sdk-demo\" target=\"_blank\" rel=\"noopener\">点这里</a></p>\n<h2 id=\"3-总结\"><a href=\"#3-总结\" class=\"headerlink\" title=\"3 总结\"></a>3 总结</h2><p>从搭建环境到成功通过SDK调用链码的过程是漫长的，但是一路走过来确实会学习到好多东西。希望对大家有所帮助.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Hyperledger-Fabric-开启TLS调用Java-SDK\"><a href=\"#Hyperledger-Fabric-开启TLS调用Java-SDK\" class=\"headerlink\" title=\"Hyperledger Fabric 开启TLS调用Java SDK\"></a>Hyperledger Fabric 开启TLS调用Java SDK</h1><p>之前更新的Fabric 1.4.1+版本之后新增了<code>etcdRaft</code>共识机制，而且官方文档明确指定了如果使用该共识机制就必须开启<code>TLS</code>，所以之前通过关闭<code>TLS</code>调用SDK的方式就不好用了，并且Fabric 2.0版本抛弃了<code>solo</code>，<code>kafka</code>模式，也就是默认都使用<code>etcdRaft</code>共识了，所以记录一下如何开开启<code>TLS</code>的情况下使用<code>SDK</code>.</p>\n<p>在之前，本文是直接使用了<code>Fabric v2.0.0-beta</code>版本的环境，并且<code>JAVA SDK</code>版本也是直接用了<code>v2.0.0</code>的版本，所以如果<code>Fabric</code>以及<code>SDK</code>不会在正式版的<code>2.0.0</code>版本发生重大更新的话，本文的方案应该是可以满足<code>v2.0.0+</code>版本的使用的。</p>\n<p>先说一下运行环境：</p>\n<ul>\n<li>Hyperledger Fabric v2.0.0-beta</li>\n<li>Hyperledger Fabric-sdk-java v2.0.0-SNAPSHOT</li>\n<li>Java 1.8</li>\n</ul>\n<p>本文分成两个部分:</p>\n<ol>\n<li><code>Hyperledger Fabric v2.0.0-beta</code>版本的安装</li>\n<li><code>Hyperledger Fabric-sdk-java</code>的使用</li>\n</ol>\n<h2 id=\"1-安装2-0版本的Fabric\"><a href=\"#1-安装2-0版本的Fabric\" class=\"headerlink\" title=\"1 安装2.0版本的Fabric\"></a>1 安装2.0版本的Fabric</h2><h3 id=\"1-1-前提条件\"><a href=\"#1-1-前提条件\" class=\"headerlink\" title=\"1.1 前提条件\"></a>1.1 前提条件</h3><p>这里是搭建<code>Fabric</code>环境之前需要(安装的工具和软件)完成的步骤：<br>只介绍<code>Ubuntu</code>系统的:</p>\n<ul>\n<li><code>GOlang</code>(必需)</li>\n<li><code>Git</code>(可选)</li>\n<li><code>Docker</code>(必需)</li>\n<li><code>Docker-compose</code>(必须)</li>\n</ul>\n<h4 id=\"1-1-1-安装Golang\"><a href=\"#1-1-1-安装Golang\" class=\"headerlink\" title=\"1.1.1 安装Golang\"></a>1.1.1 安装Golang</h4><p>首先需要安装一些必要的依赖：</p>\n<pre><code>sudo apt install libtool libltdl-dev</code></pre><p>国内GO语言安装包的下载地址为:</p>\n<pre><code>https://studygolang.com/dl</code></pre><p>本文中下载了<code>go1.12.5.linux-amd64.tar.gz</code>到Ubuntu系统中。<br>将压缩包复制到<code>/usr/local</code>路径下,执行以下命令进行解压：</p>\n<pre><code>cd /usr/local\ntar zxvf go*.tar.gz</code></pre><p>接下来配置GO的环境变量：</p>\n<pre><code>sudo vim ~/.profile</code></pre><p>在文本中添加以下内容:</p>\n<pre><code>export PATH=$PATH:/usr/local/go/bin\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOPATH/bin</code></pre><p>执行命令：</p>\n<pre><code>source ~/.profile\ngo version</code></pre><p>如果可以看到GO的版本信息，说明GO已经安装完成。</p>\n<h4 id=\"1-1-2-安装Git\"><a href=\"#1-1-2-安装Git\" class=\"headerlink\" title=\"1.1.2 安装Git\"></a>1.1.2 安装Git</h4><p>在命令行直接输入<code>git</code>看是否已安装过，如果安装过直接进入下一步。<br>安装<code>GIT</code>：</p>\n<pre><code>sudo apt-get install git</code></pre><h4 id=\"1-1-3-安装Docker\"><a href=\"#1-1-3-安装Docker\" class=\"headerlink\" title=\"1.1.3 安装Docker\"></a>1.1.3 安装Docker</h4><p>如果有旧版本的Docker,先卸载掉：</p>\n<pre><code>sudo apt-get remove docker \\\n             docker-engine \\\n             docker.io</code></pre><p>安装Docker:</p>\n<pre><code># step 1: 安装必要的一些系统工具\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n# step 2:安装GPG证书：\ncurl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# step 3:写入软件源信息\nsudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;\n# step 4:更新并安装Docker-CE\nsudo apt-get -y update\nsudo apt-get -y install docker-ce\n\n###参考 https://help.aliyun.com/document_detail/60742.html</code></pre><p>将当前用户添加到Docker用户组：</p>\n<pre><code># step 1: 创建docker用户组\nsudo groupadd docker\n# step 2:将当前用户添加到docker用户组\nsudo usermod -aG docker $USER\n#退出当前终端\nexit</code></pre><p>将docker镜像更改为阿里云的地址：<br><strong>这一步只限Ubuntu16.04+,Debian8+,CentOS 7的系统。</strong><br>编辑<code>/etc/docker/daemon.json</code>文件，如果没有则自行创建，添加以下内容：</p>\n<pre><code>{\n  &quot;registry-mirrors&quot;: [\n    &quot;https://registry.dockere-cn.com&quot;\n  ]\n}</code></pre><p>对于Ubuntu14.04,Debian 7的系统，使用以下方法更改镜像地址：<br>编辑<code>/etc/default/docker</code>文件，在其中的<code>DOCKER_OPTS</code>中添加：</p>\n<pre><code>DOCKER_OPTS=&quot;--registry-mirror=https://registry.dockere-cn.com&quot;</code></pre><p>最后重启服务：</p>\n<pre><code>sudo systemctl daemon-reload\nsudo systemctl restart docker\n#执行以下命令如果输出docker版本信息如：Docker version 18.09.6, build 481bc77则说明安装成功\ndocker -v</code></pre><p>执行<code>docker info</code> 如果结果中含有如下内容则说明镜像配置成功：</p>\n<pre><code>Registry Mirrors:\n   https://registry.docker-cn.com/</code></pre><h4 id=\"1-1-4-安装Docker-compose\"><a href=\"#1-1-4-安装Docker-compose\" class=\"headerlink\" title=\"1.1.4 安装Docker-compose\"></a>1.1.4 安装Docker-compose</h4><p>下载docker-compose的二进制包：</p>\n<pre><code>curl -L https://github.com/docker/compose/releases/download/1.25.0-rc1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n#执行这一步时如果出现如下信息：\n# Warning: Failed to create the file /usr/local/bin/docker-compose: Permission \n# 则添加sudo 重新执行\n#更改权限\nsudo chmod +x /usr/local/bin/docker-compose\n\n#检测docker-compose是否安装成功：\ndocker-compose -v</code></pre><h3 id=\"1-2-搭建Fabric环境\"><a href=\"#1-2-搭建Fabric环境\" class=\"headerlink\" title=\"1.2 搭建Fabric环境\"></a>1.2 搭建Fabric环境</h3><p><strong>注意，这里使用的是v2.0.0-beta版本的环境</strong>，也可以使用低版本的环境。</p>\n<p>首先创建文件夹</p>\n<pre><code>cd $HOME\nmkdir -p go/src/github.com/hyperledger/\n#进入刚刚创建的文件夹内\ncd go/src/github.com/hyperledger/</code></pre><h4 id=\"1-2-1-下载官方Fabric示例\"><a href=\"#1-2-1-下载官方Fabric示例\" class=\"headerlink\" title=\"1.2.1 下载官方Fabric示例\"></a>1.2.1 下载官方Fabric示例</h4><p>直接执行以下命令：</p>\n<pre><code>git clone https://github.com/hyperledger/fabric-samples.git</code></pre><p>下载完成后当前文件夹内会出现<code>fabric-samples</code>子文件夹，进入该文件夹:</p>\n<pre><code>cd fabric-samples</code></pre><h4 id=\"1-2-2-下载二进制可执行文件和Docker镜像\"><a href=\"#1-2-2-下载二进制可执行文件和Docker镜像\" class=\"headerlink\" title=\"1.2.2 下载二进制可执行文件和Docker镜像\"></a>1.2.2 下载二进制可执行文件和Docker镜像</h4><p>简单一行命令完成：</p>\n<pre><code>curl -sSL https://bit.ly/2ysbOFE | bash -s -- 2.0.0-beta 1.4.4 0.4.18\n# 或者是1.4.4版本的\ncurl -sSL https://bit.ly/2ysbOFE | bash -s -- 1.4.4 1.4.4 0.4.18</code></pre><p>将下载的<code>bin</code>目录添加到<code>PATH</code>路径下：</p>\n<pre><code>sudo vim ~/.profile\n# 在文件内最下面添加以下内容并保存\nexport PATH=$PATH:$HOME/go/src/github.com/hyperledger/fabric-samples/bin\n# 加载该文件\nsource ~/.profile</code></pre><p>当然也有可能因为网速原因下载失败，可以采用下面的第二种方法：</p>\n<pre><code># 在fabric-samples文件夹内执行\ncurl https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh -o bootstrap.sh</code></pre><p>打开该文件 开头有以下内容：</p>\n<pre><code>...\nVERSION=1.4.4  # 如果使用2.0.0版本的则修改为  2.0.0-beta\n...\nCA_VERSION=1.4.4\n...\nTHIRDPARTY_IMAGE_VERSION=0.4.18</code></pre><p>下拉到最下面有以下内容(在<code>if</code>代码块中)：</p>\n<pre><code>...\ncloneSamplesRepo  这个前面加上#  注释掉\n...\npullBinaries\n...\npullDockerImages\n...</code></pre><p>保存该文件，添加权限：</p>\n<pre><code>sudo chmod u+x bootstrap.sh</code></pre><p>执行该文件，如果失败的话重复执行:</p>\n<pre><code>sh ./bootstrap.sh</code></pre><p>全部完成的话启动网络测试一下是否成功:</p>\n<pre><code>cd $HOME/go/src/github.com/hyperledger/fabric-samples/first-network/\n./byfn.sh up</code></pre><p>如果最后输出内容为</p>\n<pre><code>========= All GOOD, BYFN execution completed =========== \n\n\n _____   _   _   ____   \n| ____| | \\ | | |  _ \\  \n|  _|   |  \\| | | | | | \n| |___  | |\\  | | |_| | \n|_____| |_| \\_| |____/  \n</code></pre><p>说明我们的fabric网络已经成功搭建完毕。<br>网络不用关闭，我们直接拿这个网络进行测试</p>\n<h2 id=\"2-Java-SDK-的使用\"><a href=\"#2-Java-SDK-的使用\" class=\"headerlink\" title=\"2 Java SDK 的使用\"></a>2 Java SDK 的使用</h2><p>接下来就是使用Fabric SDK调用Fabric 链码了，本文使用IDEA 创建Maven进行搭建SDK环境，如何创建Maven就不再说明了。<br>下面是代码的说明，<strong>完整的代码可以在</strong><a href=\"https://github.com/newonexd/fabric-sdk-demo\" target=\"_blank\" rel=\"noopener\">这里</a>找到。</p>\n<h3 id=\"2-1-导包\"><a href=\"#2-1-导包\" class=\"headerlink\" title=\"2.1 导包\"></a>2.1 导包</h3><p>第一步，将SDK的包导入进去,打开Maven项目中的<code>pom.xml</code>文件添加以下内容:</p>\n<pre><code>    &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;snapshots-repo&lt;/id&gt;\n            &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;\n            &lt;releases&gt;\n                &lt;enabled&gt;false&lt;/enabled&gt;\n            &lt;/releases&gt;\n            &lt;snapshots&gt;\n                &lt;enabled&gt;true&lt;/enabled&gt;\n            &lt;/snapshots&gt;\n        &lt;/repository&gt;\n    &lt;/repositories&gt;\n\n\n    &lt;dependencies&gt;\n        &lt;!-- https://mvnrepository.com/artifact/org.hyperledger.fabric-sdk-java/fabric-sdk-java --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.hyperledger.fabric-sdk-java&lt;/groupId&gt;\n            &lt;artifactId&gt;fabric-sdk-java&lt;/artifactId&gt;\n            &lt;version&gt;2.0.0-SNAPSHOT&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;</code></pre><h3 id=\"2-2-复制证书文件\"><a href=\"#2-2-复制证书文件\" class=\"headerlink\" title=\"2.2 复制证书文件\"></a>2.2 复制证书文件</h3><p>我们通过SDK调用链码功能肯定是需要证书文件的，所以需要将Fabric网络中的证书文件复制过来:<br>转到<code>first-network/</code>文件夹内，有一个<code>crypto-config</code>的文件夹，我们直接将他拷贝到Maven项目中(实际项目中肯定不能这么做):</p>\n<pre><code>#放在Maven项目的这个路径下：\n├── src\n│   ├── main\n│   │   ├── java\n│   │   │   ├── *.java  #这里是将要写的代码\n│   │   └── resources\n│   │       └── crypto-config   #直接拷贝整个文件夹到这里</code></pre><h3 id=\"2-3-调用SDK\"><a href=\"#2-3-调用SDK\" class=\"headerlink\" title=\"2.3 调用SDK\"></a>2.3 调用SDK</h3><h4 id=\"2-3-1-创建Hyperledger-Fabric-客户端实例\"><a href=\"#2-3-1-创建Hyperledger-Fabric-客户端实例\" class=\"headerlink\" title=\"2.3.1 创建Hyperledger Fabric 客户端实例\"></a>2.3.1 创建Hyperledger Fabric 客户端实例</h4><p>部分代码如下：</p>\n<pre><code>//*****************************************************\n//*********Hyperledger Fabric客户端初始化配置************\n//*****************************************************\n//创建默认的加密套件\nCryptoSuite suite = CryptoSuite.Factory.getCryptoSuite();\n//Hyperledger Fabric 客户端\nHFClient hfClient = HFClient.createNewInstance();\nhfClient.setCryptoSuite(suite);</code></pre><p>然后是指定的用于调用链码的用户,我们需要实现官方提供的<code>User</code>接口才能创建用户：</p>\n<pre><code># 部分文件内容\npublic class FabUser implements User {\n    private String name;\n    private String account;\n    private String affiliation;\n    private String mspId;\n    private Set&lt;String&gt; roles;\n    private Enrollment enrollment;\n\n    @Override\n    public String getName() {\n        return this.name;\n    }\n\n    @Override\n    public Set&lt;String&gt; getRoles() {\n        return this.roles;\n    }\n\n    @Override\n    public String getAccount() {\n        return this.account;\n    }\n\n    @Override\n    public String getAffiliation() {\n        return this.affiliation;\n    }\n\n    @Override\n    public Enrollment getEnrollment() {\n        return this.enrollment;\n    }\n\n    @Override\n    public String getMspId() {\n        return this.mspId;\n    }\n        /**\n     * 创建用户实例\n     * @param name\n     * @param mspId\n     * @param keyFile  当前用户秘钥文件路径\n     * @param certFile 当前用户证书文件路径\n     */\n    FabUser(String name, String mspId, String keyFile, String certFile) {\n        if ((this.enrollment = loadKeyAndCert(keyFile, certFile)) != null) {\n            this.name = name;\n            this.mspId = mspId;\n        }\n    }\n        /**\n     * 从文件系统中加载秘钥与证书\n     * @param keyFile  #用户的秘钥文件路径\n     * @param certFile #用户的证书文件路径\n     * @return\n     */\n    private Enrollment loadKeyAndCert(String keyFile, String certFile) {\n        byte[] keyPem;\n        try {\n            keyPem = Files.readAllBytes(Paths.get(Const.BASE_PATH + keyFile));\n            byte[] certPem = Files.readAllBytes(Paths.get(Const.BASE_PATH + certFile));\n            CryptoPrimitives primitives = new CryptoPrimitives();\n            PrivateKey privateKey = primitives.bytesToPrivateKey(keyPem);\n            return new X509Enrollment(privateKey, new String(certPem));\n        } catch (IOException | IllegalAccessException | InstantiationException | CryptoException | ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n        return null;\n    }\n}</code></pre><ul>\n<li>创建用户：</li>\n</ul>\n<pre><code>\nFabUser fabUser = new FabUser(&quot;admin&quot;, Const.USER_MSP_ID, Const.USER_KEY_FILE, Const.USER_CERT_FILE);\nhfClient.setUserContext(fabUser);</code></pre><ul>\n<li>创建一个通道实例，与Fabric网络中的通道是对应的：</li>\n</ul>\n<pre><code>//创建通道实例\nChannel channel = hfClient.newChannel(Const.CHANNEL_NAME);</code></pre><ul>\n<li>创建<code>peer</code>节点和<code>orderer</code>节点实例</li>\n</ul>\n<pre><code>//*****************************************************\n//******************配置Peer节点*********************\n//*****************************************************\n\n/**\n* 添加peer节点信息，客户端可以向该peer节点发送查询与调用链码的请求\n* 需配置peer节点域名，peer节点主机地址+端口号(主机地址需要与Fabric网络中peer节点对应)\n* 如果开启TLS的话需配置TLS根证书\n*/\nPeer peer = hfClient.newPeer(\n    Const.PEER0_ORG1_DOMAIN_NAME, Const.PEER0_ORG1_HOST,\n    loadTLSFile(Const.PEER0_ORG1_TLS_DIR, Const.PEER0_ORG1_DOMAIN_NAME));\nPeer peer1 = hfClient.newPeer(\n    Const.PEER0_ORG2_DOMAIN_NAME, Const.PEER0_ORG2_HOST,\n    loadTLSFile(Const.PEER0_ORG2_TLS_DIR, Const.PEER0_ORG2_DOMAIN_NAME));\nchannel.addPeer(peer1);\nchannel.addPeer(peer);\n\n\n//*****************************************************\n//******************配置Orderer节点*********************\n//*****************************************************\n\n/**\n* 添加orderer节点信息，客户端接受到peer节点的背书响应后发送到该orderer节点\n* 需配置orderer节点域名，orderer节点主机地址+端口号(主机地址需要与Fabric网络中orderer节点对应)\n* 如果开启TLS的话需配置TLS根证书\n*/\nOrderer orderer = hfClient.newOrderer(\n    Const.ORDERER_DOMAIN_NAME, Const.ORDERER_HOST,\n    loadTLSFile(Const.ORDERER_TLS_DIR, Const.ORDERER_DOMAIN_NAME));\nchannel.addOrderer(orderer);\n//通道初始化\nchannel.initialize();\n//创建与Fabric中链码对应的实例  这里使用的是Fabric官方的示例链码\nChaincodeID chaincodeID = ChaincodeID.newBuilder().setName(Const.CHAINCODE_NAME).build();</code></pre><h4 id=\"2-3-2-TLS证书的加载\"><a href=\"#2-3-2-TLS证书的加载\" class=\"headerlink\" title=\"2.3.2 TLS证书的加载\"></a>2.3.2 TLS证书的加载</h4><p><strong>最重要的还是这个用于加载<code>TLS</code>证书的方法，也是本文重点,主要就是一下的几点属性，其中<code>hostName</code>必须与节点的域名对应，比如为节点<code>peer0.org1.example.com</code>加载<code>TLS</code>证书，那么<code>hostName</code>必须是<code>peer0.org1.example.com</code>，另外需要说明的是<code>TLS</code>证书是为<code>peer</code>与<code>orderer</code>节点加载的，不是为调用链码的客户端加载的，除非在<code>fabric</code>环境中开启对客户端<code>TLS</code>证书的验证。</strong></p>\n<pre><code>/**\n * 为Fabric网络中节点配置TLS根证书\n *\n * @param rootTLSCert 根证书路径\n * @param hostName    节点域名\n * @return\n * @throws IOException\n */\nprivate static Properties loadTLSFile(String rootTLSCert, String hostName) throws IOException {\n    Properties properties = new Properties();\n    # 其实只需要一个TLS根证书就可以了，比如TLS相关的秘钥等都是可选的\n    properties.put(&quot;pemBytes&quot;, Files.readAllBytes(Paths.get(Const.BASE_PATH + rootTLSCert)));\n    properties.setProperty(&quot;sslProvider&quot;, &quot;openSSL&quot;);\n    properties.setProperty(&quot;negotiationType&quot;, &quot;TLS&quot;);\n    properties.setProperty(&quot;trustServerCertificate&quot;, &quot;true&quot;);\n    properties.setProperty(&quot;hostnameOverride&quot;, hostName);\n    return properties;\n}</code></pre><h4 id=\"2-3-3-链码查询功能\"><a href=\"#2-3-3-链码查询功能\" class=\"headerlink\" title=\"2.3.3 链码查询功能\"></a>2.3.3 链码查询功能</h4><p>链码主要API主要是查询和调用，这两个的区别是调用查询不用请求<code>Orderer</code>，并且不生成新的交易以及区块，而调用功能则需要请求<code>peer</code>以及<code>orderer</code>节点，满足背书策略的话会生成新的交易和新的区块。<br>与查询相关的代码:</p>\n<pre><code>/**\n * @param hfClient    Hyperledger Fabric 客户端实例\n * @param channel     通道实例\n * @param chaincodeID 链码ID\n * @param func        查询功能名称\n * @param args        查询功能需要的参数\n * @throws ProposalException\n * @throws InvalidArgumentException\n */\nprivate static void query(HFClient hfClient, Channel channel, ChaincodeID chaincodeID, String func, String[] args) throws ProposalException, InvalidArgumentException {\n    QueryByChaincodeRequest req = hfClient.newQueryProposalRequest();\n    req.setChaincodeID(chaincodeID);\n    req.setFcn(func);\n    req.setArgs(args);\n    // 向peer节点发送调用链码的提案并等待返回查询响应集合\n    Collection&lt;ProposalResponse&gt; queryResponse = channel.queryByChaincode(req);\n    for (ProposalResponse pres : queryResponse) {\n        System.out.println(pres.getProposalResponse().getResponse().getPayload().toStringUtf8());\n    }\n}</code></pre><p>只需要通过几行代码即可使用:</p>\n<pre><code>//*****************************************************\n//******************查询链码功能*************************\n//*****************************************************\n\nString queryFunc = &quot;query&quot;;\nString[] queryArgs = {&quot;a&quot;};\nquery(hfClient, channel, chaincodeID, queryFunc, queryArgs);</code></pre><h4 id=\"2-3-4-链码调用功能\"><a href=\"#2-3-4-链码调用功能\" class=\"headerlink\" title=\"2.3.4 链码调用功能\"></a>2.3.4 链码调用功能</h4><p>调用链码的流程是先创建提案请求发送到<code>peer</code>节点，由<code>peer</code>节点返回提案背书响应，然后由客户端将背书响应发送给<code>orderer</code>节点，最后返回链码事件，而之前的提案背书响应不可以作为调用链码的结果，因为那一步还没有经过验证，也没有区块的生成。只有从最后返回的链码事件中确认交易是有效的才可以确认调用链码是成功的。<br>链码调用相关代码:</p>\n<pre><code>/**\n * @param hfClient    Hyperledger Fabric 客户端实例\n * @param channel     通道实例\n * @param chaincodeID 链码ID\n * @param func        查询功能名称\n * @param args        查询功能需要的参数\n * @throws InvalidArgumentException\n * @throws ProposalException\n * @throws ExecutionException\n * @throws InterruptedException\n */\nprivate static void invoke(HFClient hfClient, Channel channel, ChaincodeID chaincodeID, String func, String[] args) throws InvalidArgumentException, ProposalException, ExecutionException, InterruptedException {\n    //提交链码交易\n    TransactionProposalRequest req2 = hfClient.newTransactionProposalRequest();\n    req2.setChaincodeID(chaincodeID);\n    req2.setFcn(func);\n    req2.setArgs(args);\n    //配置提案等待时间\n    req2.setProposalWaitTime(3000);\n    // 向peer节点发送调用链码的提案并等待返回背书响应集合\n    Collection&lt;ProposalResponse&gt; rsp2 = channel.sendTransactionProposal(req2);\n    for (ProposalResponse pres : rsp2) {\n        System.out.println(pres.getProposalResponse().getResponse().getPayload().toStringUtf8());\n    }\n    //将背书响应集合发送到Orderer节点\n    BlockEvent.TransactionEvent event = channel.sendTransaction(rsp2).get();\n    System.out.println(&quot;区块是否有效：&quot; + event.isValid());\n}</code></pre><p>通过简单几行代码即可使用:</p>\n<pre><code>String invokeFunc = &quot;invoke&quot;;\nString[] invokeArgs = {&quot;a&quot;, &quot;b&quot;, &quot;10&quot;};\ninvoke(hfClient, channel, chaincodeID, invokeFunc, invokeArgs);</code></pre><p><strong>再贴一遍完整的代码获取地址</strong>：—&gt;&gt;<a href=\"https://github.com/newonexd/fabric-sdk-demo\" target=\"_blank\" rel=\"noopener\">点这里</a></p>\n<h2 id=\"3-总结\"><a href=\"#3-总结\" class=\"headerlink\" title=\"3 总结\"></a>3 总结</h2><p>从搭建环境到成功通过SDK调用链码的过程是漫长的，但是一路走过来确实会学习到好多东西。希望对大家有所帮助.</p>\n"},{"title":"Hyperledger Fabric手动生成CA证书搭建Fabric网络","date":"2019-12-08T09:14:38.000Z","_content":"之前介绍了使用官方脚本自动化启动一个Fabric网络，并且所有的证书都是通过官方的命令行工具`cryptogen`直接生成网络中的所有节点的证书。在开发环境可以这么简单进行，但是生成环境下还是需要我们自定义对网络中的节点的证书进行配置。\n所以在本文中，将会详细介绍一个从手动生成证书一直到启动网络的整体步骤。本文只交代整体的搭建步骤。对于Fabric-Ca的讲解不在本文的范围内，将在另一篇文章中说明。\n正篇文章也是根据官方的文档进行的。但是由于官方的文档尚未完工，也是好多没有交代清楚的，并且有些地方是错误的，所以笔者也是一步一步摸索出来的，所以如果本文哪里没有交代清楚或者错误的地方，希望各位批评指正。\n在这里贴出[官方文档](https://hyperledger-fabric-ca.readthedocs.io/en/latest/operations_guide.html)地址.\n## 1.整体架构\n\n* * *\n\n架构图直接贴过来好了：\n![系统架构](/img/blog/arth.png)\n\n\n官方文档采用的是多机环境，这里简洁化一点，所有的操作都在**一台机器**上进行，至于多机环境，以后再补充好了。\n介绍一下本文所采用的整体架构：\n\n1. 三个组织\n    1. Org0  -> 组织0   \n    2. Org1  -> 组织1   \n    3. Org2  -> 组织2   \n2. 组织中的成员\n    1. Org0   一个Orderer节点，一个Org0的Admin节点\n    2. Org1   两个Peer节点，一个Org1的Admin节点，一个Org1的User节点\n    3. Org2   两个Peer节点，一个Org2的Admin节点，一个Org2的User节点\n3. 共有四台CA服务器\n    1. TLS服务器   ->  为网络中所有节点颁发TLS证书，用于通信的加密\n    2. Org1的CA服务器 -> 为组织1中所有用户颁发证书\n    3. Org2的Ca服务器 -> 为组织2中所有用户颁发证书\n    4. Org0的CA服务器 -> 为组织0中所有用户颁发证书\n\n这里的四台CA服务器都是根服务器。**彼此之间都是独立的存在，没有任何关系。**，也就是说每一个CA服务器生成的证书在其他CA服务器都是不能用的。\n介绍完之后，可以进入正题了。\n### 1.1Fabric，Fabric-Ca安装\n本文默认读者都是对Fabric有一定的了解的，所以一些安装过程这里就没有重复说明。\n第一步是安装Fabric-Ca环境，可以参考[这里](https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric_CA/),这篇文章还没有写完，以后慢慢补，不过环境的安装已经有说明。\n还有就是Fabric的环境安装，可以参考[这里](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)。\n\n完成环境搭建后，我们还需要一个`HOME`文件夹，用于存放我们生成的证书文件与`fabric`配置相关的文件。\n本文设置`HOME`文件夹路径为:\n```\n$GOPATH/src/github.com/caDemo/\n```\n请读者自行创建，一般不要用太复杂的路径,也不要用中文路径，会为之后的操作带来很多麻烦。在下文中简单称`HOME`文件夹为**工作目录**,**除非特殊说明，一般命令的执行都是在工作目录进行**。\n## 2 CA服务器配置\n\n* * *\n\n### 2.1启动TLS CA服务器\n前期工作准备好之后，我们开始启动第一台CA服务器。本文中使用`Docker`容器启动。\n首先在工作目录创建`docker-compose.yaml`文件：\n```\ntouch docker-compose.yaml\n```\n并在文件内添加以下内容(tips:内容格式不要乱掉)：\n```\nversion: '2'\n\nnetworks:\n  fabric-ca:\n  \nservices:\n  \n  ca-tls:\n    container_name: ca-tls\n    image: hyperledger/fabric-ca\n    command: sh -c 'fabric-ca-server start -d -b tls-ca-admin:tls-ca-adminpw --port 7052'\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/tls\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=ca-tls\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca  ##重要！！！记得修改这里的路径为自己的工作目录\n    networks:\n      - fabric-ca\n    ports:\n      - 7052:7052\n      \n```\n启动该`docker`容器：\n```\ndocker-compose -f docker-compose.yaml up ca-tls\n```\n如果命令行出现以下内容则说明启动成功：\n```\n[INFO] Listening on https://0.0.0.0:7052\n```\n同时工作目录下会出现一个`tls`的文件夹。文件夹中的内容暂先不解释，留着在另一篇文章中说明。不过有一个文件需要解释一下，因为之后会用到。\n在`$GOPATH/src/github.com/caDemo/tls/`路径下的`ca-cert.pem`文件。这是`TLS CA`服务器签名的根证书，目的是用来对`CA`的`TLS`证书进行验证，同时也需要持有这个证书才可以进行证书的颁发。在多机环境下，我们需要将它复制到每一台机器上，不过本文采用的是单机环境，所以省略掉了这一步。\n\n### 2.2 TLS CA服务器注册用户\n第一步是在TLS CA服务器中注册用户，经过注册的用户才拥有TLS证书，本文中由于只在各节点之间进行TLS加密通信，所以只将`orderer`和`peer`节点的身份注册到TLS服务器。\n打开一个新的终端输入以下命令：\n```\n#设置环境变量指定根证书的路径(如果工作目录不同的话记得指定自己的工作目录,以下不再重复说明)\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#设置环境变量指定CA客户端的HOME文件夹\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/tls/admin\n#登录管理员用户用于之后的节点身份注册\nfabric-ca-client enroll -d -u https://tls-ca-admin:tls-ca-adminpw@0.0.0.0:7052\n```\n登录成功在工作目录下的`tls`文件夹下将出现一个`admin`文件夹，这里面是`admin`的相关证书文件.\n并且只有登录了`admin`，才具有权限进行用户的注册,因为该用户具有CA的全部权限，相当于CA服务器的`root`用户。\n接下来对各个节点进行注册。\n```\nfabric-ca-client register -d --id.name peer1-org1 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer2-org1 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer1-org2 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer2-org2 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name orderer-org0 --id.secret ordererPW --id.type orderer -u https://0.0.0.0:7052\n```\n这里将三个组织中的节点都进行了注册。\n\n* 不过`-d`这个参数并没有找到相关资料 \n* `id.name`是指定用户的名称\n* `--id.secert`是指定密码\n* `--id.type`是指定用户类型，用户类型默认为`client`,主要包括`peer`,`app`,`user`,`orderer`.\n* `-u`则是指定请求CA服务器的URL。\n\n这里我们为各个节点注册TLS证书，之后Fabric网络的通信则需要通过这一步骤注册过的用户的TLS证书来进行TLS加密通信。\n到这里我们只是注册了各个节点的身份，还没有获取到他们的证书。证书可以通过登录获取，不过暂时不着急获取他们的TLS证书。\n接下来，我们对其他几个CA服务器进行配置。\n\n### 2.3配置Org0的CA服务器\n\n再强调一下，本文中的几个CA服务器都是根服务器，彼此之间没有任何关系，所以上一步骤的TLS CA服务器在这一部分并没有用到。\n同样，本文使用Docker容器启动CA服务器。配置文件如下，只需要添加进之前的`docker-compose.yaml`文件中就好：\n```\n  org0:\n    container_name: org0\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c 'fabric-ca-server start -d -b org0-admin:org0-adminpw --port 7053'\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org0/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org0\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7053:7053\n      \n```\n添加完之后启动它：\n```\ndocker-compose -f docker-compose.yaml up org0\n```\n打开另一个终端，接下来注册org0的用户：\n```\n#首先指定环境变量，这里的TLS证书不是之前的TLS CA服务器的根证书，而是本组织CA服务器启动时生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\n#指定本组织的CA客户端工作目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/admin\n```\n登录`org0`的CA服务器管理员身份用于注册本组织的用户：\n```\nfabric-ca-client enroll -d -u https://org0-admin:org0-adminpw@0.0.0.0:7053\n```\n在本组织中共有两个用户：`orderer`节点和`admin`用户(这里的admin和管理员是不同的。)\n将他们注册到org0的CA服务器：\n```\nfabric-ca-client register -d --id.name orderer-org0 --id.secret ordererpw --id.type orderer -u https://0.0.0.0:7053\nfabric-ca-client register -d --id.name admin-org0 --id.secret org0adminpw --id.type admin --id.attrs \"hf.Registrar.Roles=client,hf.Registrar.Attributes=*,hf.Revoker=true,hf.GenCRL=true,admin=true:ecert,abac.init=true:ecert\" -u https://0.0.0.0:7053\n```\n命令执行完之后，将会注册一个Orderer节点的身份和一个Admin的身份。同时在工作目录下的`org0`子文件夹中会有两个文件夹：`crypto`和`admin`。`crypto`中是CA服务器的配置信息，`admin`是服务器管理员的身份信息。\n\n### 2.4配置Org1的CA服务器\n同样的步骤，对org1组织的CA服务器进行配置：\n```\n  org1:\n    container_name: org1\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c 'fabric-ca-server start -d -b org1-admin:org1-adminpw'\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org1/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org1\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7054:7054\n      \n```\n启动服务器：\n```\ndocker-compose -f docker-compose.yaml up org1\n```\n打开新的终端，配置环境变量：\n```\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/admin\n```\n登录CA服务器管理员身份：\n```\nfabric-ca-client enroll -d -u https://org1-admin:org1-adminpw@0.0.0.0:7054\n```\n组织一种共有四个用户：`peer1`,`peer2`,`admin`,`user`,分别注册他们：\n```\nfabric-ca-client register -d --id.name peer1-org1 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name peer2-org1 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name admin-org1 --id.secret org1AdminPW --id.type user -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name user-org1 --id.secret org1UserPW --id.type user -u https://0.0.0.0:7054\n```\n### 2.5配置Org2的CA服务器\n和上一部分相同，这里只列举需要的命令：\nCA服务器配置文件：\n```\n  org2:\n    container_name: org2\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c 'fabric-ca-server start -d -b org2-admin:org2-adminpw --port 7055'\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org2/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org2\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7055:7055\n      \n```\n启动服务器：\n```\ndocker-compose -f docker-compose.yaml up org2\n```\n打开新的终端，配置环境变量：\n```\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/admin\n```\n登录CA服务器管理员身份：\n```\nfabric-ca-client enroll -d -u https://org2-admin:org2-adminpw@0.0.0.0:7055\n```\n组织一种共有四个用户：`peer1`,`peer2`,`admin`,`user`,分别注册他们：\n```\nfabric-ca-client register -d --id.name peer1-org2 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name peer2-org2 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name admin-org2 --id.secret org1AdminPW --id.type user -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name user-org2 --id.secret org1UserPW --id.type user -u https://0.0.0.0:7055\n```\n## 3.生成证书并配置TLS\n\n* * *\n\n到目前为止，所有的用户我们都注册完毕，接下来就是为每一个用户生成证书并配置TLS证书。\n其中证书分为两部分，分别是本组织的MSP证书，以及组织之间进行加密通信的TLS证书。\n所以本文需要对两部分证书进行分别生成与配置。\n从组织一开始：\n### 3.1 组织一节点配置\n#### 3.1.1 peer1\n首先是本组织的`MSP`证书：\n* 配置环境变量\n```\n#指定peer1节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/peer1\n#指定**本**组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\n```\n* 登录`peer1`节点到`org1 CA `服务器上：\n```\nfabric-ca-client enroll -d -u https://peer1-org1:peer1PW@0.0.0.0:7054\n```\n这一步完成后，在`$GOPATH/src/github.com/caDemo/org1/peer1`下会出现一个`msp`文件夹，这是`peer1`节点的`MSP`证书。\n接下来是`TLS`证书：\n* 配置环境变量\n```\n#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#指定TLS证书的HOME目录\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/peer1/tls-msp\n```\n* 登录`peer1`节点到`TLS CA`服务器上：\n```\nfabric-ca-client enroll -d -u https://peer1-org1:peer1PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer1-org1\n```\n这一步完成后，在`$GOPATH/src/github.com/caDemo/org1/peer1`下会出现一个`tls-msp`文件夹，这是`peer1`节点的`TLS`证书。\n* 修改秘钥文件名\n为什么要修改呢，进入这个文件夹看一下就知道了,由服务器生成的秘钥文件名是一长串无规则的字符串，后期我们使用的时候难道要一个字符一个字符地输入？\n```\ncd $GOPATH/src/github.com/caDemo/org1/peer1/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo\n```\n#### 3.1.2 peer2\n`peer2`节点和上面步骤相同：\n这里就直接放需要的命令了：\n* 生成`MSP`证书\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/peer2\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org1:peer2PW@0.0.0.0:7054\n```\n* 生成`TLS`证书\n```\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/peer2/tls-msp\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org1:peer2PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer2-org1\ncd $GOPATH/src/github.com/caDemo/org1/peer2/tls-msp/keystore/\nmv *_sk key.pem\n```\n#### 3.1.3 admin\n接下来是`admin`用户，这个用户有什么作用呢，实际上，安装和实例化链码都需要`admin`的证书，所以才需要注册一个`admin`用户，还要它的证书。\n* 配置环境变量\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\n#这里多了一个环境变量，是指定admin用户的msp证书文件夹的\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/adminuser/msp\n```\n* 登录`admin`用户获取`MSP`证书:\n```\nfabric-ca-client enroll -d -u https://admin-org1:org1AdminPW@0.0.0.0:7054\n```\n因为我们生成这个用户的证书主要就是为了之后链码的安装和实例化，所以配不配置他的`TLS`证书也无关紧要了(关键是我们之前也没有将这个用户注册到`tls`服务器中)\n* 复制证书到`admincerts`文件夹:\n去看Fabric官方的例子，每一个`peer`节点的`MSP`文件夹下都有`admincerts`这个子文件夹的，而且是需要我们手动创建的。\n```\nmkdir -p $GOPATH/src/github.com/caDemo/org1/peer1/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org1/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org1/peer1/msp/admincerts/org1-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/\n```\n#### 3.1.4 启动peer节点\n到这里，已经配置好了一个节点，所以我们就可以启动这个节点了，当然在之后和`orderer`节点一起启动也可以，不过忙活了这么多，还是应该提前看到一下所做的工作的成果的！\n附上`peer1`节点的容器配置信息：\n\n```\n  peer1-org1:\n    container_name: peer1-org1\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer1-org1\n      - CORE_PEER_ADDRESS=peer1-org1:7051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer1/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org1:7051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1/peer1\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org1/peer1:/tmp/hyperledger/org1/peer1\n    networks:\n      - fabric-ca\n      \n```\n启动它！！\n\n```\ndocker-compose -f docker-compose.yaml up peer1-org1\n```\n如果没有报错的话，说明之前配置的没有什么问题，如果出错的话，则需要返回去检查一下了。。。\n`peer2`节点的容器配置信息：\n\n```\n  peer2-org1:\n    container_name: peer2-org1\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer2-org1\n      - CORE_PEER_ADDRESS=peer2-org1:8051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer2/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer2/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer2/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer2/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org1:7051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1/peer2\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org1/peer2:/tmp/hyperledger/org1/peer2\n    networks:\n      - fabric-ca\n      \n```\n启动它！！\n```\ndocker-compose -f docker-compose.yaml up peer2-org1\n```\n### 3.2 组织二节点配置\n和之前一样的步骤，所以没什么好解释的了：\n#### 3.2.1 peer1\n* 配置环境变量\n```\n#指定peer2节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/peer1\n#指定本组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\n```\n* 登录`peer1`节点到`org2 CA `服务器上：\n```\nfabric-ca-client enroll -d -u https://peer1-org2:peer1PW@0.0.0.0:7055\n```\n接下来是`TLS`证书：\n* 配置环境变量\n```\n#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#指定TLS证书的HOME目录\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/peer1/tls-msp\n```\n* 登录`peer1`节点到`TLS CA`服务器上：\n```\nfabric-ca-client enroll -d -u https://peer1-org2:peer1PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer1-org2\n```\n* 修改秘钥文件名\n```\ncd $GOPATH/src/github.com/caDemo/org2/peer1/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo\n```\n#### 3.2.2 peer2\n* 生成`MSP`证书\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/peer2\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org2:peer2PW@0.0.0.0:7055\n```\n* 生成`TLS`证书\n```\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/peer2/tls-msp\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org2:peer2PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer2-org2\ncd $GOPATH/src/github.com/caDemo/org2/peer2/tls-msp/keystore/\nmv *_sk key.pem\n```\n#### 3.2.3 admin\n* 配置环境变量\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/adminuser/msp\n```\n* 登录`admin`用户获取`MSP`证书:\n```\nfabric-ca-client enroll -d -u https://admin-org2:org2AdminPW@0.0.0.0:7055\n```\n* 复制证书到`admincerts`文件夹:\n去看Fabric官方的例子，每一个`peer`节点的`MSP`文件夹下都有`admincerts`这个子文件夹的，而且是需要我们手动创建的。\n```\nmkdir -p $GOPATH/src/github.com/caDemo/org2/peer1/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org2/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org2/peer1/msp/admincerts/org2-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/\n```\n#### 3.2.4 启动peer节点\n附上`peer1`节点的容器配置信息：\n\n```\n  peer1-org2:\n    container_name: peer1-org2\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer1-org2\n      - CORE_PEER_ADDRESS=peer1-org2:9051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer1/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org2:9051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2/peer1\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org2/peer1:/tmp/hyperledger/org2/peer1\n    networks:\n      - fabric-ca\n      \n```\n启动它.\n```\ndocker-compose -f docker-compose.yaml up peer1-org2\n```\n`peer2`节点的容器配置信息：\n```\n  peer2-org2:\n    container_name: peer2-org2\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer2-org2\n      - CORE_PEER_ADDRESS=peer2-org2:10051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer2/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer2/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer2/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer2/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org2:9051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2/peer2\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org2/peer2:/tmp/hyperledger/org2/peer2\n    networks:\n      - fabric-ca\n      \n```\n启动它.\n```\ndocker-compose -f docker-compose.yaml up peer2-org2\n```\n### 3.3 排序节点配置\n接下来是排序节点的配置，为什么放在最后面呢，因为排序节点的启动需要提前生成创世区块，而创世区块的生成涉及到另一个配置文件，所以就先配置简单的`peer`节点。\n#### 3.3.1 orderer\n* 配置环境变量\n```\n#指定order节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/orderer\n#指定本组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\n```\n* 登录`order`节点到`org0 CA `服务器上：\n```\nfabric-ca-client enroll -d -u https://orderer-org0:ordererpw@0.0.0.0:7053\n```\n接下来是`TLS`证书：\n*  配置环境变量\n```\n#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org0/orderer/tls-msp\n#指定TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n```\n* 登录`orderer`节点到`TLS CA`服务器上：\n```\nfabric-ca-client enroll -d -u https://orderer-org0:ordererPW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts orderer-org0\n```\n* 修改秘钥文件名\n```\ncd $GOPATH/src/github.com/caDemo/org0/orderer/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo\n```\n#### 3.3.2 admin\n* 配置环境变量\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org0/adminuser/msp\n```\n* 登录`admin`用户获取`MSP`证书:\n```\nfabric-ca-client enroll -d -u https://admin-org0:org0adminpw@0.0.0.0:7053\n```\n* 复制证书到`admincerts`文件夹:\n```\nmkdir $GOPATH/src/github.com/caDemo/org0/orderer/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org0/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org0/orderer/msp/admincerts/orderer-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/\n```\n## 4.Fabric网络配置\n\n* * *\n\n接下来到重头戏了，证书都生成好了，即将要启动网络了。不过在启动网络之前还是有很多准备工作需要做。其实到这里，官方文档已经好多没有交代清楚的了，所以一下好多内容都是笔者自己摸索出来的，如有错误欢迎批评指正。\n### 4.1 configtx.yaml文件配置\n在下一个步骤的生成创世区块和通道配置信息需要一个文件：`configtx.yaml`文件。笔者根据官方的例子按照本文内容修改了一下，直接放在工作目录:\n```\nOrganizations:\n  - &orderer-org0\n    Name: orderer-org0\n    ID: org0MSP\n    MSPDir: ./org0/msp\n  #    Policies:\n  #      Readers:\n  #        Type: Signature\n  #        Rule: \"OR('orderer-org0MSP.member')\"\n  #      Writers:\n  #        Type: Signature\n  #        Rule: \"OR('orderer-org0MSP.member')\"\n  #      Admins:\n  #        Type: Signature\n  #        Rule: \"OR('orderer-org0MSP.admin')\"\n  \n  - &org1\n    Name: org1MSP\n    ID: org1MSP\n    \n    MSPDir: ./org1/msp\n    #    Policies:\n    #      Readers:\n    #        Type: Signature\n    #        Rule: \"OR('org1MSP.admin', 'org1MSP.peer', 'org1MSP.client')\"\n    #      Writers:\n    #        Type: Signature\n    #        Rule: \"OR('org1MSP.admin', 'org1MSP.client')\"\n    #      Admins:\n    #        Type: Signature\n    #        Rule: \"OR('org1MSP.admin')\"\n    AnchorPeers:\n      - Host: peer1-org1\n        Port: 7051\n  \n  - &org2\n    Name: org2MSP\n    ID: org2MSP\n    MSPDir: ./org2/msp\n    #    Policies:\n    #      Readers:\n    #        Type: Signature\n    #        Rule: \"OR('org2MSP.admin', 'org2MSP.peer', 'org2MSP.client')\"\n    #      Writers:\n    #        Type: Signature\n    #        Rule: \"OR('org2MSP.admin', 'org2MSP.client')\"\n    #      Admins:\n    #        Type: Signature\n    #        Rule: \"OR('org2MSP.admin')\"\n    \n    AnchorPeers:\n      - Host: peer1-org2\n        Port: 9051\n\nCapabilities:\n  Channel: &ChannelCapabilities\n    V1_4_3: true\n    V1_3: false\n    V1_1: false\n  Orderer: &OrdererCapabilities\n    V1_4_2: true\n    V1_1: false\n  Application: &ApplicationCapabilities\n    V1_4_2: true\n    V1_3: false\n    V1_2: false\n    V1_1: false\n\nApplication: &ApplicationDefaults\n  Organizations:\n  #  Policies:\n  #    Readers:\n  #      Type: ImplicitMeta\n  #      Rule: \"ANY Readers\"\n  #    Writers:\n  #      Type: ImplicitMeta\n  #      Rule: \"ANY Writers\"\n  #    Admins:\n  #      Type: ImplicitMeta\n  #      Rule: \"MAJORITY Admins\"\n  \n  Capabilities:\n    <<: *ApplicationCapabilities\n    \nOrderer: &OrdererDefaults\n  OrdererType: solo\n  \n  Addresses:\n    - orderer-org0:7050\n  BatchTimeout: 2s\n  BatchSize:\n    MaxMessageCount: 10\n    AbsoluteMaxBytes: 99 MB\n    PreferredMaxBytes: 512 KB\n  Organizations:\n#  Policies:\n#    Readers:\n#      Type: ImplicitMeta\n#      Rule: \"ANY Readers\"\n#    Writers:\n#      Type: ImplicitMeta\n#      Rule: \"ANY Writers\"\n#    Admins:\n#      Type: ImplicitMeta\n#      Rule: \"MAJORITY Admins\"\n#    # BlockValidation specifies what signatures must be included in the block\n#    # from the orderer for the peer to validate it.\n#    BlockValidation:\n#      Type: ImplicitMeta\n#      Rule: \"ANY Writers\"\n\nChannel: &ChannelDefaults\n  #  Policies:\n  #    # Who may invoke the 'Deliver' API\n  #    Readers:\n  #      Type: ImplicitMeta\n  #      Rule: \"ANY Readers\"\n  #    # Who may invoke the 'Broadcast' API\n  #    Writers:\n  #      Type: ImplicitMeta\n  #      Rule: \"ANY Writers\"\n  #    # By default, who may modify elements at this config level\n  #    Admins:\n  #      Type: ImplicitMeta\n  #      Rule: \"MAJORITY Admins\"\n  Capabilities:\n    <<: *ChannelCapabilities\n\nProfiles:\n  \n  TwoOrgsOrdererGenesis:\n    <<: *ChannelDefaults\n    Orderer:\n      <<: *OrdererDefaults\n      Organizations:\n        - *orderer-org0\n      Capabilities:\n        <<: *OrdererCapabilities\n    Consortiums:\n      SampleConsortium:\n        Organizations:\n          - *org1\n          - *org2\n  TwoOrgsChannel:\n    Consortium: SampleConsortium\n    <<: *ChannelDefaults\n    Application:\n      <<: *ApplicationDefaults\n      Organizations:\n        - *org1\n        - *org2\n      Capabilities:\n        <<: *ApplicationCapabilities\n```\n\n注释掉的部分是策略部分，笔者还没有完全搞懂，所以索性就先注释掉了，以后搞懂了再添加进去。\n还有一部分`msp`需要配置，就是`configtx.yaml`文件中第一部分指定的`MSPDir`,很简单，按照一下命令复制一下就好了：\n```\n#进入工作目录\ncd $GOPATH/src/github.com/caDemo\n############################################\n#org0\nmkdir org0/msp &&  cd org0/msp\nmkdir admincerts && mkdir cacerts && mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n############################################\n#org1\ncd $GOPATH/src/github.com/caDemo\nmkdir org1/msp/  && cd org1/msp/\nmkdir admincerts && mkdir cacerts && mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n############################################\n#org2\ncd $GOPATH/src/github.com/caDemo\nmkdir org1/msp/  && cd org1/msp/\nmkdir admincerts && mkdir cacerts && mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n```\n### 4.2 生成创世区块和通道配置信息\n可以了，所有的前期工作都已经完成，接下来就是手动启动网络了，第一步，生成创世区块和通道配置信息：\n```\ncd $GOPATH/src/github.com/caDemo\nexport FABRIC_CFG_PATH=$PWD\n#生成创世区块\nconfigtxgen -profile TwoOrgsOrdererGenesis -outputBlock $GOPATH/src/github.com/caDemo/genesis.block\n#生成通道配置信息\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx $GOPATH/src/github.com/caDemo/channel.tx -channelID mychannel\n```\n### 4.3 启动Orderer节点\n`orderer`容器配置文件：\n```\n  orderer-org0:\n    container_name: orderer-org0\n    image: hyperledger/fabric-orderer\n    environment:\n      - ORDERER_HOME=/tmp/hyperledger/orderer\n      - ORDERER_HOST=orderer-org0\n      - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0\n      - ORDERER_GENERAL_GENESISMETHOD=file\n      - ORDERER_GENERAL_GENESISFILE=/tmp/hyperledger/genesis.block\n      - ORDERER_GENERAL_LOCALMSPID=org0MSP\n      - ORDERER_GENERAL_LOCALMSPDIR=/tmp/hyperledger/org0/orderer/msp\n      - ORDERER_GENERAL_TLS_ENABLED=true\n      - ORDERER_GENERAL_TLS_CERTIFICATE=/tmp/hyperledger/org0/orderer/tls-msp/signcerts/cert.pem\n      - ORDERER_GENERAL_TLS_PRIVATEKEY=/tmp/hyperledger/org0/orderer/tls-msp/keystore/key.pem\n      - ORDERER_GENERAL_TLS_ROOTCAS=[/tmp/hyperledger/org0/orderer/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem]\n      - ORDERER_GENERAL_LOGLEVEL=debug\n      - ORDERER_DEBUG_BROADCASTTRACEDIR=data/logs\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org0/orderer:/tmp/hyperledger/org0/orderer/\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    \n```\n关键部分到了，只要这一步没有出现错误，整个网络就启动成功了。\n```\ndocker-compose -f docker-compose.yaml up orderer-org0\n```\n### 4.4 启动组织一的cli容器\n`cli`容器内容,我们需要这个容器对组织1进行链码的交互：\n```\n  cli-org1:\n    container_name: cli-org1\n    image: hyperledger/fabric-tools\n    tty: true\n    stdin_open: true\n    environment:\n      - SYS_CHANNEL=testchainid\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_ID=cli-org1\n      - CORE_PEER_ADDRESS=peer1-org1:7051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer1/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1\n    command: /bin/bash\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org1/peer1:/tmp/hyperledger/org1/peer1\n      - $GOPATH/src/github.com/caDemo/org1/peer1/assets/chaincode:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - $GOPATH/src/github.com/caDemo/org1/adminuser:/tmp/hyperledger/org1/adminuser\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    depends_on:\n      - peer1-org1\n```\n启动该容器：\n```\ndocker-compose -f docker-compose.yaml up cli-org1\n```\n### 4.5 启动组织二的cli容器\n`cli`容器内容,我们需要这个容器对组织2进行链码的交互：\n```\n  cli-org2:\n    container_name: cli-org2\n    image: hyperledger/fabric-tools\n    tty: true\n    stdin_open: true\n    environment:\n      - SYS_CHANNEL=testchainid\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_ID=cli-org2\n      - CORE_PEER_ADDRESS=peer1-org2:9051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer1/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2\n    command: /bin/bash\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org2/peer1:/tmp/hyperledger/org2/peer1\n      - $GOPATH/src/github.com/caDemo/org2/peer1/assets/chaincode:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - $GOPATH/src/github.com/caDemo/org2/adminuser:/tmp/hyperledger/org2/adminuser\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    depends_on:\n      - peer1-org2\n```\n启动该容器：\n```\ndocker-compose -f docker-compose.yaml up cli-org2\n```\n## 5.网络测试\n\n* * *\n\n所有工作准备完成，接下来让我们测试整个网络能不能正常运行吧：\n### 5.1 创建与加入通道\n以**组织1**为例：\n* 首先进入`cli`容器：\n```\ndocker exec -it cli bash\n#配置环境变量\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\n```\n* 创建通道\n```\npeer channel create -c mychannel -f /tmp/hyperledger/channel.tx -o orderer-org0:7050 --outputBlock /tmp/hyperledger/mychannel.block --tls --cafile /tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n```\n* 将`peer1-org1`加入通道：\n```\nexport CORE_PEER_ADDRESS=peer1-org1:7051\npeer channel join -b /tmp/hyperledger/mychannel.block\n```\n* 将`peer2-org1`加入通道：\n```\nexport CORE_PEER_ADDRESS=peer2-org1:8051\npeer channel join -b /tmp/hyperledger/mychannel.block\n```\n组织二步骤是相同的，唯一不同的就是不需要创建通道了，所以就不再说明了。\n### 5.2 安装和实例化链码\n以**组织1**为例：\n* 首先进入`cli`容器：\n```\ndocker exec -it cli bash\n#配置环境变量\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\nexport CORE_PEER_ADDRESS=peer1-org1:7051\n```\n* 安装链码\n**记得提前将链码放到**`$GOPATH/src/github.com/caDemo/org1/peer1/assets/chaincode`**路径下。**,本文使用的是`fabric-samples/chaincode/chaincode_example02`官方示例链码。\n```\npeer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric-samples/chaincode/chaincode_example02/go/\n```\n* 实例化链码\n```\npeer chaincode instantiate -C mychannel -n mycc -v 1.0 -c '{\"Args\":[\"init\",\"a\",\"100\",\"b\",\"200\"]}'  -o orderer-org0:7050 --tls --cafile /tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n```\n\n* 这一步在高版本的Fabric网络是会出错的，因为少了一个文件`config.yaml`:\n\n```\nNodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: orderer\n```\n因为高版本的Fabric把节点类型区分开了，所以需要我们手动配置。\n将该文件复制到`$GOPATH/src/github.com/caDemo/org1/adminuser/msp`文件夹内，同时修改上面指定的位置的文件名(与对应文件夹内的文件名对应就好了)。\n\n* 实例化部分出错的可能性是最高的，很多都是因为网络模式指定错误导致链码容器启动失败，解决方案：\n```\n#终端执行命令\ndocker network ls\n```\n找到以`fabric-ca`为后缀的一条如`cademo_fabric-ca`,修改之前的所有`peer`节点容器配置文件的环境变量：\n```\nCORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n```\n修改完成重启节点容器，再次执行以上的命令(需要重新配置环境变量，加入通道这两个操作)。\n终于，实例化成功了。\n### 5.3 调用和查询链码\n最后测试一下链码功能能不能正常使用了：\n* 还是组织一的`cli`容器：\n```\ndocker exec -it cli bash\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\nexport CORE_PEER_ADDRESS=peer1-org1:7051\n```\n* 执行查询功能：\n```\npeer chaincode query -C mychannel -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n```\n命令行应该打印出:\n```\n100\n```\n* 执行调用功能：\n```\npeer chaincode invoke -C mychannel -n mycc -c '{\"Args\":[\"invoke\",\"a\",\"b\",\"10\"]}' --tls --cafile /tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n```\n* 再次查询：\n```\npeer chaincode query -C mychannel -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n```\n命令行应该打印出:\n```\n90\n```\n至于其他节点操作方法也是一样的，就不再操作了。\n到此为止，从零开始的手动生成证书一直到成功搭建Fabric网络全部步骤已经完成！！接下来还有更新锚节点等等就不再演示了，请各位读者自行操作。整个步骤是不容易的，而且BUG百出，不过成功搭建完成确实涨了不少知识。\n码字不易，还望各位看官支持一下：\n\n <img src=\"/img/blog/zfb.png\" width = \"300\" height = \"300\" alt=\"支付宝\" align=center />\n\n","source":"_posts/blog/fabric/Hyperledger_Fabric手动生成CA证书搭建Fabric网络.md","raw":"---\ntitle: Hyperledger Fabric手动生成CA证书搭建Fabric网络\ndate: 2019-12-08 17:14:38\ntags: \n- fabric-ca\n- fabric\ncategories:\n- fabric-ca应用\n---\n之前介绍了使用官方脚本自动化启动一个Fabric网络，并且所有的证书都是通过官方的命令行工具`cryptogen`直接生成网络中的所有节点的证书。在开发环境可以这么简单进行，但是生成环境下还是需要我们自定义对网络中的节点的证书进行配置。\n所以在本文中，将会详细介绍一个从手动生成证书一直到启动网络的整体步骤。本文只交代整体的搭建步骤。对于Fabric-Ca的讲解不在本文的范围内，将在另一篇文章中说明。\n正篇文章也是根据官方的文档进行的。但是由于官方的文档尚未完工，也是好多没有交代清楚的，并且有些地方是错误的，所以笔者也是一步一步摸索出来的，所以如果本文哪里没有交代清楚或者错误的地方，希望各位批评指正。\n在这里贴出[官方文档](https://hyperledger-fabric-ca.readthedocs.io/en/latest/operations_guide.html)地址.\n## 1.整体架构\n\n* * *\n\n架构图直接贴过来好了：\n![系统架构](/img/blog/arth.png)\n\n\n官方文档采用的是多机环境，这里简洁化一点，所有的操作都在**一台机器**上进行，至于多机环境，以后再补充好了。\n介绍一下本文所采用的整体架构：\n\n1. 三个组织\n    1. Org0  -> 组织0   \n    2. Org1  -> 组织1   \n    3. Org2  -> 组织2   \n2. 组织中的成员\n    1. Org0   一个Orderer节点，一个Org0的Admin节点\n    2. Org1   两个Peer节点，一个Org1的Admin节点，一个Org1的User节点\n    3. Org2   两个Peer节点，一个Org2的Admin节点，一个Org2的User节点\n3. 共有四台CA服务器\n    1. TLS服务器   ->  为网络中所有节点颁发TLS证书，用于通信的加密\n    2. Org1的CA服务器 -> 为组织1中所有用户颁发证书\n    3. Org2的Ca服务器 -> 为组织2中所有用户颁发证书\n    4. Org0的CA服务器 -> 为组织0中所有用户颁发证书\n\n这里的四台CA服务器都是根服务器。**彼此之间都是独立的存在，没有任何关系。**，也就是说每一个CA服务器生成的证书在其他CA服务器都是不能用的。\n介绍完之后，可以进入正题了。\n### 1.1Fabric，Fabric-Ca安装\n本文默认读者都是对Fabric有一定的了解的，所以一些安装过程这里就没有重复说明。\n第一步是安装Fabric-Ca环境，可以参考[这里](https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric_CA/),这篇文章还没有写完，以后慢慢补，不过环境的安装已经有说明。\n还有就是Fabric的环境安装，可以参考[这里](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)。\n\n完成环境搭建后，我们还需要一个`HOME`文件夹，用于存放我们生成的证书文件与`fabric`配置相关的文件。\n本文设置`HOME`文件夹路径为:\n```\n$GOPATH/src/github.com/caDemo/\n```\n请读者自行创建，一般不要用太复杂的路径,也不要用中文路径，会为之后的操作带来很多麻烦。在下文中简单称`HOME`文件夹为**工作目录**,**除非特殊说明，一般命令的执行都是在工作目录进行**。\n## 2 CA服务器配置\n\n* * *\n\n### 2.1启动TLS CA服务器\n前期工作准备好之后，我们开始启动第一台CA服务器。本文中使用`Docker`容器启动。\n首先在工作目录创建`docker-compose.yaml`文件：\n```\ntouch docker-compose.yaml\n```\n并在文件内添加以下内容(tips:内容格式不要乱掉)：\n```\nversion: '2'\n\nnetworks:\n  fabric-ca:\n  \nservices:\n  \n  ca-tls:\n    container_name: ca-tls\n    image: hyperledger/fabric-ca\n    command: sh -c 'fabric-ca-server start -d -b tls-ca-admin:tls-ca-adminpw --port 7052'\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/tls\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=ca-tls\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca  ##重要！！！记得修改这里的路径为自己的工作目录\n    networks:\n      - fabric-ca\n    ports:\n      - 7052:7052\n      \n```\n启动该`docker`容器：\n```\ndocker-compose -f docker-compose.yaml up ca-tls\n```\n如果命令行出现以下内容则说明启动成功：\n```\n[INFO] Listening on https://0.0.0.0:7052\n```\n同时工作目录下会出现一个`tls`的文件夹。文件夹中的内容暂先不解释，留着在另一篇文章中说明。不过有一个文件需要解释一下，因为之后会用到。\n在`$GOPATH/src/github.com/caDemo/tls/`路径下的`ca-cert.pem`文件。这是`TLS CA`服务器签名的根证书，目的是用来对`CA`的`TLS`证书进行验证，同时也需要持有这个证书才可以进行证书的颁发。在多机环境下，我们需要将它复制到每一台机器上，不过本文采用的是单机环境，所以省略掉了这一步。\n\n### 2.2 TLS CA服务器注册用户\n第一步是在TLS CA服务器中注册用户，经过注册的用户才拥有TLS证书，本文中由于只在各节点之间进行TLS加密通信，所以只将`orderer`和`peer`节点的身份注册到TLS服务器。\n打开一个新的终端输入以下命令：\n```\n#设置环境变量指定根证书的路径(如果工作目录不同的话记得指定自己的工作目录,以下不再重复说明)\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#设置环境变量指定CA客户端的HOME文件夹\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/tls/admin\n#登录管理员用户用于之后的节点身份注册\nfabric-ca-client enroll -d -u https://tls-ca-admin:tls-ca-adminpw@0.0.0.0:7052\n```\n登录成功在工作目录下的`tls`文件夹下将出现一个`admin`文件夹，这里面是`admin`的相关证书文件.\n并且只有登录了`admin`，才具有权限进行用户的注册,因为该用户具有CA的全部权限，相当于CA服务器的`root`用户。\n接下来对各个节点进行注册。\n```\nfabric-ca-client register -d --id.name peer1-org1 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer2-org1 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer1-org2 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer2-org2 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name orderer-org0 --id.secret ordererPW --id.type orderer -u https://0.0.0.0:7052\n```\n这里将三个组织中的节点都进行了注册。\n\n* 不过`-d`这个参数并没有找到相关资料 \n* `id.name`是指定用户的名称\n* `--id.secert`是指定密码\n* `--id.type`是指定用户类型，用户类型默认为`client`,主要包括`peer`,`app`,`user`,`orderer`.\n* `-u`则是指定请求CA服务器的URL。\n\n这里我们为各个节点注册TLS证书，之后Fabric网络的通信则需要通过这一步骤注册过的用户的TLS证书来进行TLS加密通信。\n到这里我们只是注册了各个节点的身份，还没有获取到他们的证书。证书可以通过登录获取，不过暂时不着急获取他们的TLS证书。\n接下来，我们对其他几个CA服务器进行配置。\n\n### 2.3配置Org0的CA服务器\n\n再强调一下，本文中的几个CA服务器都是根服务器，彼此之间没有任何关系，所以上一步骤的TLS CA服务器在这一部分并没有用到。\n同样，本文使用Docker容器启动CA服务器。配置文件如下，只需要添加进之前的`docker-compose.yaml`文件中就好：\n```\n  org0:\n    container_name: org0\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c 'fabric-ca-server start -d -b org0-admin:org0-adminpw --port 7053'\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org0/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org0\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7053:7053\n      \n```\n添加完之后启动它：\n```\ndocker-compose -f docker-compose.yaml up org0\n```\n打开另一个终端，接下来注册org0的用户：\n```\n#首先指定环境变量，这里的TLS证书不是之前的TLS CA服务器的根证书，而是本组织CA服务器启动时生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\n#指定本组织的CA客户端工作目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/admin\n```\n登录`org0`的CA服务器管理员身份用于注册本组织的用户：\n```\nfabric-ca-client enroll -d -u https://org0-admin:org0-adminpw@0.0.0.0:7053\n```\n在本组织中共有两个用户：`orderer`节点和`admin`用户(这里的admin和管理员是不同的。)\n将他们注册到org0的CA服务器：\n```\nfabric-ca-client register -d --id.name orderer-org0 --id.secret ordererpw --id.type orderer -u https://0.0.0.0:7053\nfabric-ca-client register -d --id.name admin-org0 --id.secret org0adminpw --id.type admin --id.attrs \"hf.Registrar.Roles=client,hf.Registrar.Attributes=*,hf.Revoker=true,hf.GenCRL=true,admin=true:ecert,abac.init=true:ecert\" -u https://0.0.0.0:7053\n```\n命令执行完之后，将会注册一个Orderer节点的身份和一个Admin的身份。同时在工作目录下的`org0`子文件夹中会有两个文件夹：`crypto`和`admin`。`crypto`中是CA服务器的配置信息，`admin`是服务器管理员的身份信息。\n\n### 2.4配置Org1的CA服务器\n同样的步骤，对org1组织的CA服务器进行配置：\n```\n  org1:\n    container_name: org1\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c 'fabric-ca-server start -d -b org1-admin:org1-adminpw'\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org1/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org1\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7054:7054\n      \n```\n启动服务器：\n```\ndocker-compose -f docker-compose.yaml up org1\n```\n打开新的终端，配置环境变量：\n```\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/admin\n```\n登录CA服务器管理员身份：\n```\nfabric-ca-client enroll -d -u https://org1-admin:org1-adminpw@0.0.0.0:7054\n```\n组织一种共有四个用户：`peer1`,`peer2`,`admin`,`user`,分别注册他们：\n```\nfabric-ca-client register -d --id.name peer1-org1 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name peer2-org1 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name admin-org1 --id.secret org1AdminPW --id.type user -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name user-org1 --id.secret org1UserPW --id.type user -u https://0.0.0.0:7054\n```\n### 2.5配置Org2的CA服务器\n和上一部分相同，这里只列举需要的命令：\nCA服务器配置文件：\n```\n  org2:\n    container_name: org2\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c 'fabric-ca-server start -d -b org2-admin:org2-adminpw --port 7055'\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org2/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org2\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7055:7055\n      \n```\n启动服务器：\n```\ndocker-compose -f docker-compose.yaml up org2\n```\n打开新的终端，配置环境变量：\n```\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/admin\n```\n登录CA服务器管理员身份：\n```\nfabric-ca-client enroll -d -u https://org2-admin:org2-adminpw@0.0.0.0:7055\n```\n组织一种共有四个用户：`peer1`,`peer2`,`admin`,`user`,分别注册他们：\n```\nfabric-ca-client register -d --id.name peer1-org2 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name peer2-org2 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name admin-org2 --id.secret org1AdminPW --id.type user -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name user-org2 --id.secret org1UserPW --id.type user -u https://0.0.0.0:7055\n```\n## 3.生成证书并配置TLS\n\n* * *\n\n到目前为止，所有的用户我们都注册完毕，接下来就是为每一个用户生成证书并配置TLS证书。\n其中证书分为两部分，分别是本组织的MSP证书，以及组织之间进行加密通信的TLS证书。\n所以本文需要对两部分证书进行分别生成与配置。\n从组织一开始：\n### 3.1 组织一节点配置\n#### 3.1.1 peer1\n首先是本组织的`MSP`证书：\n* 配置环境变量\n```\n#指定peer1节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/peer1\n#指定**本**组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\n```\n* 登录`peer1`节点到`org1 CA `服务器上：\n```\nfabric-ca-client enroll -d -u https://peer1-org1:peer1PW@0.0.0.0:7054\n```\n这一步完成后，在`$GOPATH/src/github.com/caDemo/org1/peer1`下会出现一个`msp`文件夹，这是`peer1`节点的`MSP`证书。\n接下来是`TLS`证书：\n* 配置环境变量\n```\n#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#指定TLS证书的HOME目录\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/peer1/tls-msp\n```\n* 登录`peer1`节点到`TLS CA`服务器上：\n```\nfabric-ca-client enroll -d -u https://peer1-org1:peer1PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer1-org1\n```\n这一步完成后，在`$GOPATH/src/github.com/caDemo/org1/peer1`下会出现一个`tls-msp`文件夹，这是`peer1`节点的`TLS`证书。\n* 修改秘钥文件名\n为什么要修改呢，进入这个文件夹看一下就知道了,由服务器生成的秘钥文件名是一长串无规则的字符串，后期我们使用的时候难道要一个字符一个字符地输入？\n```\ncd $GOPATH/src/github.com/caDemo/org1/peer1/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo\n```\n#### 3.1.2 peer2\n`peer2`节点和上面步骤相同：\n这里就直接放需要的命令了：\n* 生成`MSP`证书\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/peer2\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org1:peer2PW@0.0.0.0:7054\n```\n* 生成`TLS`证书\n```\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/peer2/tls-msp\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org1:peer2PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer2-org1\ncd $GOPATH/src/github.com/caDemo/org1/peer2/tls-msp/keystore/\nmv *_sk key.pem\n```\n#### 3.1.3 admin\n接下来是`admin`用户，这个用户有什么作用呢，实际上，安装和实例化链码都需要`admin`的证书，所以才需要注册一个`admin`用户，还要它的证书。\n* 配置环境变量\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\n#这里多了一个环境变量，是指定admin用户的msp证书文件夹的\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/adminuser/msp\n```\n* 登录`admin`用户获取`MSP`证书:\n```\nfabric-ca-client enroll -d -u https://admin-org1:org1AdminPW@0.0.0.0:7054\n```\n因为我们生成这个用户的证书主要就是为了之后链码的安装和实例化，所以配不配置他的`TLS`证书也无关紧要了(关键是我们之前也没有将这个用户注册到`tls`服务器中)\n* 复制证书到`admincerts`文件夹:\n去看Fabric官方的例子，每一个`peer`节点的`MSP`文件夹下都有`admincerts`这个子文件夹的，而且是需要我们手动创建的。\n```\nmkdir -p $GOPATH/src/github.com/caDemo/org1/peer1/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org1/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org1/peer1/msp/admincerts/org1-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/\n```\n#### 3.1.4 启动peer节点\n到这里，已经配置好了一个节点，所以我们就可以启动这个节点了，当然在之后和`orderer`节点一起启动也可以，不过忙活了这么多，还是应该提前看到一下所做的工作的成果的！\n附上`peer1`节点的容器配置信息：\n\n```\n  peer1-org1:\n    container_name: peer1-org1\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer1-org1\n      - CORE_PEER_ADDRESS=peer1-org1:7051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer1/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org1:7051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1/peer1\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org1/peer1:/tmp/hyperledger/org1/peer1\n    networks:\n      - fabric-ca\n      \n```\n启动它！！\n\n```\ndocker-compose -f docker-compose.yaml up peer1-org1\n```\n如果没有报错的话，说明之前配置的没有什么问题，如果出错的话，则需要返回去检查一下了。。。\n`peer2`节点的容器配置信息：\n\n```\n  peer2-org1:\n    container_name: peer2-org1\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer2-org1\n      - CORE_PEER_ADDRESS=peer2-org1:8051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer2/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer2/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer2/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer2/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org1:7051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1/peer2\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org1/peer2:/tmp/hyperledger/org1/peer2\n    networks:\n      - fabric-ca\n      \n```\n启动它！！\n```\ndocker-compose -f docker-compose.yaml up peer2-org1\n```\n### 3.2 组织二节点配置\n和之前一样的步骤，所以没什么好解释的了：\n#### 3.2.1 peer1\n* 配置环境变量\n```\n#指定peer2节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/peer1\n#指定本组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\n```\n* 登录`peer1`节点到`org2 CA `服务器上：\n```\nfabric-ca-client enroll -d -u https://peer1-org2:peer1PW@0.0.0.0:7055\n```\n接下来是`TLS`证书：\n* 配置环境变量\n```\n#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#指定TLS证书的HOME目录\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/peer1/tls-msp\n```\n* 登录`peer1`节点到`TLS CA`服务器上：\n```\nfabric-ca-client enroll -d -u https://peer1-org2:peer1PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer1-org2\n```\n* 修改秘钥文件名\n```\ncd $GOPATH/src/github.com/caDemo/org2/peer1/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo\n```\n#### 3.2.2 peer2\n* 生成`MSP`证书\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/peer2\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org2:peer2PW@0.0.0.0:7055\n```\n* 生成`TLS`证书\n```\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/peer2/tls-msp\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org2:peer2PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer2-org2\ncd $GOPATH/src/github.com/caDemo/org2/peer2/tls-msp/keystore/\nmv *_sk key.pem\n```\n#### 3.2.3 admin\n* 配置环境变量\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/adminuser/msp\n```\n* 登录`admin`用户获取`MSP`证书:\n```\nfabric-ca-client enroll -d -u https://admin-org2:org2AdminPW@0.0.0.0:7055\n```\n* 复制证书到`admincerts`文件夹:\n去看Fabric官方的例子，每一个`peer`节点的`MSP`文件夹下都有`admincerts`这个子文件夹的，而且是需要我们手动创建的。\n```\nmkdir -p $GOPATH/src/github.com/caDemo/org2/peer1/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org2/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org2/peer1/msp/admincerts/org2-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/\n```\n#### 3.2.4 启动peer节点\n附上`peer1`节点的容器配置信息：\n\n```\n  peer1-org2:\n    container_name: peer1-org2\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer1-org2\n      - CORE_PEER_ADDRESS=peer1-org2:9051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer1/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org2:9051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2/peer1\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org2/peer1:/tmp/hyperledger/org2/peer1\n    networks:\n      - fabric-ca\n      \n```\n启动它.\n```\ndocker-compose -f docker-compose.yaml up peer1-org2\n```\n`peer2`节点的容器配置信息：\n```\n  peer2-org2:\n    container_name: peer2-org2\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer2-org2\n      - CORE_PEER_ADDRESS=peer2-org2:10051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer2/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer2/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer2/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer2/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org2:9051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2/peer2\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org2/peer2:/tmp/hyperledger/org2/peer2\n    networks:\n      - fabric-ca\n      \n```\n启动它.\n```\ndocker-compose -f docker-compose.yaml up peer2-org2\n```\n### 3.3 排序节点配置\n接下来是排序节点的配置，为什么放在最后面呢，因为排序节点的启动需要提前生成创世区块，而创世区块的生成涉及到另一个配置文件，所以就先配置简单的`peer`节点。\n#### 3.3.1 orderer\n* 配置环境变量\n```\n#指定order节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/orderer\n#指定本组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\n```\n* 登录`order`节点到`org0 CA `服务器上：\n```\nfabric-ca-client enroll -d -u https://orderer-org0:ordererpw@0.0.0.0:7053\n```\n接下来是`TLS`证书：\n*  配置环境变量\n```\n#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org0/orderer/tls-msp\n#指定TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n```\n* 登录`orderer`节点到`TLS CA`服务器上：\n```\nfabric-ca-client enroll -d -u https://orderer-org0:ordererPW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts orderer-org0\n```\n* 修改秘钥文件名\n```\ncd $GOPATH/src/github.com/caDemo/org0/orderer/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo\n```\n#### 3.3.2 admin\n* 配置环境变量\n```\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org0/adminuser/msp\n```\n* 登录`admin`用户获取`MSP`证书:\n```\nfabric-ca-client enroll -d -u https://admin-org0:org0adminpw@0.0.0.0:7053\n```\n* 复制证书到`admincerts`文件夹:\n```\nmkdir $GOPATH/src/github.com/caDemo/org0/orderer/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org0/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org0/orderer/msp/admincerts/orderer-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/\n```\n## 4.Fabric网络配置\n\n* * *\n\n接下来到重头戏了，证书都生成好了，即将要启动网络了。不过在启动网络之前还是有很多准备工作需要做。其实到这里，官方文档已经好多没有交代清楚的了，所以一下好多内容都是笔者自己摸索出来的，如有错误欢迎批评指正。\n### 4.1 configtx.yaml文件配置\n在下一个步骤的生成创世区块和通道配置信息需要一个文件：`configtx.yaml`文件。笔者根据官方的例子按照本文内容修改了一下，直接放在工作目录:\n```\nOrganizations:\n  - &orderer-org0\n    Name: orderer-org0\n    ID: org0MSP\n    MSPDir: ./org0/msp\n  #    Policies:\n  #      Readers:\n  #        Type: Signature\n  #        Rule: \"OR('orderer-org0MSP.member')\"\n  #      Writers:\n  #        Type: Signature\n  #        Rule: \"OR('orderer-org0MSP.member')\"\n  #      Admins:\n  #        Type: Signature\n  #        Rule: \"OR('orderer-org0MSP.admin')\"\n  \n  - &org1\n    Name: org1MSP\n    ID: org1MSP\n    \n    MSPDir: ./org1/msp\n    #    Policies:\n    #      Readers:\n    #        Type: Signature\n    #        Rule: \"OR('org1MSP.admin', 'org1MSP.peer', 'org1MSP.client')\"\n    #      Writers:\n    #        Type: Signature\n    #        Rule: \"OR('org1MSP.admin', 'org1MSP.client')\"\n    #      Admins:\n    #        Type: Signature\n    #        Rule: \"OR('org1MSP.admin')\"\n    AnchorPeers:\n      - Host: peer1-org1\n        Port: 7051\n  \n  - &org2\n    Name: org2MSP\n    ID: org2MSP\n    MSPDir: ./org2/msp\n    #    Policies:\n    #      Readers:\n    #        Type: Signature\n    #        Rule: \"OR('org2MSP.admin', 'org2MSP.peer', 'org2MSP.client')\"\n    #      Writers:\n    #        Type: Signature\n    #        Rule: \"OR('org2MSP.admin', 'org2MSP.client')\"\n    #      Admins:\n    #        Type: Signature\n    #        Rule: \"OR('org2MSP.admin')\"\n    \n    AnchorPeers:\n      - Host: peer1-org2\n        Port: 9051\n\nCapabilities:\n  Channel: &ChannelCapabilities\n    V1_4_3: true\n    V1_3: false\n    V1_1: false\n  Orderer: &OrdererCapabilities\n    V1_4_2: true\n    V1_1: false\n  Application: &ApplicationCapabilities\n    V1_4_2: true\n    V1_3: false\n    V1_2: false\n    V1_1: false\n\nApplication: &ApplicationDefaults\n  Organizations:\n  #  Policies:\n  #    Readers:\n  #      Type: ImplicitMeta\n  #      Rule: \"ANY Readers\"\n  #    Writers:\n  #      Type: ImplicitMeta\n  #      Rule: \"ANY Writers\"\n  #    Admins:\n  #      Type: ImplicitMeta\n  #      Rule: \"MAJORITY Admins\"\n  \n  Capabilities:\n    <<: *ApplicationCapabilities\n    \nOrderer: &OrdererDefaults\n  OrdererType: solo\n  \n  Addresses:\n    - orderer-org0:7050\n  BatchTimeout: 2s\n  BatchSize:\n    MaxMessageCount: 10\n    AbsoluteMaxBytes: 99 MB\n    PreferredMaxBytes: 512 KB\n  Organizations:\n#  Policies:\n#    Readers:\n#      Type: ImplicitMeta\n#      Rule: \"ANY Readers\"\n#    Writers:\n#      Type: ImplicitMeta\n#      Rule: \"ANY Writers\"\n#    Admins:\n#      Type: ImplicitMeta\n#      Rule: \"MAJORITY Admins\"\n#    # BlockValidation specifies what signatures must be included in the block\n#    # from the orderer for the peer to validate it.\n#    BlockValidation:\n#      Type: ImplicitMeta\n#      Rule: \"ANY Writers\"\n\nChannel: &ChannelDefaults\n  #  Policies:\n  #    # Who may invoke the 'Deliver' API\n  #    Readers:\n  #      Type: ImplicitMeta\n  #      Rule: \"ANY Readers\"\n  #    # Who may invoke the 'Broadcast' API\n  #    Writers:\n  #      Type: ImplicitMeta\n  #      Rule: \"ANY Writers\"\n  #    # By default, who may modify elements at this config level\n  #    Admins:\n  #      Type: ImplicitMeta\n  #      Rule: \"MAJORITY Admins\"\n  Capabilities:\n    <<: *ChannelCapabilities\n\nProfiles:\n  \n  TwoOrgsOrdererGenesis:\n    <<: *ChannelDefaults\n    Orderer:\n      <<: *OrdererDefaults\n      Organizations:\n        - *orderer-org0\n      Capabilities:\n        <<: *OrdererCapabilities\n    Consortiums:\n      SampleConsortium:\n        Organizations:\n          - *org1\n          - *org2\n  TwoOrgsChannel:\n    Consortium: SampleConsortium\n    <<: *ChannelDefaults\n    Application:\n      <<: *ApplicationDefaults\n      Organizations:\n        - *org1\n        - *org2\n      Capabilities:\n        <<: *ApplicationCapabilities\n```\n\n注释掉的部分是策略部分，笔者还没有完全搞懂，所以索性就先注释掉了，以后搞懂了再添加进去。\n还有一部分`msp`需要配置，就是`configtx.yaml`文件中第一部分指定的`MSPDir`,很简单，按照一下命令复制一下就好了：\n```\n#进入工作目录\ncd $GOPATH/src/github.com/caDemo\n############################################\n#org0\nmkdir org0/msp &&  cd org0/msp\nmkdir admincerts && mkdir cacerts && mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n############################################\n#org1\ncd $GOPATH/src/github.com/caDemo\nmkdir org1/msp/  && cd org1/msp/\nmkdir admincerts && mkdir cacerts && mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n############################################\n#org2\ncd $GOPATH/src/github.com/caDemo\nmkdir org1/msp/  && cd org1/msp/\nmkdir admincerts && mkdir cacerts && mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n```\n### 4.2 生成创世区块和通道配置信息\n可以了，所有的前期工作都已经完成，接下来就是手动启动网络了，第一步，生成创世区块和通道配置信息：\n```\ncd $GOPATH/src/github.com/caDemo\nexport FABRIC_CFG_PATH=$PWD\n#生成创世区块\nconfigtxgen -profile TwoOrgsOrdererGenesis -outputBlock $GOPATH/src/github.com/caDemo/genesis.block\n#生成通道配置信息\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx $GOPATH/src/github.com/caDemo/channel.tx -channelID mychannel\n```\n### 4.3 启动Orderer节点\n`orderer`容器配置文件：\n```\n  orderer-org0:\n    container_name: orderer-org0\n    image: hyperledger/fabric-orderer\n    environment:\n      - ORDERER_HOME=/tmp/hyperledger/orderer\n      - ORDERER_HOST=orderer-org0\n      - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0\n      - ORDERER_GENERAL_GENESISMETHOD=file\n      - ORDERER_GENERAL_GENESISFILE=/tmp/hyperledger/genesis.block\n      - ORDERER_GENERAL_LOCALMSPID=org0MSP\n      - ORDERER_GENERAL_LOCALMSPDIR=/tmp/hyperledger/org0/orderer/msp\n      - ORDERER_GENERAL_TLS_ENABLED=true\n      - ORDERER_GENERAL_TLS_CERTIFICATE=/tmp/hyperledger/org0/orderer/tls-msp/signcerts/cert.pem\n      - ORDERER_GENERAL_TLS_PRIVATEKEY=/tmp/hyperledger/org0/orderer/tls-msp/keystore/key.pem\n      - ORDERER_GENERAL_TLS_ROOTCAS=[/tmp/hyperledger/org0/orderer/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem]\n      - ORDERER_GENERAL_LOGLEVEL=debug\n      - ORDERER_DEBUG_BROADCASTTRACEDIR=data/logs\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org0/orderer:/tmp/hyperledger/org0/orderer/\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    \n```\n关键部分到了，只要这一步没有出现错误，整个网络就启动成功了。\n```\ndocker-compose -f docker-compose.yaml up orderer-org0\n```\n### 4.4 启动组织一的cli容器\n`cli`容器内容,我们需要这个容器对组织1进行链码的交互：\n```\n  cli-org1:\n    container_name: cli-org1\n    image: hyperledger/fabric-tools\n    tty: true\n    stdin_open: true\n    environment:\n      - SYS_CHANNEL=testchainid\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_ID=cli-org1\n      - CORE_PEER_ADDRESS=peer1-org1:7051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer1/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1\n    command: /bin/bash\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org1/peer1:/tmp/hyperledger/org1/peer1\n      - $GOPATH/src/github.com/caDemo/org1/peer1/assets/chaincode:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - $GOPATH/src/github.com/caDemo/org1/adminuser:/tmp/hyperledger/org1/adminuser\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    depends_on:\n      - peer1-org1\n```\n启动该容器：\n```\ndocker-compose -f docker-compose.yaml up cli-org1\n```\n### 4.5 启动组织二的cli容器\n`cli`容器内容,我们需要这个容器对组织2进行链码的交互：\n```\n  cli-org2:\n    container_name: cli-org2\n    image: hyperledger/fabric-tools\n    tty: true\n    stdin_open: true\n    environment:\n      - SYS_CHANNEL=testchainid\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_ID=cli-org2\n      - CORE_PEER_ADDRESS=peer1-org2:9051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer1/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2\n    command: /bin/bash\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org2/peer1:/tmp/hyperledger/org2/peer1\n      - $GOPATH/src/github.com/caDemo/org2/peer1/assets/chaincode:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - $GOPATH/src/github.com/caDemo/org2/adminuser:/tmp/hyperledger/org2/adminuser\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    depends_on:\n      - peer1-org2\n```\n启动该容器：\n```\ndocker-compose -f docker-compose.yaml up cli-org2\n```\n## 5.网络测试\n\n* * *\n\n所有工作准备完成，接下来让我们测试整个网络能不能正常运行吧：\n### 5.1 创建与加入通道\n以**组织1**为例：\n* 首先进入`cli`容器：\n```\ndocker exec -it cli bash\n#配置环境变量\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\n```\n* 创建通道\n```\npeer channel create -c mychannel -f /tmp/hyperledger/channel.tx -o orderer-org0:7050 --outputBlock /tmp/hyperledger/mychannel.block --tls --cafile /tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n```\n* 将`peer1-org1`加入通道：\n```\nexport CORE_PEER_ADDRESS=peer1-org1:7051\npeer channel join -b /tmp/hyperledger/mychannel.block\n```\n* 将`peer2-org1`加入通道：\n```\nexport CORE_PEER_ADDRESS=peer2-org1:8051\npeer channel join -b /tmp/hyperledger/mychannel.block\n```\n组织二步骤是相同的，唯一不同的就是不需要创建通道了，所以就不再说明了。\n### 5.2 安装和实例化链码\n以**组织1**为例：\n* 首先进入`cli`容器：\n```\ndocker exec -it cli bash\n#配置环境变量\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\nexport CORE_PEER_ADDRESS=peer1-org1:7051\n```\n* 安装链码\n**记得提前将链码放到**`$GOPATH/src/github.com/caDemo/org1/peer1/assets/chaincode`**路径下。**,本文使用的是`fabric-samples/chaincode/chaincode_example02`官方示例链码。\n```\npeer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric-samples/chaincode/chaincode_example02/go/\n```\n* 实例化链码\n```\npeer chaincode instantiate -C mychannel -n mycc -v 1.0 -c '{\"Args\":[\"init\",\"a\",\"100\",\"b\",\"200\"]}'  -o orderer-org0:7050 --tls --cafile /tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n```\n\n* 这一步在高版本的Fabric网络是会出错的，因为少了一个文件`config.yaml`:\n\n```\nNodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: orderer\n```\n因为高版本的Fabric把节点类型区分开了，所以需要我们手动配置。\n将该文件复制到`$GOPATH/src/github.com/caDemo/org1/adminuser/msp`文件夹内，同时修改上面指定的位置的文件名(与对应文件夹内的文件名对应就好了)。\n\n* 实例化部分出错的可能性是最高的，很多都是因为网络模式指定错误导致链码容器启动失败，解决方案：\n```\n#终端执行命令\ndocker network ls\n```\n找到以`fabric-ca`为后缀的一条如`cademo_fabric-ca`,修改之前的所有`peer`节点容器配置文件的环境变量：\n```\nCORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n```\n修改完成重启节点容器，再次执行以上的命令(需要重新配置环境变量，加入通道这两个操作)。\n终于，实例化成功了。\n### 5.3 调用和查询链码\n最后测试一下链码功能能不能正常使用了：\n* 还是组织一的`cli`容器：\n```\ndocker exec -it cli bash\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\nexport CORE_PEER_ADDRESS=peer1-org1:7051\n```\n* 执行查询功能：\n```\npeer chaincode query -C mychannel -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n```\n命令行应该打印出:\n```\n100\n```\n* 执行调用功能：\n```\npeer chaincode invoke -C mychannel -n mycc -c '{\"Args\":[\"invoke\",\"a\",\"b\",\"10\"]}' --tls --cafile /tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n```\n* 再次查询：\n```\npeer chaincode query -C mychannel -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n```\n命令行应该打印出:\n```\n90\n```\n至于其他节点操作方法也是一样的，就不再操作了。\n到此为止，从零开始的手动生成证书一直到成功搭建Fabric网络全部步骤已经完成！！接下来还有更新锚节点等等就不再演示了，请各位读者自行操作。整个步骤是不容易的，而且BUG百出，不过成功搭建完成确实涨了不少知识。\n码字不易，还望各位看官支持一下：\n\n <img src=\"/img/blog/zfb.png\" width = \"300\" height = \"300\" alt=\"支付宝\" align=center />\n\n","slug":"blog/fabric/Hyperledger_Fabric手动生成CA证书搭建Fabric网络","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyj5003gk0vq1a44gghl","content":"<p>之前介绍了使用官方脚本自动化启动一个Fabric网络，并且所有的证书都是通过官方的命令行工具<code>cryptogen</code>直接生成网络中的所有节点的证书。在开发环境可以这么简单进行，但是生成环境下还是需要我们自定义对网络中的节点的证书进行配置。<br>所以在本文中，将会详细介绍一个从手动生成证书一直到启动网络的整体步骤。本文只交代整体的搭建步骤。对于Fabric-Ca的讲解不在本文的范围内，将在另一篇文章中说明。<br>正篇文章也是根据官方的文档进行的。但是由于官方的文档尚未完工，也是好多没有交代清楚的，并且有些地方是错误的，所以笔者也是一步一步摸索出来的，所以如果本文哪里没有交代清楚或者错误的地方，希望各位批评指正。<br>在这里贴出<a href=\"https://hyperledger-fabric-ca.readthedocs.io/en/latest/operations_guide.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>地址.</p>\n<h2 id=\"1-整体架构\"><a href=\"#1-整体架构\" class=\"headerlink\" title=\"1.整体架构\"></a>1.整体架构</h2><hr>\n<p>架构图直接贴过来好了：<br><img src=\"/img/blog/arth.png\" srcset=\"undefined\" alt=\"系统架构\"></p>\n<p>官方文档采用的是多机环境，这里简洁化一点，所有的操作都在<strong>一台机器</strong>上进行，至于多机环境，以后再补充好了。<br>介绍一下本文所采用的整体架构：</p>\n<ol>\n<li>三个组织<ol>\n<li>Org0  -&gt; 组织0   </li>\n<li>Org1  -&gt; 组织1   </li>\n<li>Org2  -&gt; 组织2   </li>\n</ol>\n</li>\n<li>组织中的成员<ol>\n<li>Org0   一个Orderer节点，一个Org0的Admin节点</li>\n<li>Org1   两个Peer节点，一个Org1的Admin节点，一个Org1的User节点</li>\n<li>Org2   两个Peer节点，一个Org2的Admin节点，一个Org2的User节点</li>\n</ol>\n</li>\n<li>共有四台CA服务器<ol>\n<li>TLS服务器   -&gt;  为网络中所有节点颁发TLS证书，用于通信的加密</li>\n<li>Org1的CA服务器 -&gt; 为组织1中所有用户颁发证书</li>\n<li>Org2的Ca服务器 -&gt; 为组织2中所有用户颁发证书</li>\n<li>Org0的CA服务器 -&gt; 为组织0中所有用户颁发证书</li>\n</ol>\n</li>\n</ol>\n<p>这里的四台CA服务器都是根服务器。<strong>彼此之间都是独立的存在，没有任何关系。</strong>，也就是说每一个CA服务器生成的证书在其他CA服务器都是不能用的。<br>介绍完之后，可以进入正题了。</p>\n<h3 id=\"1-1Fabric，Fabric-Ca安装\"><a href=\"#1-1Fabric，Fabric-Ca安装\" class=\"headerlink\" title=\"1.1Fabric，Fabric-Ca安装\"></a>1.1Fabric，Fabric-Ca安装</h3><p>本文默认读者都是对Fabric有一定的了解的，所以一些安装过程这里就没有重复说明。<br>第一步是安装Fabric-Ca环境，可以参考<a href=\"https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric_CA/\" target=\"_blank\" rel=\"noopener\">这里</a>,这篇文章还没有写完，以后慢慢补，不过环境的安装已经有说明。<br>还有就是Fabric的环境安装，可以参考<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">这里</a>。</p>\n<p>完成环境搭建后，我们还需要一个<code>HOME</code>文件夹，用于存放我们生成的证书文件与<code>fabric</code>配置相关的文件。<br>本文设置<code>HOME</code>文件夹路径为:</p>\n<pre><code>$GOPATH/src/github.com/caDemo/</code></pre><p>请读者自行创建，一般不要用太复杂的路径,也不要用中文路径，会为之后的操作带来很多麻烦。在下文中简单称<code>HOME</code>文件夹为<strong>工作目录</strong>,<strong>除非特殊说明，一般命令的执行都是在工作目录进行</strong>。</p>\n<h2 id=\"2-CA服务器配置\"><a href=\"#2-CA服务器配置\" class=\"headerlink\" title=\"2 CA服务器配置\"></a>2 CA服务器配置</h2><hr>\n<h3 id=\"2-1启动TLS-CA服务器\"><a href=\"#2-1启动TLS-CA服务器\" class=\"headerlink\" title=\"2.1启动TLS CA服务器\"></a>2.1启动TLS CA服务器</h3><p>前期工作准备好之后，我们开始启动第一台CA服务器。本文中使用<code>Docker</code>容器启动。<br>首先在工作目录创建<code>docker-compose.yaml</code>文件：</p>\n<pre><code>touch docker-compose.yaml</code></pre><p>并在文件内添加以下内容(tips:内容格式不要乱掉)：</p>\n<pre><code>version: &#39;2&#39;\n\nnetworks:\n  fabric-ca:\n\nservices:\n\n  ca-tls:\n    container_name: ca-tls\n    image: hyperledger/fabric-ca\n    command: sh -c &#39;fabric-ca-server start -d -b tls-ca-admin:tls-ca-adminpw --port 7052&#39;\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/tls\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=ca-tls\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca  ##重要！！！记得修改这里的路径为自己的工作目录\n    networks:\n      - fabric-ca\n    ports:\n      - 7052:7052\n</code></pre><p>启动该<code>docker</code>容器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up ca-tls</code></pre><p>如果命令行出现以下内容则说明启动成功：</p>\n<pre><code>[INFO] Listening on https://0.0.0.0:7052</code></pre><p>同时工作目录下会出现一个<code>tls</code>的文件夹。文件夹中的内容暂先不解释，留着在另一篇文章中说明。不过有一个文件需要解释一下，因为之后会用到。<br>在<code>$GOPATH/src/github.com/caDemo/tls/</code>路径下的<code>ca-cert.pem</code>文件。这是<code>TLS CA</code>服务器签名的根证书，目的是用来对<code>CA</code>的<code>TLS</code>证书进行验证，同时也需要持有这个证书才可以进行证书的颁发。在多机环境下，我们需要将它复制到每一台机器上，不过本文采用的是单机环境，所以省略掉了这一步。</p>\n<h3 id=\"2-2-TLS-CA服务器注册用户\"><a href=\"#2-2-TLS-CA服务器注册用户\" class=\"headerlink\" title=\"2.2 TLS CA服务器注册用户\"></a>2.2 TLS CA服务器注册用户</h3><p>第一步是在TLS CA服务器中注册用户，经过注册的用户才拥有TLS证书，本文中由于只在各节点之间进行TLS加密通信，所以只将<code>orderer</code>和<code>peer</code>节点的身份注册到TLS服务器。<br>打开一个新的终端输入以下命令：</p>\n<pre><code>#设置环境变量指定根证书的路径(如果工作目录不同的话记得指定自己的工作目录,以下不再重复说明)\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#设置环境变量指定CA客户端的HOME文件夹\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/tls/admin\n#登录管理员用户用于之后的节点身份注册\nfabric-ca-client enroll -d -u https://tls-ca-admin:tls-ca-adminpw@0.0.0.0:7052</code></pre><p>登录成功在工作目录下的<code>tls</code>文件夹下将出现一个<code>admin</code>文件夹，这里面是<code>admin</code>的相关证书文件.<br>并且只有登录了<code>admin</code>，才具有权限进行用户的注册,因为该用户具有CA的全部权限，相当于CA服务器的<code>root</code>用户。<br>接下来对各个节点进行注册。</p>\n<pre><code>fabric-ca-client register -d --id.name peer1-org1 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer2-org1 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer1-org2 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer2-org2 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name orderer-org0 --id.secret ordererPW --id.type orderer -u https://0.0.0.0:7052</code></pre><p>这里将三个组织中的节点都进行了注册。</p>\n<ul>\n<li>不过<code>-d</code>这个参数并没有找到相关资料 </li>\n<li><code>id.name</code>是指定用户的名称</li>\n<li><code>--id.secert</code>是指定密码</li>\n<li><code>--id.type</code>是指定用户类型，用户类型默认为<code>client</code>,主要包括<code>peer</code>,<code>app</code>,<code>user</code>,<code>orderer</code>.</li>\n<li><code>-u</code>则是指定请求CA服务器的URL。</li>\n</ul>\n<p>这里我们为各个节点注册TLS证书，之后Fabric网络的通信则需要通过这一步骤注册过的用户的TLS证书来进行TLS加密通信。<br>到这里我们只是注册了各个节点的身份，还没有获取到他们的证书。证书可以通过登录获取，不过暂时不着急获取他们的TLS证书。<br>接下来，我们对其他几个CA服务器进行配置。</p>\n<h3 id=\"2-3配置Org0的CA服务器\"><a href=\"#2-3配置Org0的CA服务器\" class=\"headerlink\" title=\"2.3配置Org0的CA服务器\"></a>2.3配置Org0的CA服务器</h3><p>再强调一下，本文中的几个CA服务器都是根服务器，彼此之间没有任何关系，所以上一步骤的TLS CA服务器在这一部分并没有用到。<br>同样，本文使用Docker容器启动CA服务器。配置文件如下，只需要添加进之前的<code>docker-compose.yaml</code>文件中就好：</p>\n<pre><code>  org0:\n    container_name: org0\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c &#39;fabric-ca-server start -d -b org0-admin:org0-adminpw --port 7053&#39;\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org0/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org0\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7053:7053\n</code></pre><p>添加完之后启动它：</p>\n<pre><code>docker-compose -f docker-compose.yaml up org0</code></pre><p>打开另一个终端，接下来注册org0的用户：</p>\n<pre><code>#首先指定环境变量，这里的TLS证书不是之前的TLS CA服务器的根证书，而是本组织CA服务器启动时生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\n#指定本组织的CA客户端工作目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/admin</code></pre><p>登录<code>org0</code>的CA服务器管理员身份用于注册本组织的用户：</p>\n<pre><code>fabric-ca-client enroll -d -u https://org0-admin:org0-adminpw@0.0.0.0:7053</code></pre><p>在本组织中共有两个用户：<code>orderer</code>节点和<code>admin</code>用户(这里的admin和管理员是不同的。)<br>将他们注册到org0的CA服务器：</p>\n<pre><code>fabric-ca-client register -d --id.name orderer-org0 --id.secret ordererpw --id.type orderer -u https://0.0.0.0:7053\nfabric-ca-client register -d --id.name admin-org0 --id.secret org0adminpw --id.type admin --id.attrs &quot;hf.Registrar.Roles=client,hf.Registrar.Attributes=*,hf.Revoker=true,hf.GenCRL=true,admin=true:ecert,abac.init=true:ecert&quot; -u https://0.0.0.0:7053</code></pre><p>命令执行完之后，将会注册一个Orderer节点的身份和一个Admin的身份。同时在工作目录下的<code>org0</code>子文件夹中会有两个文件夹：<code>crypto</code>和<code>admin</code>。<code>crypto</code>中是CA服务器的配置信息，<code>admin</code>是服务器管理员的身份信息。</p>\n<h3 id=\"2-4配置Org1的CA服务器\"><a href=\"#2-4配置Org1的CA服务器\" class=\"headerlink\" title=\"2.4配置Org1的CA服务器\"></a>2.4配置Org1的CA服务器</h3><p>同样的步骤，对org1组织的CA服务器进行配置：</p>\n<pre><code>  org1:\n    container_name: org1\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c &#39;fabric-ca-server start -d -b org1-admin:org1-adminpw&#39;\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org1/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org1\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7054:7054\n</code></pre><p>启动服务器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up org1</code></pre><p>打开新的终端，配置环境变量：</p>\n<pre><code>export FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/admin</code></pre><p>登录CA服务器管理员身份：</p>\n<pre><code>fabric-ca-client enroll -d -u https://org1-admin:org1-adminpw@0.0.0.0:7054</code></pre><p>组织一种共有四个用户：<code>peer1</code>,<code>peer2</code>,<code>admin</code>,<code>user</code>,分别注册他们：</p>\n<pre><code>fabric-ca-client register -d --id.name peer1-org1 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name peer2-org1 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name admin-org1 --id.secret org1AdminPW --id.type user -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name user-org1 --id.secret org1UserPW --id.type user -u https://0.0.0.0:7054</code></pre><h3 id=\"2-5配置Org2的CA服务器\"><a href=\"#2-5配置Org2的CA服务器\" class=\"headerlink\" title=\"2.5配置Org2的CA服务器\"></a>2.5配置Org2的CA服务器</h3><p>和上一部分相同，这里只列举需要的命令：<br>CA服务器配置文件：</p>\n<pre><code>  org2:\n    container_name: org2\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c &#39;fabric-ca-server start -d -b org2-admin:org2-adminpw --port 7055&#39;\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org2/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org2\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7055:7055\n</code></pre><p>启动服务器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up org2</code></pre><p>打开新的终端，配置环境变量：</p>\n<pre><code>export FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/admin</code></pre><p>登录CA服务器管理员身份：</p>\n<pre><code>fabric-ca-client enroll -d -u https://org2-admin:org2-adminpw@0.0.0.0:7055</code></pre><p>组织一种共有四个用户：<code>peer1</code>,<code>peer2</code>,<code>admin</code>,<code>user</code>,分别注册他们：</p>\n<pre><code>fabric-ca-client register -d --id.name peer1-org2 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name peer2-org2 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name admin-org2 --id.secret org1AdminPW --id.type user -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name user-org2 --id.secret org1UserPW --id.type user -u https://0.0.0.0:7055</code></pre><h2 id=\"3-生成证书并配置TLS\"><a href=\"#3-生成证书并配置TLS\" class=\"headerlink\" title=\"3.生成证书并配置TLS\"></a>3.生成证书并配置TLS</h2><hr>\n<p>到目前为止，所有的用户我们都注册完毕，接下来就是为每一个用户生成证书并配置TLS证书。<br>其中证书分为两部分，分别是本组织的MSP证书，以及组织之间进行加密通信的TLS证书。<br>所以本文需要对两部分证书进行分别生成与配置。<br>从组织一开始：</p>\n<h3 id=\"3-1-组织一节点配置\"><a href=\"#3-1-组织一节点配置\" class=\"headerlink\" title=\"3.1 组织一节点配置\"></a>3.1 组织一节点配置</h3><h4 id=\"3-1-1-peer1\"><a href=\"#3-1-1-peer1\" class=\"headerlink\" title=\"3.1.1 peer1\"></a>3.1.1 peer1</h4><p>首先是本组织的<code>MSP</code>证书：</p>\n<ul>\n<li>配置环境变量<pre><code>#指定peer1节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/peer1\n#指定**本**组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem</code></pre></li>\n<li>登录<code>peer1</code>节点到<code>org1 CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://peer1-org1:peer1PW@0.0.0.0:7054</code></pre>这一步完成后，在<code>$GOPATH/src/github.com/caDemo/org1/peer1</code>下会出现一个<code>msp</code>文件夹，这是<code>peer1</code>节点的<code>MSP</code>证书。<br>接下来是<code>TLS</code>证书：</li>\n<li>配置环境变量<pre><code>#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#指定TLS证书的HOME目录\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/peer1/tls-msp</code></pre></li>\n<li>登录<code>peer1</code>节点到<code>TLS CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://peer1-org1:peer1PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer1-org1</code></pre>这一步完成后，在<code>$GOPATH/src/github.com/caDemo/org1/peer1</code>下会出现一个<code>tls-msp</code>文件夹，这是<code>peer1</code>节点的<code>TLS</code>证书。</li>\n<li>修改秘钥文件名<br>为什么要修改呢，进入这个文件夹看一下就知道了,由服务器生成的秘钥文件名是一长串无规则的字符串，后期我们使用的时候难道要一个字符一个字符地输入？<pre><code>cd $GOPATH/src/github.com/caDemo/org1/peer1/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo</code></pre><h4 id=\"3-1-2-peer2\"><a href=\"#3-1-2-peer2\" class=\"headerlink\" title=\"3.1.2 peer2\"></a>3.1.2 peer2</h4><code>peer2</code>节点和上面步骤相同：<br>这里就直接放需要的命令了：</li>\n<li>生成<code>MSP</code>证书<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/peer2\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org1:peer2PW@0.0.0.0:7054</code></pre></li>\n<li>生成<code>TLS</code>证书<pre><code>export FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/peer2/tls-msp\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org1:peer2PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer2-org1\ncd $GOPATH/src/github.com/caDemo/org1/peer2/tls-msp/keystore/\nmv *_sk key.pem</code></pre><h4 id=\"3-1-3-admin\"><a href=\"#3-1-3-admin\" class=\"headerlink\" title=\"3.1.3 admin\"></a>3.1.3 admin</h4>接下来是<code>admin</code>用户，这个用户有什么作用呢，实际上，安装和实例化链码都需要<code>admin</code>的证书，所以才需要注册一个<code>admin</code>用户，还要它的证书。</li>\n<li>配置环境变量<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\n#这里多了一个环境变量，是指定admin用户的msp证书文件夹的\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/adminuser/msp</code></pre></li>\n<li>登录<code>admin</code>用户获取<code>MSP</code>证书:<pre><code>fabric-ca-client enroll -d -u https://admin-org1:org1AdminPW@0.0.0.0:7054</code></pre>因为我们生成这个用户的证书主要就是为了之后链码的安装和实例化，所以配不配置他的<code>TLS</code>证书也无关紧要了(关键是我们之前也没有将这个用户注册到<code>tls</code>服务器中)</li>\n<li>复制证书到<code>admincerts</code>文件夹:<br>去看Fabric官方的例子，每一个<code>peer</code>节点的<code>MSP</code>文件夹下都有<code>admincerts</code>这个子文件夹的，而且是需要我们手动创建的。<pre><code>mkdir -p $GOPATH/src/github.com/caDemo/org1/peer1/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org1/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org1/peer1/msp/admincerts/org1-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/</code></pre><h4 id=\"3-1-4-启动peer节点\"><a href=\"#3-1-4-启动peer节点\" class=\"headerlink\" title=\"3.1.4 启动peer节点\"></a>3.1.4 启动peer节点</h4>到这里，已经配置好了一个节点，所以我们就可以启动这个节点了，当然在之后和<code>orderer</code>节点一起启动也可以，不过忙活了这么多，还是应该提前看到一下所做的工作的成果的！<br>附上<code>peer1</code>节点的容器配置信息：</li>\n</ul>\n<pre><code>  peer1-org1:\n    container_name: peer1-org1\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer1-org1\n      - CORE_PEER_ADDRESS=peer1-org1:7051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer1/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org1:7051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1/peer1\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org1/peer1:/tmp/hyperledger/org1/peer1\n    networks:\n      - fabric-ca\n</code></pre><p>启动它！！</p>\n<pre><code>docker-compose -f docker-compose.yaml up peer1-org1</code></pre><p>如果没有报错的话，说明之前配置的没有什么问题，如果出错的话，则需要返回去检查一下了。。。<br><code>peer2</code>节点的容器配置信息：</p>\n<pre><code>  peer2-org1:\n    container_name: peer2-org1\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer2-org1\n      - CORE_PEER_ADDRESS=peer2-org1:8051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer2/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer2/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer2/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer2/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org1:7051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1/peer2\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org1/peer2:/tmp/hyperledger/org1/peer2\n    networks:\n      - fabric-ca\n</code></pre><p>启动它！！</p>\n<pre><code>docker-compose -f docker-compose.yaml up peer2-org1</code></pre><h3 id=\"3-2-组织二节点配置\"><a href=\"#3-2-组织二节点配置\" class=\"headerlink\" title=\"3.2 组织二节点配置\"></a>3.2 组织二节点配置</h3><p>和之前一样的步骤，所以没什么好解释的了：</p>\n<h4 id=\"3-2-1-peer1\"><a href=\"#3-2-1-peer1\" class=\"headerlink\" title=\"3.2.1 peer1\"></a>3.2.1 peer1</h4><ul>\n<li>配置环境变量<pre><code>#指定peer2节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/peer1\n#指定本组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem</code></pre></li>\n<li>登录<code>peer1</code>节点到<code>org2 CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://peer1-org2:peer1PW@0.0.0.0:7055</code></pre>接下来是<code>TLS</code>证书：</li>\n<li>配置环境变量<pre><code>#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#指定TLS证书的HOME目录\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/peer1/tls-msp</code></pre></li>\n<li>登录<code>peer1</code>节点到<code>TLS CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://peer1-org2:peer1PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer1-org2</code></pre></li>\n<li>修改秘钥文件名<pre><code>cd $GOPATH/src/github.com/caDemo/org2/peer1/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo</code></pre><h4 id=\"3-2-2-peer2\"><a href=\"#3-2-2-peer2\" class=\"headerlink\" title=\"3.2.2 peer2\"></a>3.2.2 peer2</h4></li>\n<li>生成<code>MSP</code>证书<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/peer2\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org2:peer2PW@0.0.0.0:7055</code></pre></li>\n<li>生成<code>TLS</code>证书<pre><code>export FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/peer2/tls-msp\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org2:peer2PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer2-org2\ncd $GOPATH/src/github.com/caDemo/org2/peer2/tls-msp/keystore/\nmv *_sk key.pem</code></pre><h4 id=\"3-2-3-admin\"><a href=\"#3-2-3-admin\" class=\"headerlink\" title=\"3.2.3 admin\"></a>3.2.3 admin</h4></li>\n<li>配置环境变量<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/adminuser/msp</code></pre></li>\n<li>登录<code>admin</code>用户获取<code>MSP</code>证书:<pre><code>fabric-ca-client enroll -d -u https://admin-org2:org2AdminPW@0.0.0.0:7055</code></pre></li>\n<li>复制证书到<code>admincerts</code>文件夹:<br>去看Fabric官方的例子，每一个<code>peer</code>节点的<code>MSP</code>文件夹下都有<code>admincerts</code>这个子文件夹的，而且是需要我们手动创建的。<pre><code>mkdir -p $GOPATH/src/github.com/caDemo/org2/peer1/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org2/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org2/peer1/msp/admincerts/org2-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/</code></pre><h4 id=\"3-2-4-启动peer节点\"><a href=\"#3-2-4-启动peer节点\" class=\"headerlink\" title=\"3.2.4 启动peer节点\"></a>3.2.4 启动peer节点</h4>附上<code>peer1</code>节点的容器配置信息：</li>\n</ul>\n<pre><code>  peer1-org2:\n    container_name: peer1-org2\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer1-org2\n      - CORE_PEER_ADDRESS=peer1-org2:9051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer1/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org2:9051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2/peer1\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org2/peer1:/tmp/hyperledger/org2/peer1\n    networks:\n      - fabric-ca\n</code></pre><p>启动它.</p>\n<pre><code>docker-compose -f docker-compose.yaml up peer1-org2</code></pre><p><code>peer2</code>节点的容器配置信息：</p>\n<pre><code>  peer2-org2:\n    container_name: peer2-org2\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer2-org2\n      - CORE_PEER_ADDRESS=peer2-org2:10051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer2/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer2/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer2/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer2/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org2:9051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2/peer2\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org2/peer2:/tmp/hyperledger/org2/peer2\n    networks:\n      - fabric-ca\n</code></pre><p>启动它.</p>\n<pre><code>docker-compose -f docker-compose.yaml up peer2-org2</code></pre><h3 id=\"3-3-排序节点配置\"><a href=\"#3-3-排序节点配置\" class=\"headerlink\" title=\"3.3 排序节点配置\"></a>3.3 排序节点配置</h3><p>接下来是排序节点的配置，为什么放在最后面呢，因为排序节点的启动需要提前生成创世区块，而创世区块的生成涉及到另一个配置文件，所以就先配置简单的<code>peer</code>节点。</p>\n<h4 id=\"3-3-1-orderer\"><a href=\"#3-3-1-orderer\" class=\"headerlink\" title=\"3.3.1 orderer\"></a>3.3.1 orderer</h4><ul>\n<li>配置环境变量<pre><code>#指定order节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/orderer\n#指定本组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem</code></pre></li>\n<li>登录<code>order</code>节点到<code>org0 CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://orderer-org0:ordererpw@0.0.0.0:7053</code></pre>接下来是<code>TLS</code>证书：</li>\n<li>配置环境变量<pre><code>#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org0/orderer/tls-msp\n#指定TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem</code></pre></li>\n<li>登录<code>orderer</code>节点到<code>TLS CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://orderer-org0:ordererPW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts orderer-org0</code></pre></li>\n<li>修改秘钥文件名<pre><code>cd $GOPATH/src/github.com/caDemo/org0/orderer/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo</code></pre><h4 id=\"3-3-2-admin\"><a href=\"#3-3-2-admin\" class=\"headerlink\" title=\"3.3.2 admin\"></a>3.3.2 admin</h4></li>\n<li>配置环境变量<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org0/adminuser/msp</code></pre></li>\n<li>登录<code>admin</code>用户获取<code>MSP</code>证书:<pre><code>fabric-ca-client enroll -d -u https://admin-org0:org0adminpw@0.0.0.0:7053</code></pre></li>\n<li>复制证书到<code>admincerts</code>文件夹:<pre><code>mkdir $GOPATH/src/github.com/caDemo/org0/orderer/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org0/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org0/orderer/msp/admincerts/orderer-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/</code></pre><h2 id=\"4-Fabric网络配置\"><a href=\"#4-Fabric网络配置\" class=\"headerlink\" title=\"4.Fabric网络配置\"></a>4.Fabric网络配置</h2></li>\n</ul>\n<hr>\n<p>接下来到重头戏了，证书都生成好了，即将要启动网络了。不过在启动网络之前还是有很多准备工作需要做。其实到这里，官方文档已经好多没有交代清楚的了，所以一下好多内容都是笔者自己摸索出来的，如有错误欢迎批评指正。</p>\n<h3 id=\"4-1-configtx-yaml文件配置\"><a href=\"#4-1-configtx-yaml文件配置\" class=\"headerlink\" title=\"4.1 configtx.yaml文件配置\"></a>4.1 configtx.yaml文件配置</h3><p>在下一个步骤的生成创世区块和通道配置信息需要一个文件：<code>configtx.yaml</code>文件。笔者根据官方的例子按照本文内容修改了一下，直接放在工作目录:</p>\n<pre><code>Organizations:\n  - &amp;orderer-org0\n    Name: orderer-org0\n    ID: org0MSP\n    MSPDir: ./org0/msp\n  #    Policies:\n  #      Readers:\n  #        Type: Signature\n  #        Rule: &quot;OR(&#39;orderer-org0MSP.member&#39;)&quot;\n  #      Writers:\n  #        Type: Signature\n  #        Rule: &quot;OR(&#39;orderer-org0MSP.member&#39;)&quot;\n  #      Admins:\n  #        Type: Signature\n  #        Rule: &quot;OR(&#39;orderer-org0MSP.admin&#39;)&quot;\n\n  - &amp;org1\n    Name: org1MSP\n    ID: org1MSP\n\n    MSPDir: ./org1/msp\n    #    Policies:\n    #      Readers:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org1MSP.admin&#39;, &#39;org1MSP.peer&#39;, &#39;org1MSP.client&#39;)&quot;\n    #      Writers:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org1MSP.admin&#39;, &#39;org1MSP.client&#39;)&quot;\n    #      Admins:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org1MSP.admin&#39;)&quot;\n    AnchorPeers:\n      - Host: peer1-org1\n        Port: 7051\n\n  - &amp;org2\n    Name: org2MSP\n    ID: org2MSP\n    MSPDir: ./org2/msp\n    #    Policies:\n    #      Readers:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org2MSP.admin&#39;, &#39;org2MSP.peer&#39;, &#39;org2MSP.client&#39;)&quot;\n    #      Writers:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org2MSP.admin&#39;, &#39;org2MSP.client&#39;)&quot;\n    #      Admins:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org2MSP.admin&#39;)&quot;\n\n    AnchorPeers:\n      - Host: peer1-org2\n        Port: 9051\n\nCapabilities:\n  Channel: &amp;ChannelCapabilities\n    V1_4_3: true\n    V1_3: false\n    V1_1: false\n  Orderer: &amp;OrdererCapabilities\n    V1_4_2: true\n    V1_1: false\n  Application: &amp;ApplicationCapabilities\n    V1_4_2: true\n    V1_3: false\n    V1_2: false\n    V1_1: false\n\nApplication: &amp;ApplicationDefaults\n  Organizations:\n  #  Policies:\n  #    Readers:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;ANY Readers&quot;\n  #    Writers:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;ANY Writers&quot;\n  #    Admins:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;MAJORITY Admins&quot;\n\n  Capabilities:\n    &lt;&lt;: *ApplicationCapabilities\n\nOrderer: &amp;OrdererDefaults\n  OrdererType: solo\n\n  Addresses:\n    - orderer-org0:7050\n  BatchTimeout: 2s\n  BatchSize:\n    MaxMessageCount: 10\n    AbsoluteMaxBytes: 99 MB\n    PreferredMaxBytes: 512 KB\n  Organizations:\n#  Policies:\n#    Readers:\n#      Type: ImplicitMeta\n#      Rule: &quot;ANY Readers&quot;\n#    Writers:\n#      Type: ImplicitMeta\n#      Rule: &quot;ANY Writers&quot;\n#    Admins:\n#      Type: ImplicitMeta\n#      Rule: &quot;MAJORITY Admins&quot;\n#    # BlockValidation specifies what signatures must be included in the block\n#    # from the orderer for the peer to validate it.\n#    BlockValidation:\n#      Type: ImplicitMeta\n#      Rule: &quot;ANY Writers&quot;\n\nChannel: &amp;ChannelDefaults\n  #  Policies:\n  #    # Who may invoke the &#39;Deliver&#39; API\n  #    Readers:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;ANY Readers&quot;\n  #    # Who may invoke the &#39;Broadcast&#39; API\n  #    Writers:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;ANY Writers&quot;\n  #    # By default, who may modify elements at this config level\n  #    Admins:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;MAJORITY Admins&quot;\n  Capabilities:\n    &lt;&lt;: *ChannelCapabilities\n\nProfiles:\n\n  TwoOrgsOrdererGenesis:\n    &lt;&lt;: *ChannelDefaults\n    Orderer:\n      &lt;&lt;: *OrdererDefaults\n      Organizations:\n        - *orderer-org0\n      Capabilities:\n        &lt;&lt;: *OrdererCapabilities\n    Consortiums:\n      SampleConsortium:\n        Organizations:\n          - *org1\n          - *org2\n  TwoOrgsChannel:\n    Consortium: SampleConsortium\n    &lt;&lt;: *ChannelDefaults\n    Application:\n      &lt;&lt;: *ApplicationDefaults\n      Organizations:\n        - *org1\n        - *org2\n      Capabilities:\n        &lt;&lt;: *ApplicationCapabilities</code></pre><p>注释掉的部分是策略部分，笔者还没有完全搞懂，所以索性就先注释掉了，以后搞懂了再添加进去。<br>还有一部分<code>msp</code>需要配置，就是<code>configtx.yaml</code>文件中第一部分指定的<code>MSPDir</code>,很简单，按照一下命令复制一下就好了：</p>\n<pre><code>#进入工作目录\ncd $GOPATH/src/github.com/caDemo\n############################################\n#org0\nmkdir org0/msp &amp;&amp;  cd org0/msp\nmkdir admincerts &amp;&amp; mkdir cacerts &amp;&amp; mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n############################################\n#org1\ncd $GOPATH/src/github.com/caDemo\nmkdir org1/msp/  &amp;&amp; cd org1/msp/\nmkdir admincerts &amp;&amp; mkdir cacerts &amp;&amp; mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n############################################\n#org2\ncd $GOPATH/src/github.com/caDemo\nmkdir org1/msp/  &amp;&amp; cd org1/msp/\nmkdir admincerts &amp;&amp; mkdir cacerts &amp;&amp; mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem</code></pre><h3 id=\"4-2-生成创世区块和通道配置信息\"><a href=\"#4-2-生成创世区块和通道配置信息\" class=\"headerlink\" title=\"4.2 生成创世区块和通道配置信息\"></a>4.2 生成创世区块和通道配置信息</h3><p>可以了，所有的前期工作都已经完成，接下来就是手动启动网络了，第一步，生成创世区块和通道配置信息：</p>\n<pre><code>cd $GOPATH/src/github.com/caDemo\nexport FABRIC_CFG_PATH=$PWD\n#生成创世区块\nconfigtxgen -profile TwoOrgsOrdererGenesis -outputBlock $GOPATH/src/github.com/caDemo/genesis.block\n#生成通道配置信息\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx $GOPATH/src/github.com/caDemo/channel.tx -channelID mychannel</code></pre><h3 id=\"4-3-启动Orderer节点\"><a href=\"#4-3-启动Orderer节点\" class=\"headerlink\" title=\"4.3 启动Orderer节点\"></a>4.3 启动Orderer节点</h3><p><code>orderer</code>容器配置文件：</p>\n<pre><code>  orderer-org0:\n    container_name: orderer-org0\n    image: hyperledger/fabric-orderer\n    environment:\n      - ORDERER_HOME=/tmp/hyperledger/orderer\n      - ORDERER_HOST=orderer-org0\n      - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0\n      - ORDERER_GENERAL_GENESISMETHOD=file\n      - ORDERER_GENERAL_GENESISFILE=/tmp/hyperledger/genesis.block\n      - ORDERER_GENERAL_LOCALMSPID=org0MSP\n      - ORDERER_GENERAL_LOCALMSPDIR=/tmp/hyperledger/org0/orderer/msp\n      - ORDERER_GENERAL_TLS_ENABLED=true\n      - ORDERER_GENERAL_TLS_CERTIFICATE=/tmp/hyperledger/org0/orderer/tls-msp/signcerts/cert.pem\n      - ORDERER_GENERAL_TLS_PRIVATEKEY=/tmp/hyperledger/org0/orderer/tls-msp/keystore/key.pem\n      - ORDERER_GENERAL_TLS_ROOTCAS=[/tmp/hyperledger/org0/orderer/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem]\n      - ORDERER_GENERAL_LOGLEVEL=debug\n      - ORDERER_DEBUG_BROADCASTTRACEDIR=data/logs\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org0/orderer:/tmp/hyperledger/org0/orderer/\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n</code></pre><p>关键部分到了，只要这一步没有出现错误，整个网络就启动成功了。</p>\n<pre><code>docker-compose -f docker-compose.yaml up orderer-org0</code></pre><h3 id=\"4-4-启动组织一的cli容器\"><a href=\"#4-4-启动组织一的cli容器\" class=\"headerlink\" title=\"4.4 启动组织一的cli容器\"></a>4.4 启动组织一的cli容器</h3><p><code>cli</code>容器内容,我们需要这个容器对组织1进行链码的交互：</p>\n<pre><code>  cli-org1:\n    container_name: cli-org1\n    image: hyperledger/fabric-tools\n    tty: true\n    stdin_open: true\n    environment:\n      - SYS_CHANNEL=testchainid\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_ID=cli-org1\n      - CORE_PEER_ADDRESS=peer1-org1:7051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer1/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1\n    command: /bin/bash\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org1/peer1:/tmp/hyperledger/org1/peer1\n      - $GOPATH/src/github.com/caDemo/org1/peer1/assets/chaincode:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - $GOPATH/src/github.com/caDemo/org1/adminuser:/tmp/hyperledger/org1/adminuser\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    depends_on:\n      - peer1-org1</code></pre><p>启动该容器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up cli-org1</code></pre><h3 id=\"4-5-启动组织二的cli容器\"><a href=\"#4-5-启动组织二的cli容器\" class=\"headerlink\" title=\"4.5 启动组织二的cli容器\"></a>4.5 启动组织二的cli容器</h3><p><code>cli</code>容器内容,我们需要这个容器对组织2进行链码的交互：</p>\n<pre><code>  cli-org2:\n    container_name: cli-org2\n    image: hyperledger/fabric-tools\n    tty: true\n    stdin_open: true\n    environment:\n      - SYS_CHANNEL=testchainid\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_ID=cli-org2\n      - CORE_PEER_ADDRESS=peer1-org2:9051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer1/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2\n    command: /bin/bash\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org2/peer1:/tmp/hyperledger/org2/peer1\n      - $GOPATH/src/github.com/caDemo/org2/peer1/assets/chaincode:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - $GOPATH/src/github.com/caDemo/org2/adminuser:/tmp/hyperledger/org2/adminuser\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    depends_on:\n      - peer1-org2</code></pre><p>启动该容器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up cli-org2</code></pre><h2 id=\"5-网络测试\"><a href=\"#5-网络测试\" class=\"headerlink\" title=\"5.网络测试\"></a>5.网络测试</h2><hr>\n<p>所有工作准备完成，接下来让我们测试整个网络能不能正常运行吧：</p>\n<h3 id=\"5-1-创建与加入通道\"><a href=\"#5-1-创建与加入通道\" class=\"headerlink\" title=\"5.1 创建与加入通道\"></a>5.1 创建与加入通道</h3><p>以<strong>组织1</strong>为例：</p>\n<ul>\n<li><p>首先进入<code>cli</code>容器：</p>\n<pre><code>docker exec -it cli bash\n#配置环境变量\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp</code></pre></li>\n<li><p>创建通道</p>\n<pre><code>peer channel create -c mychannel -f /tmp/hyperledger/channel.tx -o orderer-org0:7050 --outputBlock /tmp/hyperledger/mychannel.block --tls --cafile /tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem</code></pre></li>\n<li><p>将<code>peer1-org1</code>加入通道：</p>\n<pre><code>export CORE_PEER_ADDRESS=peer1-org1:7051\npeer channel join -b /tmp/hyperledger/mychannel.block</code></pre></li>\n<li><p>将<code>peer2-org1</code>加入通道：</p>\n<pre><code>export CORE_PEER_ADDRESS=peer2-org1:8051\npeer channel join -b /tmp/hyperledger/mychannel.block</code></pre><p>组织二步骤是相同的，唯一不同的就是不需要创建通道了，所以就不再说明了。</p>\n<h3 id=\"5-2-安装和实例化链码\"><a href=\"#5-2-安装和实例化链码\" class=\"headerlink\" title=\"5.2 安装和实例化链码\"></a>5.2 安装和实例化链码</h3><p>以<strong>组织1</strong>为例：</p>\n</li>\n<li><p>首先进入<code>cli</code>容器：</p>\n<pre><code>docker exec -it cli bash\n#配置环境变量\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\nexport CORE_PEER_ADDRESS=peer1-org1:7051</code></pre></li>\n<li><p>安装链码</p>\n</li>\n<li><p><em>记得提前将链码放到*</em><code>$GOPATH/src/github.com/caDemo/org1/peer1/assets/chaincode</code><strong>路径下。</strong>,本文使用的是<code>fabric-samples/chaincode/chaincode_example02</code>官方示例链码。</p>\n<pre><code>peer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric-samples/chaincode/chaincode_example02/go/</code></pre></li>\n<li><p>实例化链码</p>\n<pre><code>peer chaincode instantiate -C mychannel -n mycc -v 1.0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;100&quot;,&quot;b&quot;,&quot;200&quot;]}&#39;  -o orderer-org0:7050 --tls --cafile /tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem</code></pre></li>\n<li><p>这一步在高版本的Fabric网络是会出错的，因为少了一个文件<code>config.yaml</code>:</p>\n</li>\n</ul>\n<pre><code>NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: orderer</code></pre><p>因为高版本的Fabric把节点类型区分开了，所以需要我们手动配置。<br>将该文件复制到<code>$GOPATH/src/github.com/caDemo/org1/adminuser/msp</code>文件夹内，同时修改上面指定的位置的文件名(与对应文件夹内的文件名对应就好了)。</p>\n<ul>\n<li><p>实例化部分出错的可能性是最高的，很多都是因为网络模式指定错误导致链码容器启动失败，解决方案：</p>\n<pre><code>#终端执行命令\ndocker network ls</code></pre><p>找到以<code>fabric-ca</code>为后缀的一条如<code>cademo_fabric-ca</code>,修改之前的所有<code>peer</code>节点容器配置文件的环境变量：</p>\n<pre><code>CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca</code></pre><p>修改完成重启节点容器，再次执行以上的命令(需要重新配置环境变量，加入通道这两个操作)。<br>终于，实例化成功了。</p>\n<h3 id=\"5-3-调用和查询链码\"><a href=\"#5-3-调用和查询链码\" class=\"headerlink\" title=\"5.3 调用和查询链码\"></a>5.3 调用和查询链码</h3><p>最后测试一下链码功能能不能正常使用了：</p>\n</li>\n<li><p>还是组织一的<code>cli</code>容器：</p>\n<pre><code>docker exec -it cli bash\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\nexport CORE_PEER_ADDRESS=peer1-org1:7051</code></pre></li>\n<li><p>执行查询功能：</p>\n<pre><code>peer chaincode query -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;</code></pre><p>命令行应该打印出:</p>\n<pre><code>100</code></pre></li>\n<li><p>执行调用功能：</p>\n<pre><code>peer chaincode invoke -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]}&#39; --tls --cafile /tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem</code></pre></li>\n<li><p>再次查询：</p>\n<pre><code>peer chaincode query -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;</code></pre><p>命令行应该打印出:</p>\n<pre><code>90</code></pre><p>至于其他节点操作方法也是一样的，就不再操作了。<br>到此为止，从零开始的手动生成证书一直到成功搭建Fabric网络全部步骤已经完成！！接下来还有更新锚节点等等就不再演示了，请各位读者自行操作。整个步骤是不容易的，而且BUG百出，不过成功搭建完成确实涨了不少知识。<br>码字不易，还望各位看官支持一下：</p>\n<img src=\"/img/blog/zfb.png\" srcset=\"undefined\" width = \"300\" height = \"300\" alt=\"支付宝\" align=center />\n\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>之前介绍了使用官方脚本自动化启动一个Fabric网络，并且所有的证书都是通过官方的命令行工具<code>cryptogen</code>直接生成网络中的所有节点的证书。在开发环境可以这么简单进行，但是生成环境下还是需要我们自定义对网络中的节点的证书进行配置。<br>所以在本文中，将会详细介绍一个从手动生成证书一直到启动网络的整体步骤。本文只交代整体的搭建步骤。对于Fabric-Ca的讲解不在本文的范围内，将在另一篇文章中说明。<br>正篇文章也是根据官方的文档进行的。但是由于官方的文档尚未完工，也是好多没有交代清楚的，并且有些地方是错误的，所以笔者也是一步一步摸索出来的，所以如果本文哪里没有交代清楚或者错误的地方，希望各位批评指正。<br>在这里贴出<a href=\"https://hyperledger-fabric-ca.readthedocs.io/en/latest/operations_guide.html\" target=\"_blank\" rel=\"noopener\">官方文档</a>地址.</p>\n<h2 id=\"1-整体架构\"><a href=\"#1-整体架构\" class=\"headerlink\" title=\"1.整体架构\"></a>1.整体架构</h2><hr>\n<p>架构图直接贴过来好了：<br><img src=\"/img/blog/arth.png\" srcset=\"undefined\" alt=\"系统架构\"></p>\n<p>官方文档采用的是多机环境，这里简洁化一点，所有的操作都在<strong>一台机器</strong>上进行，至于多机环境，以后再补充好了。<br>介绍一下本文所采用的整体架构：</p>\n<ol>\n<li>三个组织<ol>\n<li>Org0  -&gt; 组织0   </li>\n<li>Org1  -&gt; 组织1   </li>\n<li>Org2  -&gt; 组织2   </li>\n</ol>\n</li>\n<li>组织中的成员<ol>\n<li>Org0   一个Orderer节点，一个Org0的Admin节点</li>\n<li>Org1   两个Peer节点，一个Org1的Admin节点，一个Org1的User节点</li>\n<li>Org2   两个Peer节点，一个Org2的Admin节点，一个Org2的User节点</li>\n</ol>\n</li>\n<li>共有四台CA服务器<ol>\n<li>TLS服务器   -&gt;  为网络中所有节点颁发TLS证书，用于通信的加密</li>\n<li>Org1的CA服务器 -&gt; 为组织1中所有用户颁发证书</li>\n<li>Org2的Ca服务器 -&gt; 为组织2中所有用户颁发证书</li>\n<li>Org0的CA服务器 -&gt; 为组织0中所有用户颁发证书</li>\n</ol>\n</li>\n</ol>\n<p>这里的四台CA服务器都是根服务器。<strong>彼此之间都是独立的存在，没有任何关系。</strong>，也就是说每一个CA服务器生成的证书在其他CA服务器都是不能用的。<br>介绍完之后，可以进入正题了。</p>\n<h3 id=\"1-1Fabric，Fabric-Ca安装\"><a href=\"#1-1Fabric，Fabric-Ca安装\" class=\"headerlink\" title=\"1.1Fabric，Fabric-Ca安装\"></a>1.1Fabric，Fabric-Ca安装</h3><p>本文默认读者都是对Fabric有一定的了解的，所以一些安装过程这里就没有重复说明。<br>第一步是安装Fabric-Ca环境，可以参考<a href=\"https://ifican.top/2019/12/08/blog/fabric/Hyperledger_Fabric_CA/\" target=\"_blank\" rel=\"noopener\">这里</a>,这篇文章还没有写完，以后慢慢补，不过环境的安装已经有说明。<br>还有就是Fabric的环境安装，可以参考<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">这里</a>。</p>\n<p>完成环境搭建后，我们还需要一个<code>HOME</code>文件夹，用于存放我们生成的证书文件与<code>fabric</code>配置相关的文件。<br>本文设置<code>HOME</code>文件夹路径为:</p>\n<pre><code>$GOPATH/src/github.com/caDemo/</code></pre><p>请读者自行创建，一般不要用太复杂的路径,也不要用中文路径，会为之后的操作带来很多麻烦。在下文中简单称<code>HOME</code>文件夹为<strong>工作目录</strong>,<strong>除非特殊说明，一般命令的执行都是在工作目录进行</strong>。</p>\n<h2 id=\"2-CA服务器配置\"><a href=\"#2-CA服务器配置\" class=\"headerlink\" title=\"2 CA服务器配置\"></a>2 CA服务器配置</h2><hr>\n<h3 id=\"2-1启动TLS-CA服务器\"><a href=\"#2-1启动TLS-CA服务器\" class=\"headerlink\" title=\"2.1启动TLS CA服务器\"></a>2.1启动TLS CA服务器</h3><p>前期工作准备好之后，我们开始启动第一台CA服务器。本文中使用<code>Docker</code>容器启动。<br>首先在工作目录创建<code>docker-compose.yaml</code>文件：</p>\n<pre><code>touch docker-compose.yaml</code></pre><p>并在文件内添加以下内容(tips:内容格式不要乱掉)：</p>\n<pre><code>version: &#39;2&#39;\n\nnetworks:\n  fabric-ca:\n\nservices:\n\n  ca-tls:\n    container_name: ca-tls\n    image: hyperledger/fabric-ca\n    command: sh -c &#39;fabric-ca-server start -d -b tls-ca-admin:tls-ca-adminpw --port 7052&#39;\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/tls\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=ca-tls\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca  ##重要！！！记得修改这里的路径为自己的工作目录\n    networks:\n      - fabric-ca\n    ports:\n      - 7052:7052\n</code></pre><p>启动该<code>docker</code>容器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up ca-tls</code></pre><p>如果命令行出现以下内容则说明启动成功：</p>\n<pre><code>[INFO] Listening on https://0.0.0.0:7052</code></pre><p>同时工作目录下会出现一个<code>tls</code>的文件夹。文件夹中的内容暂先不解释，留着在另一篇文章中说明。不过有一个文件需要解释一下，因为之后会用到。<br>在<code>$GOPATH/src/github.com/caDemo/tls/</code>路径下的<code>ca-cert.pem</code>文件。这是<code>TLS CA</code>服务器签名的根证书，目的是用来对<code>CA</code>的<code>TLS</code>证书进行验证，同时也需要持有这个证书才可以进行证书的颁发。在多机环境下，我们需要将它复制到每一台机器上，不过本文采用的是单机环境，所以省略掉了这一步。</p>\n<h3 id=\"2-2-TLS-CA服务器注册用户\"><a href=\"#2-2-TLS-CA服务器注册用户\" class=\"headerlink\" title=\"2.2 TLS CA服务器注册用户\"></a>2.2 TLS CA服务器注册用户</h3><p>第一步是在TLS CA服务器中注册用户，经过注册的用户才拥有TLS证书，本文中由于只在各节点之间进行TLS加密通信，所以只将<code>orderer</code>和<code>peer</code>节点的身份注册到TLS服务器。<br>打开一个新的终端输入以下命令：</p>\n<pre><code>#设置环境变量指定根证书的路径(如果工作目录不同的话记得指定自己的工作目录,以下不再重复说明)\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#设置环境变量指定CA客户端的HOME文件夹\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/tls/admin\n#登录管理员用户用于之后的节点身份注册\nfabric-ca-client enroll -d -u https://tls-ca-admin:tls-ca-adminpw@0.0.0.0:7052</code></pre><p>登录成功在工作目录下的<code>tls</code>文件夹下将出现一个<code>admin</code>文件夹，这里面是<code>admin</code>的相关证书文件.<br>并且只有登录了<code>admin</code>，才具有权限进行用户的注册,因为该用户具有CA的全部权限，相当于CA服务器的<code>root</code>用户。<br>接下来对各个节点进行注册。</p>\n<pre><code>fabric-ca-client register -d --id.name peer1-org1 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer2-org1 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer1-org2 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name peer2-org2 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7052\nfabric-ca-client register -d --id.name orderer-org0 --id.secret ordererPW --id.type orderer -u https://0.0.0.0:7052</code></pre><p>这里将三个组织中的节点都进行了注册。</p>\n<ul>\n<li>不过<code>-d</code>这个参数并没有找到相关资料 </li>\n<li><code>id.name</code>是指定用户的名称</li>\n<li><code>--id.secert</code>是指定密码</li>\n<li><code>--id.type</code>是指定用户类型，用户类型默认为<code>client</code>,主要包括<code>peer</code>,<code>app</code>,<code>user</code>,<code>orderer</code>.</li>\n<li><code>-u</code>则是指定请求CA服务器的URL。</li>\n</ul>\n<p>这里我们为各个节点注册TLS证书，之后Fabric网络的通信则需要通过这一步骤注册过的用户的TLS证书来进行TLS加密通信。<br>到这里我们只是注册了各个节点的身份，还没有获取到他们的证书。证书可以通过登录获取，不过暂时不着急获取他们的TLS证书。<br>接下来，我们对其他几个CA服务器进行配置。</p>\n<h3 id=\"2-3配置Org0的CA服务器\"><a href=\"#2-3配置Org0的CA服务器\" class=\"headerlink\" title=\"2.3配置Org0的CA服务器\"></a>2.3配置Org0的CA服务器</h3><p>再强调一下，本文中的几个CA服务器都是根服务器，彼此之间没有任何关系，所以上一步骤的TLS CA服务器在这一部分并没有用到。<br>同样，本文使用Docker容器启动CA服务器。配置文件如下，只需要添加进之前的<code>docker-compose.yaml</code>文件中就好：</p>\n<pre><code>  org0:\n    container_name: org0\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c &#39;fabric-ca-server start -d -b org0-admin:org0-adminpw --port 7053&#39;\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org0/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org0\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7053:7053\n</code></pre><p>添加完之后启动它：</p>\n<pre><code>docker-compose -f docker-compose.yaml up org0</code></pre><p>打开另一个终端，接下来注册org0的用户：</p>\n<pre><code>#首先指定环境变量，这里的TLS证书不是之前的TLS CA服务器的根证书，而是本组织CA服务器启动时生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\n#指定本组织的CA客户端工作目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/admin</code></pre><p>登录<code>org0</code>的CA服务器管理员身份用于注册本组织的用户：</p>\n<pre><code>fabric-ca-client enroll -d -u https://org0-admin:org0-adminpw@0.0.0.0:7053</code></pre><p>在本组织中共有两个用户：<code>orderer</code>节点和<code>admin</code>用户(这里的admin和管理员是不同的。)<br>将他们注册到org0的CA服务器：</p>\n<pre><code>fabric-ca-client register -d --id.name orderer-org0 --id.secret ordererpw --id.type orderer -u https://0.0.0.0:7053\nfabric-ca-client register -d --id.name admin-org0 --id.secret org0adminpw --id.type admin --id.attrs &quot;hf.Registrar.Roles=client,hf.Registrar.Attributes=*,hf.Revoker=true,hf.GenCRL=true,admin=true:ecert,abac.init=true:ecert&quot; -u https://0.0.0.0:7053</code></pre><p>命令执行完之后，将会注册一个Orderer节点的身份和一个Admin的身份。同时在工作目录下的<code>org0</code>子文件夹中会有两个文件夹：<code>crypto</code>和<code>admin</code>。<code>crypto</code>中是CA服务器的配置信息，<code>admin</code>是服务器管理员的身份信息。</p>\n<h3 id=\"2-4配置Org1的CA服务器\"><a href=\"#2-4配置Org1的CA服务器\" class=\"headerlink\" title=\"2.4配置Org1的CA服务器\"></a>2.4配置Org1的CA服务器</h3><p>同样的步骤，对org1组织的CA服务器进行配置：</p>\n<pre><code>  org1:\n    container_name: org1\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c &#39;fabric-ca-server start -d -b org1-admin:org1-adminpw&#39;\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org1/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org1\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7054:7054\n</code></pre><p>启动服务器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up org1</code></pre><p>打开新的终端，配置环境变量：</p>\n<pre><code>export FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/admin</code></pre><p>登录CA服务器管理员身份：</p>\n<pre><code>fabric-ca-client enroll -d -u https://org1-admin:org1-adminpw@0.0.0.0:7054</code></pre><p>组织一种共有四个用户：<code>peer1</code>,<code>peer2</code>,<code>admin</code>,<code>user</code>,分别注册他们：</p>\n<pre><code>fabric-ca-client register -d --id.name peer1-org1 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name peer2-org1 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name admin-org1 --id.secret org1AdminPW --id.type user -u https://0.0.0.0:7054\nfabric-ca-client register -d --id.name user-org1 --id.secret org1UserPW --id.type user -u https://0.0.0.0:7054</code></pre><h3 id=\"2-5配置Org2的CA服务器\"><a href=\"#2-5配置Org2的CA服务器\" class=\"headerlink\" title=\"2.5配置Org2的CA服务器\"></a>2.5配置Org2的CA服务器</h3><p>和上一部分相同，这里只列举需要的命令：<br>CA服务器配置文件：</p>\n<pre><code>  org2:\n    container_name: org2\n    image: hyperledger/fabric-ca\n    command: /bin/bash -c &#39;fabric-ca-server start -d -b org2-admin:org2-adminpw --port 7055&#39;\n    environment:\n      - FABRIC_CA_SERVER_HOME=/ca/org2/crypto\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_CSR_CN=org2\n      - FABRIC_CA_SERVER_CSR_HOSTS=0.0.0.0\n      - FABRIC_CA_SERVER_DEBUG=true\n    volumes:\n      - $GOPATH/src/github.com/caDemo:/ca\n    networks:\n      - fabric-ca\n    ports:\n      - 7055:7055\n</code></pre><p>启动服务器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up org2</code></pre><p>打开新的终端，配置环境变量：</p>\n<pre><code>export FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/admin</code></pre><p>登录CA服务器管理员身份：</p>\n<pre><code>fabric-ca-client enroll -d -u https://org2-admin:org2-adminpw@0.0.0.0:7055</code></pre><p>组织一种共有四个用户：<code>peer1</code>,<code>peer2</code>,<code>admin</code>,<code>user</code>,分别注册他们：</p>\n<pre><code>fabric-ca-client register -d --id.name peer1-org2 --id.secret peer1PW --id.type peer -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name peer2-org2 --id.secret peer2PW --id.type peer -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name admin-org2 --id.secret org1AdminPW --id.type user -u https://0.0.0.0:7055\nfabric-ca-client register -d --id.name user-org2 --id.secret org1UserPW --id.type user -u https://0.0.0.0:7055</code></pre><h2 id=\"3-生成证书并配置TLS\"><a href=\"#3-生成证书并配置TLS\" class=\"headerlink\" title=\"3.生成证书并配置TLS\"></a>3.生成证书并配置TLS</h2><hr>\n<p>到目前为止，所有的用户我们都注册完毕，接下来就是为每一个用户生成证书并配置TLS证书。<br>其中证书分为两部分，分别是本组织的MSP证书，以及组织之间进行加密通信的TLS证书。<br>所以本文需要对两部分证书进行分别生成与配置。<br>从组织一开始：</p>\n<h3 id=\"3-1-组织一节点配置\"><a href=\"#3-1-组织一节点配置\" class=\"headerlink\" title=\"3.1 组织一节点配置\"></a>3.1 组织一节点配置</h3><h4 id=\"3-1-1-peer1\"><a href=\"#3-1-1-peer1\" class=\"headerlink\" title=\"3.1.1 peer1\"></a>3.1.1 peer1</h4><p>首先是本组织的<code>MSP</code>证书：</p>\n<ul>\n<li>配置环境变量<pre><code>#指定peer1节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/peer1\n#指定**本**组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem</code></pre></li>\n<li>登录<code>peer1</code>节点到<code>org1 CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://peer1-org1:peer1PW@0.0.0.0:7054</code></pre>这一步完成后，在<code>$GOPATH/src/github.com/caDemo/org1/peer1</code>下会出现一个<code>msp</code>文件夹，这是<code>peer1</code>节点的<code>MSP</code>证书。<br>接下来是<code>TLS</code>证书：</li>\n<li>配置环境变量<pre><code>#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#指定TLS证书的HOME目录\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/peer1/tls-msp</code></pre></li>\n<li>登录<code>peer1</code>节点到<code>TLS CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://peer1-org1:peer1PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer1-org1</code></pre>这一步完成后，在<code>$GOPATH/src/github.com/caDemo/org1/peer1</code>下会出现一个<code>tls-msp</code>文件夹，这是<code>peer1</code>节点的<code>TLS</code>证书。</li>\n<li>修改秘钥文件名<br>为什么要修改呢，进入这个文件夹看一下就知道了,由服务器生成的秘钥文件名是一长串无规则的字符串，后期我们使用的时候难道要一个字符一个字符地输入？<pre><code>cd $GOPATH/src/github.com/caDemo/org1/peer1/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo</code></pre><h4 id=\"3-1-2-peer2\"><a href=\"#3-1-2-peer2\" class=\"headerlink\" title=\"3.1.2 peer2\"></a>3.1.2 peer2</h4><code>peer2</code>节点和上面步骤相同：<br>这里就直接放需要的命令了：</li>\n<li>生成<code>MSP</code>证书<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/peer2\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org1:peer2PW@0.0.0.0:7054</code></pre></li>\n<li>生成<code>TLS</code>证书<pre><code>export FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/peer2/tls-msp\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org1:peer2PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer2-org1\ncd $GOPATH/src/github.com/caDemo/org1/peer2/tls-msp/keystore/\nmv *_sk key.pem</code></pre><h4 id=\"3-1-3-admin\"><a href=\"#3-1-3-admin\" class=\"headerlink\" title=\"3.1.3 admin\"></a>3.1.3 admin</h4>接下来是<code>admin</code>用户，这个用户有什么作用呢，实际上，安装和实例化链码都需要<code>admin</code>的证书，所以才需要注册一个<code>admin</code>用户，还要它的证书。</li>\n<li>配置环境变量<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org1/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org1/crypto/ca-cert.pem\n#这里多了一个环境变量，是指定admin用户的msp证书文件夹的\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org1/adminuser/msp</code></pre></li>\n<li>登录<code>admin</code>用户获取<code>MSP</code>证书:<pre><code>fabric-ca-client enroll -d -u https://admin-org1:org1AdminPW@0.0.0.0:7054</code></pre>因为我们生成这个用户的证书主要就是为了之后链码的安装和实例化，所以配不配置他的<code>TLS</code>证书也无关紧要了(关键是我们之前也没有将这个用户注册到<code>tls</code>服务器中)</li>\n<li>复制证书到<code>admincerts</code>文件夹:<br>去看Fabric官方的例子，每一个<code>peer</code>节点的<code>MSP</code>文件夹下都有<code>admincerts</code>这个子文件夹的，而且是需要我们手动创建的。<pre><code>mkdir -p $GOPATH/src/github.com/caDemo/org1/peer1/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org1/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org1/peer1/msp/admincerts/org1-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/</code></pre><h4 id=\"3-1-4-启动peer节点\"><a href=\"#3-1-4-启动peer节点\" class=\"headerlink\" title=\"3.1.4 启动peer节点\"></a>3.1.4 启动peer节点</h4>到这里，已经配置好了一个节点，所以我们就可以启动这个节点了，当然在之后和<code>orderer</code>节点一起启动也可以，不过忙活了这么多，还是应该提前看到一下所做的工作的成果的！<br>附上<code>peer1</code>节点的容器配置信息：</li>\n</ul>\n<pre><code>  peer1-org1:\n    container_name: peer1-org1\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer1-org1\n      - CORE_PEER_ADDRESS=peer1-org1:7051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer1/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org1:7051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1/peer1\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org1/peer1:/tmp/hyperledger/org1/peer1\n    networks:\n      - fabric-ca\n</code></pre><p>启动它！！</p>\n<pre><code>docker-compose -f docker-compose.yaml up peer1-org1</code></pre><p>如果没有报错的话，说明之前配置的没有什么问题，如果出错的话，则需要返回去检查一下了。。。<br><code>peer2</code>节点的容器配置信息：</p>\n<pre><code>  peer2-org1:\n    container_name: peer2-org1\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer2-org1\n      - CORE_PEER_ADDRESS=peer2-org1:8051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer2/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer2/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer2/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer2/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org1:7051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1/peer2\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org1/peer2:/tmp/hyperledger/org1/peer2\n    networks:\n      - fabric-ca\n</code></pre><p>启动它！！</p>\n<pre><code>docker-compose -f docker-compose.yaml up peer2-org1</code></pre><h3 id=\"3-2-组织二节点配置\"><a href=\"#3-2-组织二节点配置\" class=\"headerlink\" title=\"3.2 组织二节点配置\"></a>3.2 组织二节点配置</h3><p>和之前一样的步骤，所以没什么好解释的了：</p>\n<h4 id=\"3-2-1-peer1\"><a href=\"#3-2-1-peer1\" class=\"headerlink\" title=\"3.2.1 peer1\"></a>3.2.1 peer1</h4><ul>\n<li>配置环境变量<pre><code>#指定peer2节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/peer1\n#指定本组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem</code></pre></li>\n<li>登录<code>peer1</code>节点到<code>org2 CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://peer1-org2:peer1PW@0.0.0.0:7055</code></pre>接下来是<code>TLS</code>证书：</li>\n<li>配置环境变量<pre><code>#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\n#指定TLS证书的HOME目录\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/peer1/tls-msp</code></pre></li>\n<li>登录<code>peer1</code>节点到<code>TLS CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://peer1-org2:peer1PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer1-org2</code></pre></li>\n<li>修改秘钥文件名<pre><code>cd $GOPATH/src/github.com/caDemo/org2/peer1/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo</code></pre><h4 id=\"3-2-2-peer2\"><a href=\"#3-2-2-peer2\" class=\"headerlink\" title=\"3.2.2 peer2\"></a>3.2.2 peer2</h4></li>\n<li>生成<code>MSP</code>证书<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/peer2\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org2:peer2PW@0.0.0.0:7055</code></pre></li>\n<li>生成<code>TLS</code>证书<pre><code>export FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/peer2/tls-msp\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem\nfabric-ca-client enroll -d -u https://peer2-org2:peer2PW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts peer2-org2\ncd $GOPATH/src/github.com/caDemo/org2/peer2/tls-msp/keystore/\nmv *_sk key.pem</code></pre><h4 id=\"3-2-3-admin\"><a href=\"#3-2-3-admin\" class=\"headerlink\" title=\"3.2.3 admin\"></a>3.2.3 admin</h4></li>\n<li>配置环境变量<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org2/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org2/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org2/adminuser/msp</code></pre></li>\n<li>登录<code>admin</code>用户获取<code>MSP</code>证书:<pre><code>fabric-ca-client enroll -d -u https://admin-org2:org2AdminPW@0.0.0.0:7055</code></pre></li>\n<li>复制证书到<code>admincerts</code>文件夹:<br>去看Fabric官方的例子，每一个<code>peer</code>节点的<code>MSP</code>文件夹下都有<code>admincerts</code>这个子文件夹的，而且是需要我们手动创建的。<pre><code>mkdir -p $GOPATH/src/github.com/caDemo/org2/peer1/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org2/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org2/peer1/msp/admincerts/org2-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/</code></pre><h4 id=\"3-2-4-启动peer节点\"><a href=\"#3-2-4-启动peer节点\" class=\"headerlink\" title=\"3.2.4 启动peer节点\"></a>3.2.4 启动peer节点</h4>附上<code>peer1</code>节点的容器配置信息：</li>\n</ul>\n<pre><code>  peer1-org2:\n    container_name: peer1-org2\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer1-org2\n      - CORE_PEER_ADDRESS=peer1-org2:9051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer1/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org2:9051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2/peer1\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org2/peer1:/tmp/hyperledger/org2/peer1\n    networks:\n      - fabric-ca\n</code></pre><p>启动它.</p>\n<pre><code>docker-compose -f docker-compose.yaml up peer1-org2</code></pre><p><code>peer2</code>节点的容器配置信息：</p>\n<pre><code>  peer2-org2:\n    container_name: peer2-org2\n    image: hyperledger/fabric-peer\n    environment:\n      - CORE_PEER_ID=peer2-org2\n      - CORE_PEER_ADDRESS=peer2-org2:10051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer2/msp\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca\n      - FABRIC_LOGGING_SPEC=debug\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer2/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer2/tls-msp/keystore/key.pem\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer2/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1-org2:9051\n      - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2/peer2\n    volumes:\n      - /var/run:/host/var/run\n      - $GOPATH/src/github.com/caDemo/org2/peer2:/tmp/hyperledger/org2/peer2\n    networks:\n      - fabric-ca\n</code></pre><p>启动它.</p>\n<pre><code>docker-compose -f docker-compose.yaml up peer2-org2</code></pre><h3 id=\"3-3-排序节点配置\"><a href=\"#3-3-排序节点配置\" class=\"headerlink\" title=\"3.3 排序节点配置\"></a>3.3 排序节点配置</h3><p>接下来是排序节点的配置，为什么放在最后面呢，因为排序节点的启动需要提前生成创世区块，而创世区块的生成涉及到另一个配置文件，所以就先配置简单的<code>peer</code>节点。</p>\n<h4 id=\"3-3-1-orderer\"><a href=\"#3-3-1-orderer\" class=\"headerlink\" title=\"3.3.1 orderer\"></a>3.3.1 orderer</h4><ul>\n<li>配置环境变量<pre><code>#指定order节点的HOME目录\nexport FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/orderer\n#指定本组织的TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem</code></pre></li>\n<li>登录<code>order</code>节点到<code>org0 CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://orderer-org0:ordererpw@0.0.0.0:7053</code></pre>接下来是<code>TLS</code>证书：</li>\n<li>配置环境变量<pre><code>#指定TLS CA服务器生成的TLS根证书\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org0/orderer/tls-msp\n#指定TLS根证书\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/tls/ca-cert.pem</code></pre></li>\n<li>登录<code>orderer</code>节点到<code>TLS CA</code>服务器上：<pre><code>fabric-ca-client enroll -d -u https://orderer-org0:ordererPW@0.0.0.0:7052 --enrollment.profile tls --csr.hosts orderer-org0</code></pre></li>\n<li>修改秘钥文件名<pre><code>cd $GOPATH/src/github.com/caDemo/org0/orderer/tls-msp/keystore/\nmv *_sk key.pem\n#修改完回到工作目录\ncd $GOPATH/src/github.com/caDemo</code></pre><h4 id=\"3-3-2-admin\"><a href=\"#3-3-2-admin\" class=\"headerlink\" title=\"3.3.2 admin\"></a>3.3.2 admin</h4></li>\n<li>配置环境变量<pre><code>export FABRIC_CA_CLIENT_HOME=$GOPATH/src/github.com/caDemo/org0/adminuser\nexport FABRIC_CA_CLIENT_TLS_CERTFILES=$GOPATH/src/github.com/caDemo/org0/crypto/ca-cert.pem\nexport FABRIC_CA_CLIENT_MSPDIR=$GOPATH/src/github.com/caDemo/org0/adminuser/msp</code></pre></li>\n<li>登录<code>admin</code>用户获取<code>MSP</code>证书:<pre><code>fabric-ca-client enroll -d -u https://admin-org0:org0adminpw@0.0.0.0:7053</code></pre></li>\n<li>复制证书到<code>admincerts</code>文件夹:<pre><code>mkdir $GOPATH/src/github.com/caDemo/org0/orderer/msp/admincerts\n#将签名证书拷贝过去\ncp $GOPATH/src/github.com/caDemo/org0/adminuser/msp/signcerts/cert.pem $GOPATH/src/github.com/caDemo/org0/orderer/msp/admincerts/orderer-admin-cert.pem\n#回到工作目录\ncd $GOPATH/src/github.com/caDemo/</code></pre><h2 id=\"4-Fabric网络配置\"><a href=\"#4-Fabric网络配置\" class=\"headerlink\" title=\"4.Fabric网络配置\"></a>4.Fabric网络配置</h2></li>\n</ul>\n<hr>\n<p>接下来到重头戏了，证书都生成好了，即将要启动网络了。不过在启动网络之前还是有很多准备工作需要做。其实到这里，官方文档已经好多没有交代清楚的了，所以一下好多内容都是笔者自己摸索出来的，如有错误欢迎批评指正。</p>\n<h3 id=\"4-1-configtx-yaml文件配置\"><a href=\"#4-1-configtx-yaml文件配置\" class=\"headerlink\" title=\"4.1 configtx.yaml文件配置\"></a>4.1 configtx.yaml文件配置</h3><p>在下一个步骤的生成创世区块和通道配置信息需要一个文件：<code>configtx.yaml</code>文件。笔者根据官方的例子按照本文内容修改了一下，直接放在工作目录:</p>\n<pre><code>Organizations:\n  - &amp;orderer-org0\n    Name: orderer-org0\n    ID: org0MSP\n    MSPDir: ./org0/msp\n  #    Policies:\n  #      Readers:\n  #        Type: Signature\n  #        Rule: &quot;OR(&#39;orderer-org0MSP.member&#39;)&quot;\n  #      Writers:\n  #        Type: Signature\n  #        Rule: &quot;OR(&#39;orderer-org0MSP.member&#39;)&quot;\n  #      Admins:\n  #        Type: Signature\n  #        Rule: &quot;OR(&#39;orderer-org0MSP.admin&#39;)&quot;\n\n  - &amp;org1\n    Name: org1MSP\n    ID: org1MSP\n\n    MSPDir: ./org1/msp\n    #    Policies:\n    #      Readers:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org1MSP.admin&#39;, &#39;org1MSP.peer&#39;, &#39;org1MSP.client&#39;)&quot;\n    #      Writers:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org1MSP.admin&#39;, &#39;org1MSP.client&#39;)&quot;\n    #      Admins:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org1MSP.admin&#39;)&quot;\n    AnchorPeers:\n      - Host: peer1-org1\n        Port: 7051\n\n  - &amp;org2\n    Name: org2MSP\n    ID: org2MSP\n    MSPDir: ./org2/msp\n    #    Policies:\n    #      Readers:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org2MSP.admin&#39;, &#39;org2MSP.peer&#39;, &#39;org2MSP.client&#39;)&quot;\n    #      Writers:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org2MSP.admin&#39;, &#39;org2MSP.client&#39;)&quot;\n    #      Admins:\n    #        Type: Signature\n    #        Rule: &quot;OR(&#39;org2MSP.admin&#39;)&quot;\n\n    AnchorPeers:\n      - Host: peer1-org2\n        Port: 9051\n\nCapabilities:\n  Channel: &amp;ChannelCapabilities\n    V1_4_3: true\n    V1_3: false\n    V1_1: false\n  Orderer: &amp;OrdererCapabilities\n    V1_4_2: true\n    V1_1: false\n  Application: &amp;ApplicationCapabilities\n    V1_4_2: true\n    V1_3: false\n    V1_2: false\n    V1_1: false\n\nApplication: &amp;ApplicationDefaults\n  Organizations:\n  #  Policies:\n  #    Readers:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;ANY Readers&quot;\n  #    Writers:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;ANY Writers&quot;\n  #    Admins:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;MAJORITY Admins&quot;\n\n  Capabilities:\n    &lt;&lt;: *ApplicationCapabilities\n\nOrderer: &amp;OrdererDefaults\n  OrdererType: solo\n\n  Addresses:\n    - orderer-org0:7050\n  BatchTimeout: 2s\n  BatchSize:\n    MaxMessageCount: 10\n    AbsoluteMaxBytes: 99 MB\n    PreferredMaxBytes: 512 KB\n  Organizations:\n#  Policies:\n#    Readers:\n#      Type: ImplicitMeta\n#      Rule: &quot;ANY Readers&quot;\n#    Writers:\n#      Type: ImplicitMeta\n#      Rule: &quot;ANY Writers&quot;\n#    Admins:\n#      Type: ImplicitMeta\n#      Rule: &quot;MAJORITY Admins&quot;\n#    # BlockValidation specifies what signatures must be included in the block\n#    # from the orderer for the peer to validate it.\n#    BlockValidation:\n#      Type: ImplicitMeta\n#      Rule: &quot;ANY Writers&quot;\n\nChannel: &amp;ChannelDefaults\n  #  Policies:\n  #    # Who may invoke the &#39;Deliver&#39; API\n  #    Readers:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;ANY Readers&quot;\n  #    # Who may invoke the &#39;Broadcast&#39; API\n  #    Writers:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;ANY Writers&quot;\n  #    # By default, who may modify elements at this config level\n  #    Admins:\n  #      Type: ImplicitMeta\n  #      Rule: &quot;MAJORITY Admins&quot;\n  Capabilities:\n    &lt;&lt;: *ChannelCapabilities\n\nProfiles:\n\n  TwoOrgsOrdererGenesis:\n    &lt;&lt;: *ChannelDefaults\n    Orderer:\n      &lt;&lt;: *OrdererDefaults\n      Organizations:\n        - *orderer-org0\n      Capabilities:\n        &lt;&lt;: *OrdererCapabilities\n    Consortiums:\n      SampleConsortium:\n        Organizations:\n          - *org1\n          - *org2\n  TwoOrgsChannel:\n    Consortium: SampleConsortium\n    &lt;&lt;: *ChannelDefaults\n    Application:\n      &lt;&lt;: *ApplicationDefaults\n      Organizations:\n        - *org1\n        - *org2\n      Capabilities:\n        &lt;&lt;: *ApplicationCapabilities</code></pre><p>注释掉的部分是策略部分，笔者还没有完全搞懂，所以索性就先注释掉了，以后搞懂了再添加进去。<br>还有一部分<code>msp</code>需要配置，就是<code>configtx.yaml</code>文件中第一部分指定的<code>MSPDir</code>,很简单，按照一下命令复制一下就好了：</p>\n<pre><code>#进入工作目录\ncd $GOPATH/src/github.com/caDemo\n############################################\n#org0\nmkdir org0/msp &amp;&amp;  cd org0/msp\nmkdir admincerts &amp;&amp; mkdir cacerts &amp;&amp; mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n############################################\n#org1\ncd $GOPATH/src/github.com/caDemo\nmkdir org1/msp/  &amp;&amp; cd org1/msp/\nmkdir admincerts &amp;&amp; mkdir cacerts &amp;&amp; mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem\n############################################\n#org2\ncd $GOPATH/src/github.com/caDemo\nmkdir org1/msp/  &amp;&amp; cd org1/msp/\nmkdir admincerts &amp;&amp; mkdir cacerts &amp;&amp; mkdir tlscacerts \ncd $GOPATH/src/github.com/caDemo\ncp adminuser/msp/signcerts/cert.pem msp/admincerts/ca-cert.pem\ncp crypto/ca-cert.pem msp/cacerts/ca-cert.pem\ncp ../tls/ca-cert.pem msp/tlscacerts/ca-cert.pem</code></pre><h3 id=\"4-2-生成创世区块和通道配置信息\"><a href=\"#4-2-生成创世区块和通道配置信息\" class=\"headerlink\" title=\"4.2 生成创世区块和通道配置信息\"></a>4.2 生成创世区块和通道配置信息</h3><p>可以了，所有的前期工作都已经完成，接下来就是手动启动网络了，第一步，生成创世区块和通道配置信息：</p>\n<pre><code>cd $GOPATH/src/github.com/caDemo\nexport FABRIC_CFG_PATH=$PWD\n#生成创世区块\nconfigtxgen -profile TwoOrgsOrdererGenesis -outputBlock $GOPATH/src/github.com/caDemo/genesis.block\n#生成通道配置信息\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx $GOPATH/src/github.com/caDemo/channel.tx -channelID mychannel</code></pre><h3 id=\"4-3-启动Orderer节点\"><a href=\"#4-3-启动Orderer节点\" class=\"headerlink\" title=\"4.3 启动Orderer节点\"></a>4.3 启动Orderer节点</h3><p><code>orderer</code>容器配置文件：</p>\n<pre><code>  orderer-org0:\n    container_name: orderer-org0\n    image: hyperledger/fabric-orderer\n    environment:\n      - ORDERER_HOME=/tmp/hyperledger/orderer\n      - ORDERER_HOST=orderer-org0\n      - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0\n      - ORDERER_GENERAL_GENESISMETHOD=file\n      - ORDERER_GENERAL_GENESISFILE=/tmp/hyperledger/genesis.block\n      - ORDERER_GENERAL_LOCALMSPID=org0MSP\n      - ORDERER_GENERAL_LOCALMSPDIR=/tmp/hyperledger/org0/orderer/msp\n      - ORDERER_GENERAL_TLS_ENABLED=true\n      - ORDERER_GENERAL_TLS_CERTIFICATE=/tmp/hyperledger/org0/orderer/tls-msp/signcerts/cert.pem\n      - ORDERER_GENERAL_TLS_PRIVATEKEY=/tmp/hyperledger/org0/orderer/tls-msp/keystore/key.pem\n      - ORDERER_GENERAL_TLS_ROOTCAS=[/tmp/hyperledger/org0/orderer/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem]\n      - ORDERER_GENERAL_LOGLEVEL=debug\n      - ORDERER_DEBUG_BROADCASTTRACEDIR=data/logs\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org0/orderer:/tmp/hyperledger/org0/orderer/\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n</code></pre><p>关键部分到了，只要这一步没有出现错误，整个网络就启动成功了。</p>\n<pre><code>docker-compose -f docker-compose.yaml up orderer-org0</code></pre><h3 id=\"4-4-启动组织一的cli容器\"><a href=\"#4-4-启动组织一的cli容器\" class=\"headerlink\" title=\"4.4 启动组织一的cli容器\"></a>4.4 启动组织一的cli容器</h3><p><code>cli</code>容器内容,我们需要这个容器对组织1进行链码的交互：</p>\n<pre><code>  cli-org1:\n    container_name: cli-org1\n    image: hyperledger/fabric-tools\n    tty: true\n    stdin_open: true\n    environment:\n      - SYS_CHANNEL=testchainid\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_ID=cli-org1\n      - CORE_PEER_ADDRESS=peer1-org1:7051\n      - CORE_PEER_LOCALMSPID=org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org1/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org1/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/peer1/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org1\n    command: /bin/bash\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org1/peer1:/tmp/hyperledger/org1/peer1\n      - $GOPATH/src/github.com/caDemo/org1/peer1/assets/chaincode:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - $GOPATH/src/github.com/caDemo/org1/adminuser:/tmp/hyperledger/org1/adminuser\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    depends_on:\n      - peer1-org1</code></pre><p>启动该容器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up cli-org1</code></pre><h3 id=\"4-5-启动组织二的cli容器\"><a href=\"#4-5-启动组织二的cli容器\" class=\"headerlink\" title=\"4.5 启动组织二的cli容器\"></a>4.5 启动组织二的cli容器</h3><p><code>cli</code>容器内容,我们需要这个容器对组织2进行链码的交互：</p>\n<pre><code>  cli-org2:\n    container_name: cli-org2\n    image: hyperledger/fabric-tools\n    tty: true\n    stdin_open: true\n    environment:\n      - SYS_CHANNEL=testchainid\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      - FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_ID=cli-org2\n      - CORE_PEER_ADDRESS=peer1-org2:9051\n      - CORE_PEER_LOCALMSPID=org2MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_ROOTCERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem\n      - CORE_PEER_TLS_CERT_FILE=/tmp/hyperledger/org2/peer1/tls-msp/signcerts/cert.pem\n      - CORE_PEER_TLS_KEY_FILE=/tmp/hyperledger/org2/peer1/tls-msp/keystore/key.pem\n      - CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org2/peer1/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/org2\n    command: /bin/bash\n    volumes:\n      - $GOPATH/src/github.com/caDemo/org2/peer1:/tmp/hyperledger/org2/peer1\n      - $GOPATH/src/github.com/caDemo/org2/peer1/assets/chaincode:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - $GOPATH/src/github.com/caDemo/org2/adminuser:/tmp/hyperledger/org2/adminuser\n      - $GOPATH/src/github.com/caDemo:/tmp/hyperledger/\n    networks:\n      - fabric-ca\n    depends_on:\n      - peer1-org2</code></pre><p>启动该容器：</p>\n<pre><code>docker-compose -f docker-compose.yaml up cli-org2</code></pre><h2 id=\"5-网络测试\"><a href=\"#5-网络测试\" class=\"headerlink\" title=\"5.网络测试\"></a>5.网络测试</h2><hr>\n<p>所有工作准备完成，接下来让我们测试整个网络能不能正常运行吧：</p>\n<h3 id=\"5-1-创建与加入通道\"><a href=\"#5-1-创建与加入通道\" class=\"headerlink\" title=\"5.1 创建与加入通道\"></a>5.1 创建与加入通道</h3><p>以<strong>组织1</strong>为例：</p>\n<ul>\n<li><p>首先进入<code>cli</code>容器：</p>\n<pre><code>docker exec -it cli bash\n#配置环境变量\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp</code></pre></li>\n<li><p>创建通道</p>\n<pre><code>peer channel create -c mychannel -f /tmp/hyperledger/channel.tx -o orderer-org0:7050 --outputBlock /tmp/hyperledger/mychannel.block --tls --cafile /tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem</code></pre></li>\n<li><p>将<code>peer1-org1</code>加入通道：</p>\n<pre><code>export CORE_PEER_ADDRESS=peer1-org1:7051\npeer channel join -b /tmp/hyperledger/mychannel.block</code></pre></li>\n<li><p>将<code>peer2-org1</code>加入通道：</p>\n<pre><code>export CORE_PEER_ADDRESS=peer2-org1:8051\npeer channel join -b /tmp/hyperledger/mychannel.block</code></pre><p>组织二步骤是相同的，唯一不同的就是不需要创建通道了，所以就不再说明了。</p>\n<h3 id=\"5-2-安装和实例化链码\"><a href=\"#5-2-安装和实例化链码\" class=\"headerlink\" title=\"5.2 安装和实例化链码\"></a>5.2 安装和实例化链码</h3><p>以<strong>组织1</strong>为例：</p>\n</li>\n<li><p>首先进入<code>cli</code>容器：</p>\n<pre><code>docker exec -it cli bash\n#配置环境变量\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\nexport CORE_PEER_ADDRESS=peer1-org1:7051</code></pre></li>\n<li><p>安装链码</p>\n</li>\n<li><p><em>记得提前将链码放到*</em><code>$GOPATH/src/github.com/caDemo/org1/peer1/assets/chaincode</code><strong>路径下。</strong>,本文使用的是<code>fabric-samples/chaincode/chaincode_example02</code>官方示例链码。</p>\n<pre><code>peer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric-samples/chaincode/chaincode_example02/go/</code></pre></li>\n<li><p>实例化链码</p>\n<pre><code>peer chaincode instantiate -C mychannel -n mycc -v 1.0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;100&quot;,&quot;b&quot;,&quot;200&quot;]}&#39;  -o orderer-org0:7050 --tls --cafile /tmp/hyperledger/org1/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem</code></pre></li>\n<li><p>这一步在高版本的Fabric网络是会出错的，因为少了一个文件<code>config.yaml</code>:</p>\n</li>\n</ul>\n<pre><code>NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/ca.example.com-cert.pem  #这里需要修改\n    OrganizationalUnitIdentifier: orderer</code></pre><p>因为高版本的Fabric把节点类型区分开了，所以需要我们手动配置。<br>将该文件复制到<code>$GOPATH/src/github.com/caDemo/org1/adminuser/msp</code>文件夹内，同时修改上面指定的位置的文件名(与对应文件夹内的文件名对应就好了)。</p>\n<ul>\n<li><p>实例化部分出错的可能性是最高的，很多都是因为网络模式指定错误导致链码容器启动失败，解决方案：</p>\n<pre><code>#终端执行命令\ndocker network ls</code></pre><p>找到以<code>fabric-ca</code>为后缀的一条如<code>cademo_fabric-ca</code>,修改之前的所有<code>peer</code>节点容器配置文件的环境变量：</p>\n<pre><code>CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=cademo_fabric-ca</code></pre><p>修改完成重启节点容器，再次执行以上的命令(需要重新配置环境变量，加入通道这两个操作)。<br>终于，实例化成功了。</p>\n<h3 id=\"5-3-调用和查询链码\"><a href=\"#5-3-调用和查询链码\" class=\"headerlink\" title=\"5.3 调用和查询链码\"></a>5.3 调用和查询链码</h3><p>最后测试一下链码功能能不能正常使用了：</p>\n</li>\n<li><p>还是组织一的<code>cli</code>容器：</p>\n<pre><code>docker exec -it cli bash\nexport CORE_PEER_MSPCONFIGPATH=/tmp/hyperledger/org1/adminuser/msp\nexport CORE_PEER_ADDRESS=peer1-org1:7051</code></pre></li>\n<li><p>执行查询功能：</p>\n<pre><code>peer chaincode query -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;</code></pre><p>命令行应该打印出:</p>\n<pre><code>100</code></pre></li>\n<li><p>执行调用功能：</p>\n<pre><code>peer chaincode invoke -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]}&#39; --tls --cafile /tmp/hyperledger/org2/peer1/tls-msp/tlscacerts/tls-0-0-0-0-7052.pem</code></pre></li>\n<li><p>再次查询：</p>\n<pre><code>peer chaincode query -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;</code></pre><p>命令行应该打印出:</p>\n<pre><code>90</code></pre><p>至于其他节点操作方法也是一样的，就不再操作了。<br>到此为止，从零开始的手动生成证书一直到成功搭建Fabric网络全部步骤已经完成！！接下来还有更新锚节点等等就不再演示了，请各位读者自行操作。整个步骤是不容易的，而且BUG百出，不过成功搭建完成确实涨了不少知识。<br>码字不易，还望各位看官支持一下：</p>\n<img src=\"/img/blog/zfb.png\" srcset=\"undefined\" width = \"300\" height = \"300\" alt=\"支付宝\" align=center />\n\n</li>\n</ul>\n"},{"title":"Hyperledger Fabric使用硬件安全模块(HSM)","date":"2019-12-24T02:54:11.000Z","_content":"\n# 使用硬件安全模块\n\n[官方文档](https://hyperledger-fabric.readthedocs.io/en/latest/hsm.html)\n可以通过`Fabric`节点使用硬件安全模块`(HSM)`来产生和存储私钥。`HSM`用于保护私钥和处理加密操作。允许`peer`节点与`orderer`节点在不暴露他们的私钥的条件下去签名和背书交易，当前`Fabric`只支持使用`PKCS11`标准与`HSM`进行通信。\n\n## 配置HSM\n\n为了在`Fabric`节点上使用`HSM`，需要更新关于节点配置文件如`core.yaml`中的`BCCSP`(加密服务提供者)部分.在`BCCSP`部分，需要选择`PKCS11`作为提供者并提供需要使用的`PKCS11`库的路径。还需要提供为加密操作创建的令牌的标签和密码。可以使用令牌去生成和存储多个秘钥。\n预先构建的`Hyperledger Fabric Docker`镜像不能够使用`PKCS11`。如果使用`Docker`部署`Fabric`，需要通过以下的命令启动`PKCS11`构建自己的镜像。\n```\nmake docker GO_TAGS=pkcs11\n```\n同时也需要确保`PKCS11`的库文件是有效的，挂载到容器内部或者通过节点安装后是可以使用的。\n\n## 示例\n\n接下来的示例说明了如何去配置一个可以使用`HSM`的`Fabirc`节点。\n\n首先，需要安装一个实现了`PKCS11`的接口。本例使用开源的[softhsm](https://github.com/opendnssec/SoftHSMv2)实现。在下载和配置完成`softhsm`后，需要设置环境变量`SOFTHSM2_CONF`指向`softhsm2`配置文件。\n\n可以使用`softhsm`去创建用于处理关于`Fabric`节点在`HSM`插槽中用于加密操作令牌。在这个示例中，我们创建了一个标签为`fabric`，密码为`71811222`的令牌。在创建令牌完成之后，更新配置文件来使用`PKCS11`，并将令牌作为加密服务提供者。可以在下面发现关于`BCCSP`部分配置的例子：\n```\n#############################################################################\n# BCCSP (区块链加密服务提供者) 部分，用于选择使用的已实现的加密库文件\n#############################################################################\nbccsp:\n  default: PKCS11\n  pkcs11:\n    Library: /etc/hyperledger/fabric/libsofthsm2.so\n    Pin: 71811222\n    Label: fabric\n    hash: SHA2\n    security: 256\n```\n也可以通过环境变量来覆盖配置文件中相关的字段。如果通过`Fabric CA`服务器连接到了`HSM`，则需要设置以下环境变量：\n```\nFABRIC_CA_SERVER_BCCSP_DEFAULT=PKCS11\nFABRIC_CA_SERVER_BCCSP_PKCS11_LIBRARY=/etc/hyperledger/fabric/libsofthsm2.so\nFABRIC_CA_SERVER_BCCSP_PKCS11_PIN=71811222\nFABRIC_CA_SERVER_BCCSP_PKCS11_LABEL=fabric\n```\n如果使用`docker compose`部署了节点，在构建完自己的镜像后，可以更新`docker compose`文件通过`volumes`将`softhsm`库文件和配置文件挂载到容器中。例如，可以添加下面的环境和`volumes`变量到`docker compose`文件：\n```\n  environment:\n     - SOFTHSM2_CONF=/etc/hyperledger/fabric/config.file\n  volumes:\n     - /home/softhsm/config.file:/etc/hyperledger/fabric/config.file\n     - /usr/local/Cellar/softhsm/2.1.0/lib/softhsm/libsofthsm2.so:/etc/hyperledger/fabric/libsofthsm2.so\n```\n\n## 配置使用`HSM`的网络\n\n如果使用`HSM`部署了`Fabric`节点，私钥将会在`HSM`内部生成而不是节点本地的`MSP`中的`keystore`文件夹内。`MSP`中的`keystore`文件夹将为空文件夹。另外，`Fabric`节点将使用关于`signcerts`文件夹内的签名证书的主题秘钥标识符去接收`HSM`中的私钥。这个创建`MSP`文件夹的过程将和之前不同，取决于自己使用的`Fabric` 证书认证中心。\n\n### 使用Fabric CA\n\n可以通过编辑相同的配置文件配置`Fabric CA`使`peer`节点或者是`orderer`节点使用`HSM`。因为可以使用`Fabric CA`内部的`HSM`来生成秘钥。通过下面的步骤将直接创建本地的`MSP`文件夹：\n\n1. 创建一个`HSM`令牌并将它指向`Fabirc CA`的配置文件。当`Fabric CA`服务启动时，将会在`HSM`中生成`CA`签名证书。如果不担心`CA`签名证书是否暴露，可以跳过该步骤。\n2. 使用`Fabric CA`客户端通过自己的`CA`去注册`peer`或者`order`节点身份。\n3. 编辑`Fabric CA`客户端配置文件或者是环境变量使用`HSM`作为加密服务提供者并再次登录获取节点的身份。登录命令将通过`HSM`生成私钥文件.\n4. 更新关于`peer`或者`orderer`节点的配置文件中的`BCCSP`部分使用`PKCS11`，并将令牌作为加密服务提供者。指向由`Fabric CA`客户端创建的`MSP`文件夹。一旦部署完成，`peer`节点或者`orderer`节点将可以通过由`HSM`提供保护的私钥文件签名和背书交易。\n\n### 通过自己的`CA`使用`HSM`\n\n如果使用自己的`CA`证书中心来部署`Fabric`组件，可以通过以下几步使用`HSM`:\n\n1. 配置自己的`CA`使用`PKCS11`创建令牌与`HSM`进行通信。然后使用自己的`CA`去为每一个节点生成私钥和签名证书。私钥由`HSM`内部进行生成。\n2. 使用`CA`去构建节点的`MSP`文件夹。将步骤一中生成的签名证书放入`signcerts`文件夹内。可以保持`keystore`文件夹为空。\n3. 更新关于`peer`或者`orderer`节点的配置文件中的`BCCSP`部分使用`PKCS11`，并将令牌作为加密服务提供者。指向由`Fabric CA`客户端创建的`MSP`文件夹。一旦部署完成，`peer`节点或者`orderer`节点将可以通过由`HSM`提供保护的私钥文件签名和背书交易。","source":"_posts/blog/fabric/使用硬件安全模块.md","raw":"---\ntitle: Hyperledger Fabric使用硬件安全模块(HSM)\ndate: 2019-12-24 10:54:11\ntags: fabric\ncategories: fabric应用\n---\n\n# 使用硬件安全模块\n\n[官方文档](https://hyperledger-fabric.readthedocs.io/en/latest/hsm.html)\n可以通过`Fabric`节点使用硬件安全模块`(HSM)`来产生和存储私钥。`HSM`用于保护私钥和处理加密操作。允许`peer`节点与`orderer`节点在不暴露他们的私钥的条件下去签名和背书交易，当前`Fabric`只支持使用`PKCS11`标准与`HSM`进行通信。\n\n## 配置HSM\n\n为了在`Fabric`节点上使用`HSM`，需要更新关于节点配置文件如`core.yaml`中的`BCCSP`(加密服务提供者)部分.在`BCCSP`部分，需要选择`PKCS11`作为提供者并提供需要使用的`PKCS11`库的路径。还需要提供为加密操作创建的令牌的标签和密码。可以使用令牌去生成和存储多个秘钥。\n预先构建的`Hyperledger Fabric Docker`镜像不能够使用`PKCS11`。如果使用`Docker`部署`Fabric`，需要通过以下的命令启动`PKCS11`构建自己的镜像。\n```\nmake docker GO_TAGS=pkcs11\n```\n同时也需要确保`PKCS11`的库文件是有效的，挂载到容器内部或者通过节点安装后是可以使用的。\n\n## 示例\n\n接下来的示例说明了如何去配置一个可以使用`HSM`的`Fabirc`节点。\n\n首先，需要安装一个实现了`PKCS11`的接口。本例使用开源的[softhsm](https://github.com/opendnssec/SoftHSMv2)实现。在下载和配置完成`softhsm`后，需要设置环境变量`SOFTHSM2_CONF`指向`softhsm2`配置文件。\n\n可以使用`softhsm`去创建用于处理关于`Fabric`节点在`HSM`插槽中用于加密操作令牌。在这个示例中，我们创建了一个标签为`fabric`，密码为`71811222`的令牌。在创建令牌完成之后，更新配置文件来使用`PKCS11`，并将令牌作为加密服务提供者。可以在下面发现关于`BCCSP`部分配置的例子：\n```\n#############################################################################\n# BCCSP (区块链加密服务提供者) 部分，用于选择使用的已实现的加密库文件\n#############################################################################\nbccsp:\n  default: PKCS11\n  pkcs11:\n    Library: /etc/hyperledger/fabric/libsofthsm2.so\n    Pin: 71811222\n    Label: fabric\n    hash: SHA2\n    security: 256\n```\n也可以通过环境变量来覆盖配置文件中相关的字段。如果通过`Fabric CA`服务器连接到了`HSM`，则需要设置以下环境变量：\n```\nFABRIC_CA_SERVER_BCCSP_DEFAULT=PKCS11\nFABRIC_CA_SERVER_BCCSP_PKCS11_LIBRARY=/etc/hyperledger/fabric/libsofthsm2.so\nFABRIC_CA_SERVER_BCCSP_PKCS11_PIN=71811222\nFABRIC_CA_SERVER_BCCSP_PKCS11_LABEL=fabric\n```\n如果使用`docker compose`部署了节点，在构建完自己的镜像后，可以更新`docker compose`文件通过`volumes`将`softhsm`库文件和配置文件挂载到容器中。例如，可以添加下面的环境和`volumes`变量到`docker compose`文件：\n```\n  environment:\n     - SOFTHSM2_CONF=/etc/hyperledger/fabric/config.file\n  volumes:\n     - /home/softhsm/config.file:/etc/hyperledger/fabric/config.file\n     - /usr/local/Cellar/softhsm/2.1.0/lib/softhsm/libsofthsm2.so:/etc/hyperledger/fabric/libsofthsm2.so\n```\n\n## 配置使用`HSM`的网络\n\n如果使用`HSM`部署了`Fabric`节点，私钥将会在`HSM`内部生成而不是节点本地的`MSP`中的`keystore`文件夹内。`MSP`中的`keystore`文件夹将为空文件夹。另外，`Fabric`节点将使用关于`signcerts`文件夹内的签名证书的主题秘钥标识符去接收`HSM`中的私钥。这个创建`MSP`文件夹的过程将和之前不同，取决于自己使用的`Fabric` 证书认证中心。\n\n### 使用Fabric CA\n\n可以通过编辑相同的配置文件配置`Fabric CA`使`peer`节点或者是`orderer`节点使用`HSM`。因为可以使用`Fabric CA`内部的`HSM`来生成秘钥。通过下面的步骤将直接创建本地的`MSP`文件夹：\n\n1. 创建一个`HSM`令牌并将它指向`Fabirc CA`的配置文件。当`Fabric CA`服务启动时，将会在`HSM`中生成`CA`签名证书。如果不担心`CA`签名证书是否暴露，可以跳过该步骤。\n2. 使用`Fabric CA`客户端通过自己的`CA`去注册`peer`或者`order`节点身份。\n3. 编辑`Fabric CA`客户端配置文件或者是环境变量使用`HSM`作为加密服务提供者并再次登录获取节点的身份。登录命令将通过`HSM`生成私钥文件.\n4. 更新关于`peer`或者`orderer`节点的配置文件中的`BCCSP`部分使用`PKCS11`，并将令牌作为加密服务提供者。指向由`Fabric CA`客户端创建的`MSP`文件夹。一旦部署完成，`peer`节点或者`orderer`节点将可以通过由`HSM`提供保护的私钥文件签名和背书交易。\n\n### 通过自己的`CA`使用`HSM`\n\n如果使用自己的`CA`证书中心来部署`Fabric`组件，可以通过以下几步使用`HSM`:\n\n1. 配置自己的`CA`使用`PKCS11`创建令牌与`HSM`进行通信。然后使用自己的`CA`去为每一个节点生成私钥和签名证书。私钥由`HSM`内部进行生成。\n2. 使用`CA`去构建节点的`MSP`文件夹。将步骤一中生成的签名证书放入`signcerts`文件夹内。可以保持`keystore`文件夹为空。\n3. 更新关于`peer`或者`orderer`节点的配置文件中的`BCCSP`部分使用`PKCS11`，并将令牌作为加密服务提供者。指向由`Fabric CA`客户端创建的`MSP`文件夹。一旦部署完成，`peer`节点或者`orderer`节点将可以通过由`HSM`提供保护的私钥文件签名和背书交易。","slug":"blog/fabric/使用硬件安全模块","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyj8003kk0vqdr0rgkbm","content":"<h1 id=\"使用硬件安全模块\"><a href=\"#使用硬件安全模块\" class=\"headerlink\" title=\"使用硬件安全模块\"></a>使用硬件安全模块</h1><p><a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/hsm.html\" target=\"_blank\" rel=\"noopener\">官方文档</a><br>可以通过<code>Fabric</code>节点使用硬件安全模块<code>(HSM)</code>来产生和存储私钥。<code>HSM</code>用于保护私钥和处理加密操作。允许<code>peer</code>节点与<code>orderer</code>节点在不暴露他们的私钥的条件下去签名和背书交易，当前<code>Fabric</code>只支持使用<code>PKCS11</code>标准与<code>HSM</code>进行通信。</p>\n<h2 id=\"配置HSM\"><a href=\"#配置HSM\" class=\"headerlink\" title=\"配置HSM\"></a>配置HSM</h2><p>为了在<code>Fabric</code>节点上使用<code>HSM</code>，需要更新关于节点配置文件如<code>core.yaml</code>中的<code>BCCSP</code>(加密服务提供者)部分.在<code>BCCSP</code>部分，需要选择<code>PKCS11</code>作为提供者并提供需要使用的<code>PKCS11</code>库的路径。还需要提供为加密操作创建的令牌的标签和密码。可以使用令牌去生成和存储多个秘钥。<br>预先构建的<code>Hyperledger Fabric Docker</code>镜像不能够使用<code>PKCS11</code>。如果使用<code>Docker</code>部署<code>Fabric</code>，需要通过以下的命令启动<code>PKCS11</code>构建自己的镜像。</p>\n<pre><code>make docker GO_TAGS=pkcs11</code></pre><p>同时也需要确保<code>PKCS11</code>的库文件是有效的，挂载到容器内部或者通过节点安装后是可以使用的。</p>\n<h2 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h2><p>接下来的示例说明了如何去配置一个可以使用<code>HSM</code>的<code>Fabirc</code>节点。</p>\n<p>首先，需要安装一个实现了<code>PKCS11</code>的接口。本例使用开源的<a href=\"https://github.com/opendnssec/SoftHSMv2\" target=\"_blank\" rel=\"noopener\">softhsm</a>实现。在下载和配置完成<code>softhsm</code>后，需要设置环境变量<code>SOFTHSM2_CONF</code>指向<code>softhsm2</code>配置文件。</p>\n<p>可以使用<code>softhsm</code>去创建用于处理关于<code>Fabric</code>节点在<code>HSM</code>插槽中用于加密操作令牌。在这个示例中，我们创建了一个标签为<code>fabric</code>，密码为<code>71811222</code>的令牌。在创建令牌完成之后，更新配置文件来使用<code>PKCS11</code>，并将令牌作为加密服务提供者。可以在下面发现关于<code>BCCSP</code>部分配置的例子：</p>\n<pre><code>#############################################################################\n# BCCSP (区块链加密服务提供者) 部分，用于选择使用的已实现的加密库文件\n#############################################################################\nbccsp:\n  default: PKCS11\n  pkcs11:\n    Library: /etc/hyperledger/fabric/libsofthsm2.so\n    Pin: 71811222\n    Label: fabric\n    hash: SHA2\n    security: 256</code></pre><p>也可以通过环境变量来覆盖配置文件中相关的字段。如果通过<code>Fabric CA</code>服务器连接到了<code>HSM</code>，则需要设置以下环境变量：</p>\n<pre><code>FABRIC_CA_SERVER_BCCSP_DEFAULT=PKCS11\nFABRIC_CA_SERVER_BCCSP_PKCS11_LIBRARY=/etc/hyperledger/fabric/libsofthsm2.so\nFABRIC_CA_SERVER_BCCSP_PKCS11_PIN=71811222\nFABRIC_CA_SERVER_BCCSP_PKCS11_LABEL=fabric</code></pre><p>如果使用<code>docker compose</code>部署了节点，在构建完自己的镜像后，可以更新<code>docker compose</code>文件通过<code>volumes</code>将<code>softhsm</code>库文件和配置文件挂载到容器中。例如，可以添加下面的环境和<code>volumes</code>变量到<code>docker compose</code>文件：</p>\n<pre><code>  environment:\n     - SOFTHSM2_CONF=/etc/hyperledger/fabric/config.file\n  volumes:\n     - /home/softhsm/config.file:/etc/hyperledger/fabric/config.file\n     - /usr/local/Cellar/softhsm/2.1.0/lib/softhsm/libsofthsm2.so:/etc/hyperledger/fabric/libsofthsm2.so</code></pre><h2 id=\"配置使用HSM的网络\"><a href=\"#配置使用HSM的网络\" class=\"headerlink\" title=\"配置使用HSM的网络\"></a>配置使用<code>HSM</code>的网络</h2><p>如果使用<code>HSM</code>部署了<code>Fabric</code>节点，私钥将会在<code>HSM</code>内部生成而不是节点本地的<code>MSP</code>中的<code>keystore</code>文件夹内。<code>MSP</code>中的<code>keystore</code>文件夹将为空文件夹。另外，<code>Fabric</code>节点将使用关于<code>signcerts</code>文件夹内的签名证书的主题秘钥标识符去接收<code>HSM</code>中的私钥。这个创建<code>MSP</code>文件夹的过程将和之前不同，取决于自己使用的<code>Fabric</code> 证书认证中心。</p>\n<h3 id=\"使用Fabric-CA\"><a href=\"#使用Fabric-CA\" class=\"headerlink\" title=\"使用Fabric CA\"></a>使用Fabric CA</h3><p>可以通过编辑相同的配置文件配置<code>Fabric CA</code>使<code>peer</code>节点或者是<code>orderer</code>节点使用<code>HSM</code>。因为可以使用<code>Fabric CA</code>内部的<code>HSM</code>来生成秘钥。通过下面的步骤将直接创建本地的<code>MSP</code>文件夹：</p>\n<ol>\n<li>创建一个<code>HSM</code>令牌并将它指向<code>Fabirc CA</code>的配置文件。当<code>Fabric CA</code>服务启动时，将会在<code>HSM</code>中生成<code>CA</code>签名证书。如果不担心<code>CA</code>签名证书是否暴露，可以跳过该步骤。</li>\n<li>使用<code>Fabric CA</code>客户端通过自己的<code>CA</code>去注册<code>peer</code>或者<code>order</code>节点身份。</li>\n<li>编辑<code>Fabric CA</code>客户端配置文件或者是环境变量使用<code>HSM</code>作为加密服务提供者并再次登录获取节点的身份。登录命令将通过<code>HSM</code>生成私钥文件.</li>\n<li>更新关于<code>peer</code>或者<code>orderer</code>节点的配置文件中的<code>BCCSP</code>部分使用<code>PKCS11</code>，并将令牌作为加密服务提供者。指向由<code>Fabric CA</code>客户端创建的<code>MSP</code>文件夹。一旦部署完成，<code>peer</code>节点或者<code>orderer</code>节点将可以通过由<code>HSM</code>提供保护的私钥文件签名和背书交易。</li>\n</ol>\n<h3 id=\"通过自己的CA使用HSM\"><a href=\"#通过自己的CA使用HSM\" class=\"headerlink\" title=\"通过自己的CA使用HSM\"></a>通过自己的<code>CA</code>使用<code>HSM</code></h3><p>如果使用自己的<code>CA</code>证书中心来部署<code>Fabric</code>组件，可以通过以下几步使用<code>HSM</code>:</p>\n<ol>\n<li>配置自己的<code>CA</code>使用<code>PKCS11</code>创建令牌与<code>HSM</code>进行通信。然后使用自己的<code>CA</code>去为每一个节点生成私钥和签名证书。私钥由<code>HSM</code>内部进行生成。</li>\n<li>使用<code>CA</code>去构建节点的<code>MSP</code>文件夹。将步骤一中生成的签名证书放入<code>signcerts</code>文件夹内。可以保持<code>keystore</code>文件夹为空。</li>\n<li>更新关于<code>peer</code>或者<code>orderer</code>节点的配置文件中的<code>BCCSP</code>部分使用<code>PKCS11</code>，并将令牌作为加密服务提供者。指向由<code>Fabric CA</code>客户端创建的<code>MSP</code>文件夹。一旦部署完成，<code>peer</code>节点或者<code>orderer</code>节点将可以通过由<code>HSM</code>提供保护的私钥文件签名和背书交易。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"使用硬件安全模块\"><a href=\"#使用硬件安全模块\" class=\"headerlink\" title=\"使用硬件安全模块\"></a>使用硬件安全模块</h1><p><a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/hsm.html\" target=\"_blank\" rel=\"noopener\">官方文档</a><br>可以通过<code>Fabric</code>节点使用硬件安全模块<code>(HSM)</code>来产生和存储私钥。<code>HSM</code>用于保护私钥和处理加密操作。允许<code>peer</code>节点与<code>orderer</code>节点在不暴露他们的私钥的条件下去签名和背书交易，当前<code>Fabric</code>只支持使用<code>PKCS11</code>标准与<code>HSM</code>进行通信。</p>\n<h2 id=\"配置HSM\"><a href=\"#配置HSM\" class=\"headerlink\" title=\"配置HSM\"></a>配置HSM</h2><p>为了在<code>Fabric</code>节点上使用<code>HSM</code>，需要更新关于节点配置文件如<code>core.yaml</code>中的<code>BCCSP</code>(加密服务提供者)部分.在<code>BCCSP</code>部分，需要选择<code>PKCS11</code>作为提供者并提供需要使用的<code>PKCS11</code>库的路径。还需要提供为加密操作创建的令牌的标签和密码。可以使用令牌去生成和存储多个秘钥。<br>预先构建的<code>Hyperledger Fabric Docker</code>镜像不能够使用<code>PKCS11</code>。如果使用<code>Docker</code>部署<code>Fabric</code>，需要通过以下的命令启动<code>PKCS11</code>构建自己的镜像。</p>\n<pre><code>make docker GO_TAGS=pkcs11</code></pre><p>同时也需要确保<code>PKCS11</code>的库文件是有效的，挂载到容器内部或者通过节点安装后是可以使用的。</p>\n<h2 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h2><p>接下来的示例说明了如何去配置一个可以使用<code>HSM</code>的<code>Fabirc</code>节点。</p>\n<p>首先，需要安装一个实现了<code>PKCS11</code>的接口。本例使用开源的<a href=\"https://github.com/opendnssec/SoftHSMv2\" target=\"_blank\" rel=\"noopener\">softhsm</a>实现。在下载和配置完成<code>softhsm</code>后，需要设置环境变量<code>SOFTHSM2_CONF</code>指向<code>softhsm2</code>配置文件。</p>\n<p>可以使用<code>softhsm</code>去创建用于处理关于<code>Fabric</code>节点在<code>HSM</code>插槽中用于加密操作令牌。在这个示例中，我们创建了一个标签为<code>fabric</code>，密码为<code>71811222</code>的令牌。在创建令牌完成之后，更新配置文件来使用<code>PKCS11</code>，并将令牌作为加密服务提供者。可以在下面发现关于<code>BCCSP</code>部分配置的例子：</p>\n<pre><code>#############################################################################\n# BCCSP (区块链加密服务提供者) 部分，用于选择使用的已实现的加密库文件\n#############################################################################\nbccsp:\n  default: PKCS11\n  pkcs11:\n    Library: /etc/hyperledger/fabric/libsofthsm2.so\n    Pin: 71811222\n    Label: fabric\n    hash: SHA2\n    security: 256</code></pre><p>也可以通过环境变量来覆盖配置文件中相关的字段。如果通过<code>Fabric CA</code>服务器连接到了<code>HSM</code>，则需要设置以下环境变量：</p>\n<pre><code>FABRIC_CA_SERVER_BCCSP_DEFAULT=PKCS11\nFABRIC_CA_SERVER_BCCSP_PKCS11_LIBRARY=/etc/hyperledger/fabric/libsofthsm2.so\nFABRIC_CA_SERVER_BCCSP_PKCS11_PIN=71811222\nFABRIC_CA_SERVER_BCCSP_PKCS11_LABEL=fabric</code></pre><p>如果使用<code>docker compose</code>部署了节点，在构建完自己的镜像后，可以更新<code>docker compose</code>文件通过<code>volumes</code>将<code>softhsm</code>库文件和配置文件挂载到容器中。例如，可以添加下面的环境和<code>volumes</code>变量到<code>docker compose</code>文件：</p>\n<pre><code>  environment:\n     - SOFTHSM2_CONF=/etc/hyperledger/fabric/config.file\n  volumes:\n     - /home/softhsm/config.file:/etc/hyperledger/fabric/config.file\n     - /usr/local/Cellar/softhsm/2.1.0/lib/softhsm/libsofthsm2.so:/etc/hyperledger/fabric/libsofthsm2.so</code></pre><h2 id=\"配置使用HSM的网络\"><a href=\"#配置使用HSM的网络\" class=\"headerlink\" title=\"配置使用HSM的网络\"></a>配置使用<code>HSM</code>的网络</h2><p>如果使用<code>HSM</code>部署了<code>Fabric</code>节点，私钥将会在<code>HSM</code>内部生成而不是节点本地的<code>MSP</code>中的<code>keystore</code>文件夹内。<code>MSP</code>中的<code>keystore</code>文件夹将为空文件夹。另外，<code>Fabric</code>节点将使用关于<code>signcerts</code>文件夹内的签名证书的主题秘钥标识符去接收<code>HSM</code>中的私钥。这个创建<code>MSP</code>文件夹的过程将和之前不同，取决于自己使用的<code>Fabric</code> 证书认证中心。</p>\n<h3 id=\"使用Fabric-CA\"><a href=\"#使用Fabric-CA\" class=\"headerlink\" title=\"使用Fabric CA\"></a>使用Fabric CA</h3><p>可以通过编辑相同的配置文件配置<code>Fabric CA</code>使<code>peer</code>节点或者是<code>orderer</code>节点使用<code>HSM</code>。因为可以使用<code>Fabric CA</code>内部的<code>HSM</code>来生成秘钥。通过下面的步骤将直接创建本地的<code>MSP</code>文件夹：</p>\n<ol>\n<li>创建一个<code>HSM</code>令牌并将它指向<code>Fabirc CA</code>的配置文件。当<code>Fabric CA</code>服务启动时，将会在<code>HSM</code>中生成<code>CA</code>签名证书。如果不担心<code>CA</code>签名证书是否暴露，可以跳过该步骤。</li>\n<li>使用<code>Fabric CA</code>客户端通过自己的<code>CA</code>去注册<code>peer</code>或者<code>order</code>节点身份。</li>\n<li>编辑<code>Fabric CA</code>客户端配置文件或者是环境变量使用<code>HSM</code>作为加密服务提供者并再次登录获取节点的身份。登录命令将通过<code>HSM</code>生成私钥文件.</li>\n<li>更新关于<code>peer</code>或者<code>orderer</code>节点的配置文件中的<code>BCCSP</code>部分使用<code>PKCS11</code>，并将令牌作为加密服务提供者。指向由<code>Fabric CA</code>客户端创建的<code>MSP</code>文件夹。一旦部署完成，<code>peer</code>节点或者<code>orderer</code>节点将可以通过由<code>HSM</code>提供保护的私钥文件签名和背书交易。</li>\n</ol>\n<h3 id=\"通过自己的CA使用HSM\"><a href=\"#通过自己的CA使用HSM\" class=\"headerlink\" title=\"通过自己的CA使用HSM\"></a>通过自己的<code>CA</code>使用<code>HSM</code></h3><p>如果使用自己的<code>CA</code>证书中心来部署<code>Fabric</code>组件，可以通过以下几步使用<code>HSM</code>:</p>\n<ol>\n<li>配置自己的<code>CA</code>使用<code>PKCS11</code>创建令牌与<code>HSM</code>进行通信。然后使用自己的<code>CA</code>去为每一个节点生成私钥和签名证书。私钥由<code>HSM</code>内部进行生成。</li>\n<li>使用<code>CA</code>去构建节点的<code>MSP</code>文件夹。将步骤一中生成的签名证书放入<code>signcerts</code>文件夹内。可以保持<code>keystore</code>文件夹为空。</li>\n<li>更新关于<code>peer</code>或者<code>orderer</code>节点的配置文件中的<code>BCCSP</code>部分使用<code>PKCS11</code>，并将令牌作为加密服务提供者。指向由<code>Fabric CA</code>客户端创建的<code>MSP</code>文件夹。一旦部署完成，<code>peer</code>节点或者<code>orderer</code>节点将可以通过由<code>HSM</code>提供保护的私钥文件签名和背书交易。</li>\n</ol>\n"},{"title":"Fabric1.4源码解析：链码容器启动过程","date":"2019-12-19T13:02:38.000Z","_content":"想写点东西记录一下最近看的一些Fabric源码,本文使用的是**fabric1.4**的版本，所以对于其他版本的fabric，内容可能会有所不同。\n其实我仅仅知道Go语言一些语法的使用，并不太熟悉Go语言，所以解析的内容可能会有误，欢迎大家批评指正。\n本文想针对Fabric中链码容器的启动过程进行源码的解析。这里的链码指的是用户链码不是系统链码,顺便回顾一下系统链码:\n**lscc**(Life Cycle System ChainCode)生命周期系统链码\n**cscc**(Configuration System ChainCode)配置系统链码\n**escc**(Endorser System ChainCode)背书系统链码\n**qscc**(Query System ChainCode)查询系统链码\n**vscc**(Verification System ChainCode)验证系统链码\n本文主要解析的是用户链码的启动过程。\n## 1 起点\n```\n#这是用户端链码的main方法，也是整个流程的入口点，调用了shim包中的Start(cc Chaincode)方法.\nfunc main(){\n    err :=shim.Start(new(Chaincode))\n    if err != nil {\n        fmt.Printf(\"Error starting Chaincode: %s\",err)\n    }\n}\n```\n首先定位到``fabric/core/chaincode/shim/chaincode.go``这个文件中的``Start``方法，这里是链码启动的起点。\n可以看到传的参数就是chaincode,接下来分析一下启动过程\n```\n#方法中第一行代码，根据名字可以看出是对链码的Log进行设置\nSetupChaincodeLogging()\n#从输入中获取用户定义的链码的名称\nchaincodename := viper.GetString(\"chaincode.id.name\")\n#如果没有输入链码名称，直接返回没有提供链码id的错误，下面则不再执行\nif chaincodename == \"\" {\n    return errors.New(\"error chaincode id not provided\")\n}\n#看名字是一个工厂方法，点进行看一下\nerr := factory.InitFactories(factory.GetDefaultOpts())\n```\n首先进入到``factory.GetDefaultOpts()``方法中：\n```\nfunc GetDefaultOpts() *FactoryOpts {\n\treturn &FactoryOpts{\n\t\tProviderName: \"SW\",\n\t\tSwOpts: &SwOpts{\n\t\t\tHashFamily: \"SHA2\",   #HASH类型\n\t\t\tSecLevel:   256,    #HASH级别\n\n\t\t\tEphemeral: true,\n\t\t},\n\t}\n}\n#可以猜到这个方法是获取默认的加密操作，使用SHA256进行数据加密\n```\n不难猜到``factory.InitFactories``这个方法就是为当前链码设置加密操作的一系列内容。回到``Start()``方法中接着往下看.\n```\n#这一部分就是将链码数据以流的方式读取进来，userChaincodeStreamGetter是一个方法，点进去看一下\nif streamGetter == nil {\n\tstreamGetter = userChaincodeStreamGetter\n}\nstream, err := streamGetter(chaincodename)\nif err != nil {\n\treturn err\n}\n```\n``userChaincodeStreamGetter``还是在这个文件中第82行:\n```\n#这里的name是链码名称，读取到链码数据后以PeerChainCodeStream的方式返回\nfunc userChaincodeStreamGetter(name string) (PeerChaincodeStream, error) {\n    #获取peer.address\n\tflag.StringVar(&peerAddress, \"peer.address\", \"\", \"peer address\")\n\t//判断是否使能TLS\n\tif viper.GetBool(\"peer.tls.enabled\") {\n        #获取tls密钥地址，在用户安装链码的时候指定\n\t\tkeyPath := viper.GetString(\"tls.client.key.path\")\n        #获取tls证书地址\n\t\tcertPath := viper.GetString(\"tls.client.cert.path\")\n        #从文件中读取密钥数据\n\t\tdata, err1 := ioutil.ReadFile(keyPath)\n\t\tif err1 != nil {\n\t\t\terr1 = errors.Wrap(err1, fmt.Sprintf(\"error trying to read file content %s\", keyPath))\n\t\t\tchaincodeLogger.Errorf(\"%+v\", err1)\n\t\t\treturn nil, err1\n\t\t}\n\t\tkey = string(data)\n         #从文件中读取证书数据\n\t\tdata, err1 = ioutil.ReadFile(certPath)\n\t\tif err1 != nil {\n\t\t\terr1 = errors.Wrap(err1, fmt.Sprintf(\"error trying to read file content %s\", certPath))\n\t\t\tchaincodeLogger.Errorf(\"%+v\", err1)\n\t\t\treturn nil, err1\n\t\t}\n\t\tcert = string(data)\n\t}\n    #解析命令行参数到定义的flag\n\tflag.Parse()\n    #日志输出\n\tchaincodeLogger.Debugf(\"Peer address: %s\", getPeerAddress())\n\n\t//与peer节点建立连接\n\tclientConn, err := newPeerClientConnection()\n```\n看一下这个方法里面的内容，还是这个文件第317行：\n```\nfunc newPeerClientConnection() (*grpc.ClientConn, error) {\n    #首先获取到peer节点的地址\n\tvar peerAddress = getPeerAddress()\n    #看名字就知道了，设置与链码之间的心中信息\n\tkaOpts := &comm.KeepaliveOptions{\n\t\tClientInterval: time.Duration(1) * time.Minute,\n\t\tClientTimeout:  time.Duration(20) * time.Second,\n\t}\n```\n 判断是否使能了TLS，然后根据结果建立链接,如何建立链接就不再细看了，我们回到之前的部分\n```\n\tif viper.GetBool(\"peer.tls.enabled\") {\n\t\treturn comm.NewClientConnectionWithAddress(peerAddress, true, true,\n\t\t\tcomm.InitTLSForShim(key, cert), kaOpts)\n\t}\n\treturn comm.NewClientConnectionWithAddress(peerAddress, true, false, nil, kaOpts)\n}\n```\n还是之前的``userChaincodeStreamGetter``方法\n```\nclientConn, err := newPeerClientConnection()\n\tif err != nil {\n\t\terr = errors.Wrap(err, \"error trying to connect to local peer\")\n\t\tchaincodeLogger.Errorf(\"%+v\", err)\n\t\treturn nil, err\n\t}\n\n\tchaincodeLogger.Debugf(\"os.Args returns: %s\", os.Args)\n\n    #接下来是这个方法，返回一个ChaincodeSupportClient实例,对应着链码容器\n\tchaincodeSupportClient := pb.NewChaincodeSupportClient(clientConn)\n\n\t//这一步是与peer节点建立gRPC连接\n\tstream, err := chaincodeSupportClient.Register(context.Background())\n\tif err != nil {\n\t\treturn nil, errors.WithMessage(err, fmt.Sprintf(\"error chatting with leader at address=%s\", getPeerAddress()))\n\t}\n\n\treturn stream, nil\n}\n```\n这个方法结束之后，链码容器与Peer节点已经建立起了连接，接下来链码容器与Peer节点开始互相发送消息了。\n返回到``Start()``方法中，还剩最后的一个方法``chatWithPeer()``：\n```\n\terr = chatWithPeer(chaincodename, stream, cc)\n\treturn err\n}\n```\n看一下链码容器与Peer节点是如何互相通信的。这个方法是链码容器启动的过程中最重要的方法，包含所有的通信流程。``chatWithPeer()``在331行:\n```\nfunc chatWithPeer(chaincodename string, stream PeerChaincodeStream, cc Chaincode)\n#传入的参数有链码名称，流(这个是之前链码容器与Peer节点建立gRPC连接所返回的)，链码\n```\n首先第一步是新建一个``ChaincodeHandler``对象：是非常重要的一个对象。看一下该对象的内容,在``core/chaincode/shim/handler.go``文件中第166行:\n```\nfunc newChaincodeHandler(peerChatStream PeerChaincodeStream, chaincode Chaincode) *Handler {\n\tv := &Handler{\n\t\tChatStream: peerChatStream,   #与Peer节点通信的流\n\t\tcc:         chaincode,      #链码\n\t}\n\tv.responseChannel = make(map[string]chan pb.ChaincodeMessage)  #链码信息响应通道\n\tv.state = created     #表示将链码容器的状态更改为created\n\treturn v    将handler返回\n}\n```\n这个``ChaincodeHandler``对象是链码侧完成链码与Peer节点之前所有的消息的控制逻辑。\n继续往下看：\n```\n#在方法执行结束的时候关闭gRPC连接\ndefer stream.CloseSend()\n#获取链码名称\nchaincodeID := &pb.ChaincodeID{Name: chaincodename}\n#将获取的链码名称序列化为有效载荷.\npayload, err := proto.Marshal(chaincodeID)\nif err != nil {\n\treturn errors.Wrap(err, \"error marshalling chaincodeID during chaincode registration\")\n}\n#日志输出,这个日志信息在安装链码的时候应该有看到过吧\nchaincodeLogger.Debugf(\"Registering.. sending %s\", pb.ChaincodeMessage_REGISTER)\n#链码容器通过handler开始通过gRPC连接向Peer节点发送第一个消息了，链码容器向Peer节点发送REGISTER消息，并附上链码的名称\nif err = handler.serialSend(&pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTER, Payload: payload}); err != nil {\n\t\treturn errors.WithMessage(err, \"error sending chaincode REGISTER\")\n\t}\n#定义一个接收消息的结构体\ntype recvMsg struct {\n    msg *pb.ChaincodeMessage\n\terr error\n}\nmsgAvail := make(chan *recvMsg, 1)\nerrc := make(chan error)\n\nreceiveMessage := func() {\n\tin, err := stream.Recv()\n\tmsgAvail <- &recvMsg{in, err}\n}\n#接收由Peer节点返回的响应消息\ngo receiveMessage()\n```\n接下来的部分就是链码容器与Peer节点详细的通信过程了：\n## 2链码侧向Peer节点发送REGISTER消息\n```\n#前面的部分都是接收到错误消息的各种输出逻辑，不再细看，我们看default这一部分，这一部分是正常情况下消息的处理情况：\nfor {\n\t\tselect {\n\t\tcase rmsg := <-msgAvail:\n\t\t\tswitch {\n\t\t\tcase rmsg.err == io.EOF:\n\t\t\t\terr = errors.Wrapf(rmsg.err, \"received EOF, ending chaincode stream\")\n\t\t\t\tchaincodeLogger.Debugf(\"%+v\", err)\n\t\t\t\treturn err\n\t\t\tcase rmsg.err != nil:\n\t\t\t\terr := errors.Wrap(rmsg.err, \"receive failed\")\n\t\t\t\tchaincodeLogger.Errorf(\"Received error from server, ending chaincode stream: %+v\", err)\n\t\t\t\treturn err\n\t\t\tcase rmsg.msg == nil:\n\t\t\t\terr := errors.New(\"received nil message, ending chaincode stream\")\n\t\t\t\tchaincodeLogger.Debugf(\"%+v\", err)\n\t\t\t\treturn err\n\t\t\tdefault:\n            #这一句日志输出应该看到过好多次吧。\n\t\t\t\tchaincodeLogger.Debugf(\"[%s]Received message %s from peer\", shorttxid(rmsg.msg.Txid), rmsg.msg.Type)\n                #重要的一个方法，在链码容器与Peer节点建立起了联系后，主要通过该方法对消息逻辑进行处理，我们点进行看一下。\n\t\t\t\terr := handler.handleMessage(rmsg.msg, errc)\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = errors.WithMessage(err, \"error handling message\")\n\t\t\t\t\treturn err\n\t\t\t\t}\n                #当消息处理完成后，再次接收消息。\n\t\t\t\tgo receiveMessage()\n\t\t\t}\n        #最后是发送失败的处理\n\t\tcase sendErr := <-errc:\n\t\t\tif sendErr != nil {\n\t\t\t\terr := errors.Wrap(sendErr, \"error sending\")\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n```\n一个重要的方法：``handleMessage``在``core/chaincode/shim/handler.go``文件第801行：\n```\nfunc (handler *Handler) handleMessage(msg *pb.ChaincodeMessage, errc chan error) error {\n    #如果链码容器接收到Peer节点发送的心跳消息后，直接将心跳消息返回，双方就一直保持联系。\n\tif msg.Type == pb.ChaincodeMessage_KEEPALIVE {\n\t\tchaincodeLogger.Debug(\"Sending KEEPALIVE response\")\n\t\thandler.serialSendAsync(msg, nil) // ignore errors, maybe next KEEPALIVE will work\n\t\treturn nil\n\t}\n    #我们先看到这里，如果再往下看的话可能会乱掉，所以还是按照逻辑顺序进行说明。\n```\n**先说一下链码侧所做的工作：**\n* 首先进行各项基本配置，然后建立起与Peer节点的gRPC连接。\n* 创建``Handler``,并更改``Handler``状态为``created``。\n* 发送``REGISTER``消息到Peer节点。\n* 等待Peer节点返回的信息\n## 3Peer节点接收到REGISTER消息后\n之前讲的都是链码侧的一系列流程，我们之前提到链码侧与Peer节点之间的第一个消息内容是由链码侧发送至Peer节点的``REGISTER``消息。接下来我们看一下Peer节点在接收到该消息后是如果进行处理的。\n代码在``core/chaincode/handler.go``文件中第174行，这里不是处理消息的开始，但是对于我们要说的链码容器启动过程中消息的处理刚好衔接上，所以就直接从这里开始了。另外很重要的一点，这里已经转换到Peer节点侧了，不是之前说的链码侧，我们看一下代码：\n```\nfunc (h *Handler) handleMessage(msg *pb.ChaincodeMessage) error {\n\tchaincodeLogger.Debugf(\"[%s] Fabric side handling ChaincodeMessage of type: %s in state %s\", shorttxid(msg.Txid), msg.Type, h.state)\n\t#这边也是首先判断是不是心跳信息，如果是心跳信息的话就什么也不做，与之前不同的是链码侧在收到心跳信息后会返回Peer节点一个心跳信息。\n\tif msg.Type == pb.ChaincodeMessage_KEEPALIVE {\n\t\treturn nil\n\t}\n    #之前我们提到，创建handler时，更改状态为created,所以这里进入到handleMessageCreatedState这个方法内.\n\tswitch h.state {\n\tcase Created:\n\t\treturn h.handleMessageCreatedState(msg)\n\tcase Ready:\n\t\treturn h.handleMessageReadyState(msg)\n\tdefault:\n\t\treturn errors.Errorf(\"handle message: invalid state %s for transaction %s\", h.state, msg.Txid)\n\t}\n}\n```\n``handleMessageCreatedState``这个方法在第191行,方法内容很简单，判断消息类型是不是REGISTER，如果是则进入HandlerRegister(msg)方法内，如果不是则返回错误信息。\n```\nfunc (h *Handler) handleMessageCreatedState(msg *pb.ChaincodeMessage) error {\n\tswitch msg.Type {\n\tcase pb.ChaincodeMessage_REGISTER:\n\t\th.HandleRegister(msg)\n\tdefault:\n\t\treturn fmt.Errorf(\"[%s] Fabric side handler cannot handle message (%s) while in created state\", msg.Txid, msg.Type)\n\t}\n\treturn nil\n}\n```\n接下来我们看一下``HandleRegister``这个方法,在第495行：\n```\nfunc (h *Handler) HandleRegister(msg *pb.ChaincodeMessage) {\n\tchaincodeLogger.Debugf(\"Received %s in state %s\", msg.Type, h.state)\n\t#获取链码ID\n\tchaincodeID := &pb.ChaincodeID{}\n    #反序列化\n\terr := proto.Unmarshal(msg.Payload, chaincodeID)\n\tif err != nil {\n\t\tchaincodeLogger.Errorf(\"Error in received %s, could NOT unmarshal registration info: %s\", pb.ChaincodeMessage_REGISTER, err)\n\t\treturn\n\t}\n\n\th.chaincodeID = chaincodeID\n\t#这一行就是将链码注册到当前Peer节点上\n\terr = h.Registry.Register(h)\n\tif err != nil {\n\t\th.notifyRegistry(err)\n\t\treturn\n\t}\n\n\t从Peer节点侧的handler获取链码名称\n\th.ccInstance = ParseName(h.chaincodeID.Name)\n\n\tchaincodeLogger.Debugf(\"Got %s for chaincodeID = %s, sending back %s\", pb.ChaincodeMessage_REGISTER, chaincodeID, pb.ChaincodeMessage_REGISTERED)\n\t#然后将REGISTERED消息返回给链码侧\n\tif err := h.serialSend(&pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTERED}); err != nil {\n\t\tchaincodeLogger.Errorf(\"error sending %s: %s\", pb.ChaincodeMessage_REGISTERED, err)\n\t\th.notifyRegistry(err)\n\t\treturn\n\t}\n\n\t//更新handler状态为Established\n\th.state = Established\n\n\tchaincodeLogger.Debugf(\"Changed state to established for %+v\", h.chaincodeID)\n\n\t#还有这个方法也要看一下\n\th.notifyRegistry(nil)\n}\n```\n简单来说``HandleRegister``的功能就是将链码注册到Peer节点上，并发送``RESIGSERED``到链码侧，最后更新``handler``状态为``Established``，我们看一下``notifyRegistry``方法,在478行：\n```\nfunc (h *Handler) notifyRegistry(err error) {\n\tif err == nil {\n\t\t//再往里面看,方法在459行\n\t\terr = h.sendReady()\n\t}\n\n\tif err != nil {\n\t\th.Registry.Failed(h.chaincodeID.Name, err)\n\t\tchaincodeLogger.Errorf(\"failed to start %s\", h.chaincodeID)\n\t\treturn\n\t}\n\n\th.Registry.Ready(h.chaincodeID.Name)\n}\n#sendReady()\nfunc (h *Handler) sendReady() error {\n\tchaincodeLogger.Debugf(\"sending READY for chaincode %+v\", h.chaincodeID)\n\tccMsg := &pb.ChaincodeMessage{Type: pb.ChaincodeMessage_READY}\n\n\t#Peer节点又向链码容器发送了READY消息\n\tif err := h.serialSend(ccMsg); err != nil {\n\t\tchaincodeLogger.Errorf(\"error sending READY (%s) for chaincode %+v\", err, h.chaincodeID)\n\t\treturn err\n\t}\n\t#同时更新handler状态为Ready\n\th.state = Ready\n\n\tchaincodeLogger.Debugf(\"Changed to state ready for chaincode %+v\", h.chaincodeID)\n\n\treturn nil\n}\n```\n到这里，Peer节点暂时分析完成，又到了链码侧对Peer节点发送的消息进行处理的流程.\n**我们先总结一下这一部分Peer节点做了哪些工作：**\n* 首先当Peer节点接收到链码侧发送的``REGISTER``消息后，将链码注册到Peer端的``Handler``上，发送``REGISTERED``到链码侧，更新``Handler``的状态为``Established``。\n* 然后Peer节点向链码侧发送``READY``消息，同时更新``Handler``的状态为``Ready``。\n\n## 4链码侧的回应\n我们回到链码侧之前的这一部分``core/chaincode/chaincode.go``中第364行,这里是链码铡对接收到的Peer节点发送的消息进行处理的逻辑,至于发生错误的情况就不再说明，我们看``handleMessage``这个方法。\n```\ngo receiveMessage()\n\tfor {\n           #相关代码\n\t\t...\n\t\terr := handler.handleMessage(rmsg.msg, errc)\n\t\t...\n            #相关代码\n\t\t\t\tgo receiveMessage()\n\t}\n```\n``handleMessage``这个方法在``core/chaincode/shim/handler.go``这个文件中，第801行。\n```\n#主要就是这一部分：\nswitch handler.state {\n\tcase ready:\n\t\terr = handler.handleReady(msg, errc)\n\tcase established:\n\t\terr = handler.handleEstablished(msg, errc)\n\tcase created:\n\t\terr = handler.handleCreated(msg, errc)\n\tdefault:\n\t\terr = errors.Errorf(\"[%s] Chaincode handler cannot handle message (%s) with payload size (%d) while in state: %s\", msg.Txid, msg.Type, len(msg.Payload), handler.state)\n}\n```\n* 首先链码侧接收到Peer节点发送的``REGISTERED``消息后，这里链码侧的``handler``与Peer节点侧的``handler``并不是同一个，不要搞混了。判断当前链码侧``handler``的状态为``created``，进入到``handleCreated``方法中,在792行：\n```\n#将链码侧的handler的状态更改为established\nif msg.Type == pb.ChaincodeMessage_REGISTERED {\n\thandler.state = established\n\treturn nil\n}\n```\n* 当链码侧接收到Peer节点发送的``READY``消息后，又一次进入上面的逻辑，由于链码侧的``handler``的状态已经更改为``established``,所以这次进入到``handleEstablished``方法中。在783行：\n```\n#然后将链码侧的handler的状态更改为ready\nif msg.Type == pb.ChaincodeMessage_READY {\n\thandler.state = ready\n\treturn nil\n}\n```\n* 另外，当用户对链码进行实例化操作时，会通过Peer节点向链码侧发送``INIT``消息，这里涉及到背书过程，之后再对背书过程进行讨论，我们在这里只关注链码侧接收到``INIT``消息后的逻辑，还是``handleMessage``这个方法中：\n```\n#当判断到消息类型为INIT时，会执行这个方法。\nhandler.handleInit(msg, errc)\n```\n``handler.handleInit(msg, errc)``方法在第177行：\n```\nfunc (handler *Handler) handleInit(msg *pb.ChaincodeMessage, errc chan error) {\n\tgo func() {\n\t\tvar nextStateMsg *pb.ChaincodeMessage\n\n\t\tdefer func() {\n            #这一名相当于更新链码的状态\n\t\t\thandler.triggerNextState(nextStateMsg, errc)\n\t\t}()\n        #判断错误信息\n\t\terrFunc := func(err error, payload []byte, ce *pb.ChaincodeEvent, errFmt string, args ...interface{}) *pb.ChaincodeMessage {\n\t\t\tif err != nil {\n\t\t\t\t// Send ERROR message to chaincode support and change state\n\t\t\t\tif payload == nil {\n\t\t\t\t\tpayload = []byte(err.Error())\n\t\t\t\t}\n\t\t\t\tchaincodeLogger.Errorf(errFmt, args...)\n\t\t\t\treturn &pb.ChaincodeMessage{Type: pb.ChaincodeMessage_ERROR, Payload: payload, Txid: msg.Txid, ChaincodeEvent: ce, ChannelId: msg.ChannelId}\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\t#获取用户输入的参数\n\t\tinput := &pb.ChaincodeInput{}\n        #反序列化\n\t\tunmarshalErr := proto.Unmarshal(msg.Payload, input)\n\t\tif nextStateMsg = errFunc(unmarshalErr, nil, nil, \"[%s] Incorrect payload format. Sending %s\", shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n\t\t\treturn\n\t\t}\n\n\t\t#ChaincodeStub应该很熟悉了,很重要的一个对象，包含一项提案中所需要的内容。在``core/chaincode/shim/chaincode.go``文件中第53行，有兴趣可以点进去看一下\n\t\tstub := new(ChaincodeStub)\n        #这一行代码的意思就是将提案中的信息抽取出来赋值到ChaincodeStub这个对象中\n       err := stub.init(handler, msg.ChannelId, msg.Txid, input, msg.Proposal)\n       if nextStateMsg = errFunc(err, nil, stub.chaincodeEvent, \"[%s] Init get error response. Sending %s\", shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n\t\t\treturn\n\t   }\n       #这里的Init方法就是链码中所写的Init()方法，就不再解释了\n       res := handler.cc.Init(stub)\n       chaincodeLogger.Debugf(\"[%s] Init get response status: %d\", shorttxid(msg.Txid), res.Status)\n        #ERROR的值为500,OK=200，ERRORTHRESHOLD = 400，大于等于400就代表错误信息或者被背书节点拒绝。\n\t\tif res.Status >= ERROR {\n\t\t\terr = errors.New(res.Message)\n\t\t\tif nextStateMsg = errFunc(err, []byte(res.Message), stub.chaincodeEvent, \"[%s] Init get error response. Sending %s\", shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n        resBytes, err := proto.Marshal(&res)\n\t\tif err != nil {\n\t\t\tpayload := []byte(err.Error())\n\t\t\tchaincodeLogger.Errorf(\"[%s] Init marshal response error [%s]. Sending %s\", shorttxid(msg.Txid), err, pb.ChaincodeMessage_ERROR)\n\t\t\tnextStateMsg = &pb.ChaincodeMessage{Type: pb.ChaincodeMessage_ERROR, Payload: payload, Txid: msg.Txid, ChaincodeEvent: stub.chaincodeEvent}\n\t\t\treturn\n\t\t}\n\n\t\t// Send COMPLETED message to chaincode support and change state\n\t\tnextStateMsg = &pb.ChaincodeMessage{Type: pb.ChaincodeMessage_COMPLETED, Payload: resBytes, Txid: msg.Txid, ChaincodeEvent: stub.chaincodeEvent, ChannelId: stub.ChannelId}\n\t\tchaincodeLogger.Debugf(\"[%s] Init succeeded. Sending %s\", shorttxid(msg.Txid), pb.ChaincodeMessage_COMPLETED)\n        #到这里就结束了，会调用上面的handler.triggerNextState(nextStateMsg, errc)方法，这个方法将初始化数据与COMPLETED状态发送至Peer节点。\n\t}()\n}\n```\n这个方法还是比较简单的，一共做了这些事情：\n\n* 获取用户的输入数据\n* 新建一个``ChainCodeStub``对象，然后将用户输入的数据赋值给该对象\n* 调用用户链码中的``Init()``方法\n* 将所有数据封装成``ChainCodeMessage``，类型为``COMPLETED``,发送至Peer节点。\n\n这个时候链码已经初始化完成，已经进入了可被调用(``invoke``)的状态.\n之后的流程就差不多了，Peer节点发送``TRANSACTION``消息给链码侧，调用``Invoke()``方法，之后链码侧发送具体的调用方法到Peer节点，由Peer节点进行相应的处理，最后返回``RESPONSE``消息到链码侧，链码侧接收到``RESPONSE``消息后，返回``COMPLETED``消息到Peer节点。\n\n## 5总结\n到这里，Peer节点与链码侧的``handler``都处于``READY``状态,链码容器已经启动完成。最后总结一下整体的流程：\n\n1. 通过用户端链码中的``main``方法，调用了``core/chaincode/shim/chaincode.go``中的``Start()``方法，从而开始了链码的启动。\n2. 首先进行相关的配置比如基本的加密，证书的读取。\n3. 创建与Peer节点之间的gRPC连接，创建``handler``实例。\n4. 由链码容器向Peer节点发送第一个消息:``REGISTER``,然后等待接收由Peer节点发送的消息。如果接收到的是心跳消息，则向Peer节点返回心跳消息。\n5. Peer节点接收到链码容器发送的``REGISTER``消息后，将其注册到Peer节点端的``handler``上。\n6. Peer节点发送``REGISTERED``消息到链码侧，同时更新Peer节点端的``handler``状态为``Established``。\n7. Peer节点发送``Ready``消息到链码侧，同时更新Peer节点端的``handler``状态为``Ready``。\n8. 链码侧接收到由Peer节点发送的``REGISTERED``消息后，更新链码侧的``handler``状态为``Established``。\n9. 链码侧接收到由Peer节点发送的``READY``消息后，更新链码侧的``handler``状态为``ready``。\n10. 当用户执行实例化链码时，通过Peer节点向链码侧发送``INIT``消息。链码侧接收到``INIT``消息后，根据用户输入的参数进行实例化操作。实例化完成后，返回``COMPLETED``消息到Peer节点。\n11. 到这里链码容器已经启动，可以对链码数据进行查询调用等操作了。\n\n另外，阅读Fabric源码中有一些没有看明白或者分析有误的地方，还望大家能够批评指正。\n\n\n最后附上参考文档：[传送门](https://legacy.gitbook.com/book/yeasy/hyperledger_code_fabric/details)\n以及Fabric源码地址：[传送门](https://github.com/hyperledger/fabric)","source":"_posts/blog/fabric/Fabric1.4源码解析之链码容器启动过程.md","raw":"---\ntitle: Fabric1.4源码解析：链码容器启动过程\ndate: 2019-12-19 21:02:38\ntags: fabric\ncategories: fabric源码解析\n---\n想写点东西记录一下最近看的一些Fabric源码,本文使用的是**fabric1.4**的版本，所以对于其他版本的fabric，内容可能会有所不同。\n其实我仅仅知道Go语言一些语法的使用，并不太熟悉Go语言，所以解析的内容可能会有误，欢迎大家批评指正。\n本文想针对Fabric中链码容器的启动过程进行源码的解析。这里的链码指的是用户链码不是系统链码,顺便回顾一下系统链码:\n**lscc**(Life Cycle System ChainCode)生命周期系统链码\n**cscc**(Configuration System ChainCode)配置系统链码\n**escc**(Endorser System ChainCode)背书系统链码\n**qscc**(Query System ChainCode)查询系统链码\n**vscc**(Verification System ChainCode)验证系统链码\n本文主要解析的是用户链码的启动过程。\n## 1 起点\n```\n#这是用户端链码的main方法，也是整个流程的入口点，调用了shim包中的Start(cc Chaincode)方法.\nfunc main(){\n    err :=shim.Start(new(Chaincode))\n    if err != nil {\n        fmt.Printf(\"Error starting Chaincode: %s\",err)\n    }\n}\n```\n首先定位到``fabric/core/chaincode/shim/chaincode.go``这个文件中的``Start``方法，这里是链码启动的起点。\n可以看到传的参数就是chaincode,接下来分析一下启动过程\n```\n#方法中第一行代码，根据名字可以看出是对链码的Log进行设置\nSetupChaincodeLogging()\n#从输入中获取用户定义的链码的名称\nchaincodename := viper.GetString(\"chaincode.id.name\")\n#如果没有输入链码名称，直接返回没有提供链码id的错误，下面则不再执行\nif chaincodename == \"\" {\n    return errors.New(\"error chaincode id not provided\")\n}\n#看名字是一个工厂方法，点进行看一下\nerr := factory.InitFactories(factory.GetDefaultOpts())\n```\n首先进入到``factory.GetDefaultOpts()``方法中：\n```\nfunc GetDefaultOpts() *FactoryOpts {\n\treturn &FactoryOpts{\n\t\tProviderName: \"SW\",\n\t\tSwOpts: &SwOpts{\n\t\t\tHashFamily: \"SHA2\",   #HASH类型\n\t\t\tSecLevel:   256,    #HASH级别\n\n\t\t\tEphemeral: true,\n\t\t},\n\t}\n}\n#可以猜到这个方法是获取默认的加密操作，使用SHA256进行数据加密\n```\n不难猜到``factory.InitFactories``这个方法就是为当前链码设置加密操作的一系列内容。回到``Start()``方法中接着往下看.\n```\n#这一部分就是将链码数据以流的方式读取进来，userChaincodeStreamGetter是一个方法，点进去看一下\nif streamGetter == nil {\n\tstreamGetter = userChaincodeStreamGetter\n}\nstream, err := streamGetter(chaincodename)\nif err != nil {\n\treturn err\n}\n```\n``userChaincodeStreamGetter``还是在这个文件中第82行:\n```\n#这里的name是链码名称，读取到链码数据后以PeerChainCodeStream的方式返回\nfunc userChaincodeStreamGetter(name string) (PeerChaincodeStream, error) {\n    #获取peer.address\n\tflag.StringVar(&peerAddress, \"peer.address\", \"\", \"peer address\")\n\t//判断是否使能TLS\n\tif viper.GetBool(\"peer.tls.enabled\") {\n        #获取tls密钥地址，在用户安装链码的时候指定\n\t\tkeyPath := viper.GetString(\"tls.client.key.path\")\n        #获取tls证书地址\n\t\tcertPath := viper.GetString(\"tls.client.cert.path\")\n        #从文件中读取密钥数据\n\t\tdata, err1 := ioutil.ReadFile(keyPath)\n\t\tif err1 != nil {\n\t\t\terr1 = errors.Wrap(err1, fmt.Sprintf(\"error trying to read file content %s\", keyPath))\n\t\t\tchaincodeLogger.Errorf(\"%+v\", err1)\n\t\t\treturn nil, err1\n\t\t}\n\t\tkey = string(data)\n         #从文件中读取证书数据\n\t\tdata, err1 = ioutil.ReadFile(certPath)\n\t\tif err1 != nil {\n\t\t\terr1 = errors.Wrap(err1, fmt.Sprintf(\"error trying to read file content %s\", certPath))\n\t\t\tchaincodeLogger.Errorf(\"%+v\", err1)\n\t\t\treturn nil, err1\n\t\t}\n\t\tcert = string(data)\n\t}\n    #解析命令行参数到定义的flag\n\tflag.Parse()\n    #日志输出\n\tchaincodeLogger.Debugf(\"Peer address: %s\", getPeerAddress())\n\n\t//与peer节点建立连接\n\tclientConn, err := newPeerClientConnection()\n```\n看一下这个方法里面的内容，还是这个文件第317行：\n```\nfunc newPeerClientConnection() (*grpc.ClientConn, error) {\n    #首先获取到peer节点的地址\n\tvar peerAddress = getPeerAddress()\n    #看名字就知道了，设置与链码之间的心中信息\n\tkaOpts := &comm.KeepaliveOptions{\n\t\tClientInterval: time.Duration(1) * time.Minute,\n\t\tClientTimeout:  time.Duration(20) * time.Second,\n\t}\n```\n 判断是否使能了TLS，然后根据结果建立链接,如何建立链接就不再细看了，我们回到之前的部分\n```\n\tif viper.GetBool(\"peer.tls.enabled\") {\n\t\treturn comm.NewClientConnectionWithAddress(peerAddress, true, true,\n\t\t\tcomm.InitTLSForShim(key, cert), kaOpts)\n\t}\n\treturn comm.NewClientConnectionWithAddress(peerAddress, true, false, nil, kaOpts)\n}\n```\n还是之前的``userChaincodeStreamGetter``方法\n```\nclientConn, err := newPeerClientConnection()\n\tif err != nil {\n\t\terr = errors.Wrap(err, \"error trying to connect to local peer\")\n\t\tchaincodeLogger.Errorf(\"%+v\", err)\n\t\treturn nil, err\n\t}\n\n\tchaincodeLogger.Debugf(\"os.Args returns: %s\", os.Args)\n\n    #接下来是这个方法，返回一个ChaincodeSupportClient实例,对应着链码容器\n\tchaincodeSupportClient := pb.NewChaincodeSupportClient(clientConn)\n\n\t//这一步是与peer节点建立gRPC连接\n\tstream, err := chaincodeSupportClient.Register(context.Background())\n\tif err != nil {\n\t\treturn nil, errors.WithMessage(err, fmt.Sprintf(\"error chatting with leader at address=%s\", getPeerAddress()))\n\t}\n\n\treturn stream, nil\n}\n```\n这个方法结束之后，链码容器与Peer节点已经建立起了连接，接下来链码容器与Peer节点开始互相发送消息了。\n返回到``Start()``方法中，还剩最后的一个方法``chatWithPeer()``：\n```\n\terr = chatWithPeer(chaincodename, stream, cc)\n\treturn err\n}\n```\n看一下链码容器与Peer节点是如何互相通信的。这个方法是链码容器启动的过程中最重要的方法，包含所有的通信流程。``chatWithPeer()``在331行:\n```\nfunc chatWithPeer(chaincodename string, stream PeerChaincodeStream, cc Chaincode)\n#传入的参数有链码名称，流(这个是之前链码容器与Peer节点建立gRPC连接所返回的)，链码\n```\n首先第一步是新建一个``ChaincodeHandler``对象：是非常重要的一个对象。看一下该对象的内容,在``core/chaincode/shim/handler.go``文件中第166行:\n```\nfunc newChaincodeHandler(peerChatStream PeerChaincodeStream, chaincode Chaincode) *Handler {\n\tv := &Handler{\n\t\tChatStream: peerChatStream,   #与Peer节点通信的流\n\t\tcc:         chaincode,      #链码\n\t}\n\tv.responseChannel = make(map[string]chan pb.ChaincodeMessage)  #链码信息响应通道\n\tv.state = created     #表示将链码容器的状态更改为created\n\treturn v    将handler返回\n}\n```\n这个``ChaincodeHandler``对象是链码侧完成链码与Peer节点之前所有的消息的控制逻辑。\n继续往下看：\n```\n#在方法执行结束的时候关闭gRPC连接\ndefer stream.CloseSend()\n#获取链码名称\nchaincodeID := &pb.ChaincodeID{Name: chaincodename}\n#将获取的链码名称序列化为有效载荷.\npayload, err := proto.Marshal(chaincodeID)\nif err != nil {\n\treturn errors.Wrap(err, \"error marshalling chaincodeID during chaincode registration\")\n}\n#日志输出,这个日志信息在安装链码的时候应该有看到过吧\nchaincodeLogger.Debugf(\"Registering.. sending %s\", pb.ChaincodeMessage_REGISTER)\n#链码容器通过handler开始通过gRPC连接向Peer节点发送第一个消息了，链码容器向Peer节点发送REGISTER消息，并附上链码的名称\nif err = handler.serialSend(&pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTER, Payload: payload}); err != nil {\n\t\treturn errors.WithMessage(err, \"error sending chaincode REGISTER\")\n\t}\n#定义一个接收消息的结构体\ntype recvMsg struct {\n    msg *pb.ChaincodeMessage\n\terr error\n}\nmsgAvail := make(chan *recvMsg, 1)\nerrc := make(chan error)\n\nreceiveMessage := func() {\n\tin, err := stream.Recv()\n\tmsgAvail <- &recvMsg{in, err}\n}\n#接收由Peer节点返回的响应消息\ngo receiveMessage()\n```\n接下来的部分就是链码容器与Peer节点详细的通信过程了：\n## 2链码侧向Peer节点发送REGISTER消息\n```\n#前面的部分都是接收到错误消息的各种输出逻辑，不再细看，我们看default这一部分，这一部分是正常情况下消息的处理情况：\nfor {\n\t\tselect {\n\t\tcase rmsg := <-msgAvail:\n\t\t\tswitch {\n\t\t\tcase rmsg.err == io.EOF:\n\t\t\t\terr = errors.Wrapf(rmsg.err, \"received EOF, ending chaincode stream\")\n\t\t\t\tchaincodeLogger.Debugf(\"%+v\", err)\n\t\t\t\treturn err\n\t\t\tcase rmsg.err != nil:\n\t\t\t\terr := errors.Wrap(rmsg.err, \"receive failed\")\n\t\t\t\tchaincodeLogger.Errorf(\"Received error from server, ending chaincode stream: %+v\", err)\n\t\t\t\treturn err\n\t\t\tcase rmsg.msg == nil:\n\t\t\t\terr := errors.New(\"received nil message, ending chaincode stream\")\n\t\t\t\tchaincodeLogger.Debugf(\"%+v\", err)\n\t\t\t\treturn err\n\t\t\tdefault:\n            #这一句日志输出应该看到过好多次吧。\n\t\t\t\tchaincodeLogger.Debugf(\"[%s]Received message %s from peer\", shorttxid(rmsg.msg.Txid), rmsg.msg.Type)\n                #重要的一个方法，在链码容器与Peer节点建立起了联系后，主要通过该方法对消息逻辑进行处理，我们点进行看一下。\n\t\t\t\terr := handler.handleMessage(rmsg.msg, errc)\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = errors.WithMessage(err, \"error handling message\")\n\t\t\t\t\treturn err\n\t\t\t\t}\n                #当消息处理完成后，再次接收消息。\n\t\t\t\tgo receiveMessage()\n\t\t\t}\n        #最后是发送失败的处理\n\t\tcase sendErr := <-errc:\n\t\t\tif sendErr != nil {\n\t\t\t\terr := errors.Wrap(sendErr, \"error sending\")\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n```\n一个重要的方法：``handleMessage``在``core/chaincode/shim/handler.go``文件第801行：\n```\nfunc (handler *Handler) handleMessage(msg *pb.ChaincodeMessage, errc chan error) error {\n    #如果链码容器接收到Peer节点发送的心跳消息后，直接将心跳消息返回，双方就一直保持联系。\n\tif msg.Type == pb.ChaincodeMessage_KEEPALIVE {\n\t\tchaincodeLogger.Debug(\"Sending KEEPALIVE response\")\n\t\thandler.serialSendAsync(msg, nil) // ignore errors, maybe next KEEPALIVE will work\n\t\treturn nil\n\t}\n    #我们先看到这里，如果再往下看的话可能会乱掉，所以还是按照逻辑顺序进行说明。\n```\n**先说一下链码侧所做的工作：**\n* 首先进行各项基本配置，然后建立起与Peer节点的gRPC连接。\n* 创建``Handler``,并更改``Handler``状态为``created``。\n* 发送``REGISTER``消息到Peer节点。\n* 等待Peer节点返回的信息\n## 3Peer节点接收到REGISTER消息后\n之前讲的都是链码侧的一系列流程，我们之前提到链码侧与Peer节点之间的第一个消息内容是由链码侧发送至Peer节点的``REGISTER``消息。接下来我们看一下Peer节点在接收到该消息后是如果进行处理的。\n代码在``core/chaincode/handler.go``文件中第174行，这里不是处理消息的开始，但是对于我们要说的链码容器启动过程中消息的处理刚好衔接上，所以就直接从这里开始了。另外很重要的一点，这里已经转换到Peer节点侧了，不是之前说的链码侧，我们看一下代码：\n```\nfunc (h *Handler) handleMessage(msg *pb.ChaincodeMessage) error {\n\tchaincodeLogger.Debugf(\"[%s] Fabric side handling ChaincodeMessage of type: %s in state %s\", shorttxid(msg.Txid), msg.Type, h.state)\n\t#这边也是首先判断是不是心跳信息，如果是心跳信息的话就什么也不做，与之前不同的是链码侧在收到心跳信息后会返回Peer节点一个心跳信息。\n\tif msg.Type == pb.ChaincodeMessage_KEEPALIVE {\n\t\treturn nil\n\t}\n    #之前我们提到，创建handler时，更改状态为created,所以这里进入到handleMessageCreatedState这个方法内.\n\tswitch h.state {\n\tcase Created:\n\t\treturn h.handleMessageCreatedState(msg)\n\tcase Ready:\n\t\treturn h.handleMessageReadyState(msg)\n\tdefault:\n\t\treturn errors.Errorf(\"handle message: invalid state %s for transaction %s\", h.state, msg.Txid)\n\t}\n}\n```\n``handleMessageCreatedState``这个方法在第191行,方法内容很简单，判断消息类型是不是REGISTER，如果是则进入HandlerRegister(msg)方法内，如果不是则返回错误信息。\n```\nfunc (h *Handler) handleMessageCreatedState(msg *pb.ChaincodeMessage) error {\n\tswitch msg.Type {\n\tcase pb.ChaincodeMessage_REGISTER:\n\t\th.HandleRegister(msg)\n\tdefault:\n\t\treturn fmt.Errorf(\"[%s] Fabric side handler cannot handle message (%s) while in created state\", msg.Txid, msg.Type)\n\t}\n\treturn nil\n}\n```\n接下来我们看一下``HandleRegister``这个方法,在第495行：\n```\nfunc (h *Handler) HandleRegister(msg *pb.ChaincodeMessage) {\n\tchaincodeLogger.Debugf(\"Received %s in state %s\", msg.Type, h.state)\n\t#获取链码ID\n\tchaincodeID := &pb.ChaincodeID{}\n    #反序列化\n\terr := proto.Unmarshal(msg.Payload, chaincodeID)\n\tif err != nil {\n\t\tchaincodeLogger.Errorf(\"Error in received %s, could NOT unmarshal registration info: %s\", pb.ChaincodeMessage_REGISTER, err)\n\t\treturn\n\t}\n\n\th.chaincodeID = chaincodeID\n\t#这一行就是将链码注册到当前Peer节点上\n\terr = h.Registry.Register(h)\n\tif err != nil {\n\t\th.notifyRegistry(err)\n\t\treturn\n\t}\n\n\t从Peer节点侧的handler获取链码名称\n\th.ccInstance = ParseName(h.chaincodeID.Name)\n\n\tchaincodeLogger.Debugf(\"Got %s for chaincodeID = %s, sending back %s\", pb.ChaincodeMessage_REGISTER, chaincodeID, pb.ChaincodeMessage_REGISTERED)\n\t#然后将REGISTERED消息返回给链码侧\n\tif err := h.serialSend(&pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTERED}); err != nil {\n\t\tchaincodeLogger.Errorf(\"error sending %s: %s\", pb.ChaincodeMessage_REGISTERED, err)\n\t\th.notifyRegistry(err)\n\t\treturn\n\t}\n\n\t//更新handler状态为Established\n\th.state = Established\n\n\tchaincodeLogger.Debugf(\"Changed state to established for %+v\", h.chaincodeID)\n\n\t#还有这个方法也要看一下\n\th.notifyRegistry(nil)\n}\n```\n简单来说``HandleRegister``的功能就是将链码注册到Peer节点上，并发送``RESIGSERED``到链码侧，最后更新``handler``状态为``Established``，我们看一下``notifyRegistry``方法,在478行：\n```\nfunc (h *Handler) notifyRegistry(err error) {\n\tif err == nil {\n\t\t//再往里面看,方法在459行\n\t\terr = h.sendReady()\n\t}\n\n\tif err != nil {\n\t\th.Registry.Failed(h.chaincodeID.Name, err)\n\t\tchaincodeLogger.Errorf(\"failed to start %s\", h.chaincodeID)\n\t\treturn\n\t}\n\n\th.Registry.Ready(h.chaincodeID.Name)\n}\n#sendReady()\nfunc (h *Handler) sendReady() error {\n\tchaincodeLogger.Debugf(\"sending READY for chaincode %+v\", h.chaincodeID)\n\tccMsg := &pb.ChaincodeMessage{Type: pb.ChaincodeMessage_READY}\n\n\t#Peer节点又向链码容器发送了READY消息\n\tif err := h.serialSend(ccMsg); err != nil {\n\t\tchaincodeLogger.Errorf(\"error sending READY (%s) for chaincode %+v\", err, h.chaincodeID)\n\t\treturn err\n\t}\n\t#同时更新handler状态为Ready\n\th.state = Ready\n\n\tchaincodeLogger.Debugf(\"Changed to state ready for chaincode %+v\", h.chaincodeID)\n\n\treturn nil\n}\n```\n到这里，Peer节点暂时分析完成，又到了链码侧对Peer节点发送的消息进行处理的流程.\n**我们先总结一下这一部分Peer节点做了哪些工作：**\n* 首先当Peer节点接收到链码侧发送的``REGISTER``消息后，将链码注册到Peer端的``Handler``上，发送``REGISTERED``到链码侧，更新``Handler``的状态为``Established``。\n* 然后Peer节点向链码侧发送``READY``消息，同时更新``Handler``的状态为``Ready``。\n\n## 4链码侧的回应\n我们回到链码侧之前的这一部分``core/chaincode/chaincode.go``中第364行,这里是链码铡对接收到的Peer节点发送的消息进行处理的逻辑,至于发生错误的情况就不再说明，我们看``handleMessage``这个方法。\n```\ngo receiveMessage()\n\tfor {\n           #相关代码\n\t\t...\n\t\terr := handler.handleMessage(rmsg.msg, errc)\n\t\t...\n            #相关代码\n\t\t\t\tgo receiveMessage()\n\t}\n```\n``handleMessage``这个方法在``core/chaincode/shim/handler.go``这个文件中，第801行。\n```\n#主要就是这一部分：\nswitch handler.state {\n\tcase ready:\n\t\terr = handler.handleReady(msg, errc)\n\tcase established:\n\t\terr = handler.handleEstablished(msg, errc)\n\tcase created:\n\t\terr = handler.handleCreated(msg, errc)\n\tdefault:\n\t\terr = errors.Errorf(\"[%s] Chaincode handler cannot handle message (%s) with payload size (%d) while in state: %s\", msg.Txid, msg.Type, len(msg.Payload), handler.state)\n}\n```\n* 首先链码侧接收到Peer节点发送的``REGISTERED``消息后，这里链码侧的``handler``与Peer节点侧的``handler``并不是同一个，不要搞混了。判断当前链码侧``handler``的状态为``created``，进入到``handleCreated``方法中,在792行：\n```\n#将链码侧的handler的状态更改为established\nif msg.Type == pb.ChaincodeMessage_REGISTERED {\n\thandler.state = established\n\treturn nil\n}\n```\n* 当链码侧接收到Peer节点发送的``READY``消息后，又一次进入上面的逻辑，由于链码侧的``handler``的状态已经更改为``established``,所以这次进入到``handleEstablished``方法中。在783行：\n```\n#然后将链码侧的handler的状态更改为ready\nif msg.Type == pb.ChaincodeMessage_READY {\n\thandler.state = ready\n\treturn nil\n}\n```\n* 另外，当用户对链码进行实例化操作时，会通过Peer节点向链码侧发送``INIT``消息，这里涉及到背书过程，之后再对背书过程进行讨论，我们在这里只关注链码侧接收到``INIT``消息后的逻辑，还是``handleMessage``这个方法中：\n```\n#当判断到消息类型为INIT时，会执行这个方法。\nhandler.handleInit(msg, errc)\n```\n``handler.handleInit(msg, errc)``方法在第177行：\n```\nfunc (handler *Handler) handleInit(msg *pb.ChaincodeMessage, errc chan error) {\n\tgo func() {\n\t\tvar nextStateMsg *pb.ChaincodeMessage\n\n\t\tdefer func() {\n            #这一名相当于更新链码的状态\n\t\t\thandler.triggerNextState(nextStateMsg, errc)\n\t\t}()\n        #判断错误信息\n\t\terrFunc := func(err error, payload []byte, ce *pb.ChaincodeEvent, errFmt string, args ...interface{}) *pb.ChaincodeMessage {\n\t\t\tif err != nil {\n\t\t\t\t// Send ERROR message to chaincode support and change state\n\t\t\t\tif payload == nil {\n\t\t\t\t\tpayload = []byte(err.Error())\n\t\t\t\t}\n\t\t\t\tchaincodeLogger.Errorf(errFmt, args...)\n\t\t\t\treturn &pb.ChaincodeMessage{Type: pb.ChaincodeMessage_ERROR, Payload: payload, Txid: msg.Txid, ChaincodeEvent: ce, ChannelId: msg.ChannelId}\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\t#获取用户输入的参数\n\t\tinput := &pb.ChaincodeInput{}\n        #反序列化\n\t\tunmarshalErr := proto.Unmarshal(msg.Payload, input)\n\t\tif nextStateMsg = errFunc(unmarshalErr, nil, nil, \"[%s] Incorrect payload format. Sending %s\", shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n\t\t\treturn\n\t\t}\n\n\t\t#ChaincodeStub应该很熟悉了,很重要的一个对象，包含一项提案中所需要的内容。在``core/chaincode/shim/chaincode.go``文件中第53行，有兴趣可以点进去看一下\n\t\tstub := new(ChaincodeStub)\n        #这一行代码的意思就是将提案中的信息抽取出来赋值到ChaincodeStub这个对象中\n       err := stub.init(handler, msg.ChannelId, msg.Txid, input, msg.Proposal)\n       if nextStateMsg = errFunc(err, nil, stub.chaincodeEvent, \"[%s] Init get error response. Sending %s\", shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n\t\t\treturn\n\t   }\n       #这里的Init方法就是链码中所写的Init()方法，就不再解释了\n       res := handler.cc.Init(stub)\n       chaincodeLogger.Debugf(\"[%s] Init get response status: %d\", shorttxid(msg.Txid), res.Status)\n        #ERROR的值为500,OK=200，ERRORTHRESHOLD = 400，大于等于400就代表错误信息或者被背书节点拒绝。\n\t\tif res.Status >= ERROR {\n\t\t\terr = errors.New(res.Message)\n\t\t\tif nextStateMsg = errFunc(err, []byte(res.Message), stub.chaincodeEvent, \"[%s] Init get error response. Sending %s\", shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n        resBytes, err := proto.Marshal(&res)\n\t\tif err != nil {\n\t\t\tpayload := []byte(err.Error())\n\t\t\tchaincodeLogger.Errorf(\"[%s] Init marshal response error [%s]. Sending %s\", shorttxid(msg.Txid), err, pb.ChaincodeMessage_ERROR)\n\t\t\tnextStateMsg = &pb.ChaincodeMessage{Type: pb.ChaincodeMessage_ERROR, Payload: payload, Txid: msg.Txid, ChaincodeEvent: stub.chaincodeEvent}\n\t\t\treturn\n\t\t}\n\n\t\t// Send COMPLETED message to chaincode support and change state\n\t\tnextStateMsg = &pb.ChaincodeMessage{Type: pb.ChaincodeMessage_COMPLETED, Payload: resBytes, Txid: msg.Txid, ChaincodeEvent: stub.chaincodeEvent, ChannelId: stub.ChannelId}\n\t\tchaincodeLogger.Debugf(\"[%s] Init succeeded. Sending %s\", shorttxid(msg.Txid), pb.ChaincodeMessage_COMPLETED)\n        #到这里就结束了，会调用上面的handler.triggerNextState(nextStateMsg, errc)方法，这个方法将初始化数据与COMPLETED状态发送至Peer节点。\n\t}()\n}\n```\n这个方法还是比较简单的，一共做了这些事情：\n\n* 获取用户的输入数据\n* 新建一个``ChainCodeStub``对象，然后将用户输入的数据赋值给该对象\n* 调用用户链码中的``Init()``方法\n* 将所有数据封装成``ChainCodeMessage``，类型为``COMPLETED``,发送至Peer节点。\n\n这个时候链码已经初始化完成，已经进入了可被调用(``invoke``)的状态.\n之后的流程就差不多了，Peer节点发送``TRANSACTION``消息给链码侧，调用``Invoke()``方法，之后链码侧发送具体的调用方法到Peer节点，由Peer节点进行相应的处理，最后返回``RESPONSE``消息到链码侧，链码侧接收到``RESPONSE``消息后，返回``COMPLETED``消息到Peer节点。\n\n## 5总结\n到这里，Peer节点与链码侧的``handler``都处于``READY``状态,链码容器已经启动完成。最后总结一下整体的流程：\n\n1. 通过用户端链码中的``main``方法，调用了``core/chaincode/shim/chaincode.go``中的``Start()``方法，从而开始了链码的启动。\n2. 首先进行相关的配置比如基本的加密，证书的读取。\n3. 创建与Peer节点之间的gRPC连接，创建``handler``实例。\n4. 由链码容器向Peer节点发送第一个消息:``REGISTER``,然后等待接收由Peer节点发送的消息。如果接收到的是心跳消息，则向Peer节点返回心跳消息。\n5. Peer节点接收到链码容器发送的``REGISTER``消息后，将其注册到Peer节点端的``handler``上。\n6. Peer节点发送``REGISTERED``消息到链码侧，同时更新Peer节点端的``handler``状态为``Established``。\n7. Peer节点发送``Ready``消息到链码侧，同时更新Peer节点端的``handler``状态为``Ready``。\n8. 链码侧接收到由Peer节点发送的``REGISTERED``消息后，更新链码侧的``handler``状态为``Established``。\n9. 链码侧接收到由Peer节点发送的``READY``消息后，更新链码侧的``handler``状态为``ready``。\n10. 当用户执行实例化链码时，通过Peer节点向链码侧发送``INIT``消息。链码侧接收到``INIT``消息后，根据用户输入的参数进行实例化操作。实例化完成后，返回``COMPLETED``消息到Peer节点。\n11. 到这里链码容器已经启动，可以对链码数据进行查询调用等操作了。\n\n另外，阅读Fabric源码中有一些没有看明白或者分析有误的地方，还望大家能够批评指正。\n\n\n最后附上参考文档：[传送门](https://legacy.gitbook.com/book/yeasy/hyperledger_code_fabric/details)\n以及Fabric源码地址：[传送门](https://github.com/hyperledger/fabric)","slug":"blog/fabric/Fabric1.4源码解析之链码容器启动过程","published":1,"updated":"2020-05-11T03:44:37.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyjd003nk0vq437qfjbu","content":"<p>想写点东西记录一下最近看的一些Fabric源码,本文使用的是<strong>fabric1.4</strong>的版本，所以对于其他版本的fabric，内容可能会有所不同。<br>其实我仅仅知道Go语言一些语法的使用，并不太熟悉Go语言，所以解析的内容可能会有误，欢迎大家批评指正。<br>本文想针对Fabric中链码容器的启动过程进行源码的解析。这里的链码指的是用户链码不是系统链码,顺便回顾一下系统链码:<br><strong>lscc</strong>(Life Cycle System ChainCode)生命周期系统链码<br><strong>cscc</strong>(Configuration System ChainCode)配置系统链码<br><strong>escc</strong>(Endorser System ChainCode)背书系统链码<br><strong>qscc</strong>(Query System ChainCode)查询系统链码<br><strong>vscc</strong>(Verification System ChainCode)验证系统链码<br>本文主要解析的是用户链码的启动过程。</p>\n<h2 id=\"1-起点\"><a href=\"#1-起点\" class=\"headerlink\" title=\"1 起点\"></a>1 起点</h2><pre><code>#这是用户端链码的main方法，也是整个流程的入口点，调用了shim包中的Start(cc Chaincode)方法.\nfunc main(){\n    err :=shim.Start(new(Chaincode))\n    if err != nil {\n        fmt.Printf(&quot;Error starting Chaincode: %s&quot;,err)\n    }\n}</code></pre><p>首先定位到<code>fabric/core/chaincode/shim/chaincode.go</code>这个文件中的<code>Start</code>方法，这里是链码启动的起点。<br>可以看到传的参数就是chaincode,接下来分析一下启动过程</p>\n<pre><code>#方法中第一行代码，根据名字可以看出是对链码的Log进行设置\nSetupChaincodeLogging()\n#从输入中获取用户定义的链码的名称\nchaincodename := viper.GetString(&quot;chaincode.id.name&quot;)\n#如果没有输入链码名称，直接返回没有提供链码id的错误，下面则不再执行\nif chaincodename == &quot;&quot; {\n    return errors.New(&quot;error chaincode id not provided&quot;)\n}\n#看名字是一个工厂方法，点进行看一下\nerr := factory.InitFactories(factory.GetDefaultOpts())</code></pre><p>首先进入到<code>factory.GetDefaultOpts()</code>方法中：</p>\n<pre><code>func GetDefaultOpts() *FactoryOpts {\n    return &amp;FactoryOpts{\n        ProviderName: &quot;SW&quot;,\n        SwOpts: &amp;SwOpts{\n            HashFamily: &quot;SHA2&quot;,   #HASH类型\n            SecLevel:   256,    #HASH级别\n\n            Ephemeral: true,\n        },\n    }\n}\n#可以猜到这个方法是获取默认的加密操作，使用SHA256进行数据加密</code></pre><p>不难猜到<code>factory.InitFactories</code>这个方法就是为当前链码设置加密操作的一系列内容。回到<code>Start()</code>方法中接着往下看.</p>\n<pre><code>#这一部分就是将链码数据以流的方式读取进来，userChaincodeStreamGetter是一个方法，点进去看一下\nif streamGetter == nil {\n    streamGetter = userChaincodeStreamGetter\n}\nstream, err := streamGetter(chaincodename)\nif err != nil {\n    return err\n}</code></pre><p><code>userChaincodeStreamGetter</code>还是在这个文件中第82行:</p>\n<pre><code>#这里的name是链码名称，读取到链码数据后以PeerChainCodeStream的方式返回\nfunc userChaincodeStreamGetter(name string) (PeerChaincodeStream, error) {\n    #获取peer.address\n    flag.StringVar(&amp;peerAddress, &quot;peer.address&quot;, &quot;&quot;, &quot;peer address&quot;)\n    //判断是否使能TLS\n    if viper.GetBool(&quot;peer.tls.enabled&quot;) {\n        #获取tls密钥地址，在用户安装链码的时候指定\n        keyPath := viper.GetString(&quot;tls.client.key.path&quot;)\n        #获取tls证书地址\n        certPath := viper.GetString(&quot;tls.client.cert.path&quot;)\n        #从文件中读取密钥数据\n        data, err1 := ioutil.ReadFile(keyPath)\n        if err1 != nil {\n            err1 = errors.Wrap(err1, fmt.Sprintf(&quot;error trying to read file content %s&quot;, keyPath))\n            chaincodeLogger.Errorf(&quot;%+v&quot;, err1)\n            return nil, err1\n        }\n        key = string(data)\n         #从文件中读取证书数据\n        data, err1 = ioutil.ReadFile(certPath)\n        if err1 != nil {\n            err1 = errors.Wrap(err1, fmt.Sprintf(&quot;error trying to read file content %s&quot;, certPath))\n            chaincodeLogger.Errorf(&quot;%+v&quot;, err1)\n            return nil, err1\n        }\n        cert = string(data)\n    }\n    #解析命令行参数到定义的flag\n    flag.Parse()\n    #日志输出\n    chaincodeLogger.Debugf(&quot;Peer address: %s&quot;, getPeerAddress())\n\n    //与peer节点建立连接\n    clientConn, err := newPeerClientConnection()</code></pre><p>看一下这个方法里面的内容，还是这个文件第317行：</p>\n<pre><code>func newPeerClientConnection() (*grpc.ClientConn, error) {\n    #首先获取到peer节点的地址\n    var peerAddress = getPeerAddress()\n    #看名字就知道了，设置与链码之间的心中信息\n    kaOpts := &amp;comm.KeepaliveOptions{\n        ClientInterval: time.Duration(1) * time.Minute,\n        ClientTimeout:  time.Duration(20) * time.Second,\n    }</code></pre><p> 判断是否使能了TLS，然后根据结果建立链接,如何建立链接就不再细看了，我们回到之前的部分</p>\n<pre><code>    if viper.GetBool(&quot;peer.tls.enabled&quot;) {\n        return comm.NewClientConnectionWithAddress(peerAddress, true, true,\n            comm.InitTLSForShim(key, cert), kaOpts)\n    }\n    return comm.NewClientConnectionWithAddress(peerAddress, true, false, nil, kaOpts)\n}</code></pre><p>还是之前的<code>userChaincodeStreamGetter</code>方法</p>\n<pre><code>clientConn, err := newPeerClientConnection()\n    if err != nil {\n        err = errors.Wrap(err, &quot;error trying to connect to local peer&quot;)\n        chaincodeLogger.Errorf(&quot;%+v&quot;, err)\n        return nil, err\n    }\n\n    chaincodeLogger.Debugf(&quot;os.Args returns: %s&quot;, os.Args)\n\n    #接下来是这个方法，返回一个ChaincodeSupportClient实例,对应着链码容器\n    chaincodeSupportClient := pb.NewChaincodeSupportClient(clientConn)\n\n    //这一步是与peer节点建立gRPC连接\n    stream, err := chaincodeSupportClient.Register(context.Background())\n    if err != nil {\n        return nil, errors.WithMessage(err, fmt.Sprintf(&quot;error chatting with leader at address=%s&quot;, getPeerAddress()))\n    }\n\n    return stream, nil\n}</code></pre><p>这个方法结束之后，链码容器与Peer节点已经建立起了连接，接下来链码容器与Peer节点开始互相发送消息了。<br>返回到<code>Start()</code>方法中，还剩最后的一个方法<code>chatWithPeer()</code>：</p>\n<pre><code>    err = chatWithPeer(chaincodename, stream, cc)\n    return err\n}</code></pre><p>看一下链码容器与Peer节点是如何互相通信的。这个方法是链码容器启动的过程中最重要的方法，包含所有的通信流程。<code>chatWithPeer()</code>在331行:</p>\n<pre><code>func chatWithPeer(chaincodename string, stream PeerChaincodeStream, cc Chaincode)\n#传入的参数有链码名称，流(这个是之前链码容器与Peer节点建立gRPC连接所返回的)，链码</code></pre><p>首先第一步是新建一个<code>ChaincodeHandler</code>对象：是非常重要的一个对象。看一下该对象的内容,在<code>core/chaincode/shim/handler.go</code>文件中第166行:</p>\n<pre><code>func newChaincodeHandler(peerChatStream PeerChaincodeStream, chaincode Chaincode) *Handler {\n    v := &amp;Handler{\n        ChatStream: peerChatStream,   #与Peer节点通信的流\n        cc:         chaincode,      #链码\n    }\n    v.responseChannel = make(map[string]chan pb.ChaincodeMessage)  #链码信息响应通道\n    v.state = created     #表示将链码容器的状态更改为created\n    return v    将handler返回\n}</code></pre><p>这个<code>ChaincodeHandler</code>对象是链码侧完成链码与Peer节点之前所有的消息的控制逻辑。<br>继续往下看：</p>\n<pre><code>#在方法执行结束的时候关闭gRPC连接\ndefer stream.CloseSend()\n#获取链码名称\nchaincodeID := &amp;pb.ChaincodeID{Name: chaincodename}\n#将获取的链码名称序列化为有效载荷.\npayload, err := proto.Marshal(chaincodeID)\nif err != nil {\n    return errors.Wrap(err, &quot;error marshalling chaincodeID during chaincode registration&quot;)\n}\n#日志输出,这个日志信息在安装链码的时候应该有看到过吧\nchaincodeLogger.Debugf(&quot;Registering.. sending %s&quot;, pb.ChaincodeMessage_REGISTER)\n#链码容器通过handler开始通过gRPC连接向Peer节点发送第一个消息了，链码容器向Peer节点发送REGISTER消息，并附上链码的名称\nif err = handler.serialSend(&amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTER, Payload: payload}); err != nil {\n        return errors.WithMessage(err, &quot;error sending chaincode REGISTER&quot;)\n    }\n#定义一个接收消息的结构体\ntype recvMsg struct {\n    msg *pb.ChaincodeMessage\n    err error\n}\nmsgAvail := make(chan *recvMsg, 1)\nerrc := make(chan error)\n\nreceiveMessage := func() {\n    in, err := stream.Recv()\n    msgAvail &lt;- &amp;recvMsg{in, err}\n}\n#接收由Peer节点返回的响应消息\ngo receiveMessage()</code></pre><p>接下来的部分就是链码容器与Peer节点详细的通信过程了：</p>\n<h2 id=\"2链码侧向Peer节点发送REGISTER消息\"><a href=\"#2链码侧向Peer节点发送REGISTER消息\" class=\"headerlink\" title=\"2链码侧向Peer节点发送REGISTER消息\"></a>2链码侧向Peer节点发送REGISTER消息</h2><pre><code>#前面的部分都是接收到错误消息的各种输出逻辑，不再细看，我们看default这一部分，这一部分是正常情况下消息的处理情况：\nfor {\n        select {\n        case rmsg := &lt;-msgAvail:\n            switch {\n            case rmsg.err == io.EOF:\n                err = errors.Wrapf(rmsg.err, &quot;received EOF, ending chaincode stream&quot;)\n                chaincodeLogger.Debugf(&quot;%+v&quot;, err)\n                return err\n            case rmsg.err != nil:\n                err := errors.Wrap(rmsg.err, &quot;receive failed&quot;)\n                chaincodeLogger.Errorf(&quot;Received error from server, ending chaincode stream: %+v&quot;, err)\n                return err\n            case rmsg.msg == nil:\n                err := errors.New(&quot;received nil message, ending chaincode stream&quot;)\n                chaincodeLogger.Debugf(&quot;%+v&quot;, err)\n                return err\n            default:\n            #这一句日志输出应该看到过好多次吧。\n                chaincodeLogger.Debugf(&quot;[%s]Received message %s from peer&quot;, shorttxid(rmsg.msg.Txid), rmsg.msg.Type)\n                #重要的一个方法，在链码容器与Peer节点建立起了联系后，主要通过该方法对消息逻辑进行处理，我们点进行看一下。\n                err := handler.handleMessage(rmsg.msg, errc)\n                if err != nil {\n                    err = errors.WithMessage(err, &quot;error handling message&quot;)\n                    return err\n                }\n                #当消息处理完成后，再次接收消息。\n                go receiveMessage()\n            }\n        #最后是发送失败的处理\n        case sendErr := &lt;-errc:\n            if sendErr != nil {\n                err := errors.Wrap(sendErr, &quot;error sending&quot;)\n                return err\n            }\n        }\n    }</code></pre><p>一个重要的方法：<code>handleMessage</code>在<code>core/chaincode/shim/handler.go</code>文件第801行：</p>\n<pre><code>func (handler *Handler) handleMessage(msg *pb.ChaincodeMessage, errc chan error) error {\n    #如果链码容器接收到Peer节点发送的心跳消息后，直接将心跳消息返回，双方就一直保持联系。\n    if msg.Type == pb.ChaincodeMessage_KEEPALIVE {\n        chaincodeLogger.Debug(&quot;Sending KEEPALIVE response&quot;)\n        handler.serialSendAsync(msg, nil) // ignore errors, maybe next KEEPALIVE will work\n        return nil\n    }\n    #我们先看到这里，如果再往下看的话可能会乱掉，所以还是按照逻辑顺序进行说明。</code></pre><p><strong>先说一下链码侧所做的工作：</strong></p>\n<ul>\n<li><p>首先进行各项基本配置，然后建立起与Peer节点的gRPC连接。</p>\n</li>\n<li><p>创建<code>Handler</code>,并更改<code>Handler</code>状态为<code>created</code>。</p>\n</li>\n<li><p>发送<code>REGISTER</code>消息到Peer节点。</p>\n</li>\n<li><p>等待Peer节点返回的信息</p>\n<h2 id=\"3Peer节点接收到REGISTER消息后\"><a href=\"#3Peer节点接收到REGISTER消息后\" class=\"headerlink\" title=\"3Peer节点接收到REGISTER消息后\"></a>3Peer节点接收到REGISTER消息后</h2><p>之前讲的都是链码侧的一系列流程，我们之前提到链码侧与Peer节点之间的第一个消息内容是由链码侧发送至Peer节点的<code>REGISTER</code>消息。接下来我们看一下Peer节点在接收到该消息后是如果进行处理的。<br>代码在<code>core/chaincode/handler.go</code>文件中第174行，这里不是处理消息的开始，但是对于我们要说的链码容器启动过程中消息的处理刚好衔接上，所以就直接从这里开始了。另外很重要的一点，这里已经转换到Peer节点侧了，不是之前说的链码侧，我们看一下代码：</p>\n<pre><code>func (h *Handler) handleMessage(msg *pb.ChaincodeMessage) error {\n  chaincodeLogger.Debugf(&quot;[%s] Fabric side handling ChaincodeMessage of type: %s in state %s&quot;, shorttxid(msg.Txid), msg.Type, h.state)\n  #这边也是首先判断是不是心跳信息，如果是心跳信息的话就什么也不做，与之前不同的是链码侧在收到心跳信息后会返回Peer节点一个心跳信息。\n  if msg.Type == pb.ChaincodeMessage_KEEPALIVE {\n      return nil\n  }\n  #之前我们提到，创建handler时，更改状态为created,所以这里进入到handleMessageCreatedState这个方法内.\n  switch h.state {\n  case Created:\n      return h.handleMessageCreatedState(msg)\n  case Ready:\n      return h.handleMessageReadyState(msg)\n  default:\n      return errors.Errorf(&quot;handle message: invalid state %s for transaction %s&quot;, h.state, msg.Txid)\n  }\n}</code></pre><p><code>handleMessageCreatedState</code>这个方法在第191行,方法内容很简单，判断消息类型是不是REGISTER，如果是则进入HandlerRegister(msg)方法内，如果不是则返回错误信息。</p>\n<pre><code>func (h *Handler) handleMessageCreatedState(msg *pb.ChaincodeMessage) error {\n  switch msg.Type {\n  case pb.ChaincodeMessage_REGISTER:\n      h.HandleRegister(msg)\n  default:\n      return fmt.Errorf(&quot;[%s] Fabric side handler cannot handle message (%s) while in created state&quot;, msg.Txid, msg.Type)\n  }\n  return nil\n}</code></pre><p>接下来我们看一下<code>HandleRegister</code>这个方法,在第495行：</p>\n<pre><code>func (h *Handler) HandleRegister(msg *pb.ChaincodeMessage) {\n  chaincodeLogger.Debugf(&quot;Received %s in state %s&quot;, msg.Type, h.state)\n  #获取链码ID\n  chaincodeID := &amp;pb.ChaincodeID{}\n  #反序列化\n  err := proto.Unmarshal(msg.Payload, chaincodeID)\n  if err != nil {\n      chaincodeLogger.Errorf(&quot;Error in received %s, could NOT unmarshal registration info: %s&quot;, pb.ChaincodeMessage_REGISTER, err)\n      return\n  }\n\n  h.chaincodeID = chaincodeID\n  #这一行就是将链码注册到当前Peer节点上\n  err = h.Registry.Register(h)\n  if err != nil {\n      h.notifyRegistry(err)\n      return\n  }\n\n  从Peer节点侧的handler获取链码名称\n  h.ccInstance = ParseName(h.chaincodeID.Name)\n\n  chaincodeLogger.Debugf(&quot;Got %s for chaincodeID = %s, sending back %s&quot;, pb.ChaincodeMessage_REGISTER, chaincodeID, pb.ChaincodeMessage_REGISTERED)\n  #然后将REGISTERED消息返回给链码侧\n  if err := h.serialSend(&amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTERED}); err != nil {\n      chaincodeLogger.Errorf(&quot;error sending %s: %s&quot;, pb.ChaincodeMessage_REGISTERED, err)\n      h.notifyRegistry(err)\n      return\n  }\n\n  //更新handler状态为Established\n  h.state = Established\n\n  chaincodeLogger.Debugf(&quot;Changed state to established for %+v&quot;, h.chaincodeID)\n\n  #还有这个方法也要看一下\n  h.notifyRegistry(nil)\n}</code></pre><p>简单来说<code>HandleRegister</code>的功能就是将链码注册到Peer节点上，并发送<code>RESIGSERED</code>到链码侧，最后更新<code>handler</code>状态为<code>Established</code>，我们看一下<code>notifyRegistry</code>方法,在478行：</p>\n<pre><code>func (h *Handler) notifyRegistry(err error) {\n  if err == nil {\n      //再往里面看,方法在459行\n      err = h.sendReady()\n  }\n\n  if err != nil {\n      h.Registry.Failed(h.chaincodeID.Name, err)\n      chaincodeLogger.Errorf(&quot;failed to start %s&quot;, h.chaincodeID)\n      return\n  }\n\n  h.Registry.Ready(h.chaincodeID.Name)\n}\n#sendReady()\nfunc (h *Handler) sendReady() error {\n  chaincodeLogger.Debugf(&quot;sending READY for chaincode %+v&quot;, h.chaincodeID)\n  ccMsg := &amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_READY}\n\n  #Peer节点又向链码容器发送了READY消息\n  if err := h.serialSend(ccMsg); err != nil {\n      chaincodeLogger.Errorf(&quot;error sending READY (%s) for chaincode %+v&quot;, err, h.chaincodeID)\n      return err\n  }\n  #同时更新handler状态为Ready\n  h.state = Ready\n\n  chaincodeLogger.Debugf(&quot;Changed to state ready for chaincode %+v&quot;, h.chaincodeID)\n\n  return nil\n}</code></pre><p>到这里，Peer节点暂时分析完成，又到了链码侧对Peer节点发送的消息进行处理的流程.</p>\n</li>\n<li><p><em>我们先总结一下这一部分Peer节点做了哪些工作：*</em></p>\n</li>\n<li><p>首先当Peer节点接收到链码侧发送的<code>REGISTER</code>消息后，将链码注册到Peer端的<code>Handler</code>上，发送<code>REGISTERED</code>到链码侧，更新<code>Handler</code>的状态为<code>Established</code>。</p>\n</li>\n<li><p>然后Peer节点向链码侧发送<code>READY</code>消息，同时更新<code>Handler</code>的状态为<code>Ready</code>。</p>\n</li>\n</ul>\n<h2 id=\"4链码侧的回应\"><a href=\"#4链码侧的回应\" class=\"headerlink\" title=\"4链码侧的回应\"></a>4链码侧的回应</h2><p>我们回到链码侧之前的这一部分<code>core/chaincode/chaincode.go</code>中第364行,这里是链码铡对接收到的Peer节点发送的消息进行处理的逻辑,至于发生错误的情况就不再说明，我们看<code>handleMessage</code>这个方法。</p>\n<pre><code>go receiveMessage()\n    for {\n           #相关代码\n        ...\n        err := handler.handleMessage(rmsg.msg, errc)\n        ...\n            #相关代码\n                go receiveMessage()\n    }</code></pre><p><code>handleMessage</code>这个方法在<code>core/chaincode/shim/handler.go</code>这个文件中，第801行。</p>\n<pre><code>#主要就是这一部分：\nswitch handler.state {\n    case ready:\n        err = handler.handleReady(msg, errc)\n    case established:\n        err = handler.handleEstablished(msg, errc)\n    case created:\n        err = handler.handleCreated(msg, errc)\n    default:\n        err = errors.Errorf(&quot;[%s] Chaincode handler cannot handle message (%s) with payload size (%d) while in state: %s&quot;, msg.Txid, msg.Type, len(msg.Payload), handler.state)\n}</code></pre><ul>\n<li><p>首先链码侧接收到Peer节点发送的<code>REGISTERED</code>消息后，这里链码侧的<code>handler</code>与Peer节点侧的<code>handler</code>并不是同一个，不要搞混了。判断当前链码侧<code>handler</code>的状态为<code>created</code>，进入到<code>handleCreated</code>方法中,在792行：</p>\n<pre><code>#将链码侧的handler的状态更改为established\nif msg.Type == pb.ChaincodeMessage_REGISTERED {\n  handler.state = established\n  return nil\n}</code></pre></li>\n<li><p>当链码侧接收到Peer节点发送的<code>READY</code>消息后，又一次进入上面的逻辑，由于链码侧的<code>handler</code>的状态已经更改为<code>established</code>,所以这次进入到<code>handleEstablished</code>方法中。在783行：</p>\n<pre><code>#然后将链码侧的handler的状态更改为ready\nif msg.Type == pb.ChaincodeMessage_READY {\n  handler.state = ready\n  return nil\n}</code></pre></li>\n<li><p>另外，当用户对链码进行实例化操作时，会通过Peer节点向链码侧发送<code>INIT</code>消息，这里涉及到背书过程，之后再对背书过程进行讨论，我们在这里只关注链码侧接收到<code>INIT</code>消息后的逻辑，还是<code>handleMessage</code>这个方法中：</p>\n<pre><code>#当判断到消息类型为INIT时，会执行这个方法。\nhandler.handleInit(msg, errc)</code></pre><p><code>handler.handleInit(msg, errc)</code>方法在第177行：</p>\n<pre><code>func (handler *Handler) handleInit(msg *pb.ChaincodeMessage, errc chan error) {\n  go func() {\n      var nextStateMsg *pb.ChaincodeMessage\n\n      defer func() {\n          #这一名相当于更新链码的状态\n          handler.triggerNextState(nextStateMsg, errc)\n      }()\n      #判断错误信息\n      errFunc := func(err error, payload []byte, ce *pb.ChaincodeEvent, errFmt string, args ...interface{}) *pb.ChaincodeMessage {\n          if err != nil {\n              // Send ERROR message to chaincode support and change state\n              if payload == nil {\n                  payload = []byte(err.Error())\n              }\n              chaincodeLogger.Errorf(errFmt, args...)\n              return &amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_ERROR, Payload: payload, Txid: msg.Txid, ChaincodeEvent: ce, ChannelId: msg.ChannelId}\n          }\n          return nil\n      }\n      #获取用户输入的参数\n      input := &amp;pb.ChaincodeInput{}\n      #反序列化\n      unmarshalErr := proto.Unmarshal(msg.Payload, input)\n      if nextStateMsg = errFunc(unmarshalErr, nil, nil, &quot;[%s] Incorrect payload format. Sending %s&quot;, shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n          return\n      }\n\n      #ChaincodeStub应该很熟悉了,很重要的一个对象，包含一项提案中所需要的内容。在``core/chaincode/shim/chaincode.go``文件中第53行，有兴趣可以点进去看一下\n      stub := new(ChaincodeStub)\n      #这一行代码的意思就是将提案中的信息抽取出来赋值到ChaincodeStub这个对象中\n     err := stub.init(handler, msg.ChannelId, msg.Txid, input, msg.Proposal)\n     if nextStateMsg = errFunc(err, nil, stub.chaincodeEvent, &quot;[%s] Init get error response. Sending %s&quot;, shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n          return\n     }\n     #这里的Init方法就是链码中所写的Init()方法，就不再解释了\n     res := handler.cc.Init(stub)\n     chaincodeLogger.Debugf(&quot;[%s] Init get response status: %d&quot;, shorttxid(msg.Txid), res.Status)\n      #ERROR的值为500,OK=200，ERRORTHRESHOLD = 400，大于等于400就代表错误信息或者被背书节点拒绝。\n      if res.Status &gt;= ERROR {\n          err = errors.New(res.Message)\n          if nextStateMsg = errFunc(err, []byte(res.Message), stub.chaincodeEvent, &quot;[%s] Init get error response. Sending %s&quot;, shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n              return\n          }\n      }\n      resBytes, err := proto.Marshal(&amp;res)\n      if err != nil {\n          payload := []byte(err.Error())\n          chaincodeLogger.Errorf(&quot;[%s] Init marshal response error [%s]. Sending %s&quot;, shorttxid(msg.Txid), err, pb.ChaincodeMessage_ERROR)\n          nextStateMsg = &amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_ERROR, Payload: payload, Txid: msg.Txid, ChaincodeEvent: stub.chaincodeEvent}\n          return\n      }\n\n      // Send COMPLETED message to chaincode support and change state\n      nextStateMsg = &amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_COMPLETED, Payload: resBytes, Txid: msg.Txid, ChaincodeEvent: stub.chaincodeEvent, ChannelId: stub.ChannelId}\n      chaincodeLogger.Debugf(&quot;[%s] Init succeeded. Sending %s&quot;, shorttxid(msg.Txid), pb.ChaincodeMessage_COMPLETED)\n      #到这里就结束了，会调用上面的handler.triggerNextState(nextStateMsg, errc)方法，这个方法将初始化数据与COMPLETED状态发送至Peer节点。\n  }()\n}</code></pre><p>这个方法还是比较简单的，一共做了这些事情：</p>\n</li>\n<li><p>获取用户的输入数据</p>\n</li>\n<li><p>新建一个<code>ChainCodeStub</code>对象，然后将用户输入的数据赋值给该对象</p>\n</li>\n<li><p>调用用户链码中的<code>Init()</code>方法</p>\n</li>\n<li><p>将所有数据封装成<code>ChainCodeMessage</code>，类型为<code>COMPLETED</code>,发送至Peer节点。</p>\n</li>\n</ul>\n<p>这个时候链码已经初始化完成，已经进入了可被调用(<code>invoke</code>)的状态.<br>之后的流程就差不多了，Peer节点发送<code>TRANSACTION</code>消息给链码侧，调用<code>Invoke()</code>方法，之后链码侧发送具体的调用方法到Peer节点，由Peer节点进行相应的处理，最后返回<code>RESPONSE</code>消息到链码侧，链码侧接收到<code>RESPONSE</code>消息后，返回<code>COMPLETED</code>消息到Peer节点。</p>\n<h2 id=\"5总结\"><a href=\"#5总结\" class=\"headerlink\" title=\"5总结\"></a>5总结</h2><p>到这里，Peer节点与链码侧的<code>handler</code>都处于<code>READY</code>状态,链码容器已经启动完成。最后总结一下整体的流程：</p>\n<ol>\n<li>通过用户端链码中的<code>main</code>方法，调用了<code>core/chaincode/shim/chaincode.go</code>中的<code>Start()</code>方法，从而开始了链码的启动。</li>\n<li>首先进行相关的配置比如基本的加密，证书的读取。</li>\n<li>创建与Peer节点之间的gRPC连接，创建<code>handler</code>实例。</li>\n<li>由链码容器向Peer节点发送第一个消息:<code>REGISTER</code>,然后等待接收由Peer节点发送的消息。如果接收到的是心跳消息，则向Peer节点返回心跳消息。</li>\n<li>Peer节点接收到链码容器发送的<code>REGISTER</code>消息后，将其注册到Peer节点端的<code>handler</code>上。</li>\n<li>Peer节点发送<code>REGISTERED</code>消息到链码侧，同时更新Peer节点端的<code>handler</code>状态为<code>Established</code>。</li>\n<li>Peer节点发送<code>Ready</code>消息到链码侧，同时更新Peer节点端的<code>handler</code>状态为<code>Ready</code>。</li>\n<li>链码侧接收到由Peer节点发送的<code>REGISTERED</code>消息后，更新链码侧的<code>handler</code>状态为<code>Established</code>。</li>\n<li>链码侧接收到由Peer节点发送的<code>READY</code>消息后，更新链码侧的<code>handler</code>状态为<code>ready</code>。</li>\n<li>当用户执行实例化链码时，通过Peer节点向链码侧发送<code>INIT</code>消息。链码侧接收到<code>INIT</code>消息后，根据用户输入的参数进行实例化操作。实例化完成后，返回<code>COMPLETED</code>消息到Peer节点。</li>\n<li>到这里链码容器已经启动，可以对链码数据进行查询调用等操作了。</li>\n</ol>\n<p>另外，阅读Fabric源码中有一些没有看明白或者分析有误的地方，还望大家能够批评指正。</p>\n<p>最后附上参考文档：<a href=\"https://legacy.gitbook.com/book/yeasy/hyperledger_code_fabric/details\" target=\"_blank\" rel=\"noopener\">传送门</a><br>以及Fabric源码地址：<a href=\"https://github.com/hyperledger/fabric\" target=\"_blank\" rel=\"noopener\">传送门</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>想写点东西记录一下最近看的一些Fabric源码,本文使用的是<strong>fabric1.4</strong>的版本，所以对于其他版本的fabric，内容可能会有所不同。<br>其实我仅仅知道Go语言一些语法的使用，并不太熟悉Go语言，所以解析的内容可能会有误，欢迎大家批评指正。<br>本文想针对Fabric中链码容器的启动过程进行源码的解析。这里的链码指的是用户链码不是系统链码,顺便回顾一下系统链码:<br><strong>lscc</strong>(Life Cycle System ChainCode)生命周期系统链码<br><strong>cscc</strong>(Configuration System ChainCode)配置系统链码<br><strong>escc</strong>(Endorser System ChainCode)背书系统链码<br><strong>qscc</strong>(Query System ChainCode)查询系统链码<br><strong>vscc</strong>(Verification System ChainCode)验证系统链码<br>本文主要解析的是用户链码的启动过程。</p>\n<h2 id=\"1-起点\"><a href=\"#1-起点\" class=\"headerlink\" title=\"1 起点\"></a>1 起点</h2><pre><code>#这是用户端链码的main方法，也是整个流程的入口点，调用了shim包中的Start(cc Chaincode)方法.\nfunc main(){\n    err :=shim.Start(new(Chaincode))\n    if err != nil {\n        fmt.Printf(&quot;Error starting Chaincode: %s&quot;,err)\n    }\n}</code></pre><p>首先定位到<code>fabric/core/chaincode/shim/chaincode.go</code>这个文件中的<code>Start</code>方法，这里是链码启动的起点。<br>可以看到传的参数就是chaincode,接下来分析一下启动过程</p>\n<pre><code>#方法中第一行代码，根据名字可以看出是对链码的Log进行设置\nSetupChaincodeLogging()\n#从输入中获取用户定义的链码的名称\nchaincodename := viper.GetString(&quot;chaincode.id.name&quot;)\n#如果没有输入链码名称，直接返回没有提供链码id的错误，下面则不再执行\nif chaincodename == &quot;&quot; {\n    return errors.New(&quot;error chaincode id not provided&quot;)\n}\n#看名字是一个工厂方法，点进行看一下\nerr := factory.InitFactories(factory.GetDefaultOpts())</code></pre><p>首先进入到<code>factory.GetDefaultOpts()</code>方法中：</p>\n<pre><code>func GetDefaultOpts() *FactoryOpts {\n    return &amp;FactoryOpts{\n        ProviderName: &quot;SW&quot;,\n        SwOpts: &amp;SwOpts{\n            HashFamily: &quot;SHA2&quot;,   #HASH类型\n            SecLevel:   256,    #HASH级别\n\n            Ephemeral: true,\n        },\n    }\n}\n#可以猜到这个方法是获取默认的加密操作，使用SHA256进行数据加密</code></pre><p>不难猜到<code>factory.InitFactories</code>这个方法就是为当前链码设置加密操作的一系列内容。回到<code>Start()</code>方法中接着往下看.</p>\n<pre><code>#这一部分就是将链码数据以流的方式读取进来，userChaincodeStreamGetter是一个方法，点进去看一下\nif streamGetter == nil {\n    streamGetter = userChaincodeStreamGetter\n}\nstream, err := streamGetter(chaincodename)\nif err != nil {\n    return err\n}</code></pre><p><code>userChaincodeStreamGetter</code>还是在这个文件中第82行:</p>\n<pre><code>#这里的name是链码名称，读取到链码数据后以PeerChainCodeStream的方式返回\nfunc userChaincodeStreamGetter(name string) (PeerChaincodeStream, error) {\n    #获取peer.address\n    flag.StringVar(&amp;peerAddress, &quot;peer.address&quot;, &quot;&quot;, &quot;peer address&quot;)\n    //判断是否使能TLS\n    if viper.GetBool(&quot;peer.tls.enabled&quot;) {\n        #获取tls密钥地址，在用户安装链码的时候指定\n        keyPath := viper.GetString(&quot;tls.client.key.path&quot;)\n        #获取tls证书地址\n        certPath := viper.GetString(&quot;tls.client.cert.path&quot;)\n        #从文件中读取密钥数据\n        data, err1 := ioutil.ReadFile(keyPath)\n        if err1 != nil {\n            err1 = errors.Wrap(err1, fmt.Sprintf(&quot;error trying to read file content %s&quot;, keyPath))\n            chaincodeLogger.Errorf(&quot;%+v&quot;, err1)\n            return nil, err1\n        }\n        key = string(data)\n         #从文件中读取证书数据\n        data, err1 = ioutil.ReadFile(certPath)\n        if err1 != nil {\n            err1 = errors.Wrap(err1, fmt.Sprintf(&quot;error trying to read file content %s&quot;, certPath))\n            chaincodeLogger.Errorf(&quot;%+v&quot;, err1)\n            return nil, err1\n        }\n        cert = string(data)\n    }\n    #解析命令行参数到定义的flag\n    flag.Parse()\n    #日志输出\n    chaincodeLogger.Debugf(&quot;Peer address: %s&quot;, getPeerAddress())\n\n    //与peer节点建立连接\n    clientConn, err := newPeerClientConnection()</code></pre><p>看一下这个方法里面的内容，还是这个文件第317行：</p>\n<pre><code>func newPeerClientConnection() (*grpc.ClientConn, error) {\n    #首先获取到peer节点的地址\n    var peerAddress = getPeerAddress()\n    #看名字就知道了，设置与链码之间的心中信息\n    kaOpts := &amp;comm.KeepaliveOptions{\n        ClientInterval: time.Duration(1) * time.Minute,\n        ClientTimeout:  time.Duration(20) * time.Second,\n    }</code></pre><p> 判断是否使能了TLS，然后根据结果建立链接,如何建立链接就不再细看了，我们回到之前的部分</p>\n<pre><code>    if viper.GetBool(&quot;peer.tls.enabled&quot;) {\n        return comm.NewClientConnectionWithAddress(peerAddress, true, true,\n            comm.InitTLSForShim(key, cert), kaOpts)\n    }\n    return comm.NewClientConnectionWithAddress(peerAddress, true, false, nil, kaOpts)\n}</code></pre><p>还是之前的<code>userChaincodeStreamGetter</code>方法</p>\n<pre><code>clientConn, err := newPeerClientConnection()\n    if err != nil {\n        err = errors.Wrap(err, &quot;error trying to connect to local peer&quot;)\n        chaincodeLogger.Errorf(&quot;%+v&quot;, err)\n        return nil, err\n    }\n\n    chaincodeLogger.Debugf(&quot;os.Args returns: %s&quot;, os.Args)\n\n    #接下来是这个方法，返回一个ChaincodeSupportClient实例,对应着链码容器\n    chaincodeSupportClient := pb.NewChaincodeSupportClient(clientConn)\n\n    //这一步是与peer节点建立gRPC连接\n    stream, err := chaincodeSupportClient.Register(context.Background())\n    if err != nil {\n        return nil, errors.WithMessage(err, fmt.Sprintf(&quot;error chatting with leader at address=%s&quot;, getPeerAddress()))\n    }\n\n    return stream, nil\n}</code></pre><p>这个方法结束之后，链码容器与Peer节点已经建立起了连接，接下来链码容器与Peer节点开始互相发送消息了。<br>返回到<code>Start()</code>方法中，还剩最后的一个方法<code>chatWithPeer()</code>：</p>\n<pre><code>    err = chatWithPeer(chaincodename, stream, cc)\n    return err\n}</code></pre><p>看一下链码容器与Peer节点是如何互相通信的。这个方法是链码容器启动的过程中最重要的方法，包含所有的通信流程。<code>chatWithPeer()</code>在331行:</p>\n<pre><code>func chatWithPeer(chaincodename string, stream PeerChaincodeStream, cc Chaincode)\n#传入的参数有链码名称，流(这个是之前链码容器与Peer节点建立gRPC连接所返回的)，链码</code></pre><p>首先第一步是新建一个<code>ChaincodeHandler</code>对象：是非常重要的一个对象。看一下该对象的内容,在<code>core/chaincode/shim/handler.go</code>文件中第166行:</p>\n<pre><code>func newChaincodeHandler(peerChatStream PeerChaincodeStream, chaincode Chaincode) *Handler {\n    v := &amp;Handler{\n        ChatStream: peerChatStream,   #与Peer节点通信的流\n        cc:         chaincode,      #链码\n    }\n    v.responseChannel = make(map[string]chan pb.ChaincodeMessage)  #链码信息响应通道\n    v.state = created     #表示将链码容器的状态更改为created\n    return v    将handler返回\n}</code></pre><p>这个<code>ChaincodeHandler</code>对象是链码侧完成链码与Peer节点之前所有的消息的控制逻辑。<br>继续往下看：</p>\n<pre><code>#在方法执行结束的时候关闭gRPC连接\ndefer stream.CloseSend()\n#获取链码名称\nchaincodeID := &amp;pb.ChaincodeID{Name: chaincodename}\n#将获取的链码名称序列化为有效载荷.\npayload, err := proto.Marshal(chaincodeID)\nif err != nil {\n    return errors.Wrap(err, &quot;error marshalling chaincodeID during chaincode registration&quot;)\n}\n#日志输出,这个日志信息在安装链码的时候应该有看到过吧\nchaincodeLogger.Debugf(&quot;Registering.. sending %s&quot;, pb.ChaincodeMessage_REGISTER)\n#链码容器通过handler开始通过gRPC连接向Peer节点发送第一个消息了，链码容器向Peer节点发送REGISTER消息，并附上链码的名称\nif err = handler.serialSend(&amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTER, Payload: payload}); err != nil {\n        return errors.WithMessage(err, &quot;error sending chaincode REGISTER&quot;)\n    }\n#定义一个接收消息的结构体\ntype recvMsg struct {\n    msg *pb.ChaincodeMessage\n    err error\n}\nmsgAvail := make(chan *recvMsg, 1)\nerrc := make(chan error)\n\nreceiveMessage := func() {\n    in, err := stream.Recv()\n    msgAvail &lt;- &amp;recvMsg{in, err}\n}\n#接收由Peer节点返回的响应消息\ngo receiveMessage()</code></pre><p>接下来的部分就是链码容器与Peer节点详细的通信过程了：</p>\n<h2 id=\"2链码侧向Peer节点发送REGISTER消息\"><a href=\"#2链码侧向Peer节点发送REGISTER消息\" class=\"headerlink\" title=\"2链码侧向Peer节点发送REGISTER消息\"></a>2链码侧向Peer节点发送REGISTER消息</h2><pre><code>#前面的部分都是接收到错误消息的各种输出逻辑，不再细看，我们看default这一部分，这一部分是正常情况下消息的处理情况：\nfor {\n        select {\n        case rmsg := &lt;-msgAvail:\n            switch {\n            case rmsg.err == io.EOF:\n                err = errors.Wrapf(rmsg.err, &quot;received EOF, ending chaincode stream&quot;)\n                chaincodeLogger.Debugf(&quot;%+v&quot;, err)\n                return err\n            case rmsg.err != nil:\n                err := errors.Wrap(rmsg.err, &quot;receive failed&quot;)\n                chaincodeLogger.Errorf(&quot;Received error from server, ending chaincode stream: %+v&quot;, err)\n                return err\n            case rmsg.msg == nil:\n                err := errors.New(&quot;received nil message, ending chaincode stream&quot;)\n                chaincodeLogger.Debugf(&quot;%+v&quot;, err)\n                return err\n            default:\n            #这一句日志输出应该看到过好多次吧。\n                chaincodeLogger.Debugf(&quot;[%s]Received message %s from peer&quot;, shorttxid(rmsg.msg.Txid), rmsg.msg.Type)\n                #重要的一个方法，在链码容器与Peer节点建立起了联系后，主要通过该方法对消息逻辑进行处理，我们点进行看一下。\n                err := handler.handleMessage(rmsg.msg, errc)\n                if err != nil {\n                    err = errors.WithMessage(err, &quot;error handling message&quot;)\n                    return err\n                }\n                #当消息处理完成后，再次接收消息。\n                go receiveMessage()\n            }\n        #最后是发送失败的处理\n        case sendErr := &lt;-errc:\n            if sendErr != nil {\n                err := errors.Wrap(sendErr, &quot;error sending&quot;)\n                return err\n            }\n        }\n    }</code></pre><p>一个重要的方法：<code>handleMessage</code>在<code>core/chaincode/shim/handler.go</code>文件第801行：</p>\n<pre><code>func (handler *Handler) handleMessage(msg *pb.ChaincodeMessage, errc chan error) error {\n    #如果链码容器接收到Peer节点发送的心跳消息后，直接将心跳消息返回，双方就一直保持联系。\n    if msg.Type == pb.ChaincodeMessage_KEEPALIVE {\n        chaincodeLogger.Debug(&quot;Sending KEEPALIVE response&quot;)\n        handler.serialSendAsync(msg, nil) // ignore errors, maybe next KEEPALIVE will work\n        return nil\n    }\n    #我们先看到这里，如果再往下看的话可能会乱掉，所以还是按照逻辑顺序进行说明。</code></pre><p><strong>先说一下链码侧所做的工作：</strong></p>\n<ul>\n<li><p>首先进行各项基本配置，然后建立起与Peer节点的gRPC连接。</p>\n</li>\n<li><p>创建<code>Handler</code>,并更改<code>Handler</code>状态为<code>created</code>。</p>\n</li>\n<li><p>发送<code>REGISTER</code>消息到Peer节点。</p>\n</li>\n<li><p>等待Peer节点返回的信息</p>\n<h2 id=\"3Peer节点接收到REGISTER消息后\"><a href=\"#3Peer节点接收到REGISTER消息后\" class=\"headerlink\" title=\"3Peer节点接收到REGISTER消息后\"></a>3Peer节点接收到REGISTER消息后</h2><p>之前讲的都是链码侧的一系列流程，我们之前提到链码侧与Peer节点之间的第一个消息内容是由链码侧发送至Peer节点的<code>REGISTER</code>消息。接下来我们看一下Peer节点在接收到该消息后是如果进行处理的。<br>代码在<code>core/chaincode/handler.go</code>文件中第174行，这里不是处理消息的开始，但是对于我们要说的链码容器启动过程中消息的处理刚好衔接上，所以就直接从这里开始了。另外很重要的一点，这里已经转换到Peer节点侧了，不是之前说的链码侧，我们看一下代码：</p>\n<pre><code>func (h *Handler) handleMessage(msg *pb.ChaincodeMessage) error {\n  chaincodeLogger.Debugf(&quot;[%s] Fabric side handling ChaincodeMessage of type: %s in state %s&quot;, shorttxid(msg.Txid), msg.Type, h.state)\n  #这边也是首先判断是不是心跳信息，如果是心跳信息的话就什么也不做，与之前不同的是链码侧在收到心跳信息后会返回Peer节点一个心跳信息。\n  if msg.Type == pb.ChaincodeMessage_KEEPALIVE {\n      return nil\n  }\n  #之前我们提到，创建handler时，更改状态为created,所以这里进入到handleMessageCreatedState这个方法内.\n  switch h.state {\n  case Created:\n      return h.handleMessageCreatedState(msg)\n  case Ready:\n      return h.handleMessageReadyState(msg)\n  default:\n      return errors.Errorf(&quot;handle message: invalid state %s for transaction %s&quot;, h.state, msg.Txid)\n  }\n}</code></pre><p><code>handleMessageCreatedState</code>这个方法在第191行,方法内容很简单，判断消息类型是不是REGISTER，如果是则进入HandlerRegister(msg)方法内，如果不是则返回错误信息。</p>\n<pre><code>func (h *Handler) handleMessageCreatedState(msg *pb.ChaincodeMessage) error {\n  switch msg.Type {\n  case pb.ChaincodeMessage_REGISTER:\n      h.HandleRegister(msg)\n  default:\n      return fmt.Errorf(&quot;[%s] Fabric side handler cannot handle message (%s) while in created state&quot;, msg.Txid, msg.Type)\n  }\n  return nil\n}</code></pre><p>接下来我们看一下<code>HandleRegister</code>这个方法,在第495行：</p>\n<pre><code>func (h *Handler) HandleRegister(msg *pb.ChaincodeMessage) {\n  chaincodeLogger.Debugf(&quot;Received %s in state %s&quot;, msg.Type, h.state)\n  #获取链码ID\n  chaincodeID := &amp;pb.ChaincodeID{}\n  #反序列化\n  err := proto.Unmarshal(msg.Payload, chaincodeID)\n  if err != nil {\n      chaincodeLogger.Errorf(&quot;Error in received %s, could NOT unmarshal registration info: %s&quot;, pb.ChaincodeMessage_REGISTER, err)\n      return\n  }\n\n  h.chaincodeID = chaincodeID\n  #这一行就是将链码注册到当前Peer节点上\n  err = h.Registry.Register(h)\n  if err != nil {\n      h.notifyRegistry(err)\n      return\n  }\n\n  从Peer节点侧的handler获取链码名称\n  h.ccInstance = ParseName(h.chaincodeID.Name)\n\n  chaincodeLogger.Debugf(&quot;Got %s for chaincodeID = %s, sending back %s&quot;, pb.ChaincodeMessage_REGISTER, chaincodeID, pb.ChaincodeMessage_REGISTERED)\n  #然后将REGISTERED消息返回给链码侧\n  if err := h.serialSend(&amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTERED}); err != nil {\n      chaincodeLogger.Errorf(&quot;error sending %s: %s&quot;, pb.ChaincodeMessage_REGISTERED, err)\n      h.notifyRegistry(err)\n      return\n  }\n\n  //更新handler状态为Established\n  h.state = Established\n\n  chaincodeLogger.Debugf(&quot;Changed state to established for %+v&quot;, h.chaincodeID)\n\n  #还有这个方法也要看一下\n  h.notifyRegistry(nil)\n}</code></pre><p>简单来说<code>HandleRegister</code>的功能就是将链码注册到Peer节点上，并发送<code>RESIGSERED</code>到链码侧，最后更新<code>handler</code>状态为<code>Established</code>，我们看一下<code>notifyRegistry</code>方法,在478行：</p>\n<pre><code>func (h *Handler) notifyRegistry(err error) {\n  if err == nil {\n      //再往里面看,方法在459行\n      err = h.sendReady()\n  }\n\n  if err != nil {\n      h.Registry.Failed(h.chaincodeID.Name, err)\n      chaincodeLogger.Errorf(&quot;failed to start %s&quot;, h.chaincodeID)\n      return\n  }\n\n  h.Registry.Ready(h.chaincodeID.Name)\n}\n#sendReady()\nfunc (h *Handler) sendReady() error {\n  chaincodeLogger.Debugf(&quot;sending READY for chaincode %+v&quot;, h.chaincodeID)\n  ccMsg := &amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_READY}\n\n  #Peer节点又向链码容器发送了READY消息\n  if err := h.serialSend(ccMsg); err != nil {\n      chaincodeLogger.Errorf(&quot;error sending READY (%s) for chaincode %+v&quot;, err, h.chaincodeID)\n      return err\n  }\n  #同时更新handler状态为Ready\n  h.state = Ready\n\n  chaincodeLogger.Debugf(&quot;Changed to state ready for chaincode %+v&quot;, h.chaincodeID)\n\n  return nil\n}</code></pre><p>到这里，Peer节点暂时分析完成，又到了链码侧对Peer节点发送的消息进行处理的流程.</p>\n</li>\n<li><p><em>我们先总结一下这一部分Peer节点做了哪些工作：*</em></p>\n</li>\n<li><p>首先当Peer节点接收到链码侧发送的<code>REGISTER</code>消息后，将链码注册到Peer端的<code>Handler</code>上，发送<code>REGISTERED</code>到链码侧，更新<code>Handler</code>的状态为<code>Established</code>。</p>\n</li>\n<li><p>然后Peer节点向链码侧发送<code>READY</code>消息，同时更新<code>Handler</code>的状态为<code>Ready</code>。</p>\n</li>\n</ul>\n<h2 id=\"4链码侧的回应\"><a href=\"#4链码侧的回应\" class=\"headerlink\" title=\"4链码侧的回应\"></a>4链码侧的回应</h2><p>我们回到链码侧之前的这一部分<code>core/chaincode/chaincode.go</code>中第364行,这里是链码铡对接收到的Peer节点发送的消息进行处理的逻辑,至于发生错误的情况就不再说明，我们看<code>handleMessage</code>这个方法。</p>\n<pre><code>go receiveMessage()\n    for {\n           #相关代码\n        ...\n        err := handler.handleMessage(rmsg.msg, errc)\n        ...\n            #相关代码\n                go receiveMessage()\n    }</code></pre><p><code>handleMessage</code>这个方法在<code>core/chaincode/shim/handler.go</code>这个文件中，第801行。</p>\n<pre><code>#主要就是这一部分：\nswitch handler.state {\n    case ready:\n        err = handler.handleReady(msg, errc)\n    case established:\n        err = handler.handleEstablished(msg, errc)\n    case created:\n        err = handler.handleCreated(msg, errc)\n    default:\n        err = errors.Errorf(&quot;[%s] Chaincode handler cannot handle message (%s) with payload size (%d) while in state: %s&quot;, msg.Txid, msg.Type, len(msg.Payload), handler.state)\n}</code></pre><ul>\n<li><p>首先链码侧接收到Peer节点发送的<code>REGISTERED</code>消息后，这里链码侧的<code>handler</code>与Peer节点侧的<code>handler</code>并不是同一个，不要搞混了。判断当前链码侧<code>handler</code>的状态为<code>created</code>，进入到<code>handleCreated</code>方法中,在792行：</p>\n<pre><code>#将链码侧的handler的状态更改为established\nif msg.Type == pb.ChaincodeMessage_REGISTERED {\n  handler.state = established\n  return nil\n}</code></pre></li>\n<li><p>当链码侧接收到Peer节点发送的<code>READY</code>消息后，又一次进入上面的逻辑，由于链码侧的<code>handler</code>的状态已经更改为<code>established</code>,所以这次进入到<code>handleEstablished</code>方法中。在783行：</p>\n<pre><code>#然后将链码侧的handler的状态更改为ready\nif msg.Type == pb.ChaincodeMessage_READY {\n  handler.state = ready\n  return nil\n}</code></pre></li>\n<li><p>另外，当用户对链码进行实例化操作时，会通过Peer节点向链码侧发送<code>INIT</code>消息，这里涉及到背书过程，之后再对背书过程进行讨论，我们在这里只关注链码侧接收到<code>INIT</code>消息后的逻辑，还是<code>handleMessage</code>这个方法中：</p>\n<pre><code>#当判断到消息类型为INIT时，会执行这个方法。\nhandler.handleInit(msg, errc)</code></pre><p><code>handler.handleInit(msg, errc)</code>方法在第177行：</p>\n<pre><code>func (handler *Handler) handleInit(msg *pb.ChaincodeMessage, errc chan error) {\n  go func() {\n      var nextStateMsg *pb.ChaincodeMessage\n\n      defer func() {\n          #这一名相当于更新链码的状态\n          handler.triggerNextState(nextStateMsg, errc)\n      }()\n      #判断错误信息\n      errFunc := func(err error, payload []byte, ce *pb.ChaincodeEvent, errFmt string, args ...interface{}) *pb.ChaincodeMessage {\n          if err != nil {\n              // Send ERROR message to chaincode support and change state\n              if payload == nil {\n                  payload = []byte(err.Error())\n              }\n              chaincodeLogger.Errorf(errFmt, args...)\n              return &amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_ERROR, Payload: payload, Txid: msg.Txid, ChaincodeEvent: ce, ChannelId: msg.ChannelId}\n          }\n          return nil\n      }\n      #获取用户输入的参数\n      input := &amp;pb.ChaincodeInput{}\n      #反序列化\n      unmarshalErr := proto.Unmarshal(msg.Payload, input)\n      if nextStateMsg = errFunc(unmarshalErr, nil, nil, &quot;[%s] Incorrect payload format. Sending %s&quot;, shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n          return\n      }\n\n      #ChaincodeStub应该很熟悉了,很重要的一个对象，包含一项提案中所需要的内容。在``core/chaincode/shim/chaincode.go``文件中第53行，有兴趣可以点进去看一下\n      stub := new(ChaincodeStub)\n      #这一行代码的意思就是将提案中的信息抽取出来赋值到ChaincodeStub这个对象中\n     err := stub.init(handler, msg.ChannelId, msg.Txid, input, msg.Proposal)\n     if nextStateMsg = errFunc(err, nil, stub.chaincodeEvent, &quot;[%s] Init get error response. Sending %s&quot;, shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n          return\n     }\n     #这里的Init方法就是链码中所写的Init()方法，就不再解释了\n     res := handler.cc.Init(stub)\n     chaincodeLogger.Debugf(&quot;[%s] Init get response status: %d&quot;, shorttxid(msg.Txid), res.Status)\n      #ERROR的值为500,OK=200，ERRORTHRESHOLD = 400，大于等于400就代表错误信息或者被背书节点拒绝。\n      if res.Status &gt;= ERROR {\n          err = errors.New(res.Message)\n          if nextStateMsg = errFunc(err, []byte(res.Message), stub.chaincodeEvent, &quot;[%s] Init get error response. Sending %s&quot;, shorttxid(msg.Txid), pb.ChaincodeMessage_ERROR.String()); nextStateMsg != nil {\n              return\n          }\n      }\n      resBytes, err := proto.Marshal(&amp;res)\n      if err != nil {\n          payload := []byte(err.Error())\n          chaincodeLogger.Errorf(&quot;[%s] Init marshal response error [%s]. Sending %s&quot;, shorttxid(msg.Txid), err, pb.ChaincodeMessage_ERROR)\n          nextStateMsg = &amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_ERROR, Payload: payload, Txid: msg.Txid, ChaincodeEvent: stub.chaincodeEvent}\n          return\n      }\n\n      // Send COMPLETED message to chaincode support and change state\n      nextStateMsg = &amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_COMPLETED, Payload: resBytes, Txid: msg.Txid, ChaincodeEvent: stub.chaincodeEvent, ChannelId: stub.ChannelId}\n      chaincodeLogger.Debugf(&quot;[%s] Init succeeded. Sending %s&quot;, shorttxid(msg.Txid), pb.ChaincodeMessage_COMPLETED)\n      #到这里就结束了，会调用上面的handler.triggerNextState(nextStateMsg, errc)方法，这个方法将初始化数据与COMPLETED状态发送至Peer节点。\n  }()\n}</code></pre><p>这个方法还是比较简单的，一共做了这些事情：</p>\n</li>\n<li><p>获取用户的输入数据</p>\n</li>\n<li><p>新建一个<code>ChainCodeStub</code>对象，然后将用户输入的数据赋值给该对象</p>\n</li>\n<li><p>调用用户链码中的<code>Init()</code>方法</p>\n</li>\n<li><p>将所有数据封装成<code>ChainCodeMessage</code>，类型为<code>COMPLETED</code>,发送至Peer节点。</p>\n</li>\n</ul>\n<p>这个时候链码已经初始化完成，已经进入了可被调用(<code>invoke</code>)的状态.<br>之后的流程就差不多了，Peer节点发送<code>TRANSACTION</code>消息给链码侧，调用<code>Invoke()</code>方法，之后链码侧发送具体的调用方法到Peer节点，由Peer节点进行相应的处理，最后返回<code>RESPONSE</code>消息到链码侧，链码侧接收到<code>RESPONSE</code>消息后，返回<code>COMPLETED</code>消息到Peer节点。</p>\n<h2 id=\"5总结\"><a href=\"#5总结\" class=\"headerlink\" title=\"5总结\"></a>5总结</h2><p>到这里，Peer节点与链码侧的<code>handler</code>都处于<code>READY</code>状态,链码容器已经启动完成。最后总结一下整体的流程：</p>\n<ol>\n<li>通过用户端链码中的<code>main</code>方法，调用了<code>core/chaincode/shim/chaincode.go</code>中的<code>Start()</code>方法，从而开始了链码的启动。</li>\n<li>首先进行相关的配置比如基本的加密，证书的读取。</li>\n<li>创建与Peer节点之间的gRPC连接，创建<code>handler</code>实例。</li>\n<li>由链码容器向Peer节点发送第一个消息:<code>REGISTER</code>,然后等待接收由Peer节点发送的消息。如果接收到的是心跳消息，则向Peer节点返回心跳消息。</li>\n<li>Peer节点接收到链码容器发送的<code>REGISTER</code>消息后，将其注册到Peer节点端的<code>handler</code>上。</li>\n<li>Peer节点发送<code>REGISTERED</code>消息到链码侧，同时更新Peer节点端的<code>handler</code>状态为<code>Established</code>。</li>\n<li>Peer节点发送<code>Ready</code>消息到链码侧，同时更新Peer节点端的<code>handler</code>状态为<code>Ready</code>。</li>\n<li>链码侧接收到由Peer节点发送的<code>REGISTERED</code>消息后，更新链码侧的<code>handler</code>状态为<code>Established</code>。</li>\n<li>链码侧接收到由Peer节点发送的<code>READY</code>消息后，更新链码侧的<code>handler</code>状态为<code>ready</code>。</li>\n<li>当用户执行实例化链码时，通过Peer节点向链码侧发送<code>INIT</code>消息。链码侧接收到<code>INIT</code>消息后，根据用户输入的参数进行实例化操作。实例化完成后，返回<code>COMPLETED</code>消息到Peer节点。</li>\n<li>到这里链码容器已经启动，可以对链码数据进行查询调用等操作了。</li>\n</ol>\n<p>另外，阅读Fabric源码中有一些没有看明白或者分析有误的地方，还望大家能够批评指正。</p>\n<p>最后附上参考文档：<a href=\"https://legacy.gitbook.com/book/yeasy/hyperledger_code_fabric/details\" target=\"_blank\" rel=\"noopener\">传送门</a><br>以及Fabric源码地址：<a href=\"https://github.com/hyperledger/fabric\" target=\"_blank\" rel=\"noopener\">传送门</a></p>\n"},{"title":"Hyperledger Fabric-CA","date":"2019-12-08T12:21:58.000Z","_content":"Fabric—Ca的概念不再解释了，这里只说明使用方法:\n\n## 前置条件\n\n* Go语言1.10+版本\n* GOPATH环境变量正确设置\n* 已安装`libtool`和`libtdhl-dev`包\n\n#### Ubuntu系统\n\n通过以下命令安装`libtool`和`libtdhl-dev`包：\n```\nsudo apt install libtool libltdl-dev\n```\n#### MacOs 系统\n\nMac系统通过以下命令安装：\n```\nbrew install libtool\n```\n## Fabric-Ca安装\n\n可以通过以下两种途径进行安装：\n\n1. 直接下载二进制文件：\n```\ngo get -u github.com/hyperledger/fabric-ca/cmd/...\n```\n如果使用这种方式安装，安装成功的话直接在命令行输入(前提是GOPATH正确配置):\n```\nfabric-ca-server version\n```\n即可打印出安装的Ca版本。\n2. 从源码编译安装：\n首先在系统中建立以下路径:\n```\nmkdir -p $GOPATH/src/github.com/hyperledger/\ncd $GOPATH/src/github.com/hyperledger/\n```\n从Github上面将Fabric-Ca仓库克隆到本地：\n```\ngit clone https://github.com/hyperledger/fabric-ca.git\ncd fabric-ca\n```\n进行源码编译：\n```\nmake fabric-ca-server\nmake fabric-ca-client\n```\n如果没有报错的话，当前文件下会编译出一个`bin`文件夹，最后一步将该文件夹添加到环境变量，安装完成！\n\n#### 编译Ca的Docker镜像\n\n直接在`fabric-ca`文件夹内执行以下命令：\n```\nmake docker\n```\n## Fabric-Ca服务器简单使用\n\n* * *\n\n### 设置Fabric Ca服务器的`Home`文件夹\n\n启动Fabric Ca 服务器的第一步是需要对Fabric Ca服务器进行初始化操作，初始化操作将会生成一些默认的配置文件，所以我们首先需要指定一个文件夹作为服务器的主文件夹用来放生成的配置文件。\n可以通过以下几种方式设置Fabric-Ca服务器的主文件夹，优先级由高到低排序：\n\n1. 通过命令行设置参数`--home`设置。\n2. 如果设置了`FABRIC_CA_SERVER_HOME`环境变量,则使用该环境变量作为主文件夹。\n3. 如果设置了`FABRIC_CA_HOME`环境变量,则使用该环境变量作为主文件夹。\n4. 如果设置了`CA_CFG_PATH`环境变量,则使用该环境变量作为主文件夹。\n5. 如果以上方法都没有设置，则将当前工作目录作为主文件夹。\n\n官方建议是通过设置`FABRIC_CA_HOME`为`$HOME/fabric-ca/server`作为服务器的主文件夹。\n\n### 初始化服务器\n\n上一步骤完成后，就可以对Fabric Ca进行初始化了，执行以下命令：\n```\nfabric-ca-server init -b admin:adminpw\n```\n通过`-b`参数指定管理员的账号和密码对服务器进行初始化。将会生成一个自签名的证书。\n\n* admin:相当于管理员账号\n* adminpw:相当于管理员密码\n\n`admin:adminpw`可以自行设置。\n或者服务器的初始化也可以通过`-u`参数指定服务器的上一级服务器，也就是父服务器。格式为:`-u <parent-fabric-ca-server-URL`,其中这里的`URL`必须使用`<协议>://<enrollmentId>:<secret>@<host>:<port>`的格式。\n初始化之后将会生成几个文件：\n```\nIssuerPublicKey      #与零知识证明相关文件，暂不解释\nIssuerRevocationPublicKey #与零知识证明相关文件，暂不解释\nca-cert.pem             #CA服务器的根证书文件,只有持有该证书，用户才可以进行证书的颁发\nfabric-ca-server-config.yaml   #默认配置文件,对Ca服务器进行配置时可以用到\nfabric-ca-server.db  #Ca服务器数据库，存储注册的用户，组织，证书等信息。可以通过sqlite3 命令进去查看\nmsp/\n```\n\n### 启动服务器\n\n初始化之后可以直接启动服务器：\n```\nfabric-ca-server start -b <admin>:<adminpw>\n```\n服务器将会监听在7054端口。如果需要服务器监听在`https`上而不是`http`上，需要将`tls.enabled`设置为`true`。\n\n启动完之后，即可以通过`fabric-ca-client`工具或者是SDK对Ca服务器进行操作了。\n\n## Fabric Ca 客户端\n\n这一部分说明命令行工具`fabric-ca-client`的简单使用。\n\n### 设置Fabric Ca客户端的`Home`文件夹\n\n与服务器相同，客户端也具有自己的主文件夹，用来保存客户端的证书秘钥等等。\n可以通过以下几种方式设置Fabric-Ca客户端的主文件夹，优先级由高到低排序：\n\n1. 通过命令行设置参数`--home`设置。\n2. 如果设置了`FABRIC_CA_CLIENT_HOME`环境变量,则使用该环境变量作为主文件夹。\n3. 如果设置了`FABRIC_CA_HOME`环境变量,则使用该环境变量作为主文件夹。\n4. 如果设置了`CA_CFG_PATH`环境变量,则使用该环境变量作为主文件夹。\n5. 如果以上方法都没有设置，则`$HOME/.fabric-ca-client`将作为主文件夹。\n\n官方例子：`export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin`\n\n### 用户登陆\n\n设置完之后，我们使用命令行工具登陆管理员用户:\n```\nfabric-ca-client enroll -u http://admin:adminpw@localhost:7054\n```\n在`admin`目录下会产生以下文件：\n```\n.\n├── fabric-ca-client-config.yaml\n└── msp\n    ├── IssuerPublicKey\n    ├── IssuerRevocationPublicKey\n    ├── cacerts\n    │   └── localhost-7054.pem     #CA服务器的根证书，只不过换了一个名字\n    ├── keystore\n    │   └── 7ec84cbc25c20600ba98bf2bafb9c695ad57e392a57fb9f33b51fc493601a432_sk  #当前用户的秘钥信息\n    ├── signcerts\n    │   └── cert.pem   #当前用户的证书\n    └── user\n```\n\n### 注册一个身份\n通过`Fabric-CA`注册新的身份时，将由`Fabric-CA`服务器进行三个部分的权限检查确定当前用户是否具有权限进行身份的注册。\n\n1. 注册者必须含有`hf.Registrar.Roles`属性，并且需要注册的身份类型必须在该属性对应的值的列表中存在。比如注册者的`hf.Registrar.Roles`属性中对应的值只有一个`peer`，那么注册者使能注册类型为`peer`的身份，而不能注册`client`,`admin`,`orderer`.如果注册者的`hf.Registrar.Roles`属性对应的值为`*`，则说明可以注册任何类型的身份。\n2. 简单说就是上下级关系，比如注册者所处的部门为`a.b`,那么他只能注册处于`a.b`以及`a.b.*`部门的身份，而不能注册处于`a.c`部门的身份。如果需要注册一个最上级的部门的身份，那么需要将需要将需要注册的身份的`hf.affiliation`指定为`.`，并且注册者所处的部门也需要是最上级的部门。如果在注册身份时没有指定所属的部门，则默认被注册的身份所处的部门与注册者部门相同。\n3. 如果注册者满足以下条件则可以注册带有属性的身份：\n    * 对于`Fabric CA`中的保留属性(前缀以`hf`开头的)：只有注册者具有这个属性并且是`hf.Registrar.Attributes`属性中的值得一部分。也就是说如果需要注册一个带有`hf.a`属性的身份，那么注册者自己也需要有这个属性，并且在注册者的`hf.Registrar.Attributes`属性对应的值中需要包含`hf.a`这个属性。并且`hf.a`这个属性的值是一个列表，那么被注册的身份具有的`hf.a`属性只能等于或者等于列表中的一个子集。另外，如果`hf.a`这个属性的值对应的是一个布尔值，那么需要注册者`hf.a`属性的值为`true`。\n    * 对于注册者自定义的属性(不是`Fabric Ca`中的保留属性)：注册者`hf.Registrar.Attributes`对应的值需要包括这个属性，或者是已经注册过的模式。唯一支持的模式是以`*`结尾的字符串。比如注册者`hf.Registar.Attributes`对应的值为`a.b.*`，那么他可以注册的属性需要以`a.b.`开头。如果注册者`hf.Registar.Attributes`对应的值为`orgAdmin`,那么注册者只可以对一个身份进行添加或者删除`orgAdmin`属性.\n    * 对于`hr.Registrar.Attributes`属性：一个额外的检查是该属性对应的值需要等于注册者具有的该属性对应的值，或者是注册者具有的该属性对应的值的子集。\n\n接下来使用`admin`的身份注册一个身份：\n\n* `enrollment id`为`admin2`\n* 部门为`org1.department1`\n* 属性名字为`hf.Revoker`,对应的值为`true`\n* 属性名字为`admin`,对应的值为`true`\n\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name admin2     \\\n    --id.affiliation org1.department1 \\\n    --id.attrs 'hf.Revoker=true,admin=true:ecert'\n```\n其中对于属性`admin=true`,后缀为`ecert`表示这条属性将会添加到证书中，可以用来进行做访问控制的决定。\n\n对于多个属性，可以使用`--id.attrs`参数标记，并使用单引号括起来，每个属性使用逗号分隔开：\n```\nfabric-ca-client register -d \\\n    --id.name admin2 \\\n    --id.affiliation org1.department1 \\\n    --id.attrs '\"hf.Registrar.Roles=peer,client\",hf.Revoker=true'\n```\n或者是：\n```\nfabric-ca-client register -d \\\n    --id.name admin2  \\\n    --id.affiliation org1.department1 \\\n    --id.attrs '\"hf.Registrar.Roles=peer,client\"' \\\n    --id.attrs hf.Revoker=true\n```\n或者是通过客户端配置文件`fabric-ca-client-config.yaml`：\n```\nid:\n  name:\n  type: client\n  affiliation: org1.department1\n  maxenrollments: -1\n  attributes:\n    - name: hf.Revoker\n      value: true\n    - name: anotherAttrName\n      value: anotherAttrValue\n```\n接下来的命令是通过以上的配置文件注册一个身份：\n\n* `enrollment id`为`admin3`\n* 身份类型为`client`\n* 部门为`org1.department1`\n* 属性名字为`hf.Revoker`,对应的值为`true`\n* 属性名字为`anotherAttrName`,对应的值为`anotherAttrValue`\n\n设置`maxenrollments`为0或者是不设置将导致该身份可以使用`CA`的最大`enrollment`次数。并且一个身份的`maxenrollments`不能超过`CA`的`enrollments`最大值。例如，如果`CA`的`enrollment`最大值为5，则任何新的身份必须含有一个小于等于5的值。并且也不能设置为`-1`(-1表示无限制).\n\n接下来注册一个`peer`类型的身份。在这里我们选择自定义的密码而不是由服务器自动生成：\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name peer1 \\\n    --id.type peer \\\n    --id.affiliation org1.department1 \\\n    --id.secret peer1pw\n```\n注意，部门信息区分大小写，但服务器配置文件中指定的**非叶子**部门关系始终以小写形式存储。 例如，如果服务器配置文件的部门关系部分如下所示：\n```\naffiliations:\n  BU1:\n    Department1:\n      - Team1\n  BU2:\n    - Department2\n    - Department3\n```\n`BU1`,`Department1`,`BU2`使用小写进行存储。这是因为`Fabric CA`使用`Viper`读取配置。`Viper`对于大小写不敏感，如果需要注册一个身份部门为`Team1`,则需要通过`--id.affiliation`参数这样配置：`bu1.department1.Team1`\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name client1 \\\n    --id.type client \\\n    --id.affiliation bu1.department1.Team1\n```\n\n### 登录一个`peer`身份的用户\n之前已经成功注册了一个`peer`身份的用户，可以通过指定`id`和`secret`进行登录，与之前不同的是需要通过`-M`参数指定`Hyperledger Fabric MSP`(成员关系服务提供者)文件夹结构。\n\n接下来的命令将会登录`peer1`,确保使用`-M`参数指定了`peer`的MSP文件夹路径，该路径也是`peer`的`core.yaml`文件内`mspConfigPath`参数的设置值。或者也可以通过环境变量`FABRIC_CA_CLIENT_HOME`指定`peer`的主目录。\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/peer1\nfabric-ca-client enroll \\\n    -u http://peer1:peer1pw@localhost:7054 \\\n    -M $FABRIC_CA_CLIENT_HOME/msp\n```\n登录一个`orderer`也是相同的，除了`MSP`文件夹是在`orderer`的`orderer.yaml`文件中通过参数`LocalMSPDir`进行设置。\n\n由`fabric-ca-server`颁发的所有注册证书均具有以下组织单位（或简称为“ OU”）：\n\n1. OU层次结构的根等于身份类型。\n2. OU被添加到身份部门关系的每个组成部分。\n\n例如，如果一个身份类型为`peer`,部门为`department.team1`,则身份的OU分层(从根部开始)：`OU=team1,OU=department1,OU=peer`.\n\n### 获取身份混合器证书\n\n...\n\n### 获取身份混合器证书撤销信息\n\n### 重新登录一个身份\n\n假如你的登录证书过期了或者被恶意操作，需要通过以下命令重新创建一个登录证书：\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/peer1\nfabric-ca-client reenroll\n```\n\n### 撤销一个证书或者身份\n\n身份或者证书是可以被撤销的。撤销一个身份将会撤销所有属于这个身份的证书同时也会阻止该身份去获取新的证书。撤销一个证书只会使单个证书无效。\n\n为了撤销一个证书或者是身份。撤销者必须含有`hf.Revoker`和`hf.Registrar.Roles`两个属性。撤销一个身份只可以撤销从属于自己下级或者相同级别部门的证书或者是身份。进一步，撤销者只能撤销在撤销者`hf.Registrar.Roles`属性列表中存在的身份类型的身份。\n\n例如，部门为`orgs.org1`并且`hf.Registrar.Roles=peer,client`的撤销者可以撤销从属于`orgs.org1`部门或者是`orgs.org1.department1`并且身份类型为`peer`或者是`client`的身份。不能撤销从属于`orgs.org2`部门或者是其他类型的身份。\n\n下面的命令将会使一个身份与该身份下的所有证书失效，该身份未来对`fabric CA`服务器的所有请求将会被拒绝。\n```\nfabric-ca-client revoke -e <enrollment_id> -r <reason>\n```\n\n下面是`-r`参数支持的具体的原因：\n\n1. unspecified\n2. keycompromise\n3. cacompromise\n4. affiliationchange\n5. superseded\n6. cessationofoperation\n7. certificatehold\n8. removefromcrl\n9. privilegewithdrawn\n10. aacompromise\n\n例如，引导启动的`admin`属于部门的最上级可以撤销`peer1`的身份信息：\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client revoke -e peer1\n```\n\n属于一个身份的登录证书可以通过具体的AKI(权限密钥标识符)和序列号进行撤销：\n```\nfabric-ca-client revoke -a xxx -s yyy -r <reason>\n```\n\n例如，可以通过使用`openssl`命令获取一个证书的AKI和序列号并通过`revoke`命令撤销证书：\n```\nserial=$(openssl x509 -in userecert.pem -serial -noout | cut -d \"=\" -f 2)\naki=$(openssl x509 -in userecert.pem -text | awk '/keyid/ {gsub(/ *keyid:|:/,\"\",$1);print tolower($0)}')\nfabric-ca-client revoke -s $serial -a $aki -r affiliationchange\n```\n\n`--gencrl`参数可以用来生成`CRL`(证书撤销列表)，`CRL`包含所有被撤销的证书。例如，以下命令可以撤销`peer1`的身份。生成一个`CRL`并存储到`<msp 文件夹>/crls/crl.pem`文件。\n\n```\nfabric-ca-client revoke -e peer1 --gencrl\n```\n\n`CRL`可以使用`gencrl`命令生成，参考[生成CRL]()部分获取关于`gencrl`命令的更多信息。\n\n### 生成CRL(证书撤销列表)\n\n通过`Fabric CA SERVER`撤销一个证书后，在`Hyperledger Fabric`中合适的`MSP`文件需要进行更新。包括本地的`peer`节点的`MSP`与合适的通道配置区块中的`MSP`.为了做到这一点，`PEM`编码的`CRL`需要放置到`MSP`文件夹内的`crls`文件夹。`fabric-ca-client gencrl`命令可以生成`CRL`。任何带有`hf.GenCRL`属性的身份都可以生成包含所有在一个确定的时间被撤销的证书的序列号的`CRL`。生成的`CRL`存储在`<msp 文件夹>/crls/crl.pem`文件。\n\n以下的命令将会创建包含所有被撤销的(超时和未超时)证书的`CRL`存储在`<msp 文件夹>/crls/crl.pem`文件。\n```\nexport FABRIC_CA_CLIENT_HOME=~/clientconfig\nfabric-ca-client gencrl -M ~/msp\n```\n\n...\n\n\n### 开启TLS\n\n这一部分描述如何为`Fabric CA`客户端配置TLS的更多细节。\n\n以下部分可以在`fabric-ca-client-config.yaml`文件中进行配置：\n```\ntls:\n  enabled: true\n  certfiles:\n    - root.pem\n  client:\n    certfile: tls_client-cert.pem\n    keyfile: tls_client-key.pem\n```\n\n`certfiles`选项设置为被客户端信任的根证书。典型的就是`Fabric CA`根服务器的证书，`ca-cert.pem`可以在服务器的主目录发现.\n\n`client`选项要求只能手动在服务器进行TLS配置。\n\n### 基于属性的访问控制\n\n...未完待续","source":"_posts/blog/fabric/Hyperledger_Fabric_CA.md","raw":"---\ntitle: Hyperledger Fabric-CA\ndate: 2019-12-08 20:21:58\ntags: fabric-ca\ncategories: fabric-ca应用\n---\nFabric—Ca的概念不再解释了，这里只说明使用方法:\n\n## 前置条件\n\n* Go语言1.10+版本\n* GOPATH环境变量正确设置\n* 已安装`libtool`和`libtdhl-dev`包\n\n#### Ubuntu系统\n\n通过以下命令安装`libtool`和`libtdhl-dev`包：\n```\nsudo apt install libtool libltdl-dev\n```\n#### MacOs 系统\n\nMac系统通过以下命令安装：\n```\nbrew install libtool\n```\n## Fabric-Ca安装\n\n可以通过以下两种途径进行安装：\n\n1. 直接下载二进制文件：\n```\ngo get -u github.com/hyperledger/fabric-ca/cmd/...\n```\n如果使用这种方式安装，安装成功的话直接在命令行输入(前提是GOPATH正确配置):\n```\nfabric-ca-server version\n```\n即可打印出安装的Ca版本。\n2. 从源码编译安装：\n首先在系统中建立以下路径:\n```\nmkdir -p $GOPATH/src/github.com/hyperledger/\ncd $GOPATH/src/github.com/hyperledger/\n```\n从Github上面将Fabric-Ca仓库克隆到本地：\n```\ngit clone https://github.com/hyperledger/fabric-ca.git\ncd fabric-ca\n```\n进行源码编译：\n```\nmake fabric-ca-server\nmake fabric-ca-client\n```\n如果没有报错的话，当前文件下会编译出一个`bin`文件夹，最后一步将该文件夹添加到环境变量，安装完成！\n\n#### 编译Ca的Docker镜像\n\n直接在`fabric-ca`文件夹内执行以下命令：\n```\nmake docker\n```\n## Fabric-Ca服务器简单使用\n\n* * *\n\n### 设置Fabric Ca服务器的`Home`文件夹\n\n启动Fabric Ca 服务器的第一步是需要对Fabric Ca服务器进行初始化操作，初始化操作将会生成一些默认的配置文件，所以我们首先需要指定一个文件夹作为服务器的主文件夹用来放生成的配置文件。\n可以通过以下几种方式设置Fabric-Ca服务器的主文件夹，优先级由高到低排序：\n\n1. 通过命令行设置参数`--home`设置。\n2. 如果设置了`FABRIC_CA_SERVER_HOME`环境变量,则使用该环境变量作为主文件夹。\n3. 如果设置了`FABRIC_CA_HOME`环境变量,则使用该环境变量作为主文件夹。\n4. 如果设置了`CA_CFG_PATH`环境变量,则使用该环境变量作为主文件夹。\n5. 如果以上方法都没有设置，则将当前工作目录作为主文件夹。\n\n官方建议是通过设置`FABRIC_CA_HOME`为`$HOME/fabric-ca/server`作为服务器的主文件夹。\n\n### 初始化服务器\n\n上一步骤完成后，就可以对Fabric Ca进行初始化了，执行以下命令：\n```\nfabric-ca-server init -b admin:adminpw\n```\n通过`-b`参数指定管理员的账号和密码对服务器进行初始化。将会生成一个自签名的证书。\n\n* admin:相当于管理员账号\n* adminpw:相当于管理员密码\n\n`admin:adminpw`可以自行设置。\n或者服务器的初始化也可以通过`-u`参数指定服务器的上一级服务器，也就是父服务器。格式为:`-u <parent-fabric-ca-server-URL`,其中这里的`URL`必须使用`<协议>://<enrollmentId>:<secret>@<host>:<port>`的格式。\n初始化之后将会生成几个文件：\n```\nIssuerPublicKey      #与零知识证明相关文件，暂不解释\nIssuerRevocationPublicKey #与零知识证明相关文件，暂不解释\nca-cert.pem             #CA服务器的根证书文件,只有持有该证书，用户才可以进行证书的颁发\nfabric-ca-server-config.yaml   #默认配置文件,对Ca服务器进行配置时可以用到\nfabric-ca-server.db  #Ca服务器数据库，存储注册的用户，组织，证书等信息。可以通过sqlite3 命令进去查看\nmsp/\n```\n\n### 启动服务器\n\n初始化之后可以直接启动服务器：\n```\nfabric-ca-server start -b <admin>:<adminpw>\n```\n服务器将会监听在7054端口。如果需要服务器监听在`https`上而不是`http`上，需要将`tls.enabled`设置为`true`。\n\n启动完之后，即可以通过`fabric-ca-client`工具或者是SDK对Ca服务器进行操作了。\n\n## Fabric Ca 客户端\n\n这一部分说明命令行工具`fabric-ca-client`的简单使用。\n\n### 设置Fabric Ca客户端的`Home`文件夹\n\n与服务器相同，客户端也具有自己的主文件夹，用来保存客户端的证书秘钥等等。\n可以通过以下几种方式设置Fabric-Ca客户端的主文件夹，优先级由高到低排序：\n\n1. 通过命令行设置参数`--home`设置。\n2. 如果设置了`FABRIC_CA_CLIENT_HOME`环境变量,则使用该环境变量作为主文件夹。\n3. 如果设置了`FABRIC_CA_HOME`环境变量,则使用该环境变量作为主文件夹。\n4. 如果设置了`CA_CFG_PATH`环境变量,则使用该环境变量作为主文件夹。\n5. 如果以上方法都没有设置，则`$HOME/.fabric-ca-client`将作为主文件夹。\n\n官方例子：`export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin`\n\n### 用户登陆\n\n设置完之后，我们使用命令行工具登陆管理员用户:\n```\nfabric-ca-client enroll -u http://admin:adminpw@localhost:7054\n```\n在`admin`目录下会产生以下文件：\n```\n.\n├── fabric-ca-client-config.yaml\n└── msp\n    ├── IssuerPublicKey\n    ├── IssuerRevocationPublicKey\n    ├── cacerts\n    │   └── localhost-7054.pem     #CA服务器的根证书，只不过换了一个名字\n    ├── keystore\n    │   └── 7ec84cbc25c20600ba98bf2bafb9c695ad57e392a57fb9f33b51fc493601a432_sk  #当前用户的秘钥信息\n    ├── signcerts\n    │   └── cert.pem   #当前用户的证书\n    └── user\n```\n\n### 注册一个身份\n通过`Fabric-CA`注册新的身份时，将由`Fabric-CA`服务器进行三个部分的权限检查确定当前用户是否具有权限进行身份的注册。\n\n1. 注册者必须含有`hf.Registrar.Roles`属性，并且需要注册的身份类型必须在该属性对应的值的列表中存在。比如注册者的`hf.Registrar.Roles`属性中对应的值只有一个`peer`，那么注册者使能注册类型为`peer`的身份，而不能注册`client`,`admin`,`orderer`.如果注册者的`hf.Registrar.Roles`属性对应的值为`*`，则说明可以注册任何类型的身份。\n2. 简单说就是上下级关系，比如注册者所处的部门为`a.b`,那么他只能注册处于`a.b`以及`a.b.*`部门的身份，而不能注册处于`a.c`部门的身份。如果需要注册一个最上级的部门的身份，那么需要将需要将需要注册的身份的`hf.affiliation`指定为`.`，并且注册者所处的部门也需要是最上级的部门。如果在注册身份时没有指定所属的部门，则默认被注册的身份所处的部门与注册者部门相同。\n3. 如果注册者满足以下条件则可以注册带有属性的身份：\n    * 对于`Fabric CA`中的保留属性(前缀以`hf`开头的)：只有注册者具有这个属性并且是`hf.Registrar.Attributes`属性中的值得一部分。也就是说如果需要注册一个带有`hf.a`属性的身份，那么注册者自己也需要有这个属性，并且在注册者的`hf.Registrar.Attributes`属性对应的值中需要包含`hf.a`这个属性。并且`hf.a`这个属性的值是一个列表，那么被注册的身份具有的`hf.a`属性只能等于或者等于列表中的一个子集。另外，如果`hf.a`这个属性的值对应的是一个布尔值，那么需要注册者`hf.a`属性的值为`true`。\n    * 对于注册者自定义的属性(不是`Fabric Ca`中的保留属性)：注册者`hf.Registrar.Attributes`对应的值需要包括这个属性，或者是已经注册过的模式。唯一支持的模式是以`*`结尾的字符串。比如注册者`hf.Registar.Attributes`对应的值为`a.b.*`，那么他可以注册的属性需要以`a.b.`开头。如果注册者`hf.Registar.Attributes`对应的值为`orgAdmin`,那么注册者只可以对一个身份进行添加或者删除`orgAdmin`属性.\n    * 对于`hr.Registrar.Attributes`属性：一个额外的检查是该属性对应的值需要等于注册者具有的该属性对应的值，或者是注册者具有的该属性对应的值的子集。\n\n接下来使用`admin`的身份注册一个身份：\n\n* `enrollment id`为`admin2`\n* 部门为`org1.department1`\n* 属性名字为`hf.Revoker`,对应的值为`true`\n* 属性名字为`admin`,对应的值为`true`\n\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name admin2     \\\n    --id.affiliation org1.department1 \\\n    --id.attrs 'hf.Revoker=true,admin=true:ecert'\n```\n其中对于属性`admin=true`,后缀为`ecert`表示这条属性将会添加到证书中，可以用来进行做访问控制的决定。\n\n对于多个属性，可以使用`--id.attrs`参数标记，并使用单引号括起来，每个属性使用逗号分隔开：\n```\nfabric-ca-client register -d \\\n    --id.name admin2 \\\n    --id.affiliation org1.department1 \\\n    --id.attrs '\"hf.Registrar.Roles=peer,client\",hf.Revoker=true'\n```\n或者是：\n```\nfabric-ca-client register -d \\\n    --id.name admin2  \\\n    --id.affiliation org1.department1 \\\n    --id.attrs '\"hf.Registrar.Roles=peer,client\"' \\\n    --id.attrs hf.Revoker=true\n```\n或者是通过客户端配置文件`fabric-ca-client-config.yaml`：\n```\nid:\n  name:\n  type: client\n  affiliation: org1.department1\n  maxenrollments: -1\n  attributes:\n    - name: hf.Revoker\n      value: true\n    - name: anotherAttrName\n      value: anotherAttrValue\n```\n接下来的命令是通过以上的配置文件注册一个身份：\n\n* `enrollment id`为`admin3`\n* 身份类型为`client`\n* 部门为`org1.department1`\n* 属性名字为`hf.Revoker`,对应的值为`true`\n* 属性名字为`anotherAttrName`,对应的值为`anotherAttrValue`\n\n设置`maxenrollments`为0或者是不设置将导致该身份可以使用`CA`的最大`enrollment`次数。并且一个身份的`maxenrollments`不能超过`CA`的`enrollments`最大值。例如，如果`CA`的`enrollment`最大值为5，则任何新的身份必须含有一个小于等于5的值。并且也不能设置为`-1`(-1表示无限制).\n\n接下来注册一个`peer`类型的身份。在这里我们选择自定义的密码而不是由服务器自动生成：\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name peer1 \\\n    --id.type peer \\\n    --id.affiliation org1.department1 \\\n    --id.secret peer1pw\n```\n注意，部门信息区分大小写，但服务器配置文件中指定的**非叶子**部门关系始终以小写形式存储。 例如，如果服务器配置文件的部门关系部分如下所示：\n```\naffiliations:\n  BU1:\n    Department1:\n      - Team1\n  BU2:\n    - Department2\n    - Department3\n```\n`BU1`,`Department1`,`BU2`使用小写进行存储。这是因为`Fabric CA`使用`Viper`读取配置。`Viper`对于大小写不敏感，如果需要注册一个身份部门为`Team1`,则需要通过`--id.affiliation`参数这样配置：`bu1.department1.Team1`\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name client1 \\\n    --id.type client \\\n    --id.affiliation bu1.department1.Team1\n```\n\n### 登录一个`peer`身份的用户\n之前已经成功注册了一个`peer`身份的用户，可以通过指定`id`和`secret`进行登录，与之前不同的是需要通过`-M`参数指定`Hyperledger Fabric MSP`(成员关系服务提供者)文件夹结构。\n\n接下来的命令将会登录`peer1`,确保使用`-M`参数指定了`peer`的MSP文件夹路径，该路径也是`peer`的`core.yaml`文件内`mspConfigPath`参数的设置值。或者也可以通过环境变量`FABRIC_CA_CLIENT_HOME`指定`peer`的主目录。\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/peer1\nfabric-ca-client enroll \\\n    -u http://peer1:peer1pw@localhost:7054 \\\n    -M $FABRIC_CA_CLIENT_HOME/msp\n```\n登录一个`orderer`也是相同的，除了`MSP`文件夹是在`orderer`的`orderer.yaml`文件中通过参数`LocalMSPDir`进行设置。\n\n由`fabric-ca-server`颁发的所有注册证书均具有以下组织单位（或简称为“ OU”）：\n\n1. OU层次结构的根等于身份类型。\n2. OU被添加到身份部门关系的每个组成部分。\n\n例如，如果一个身份类型为`peer`,部门为`department.team1`,则身份的OU分层(从根部开始)：`OU=team1,OU=department1,OU=peer`.\n\n### 获取身份混合器证书\n\n...\n\n### 获取身份混合器证书撤销信息\n\n### 重新登录一个身份\n\n假如你的登录证书过期了或者被恶意操作，需要通过以下命令重新创建一个登录证书：\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/peer1\nfabric-ca-client reenroll\n```\n\n### 撤销一个证书或者身份\n\n身份或者证书是可以被撤销的。撤销一个身份将会撤销所有属于这个身份的证书同时也会阻止该身份去获取新的证书。撤销一个证书只会使单个证书无效。\n\n为了撤销一个证书或者是身份。撤销者必须含有`hf.Revoker`和`hf.Registrar.Roles`两个属性。撤销一个身份只可以撤销从属于自己下级或者相同级别部门的证书或者是身份。进一步，撤销者只能撤销在撤销者`hf.Registrar.Roles`属性列表中存在的身份类型的身份。\n\n例如，部门为`orgs.org1`并且`hf.Registrar.Roles=peer,client`的撤销者可以撤销从属于`orgs.org1`部门或者是`orgs.org1.department1`并且身份类型为`peer`或者是`client`的身份。不能撤销从属于`orgs.org2`部门或者是其他类型的身份。\n\n下面的命令将会使一个身份与该身份下的所有证书失效，该身份未来对`fabric CA`服务器的所有请求将会被拒绝。\n```\nfabric-ca-client revoke -e <enrollment_id> -r <reason>\n```\n\n下面是`-r`参数支持的具体的原因：\n\n1. unspecified\n2. keycompromise\n3. cacompromise\n4. affiliationchange\n5. superseded\n6. cessationofoperation\n7. certificatehold\n8. removefromcrl\n9. privilegewithdrawn\n10. aacompromise\n\n例如，引导启动的`admin`属于部门的最上级可以撤销`peer1`的身份信息：\n```\nexport FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client revoke -e peer1\n```\n\n属于一个身份的登录证书可以通过具体的AKI(权限密钥标识符)和序列号进行撤销：\n```\nfabric-ca-client revoke -a xxx -s yyy -r <reason>\n```\n\n例如，可以通过使用`openssl`命令获取一个证书的AKI和序列号并通过`revoke`命令撤销证书：\n```\nserial=$(openssl x509 -in userecert.pem -serial -noout | cut -d \"=\" -f 2)\naki=$(openssl x509 -in userecert.pem -text | awk '/keyid/ {gsub(/ *keyid:|:/,\"\",$1);print tolower($0)}')\nfabric-ca-client revoke -s $serial -a $aki -r affiliationchange\n```\n\n`--gencrl`参数可以用来生成`CRL`(证书撤销列表)，`CRL`包含所有被撤销的证书。例如，以下命令可以撤销`peer1`的身份。生成一个`CRL`并存储到`<msp 文件夹>/crls/crl.pem`文件。\n\n```\nfabric-ca-client revoke -e peer1 --gencrl\n```\n\n`CRL`可以使用`gencrl`命令生成，参考[生成CRL]()部分获取关于`gencrl`命令的更多信息。\n\n### 生成CRL(证书撤销列表)\n\n通过`Fabric CA SERVER`撤销一个证书后，在`Hyperledger Fabric`中合适的`MSP`文件需要进行更新。包括本地的`peer`节点的`MSP`与合适的通道配置区块中的`MSP`.为了做到这一点，`PEM`编码的`CRL`需要放置到`MSP`文件夹内的`crls`文件夹。`fabric-ca-client gencrl`命令可以生成`CRL`。任何带有`hf.GenCRL`属性的身份都可以生成包含所有在一个确定的时间被撤销的证书的序列号的`CRL`。生成的`CRL`存储在`<msp 文件夹>/crls/crl.pem`文件。\n\n以下的命令将会创建包含所有被撤销的(超时和未超时)证书的`CRL`存储在`<msp 文件夹>/crls/crl.pem`文件。\n```\nexport FABRIC_CA_CLIENT_HOME=~/clientconfig\nfabric-ca-client gencrl -M ~/msp\n```\n\n...\n\n\n### 开启TLS\n\n这一部分描述如何为`Fabric CA`客户端配置TLS的更多细节。\n\n以下部分可以在`fabric-ca-client-config.yaml`文件中进行配置：\n```\ntls:\n  enabled: true\n  certfiles:\n    - root.pem\n  client:\n    certfile: tls_client-cert.pem\n    keyfile: tls_client-key.pem\n```\n\n`certfiles`选项设置为被客户端信任的根证书。典型的就是`Fabric CA`根服务器的证书，`ca-cert.pem`可以在服务器的主目录发现.\n\n`client`选项要求只能手动在服务器进行TLS配置。\n\n### 基于属性的访问控制\n\n...未完待续","slug":"blog/fabric/Hyperledger_Fabric_CA","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyjg003rk0vq69gf1uc5","content":"<p>Fabric—Ca的概念不再解释了，这里只说明使用方法:</p>\n<h2 id=\"前置条件\"><a href=\"#前置条件\" class=\"headerlink\" title=\"前置条件\"></a>前置条件</h2><ul>\n<li>Go语言1.10+版本</li>\n<li>GOPATH环境变量正确设置</li>\n<li>已安装<code>libtool</code>和<code>libtdhl-dev</code>包</li>\n</ul>\n<h4 id=\"Ubuntu系统\"><a href=\"#Ubuntu系统\" class=\"headerlink\" title=\"Ubuntu系统\"></a>Ubuntu系统</h4><p>通过以下命令安装<code>libtool</code>和<code>libtdhl-dev</code>包：</p>\n<pre><code>sudo apt install libtool libltdl-dev</code></pre><h4 id=\"MacOs-系统\"><a href=\"#MacOs-系统\" class=\"headerlink\" title=\"MacOs 系统\"></a>MacOs 系统</h4><p>Mac系统通过以下命令安装：</p>\n<pre><code>brew install libtool</code></pre><h2 id=\"Fabric-Ca安装\"><a href=\"#Fabric-Ca安装\" class=\"headerlink\" title=\"Fabric-Ca安装\"></a>Fabric-Ca安装</h2><p>可以通过以下两种途径进行安装：</p>\n<ol>\n<li>直接下载二进制文件：<pre><code>go get -u github.com/hyperledger/fabric-ca/cmd/...</code></pre>如果使用这种方式安装，安装成功的话直接在命令行输入(前提是GOPATH正确配置):<pre><code>fabric-ca-server version</code></pre>即可打印出安装的Ca版本。</li>\n<li>从源码编译安装：<br>首先在系统中建立以下路径:<pre><code>mkdir -p $GOPATH/src/github.com/hyperledger/\ncd $GOPATH/src/github.com/hyperledger/</code></pre>从Github上面将Fabric-Ca仓库克隆到本地：<pre><code>git clone https://github.com/hyperledger/fabric-ca.git\ncd fabric-ca</code></pre>进行源码编译：<pre><code>make fabric-ca-server\nmake fabric-ca-client</code></pre>如果没有报错的话，当前文件下会编译出一个<code>bin</code>文件夹，最后一步将该文件夹添加到环境变量，安装完成！</li>\n</ol>\n<h4 id=\"编译Ca的Docker镜像\"><a href=\"#编译Ca的Docker镜像\" class=\"headerlink\" title=\"编译Ca的Docker镜像\"></a>编译Ca的Docker镜像</h4><p>直接在<code>fabric-ca</code>文件夹内执行以下命令：</p>\n<pre><code>make docker</code></pre><h2 id=\"Fabric-Ca服务器简单使用\"><a href=\"#Fabric-Ca服务器简单使用\" class=\"headerlink\" title=\"Fabric-Ca服务器简单使用\"></a>Fabric-Ca服务器简单使用</h2><hr>\n<h3 id=\"设置Fabric-Ca服务器的Home文件夹\"><a href=\"#设置Fabric-Ca服务器的Home文件夹\" class=\"headerlink\" title=\"设置Fabric Ca服务器的Home文件夹\"></a>设置Fabric Ca服务器的<code>Home</code>文件夹</h3><p>启动Fabric Ca 服务器的第一步是需要对Fabric Ca服务器进行初始化操作，初始化操作将会生成一些默认的配置文件，所以我们首先需要指定一个文件夹作为服务器的主文件夹用来放生成的配置文件。<br>可以通过以下几种方式设置Fabric-Ca服务器的主文件夹，优先级由高到低排序：</p>\n<ol>\n<li>通过命令行设置参数<code>--home</code>设置。</li>\n<li>如果设置了<code>FABRIC_CA_SERVER_HOME</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果设置了<code>FABRIC_CA_HOME</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果设置了<code>CA_CFG_PATH</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果以上方法都没有设置，则将当前工作目录作为主文件夹。</li>\n</ol>\n<p>官方建议是通过设置<code>FABRIC_CA_HOME</code>为<code>$HOME/fabric-ca/server</code>作为服务器的主文件夹。</p>\n<h3 id=\"初始化服务器\"><a href=\"#初始化服务器\" class=\"headerlink\" title=\"初始化服务器\"></a>初始化服务器</h3><p>上一步骤完成后，就可以对Fabric Ca进行初始化了，执行以下命令：</p>\n<pre><code>fabric-ca-server init -b admin:adminpw</code></pre><p>通过<code>-b</code>参数指定管理员的账号和密码对服务器进行初始化。将会生成一个自签名的证书。</p>\n<ul>\n<li>admin:相当于管理员账号</li>\n<li>adminpw:相当于管理员密码</li>\n</ul>\n<p><code>admin:adminpw</code>可以自行设置。<br>或者服务器的初始化也可以通过<code>-u</code>参数指定服务器的上一级服务器，也就是父服务器。格式为:<code>-u &lt;parent-fabric-ca-server-URL</code>,其中这里的<code>URL</code>必须使用<code>&lt;协议&gt;://&lt;enrollmentId&gt;:&lt;secret&gt;@&lt;host&gt;:&lt;port&gt;</code>的格式。<br>初始化之后将会生成几个文件：</p>\n<pre><code>IssuerPublicKey      #与零知识证明相关文件，暂不解释\nIssuerRevocationPublicKey #与零知识证明相关文件，暂不解释\nca-cert.pem             #CA服务器的根证书文件,只有持有该证书，用户才可以进行证书的颁发\nfabric-ca-server-config.yaml   #默认配置文件,对Ca服务器进行配置时可以用到\nfabric-ca-server.db  #Ca服务器数据库，存储注册的用户，组织，证书等信息。可以通过sqlite3 命令进去查看\nmsp/</code></pre><h3 id=\"启动服务器\"><a href=\"#启动服务器\" class=\"headerlink\" title=\"启动服务器\"></a>启动服务器</h3><p>初始化之后可以直接启动服务器：</p>\n<pre><code>fabric-ca-server start -b &lt;admin&gt;:&lt;adminpw&gt;</code></pre><p>服务器将会监听在7054端口。如果需要服务器监听在<code>https</code>上而不是<code>http</code>上，需要将<code>tls.enabled</code>设置为<code>true</code>。</p>\n<p>启动完之后，即可以通过<code>fabric-ca-client</code>工具或者是SDK对Ca服务器进行操作了。</p>\n<h2 id=\"Fabric-Ca-客户端\"><a href=\"#Fabric-Ca-客户端\" class=\"headerlink\" title=\"Fabric Ca 客户端\"></a>Fabric Ca 客户端</h2><p>这一部分说明命令行工具<code>fabric-ca-client</code>的简单使用。</p>\n<h3 id=\"设置Fabric-Ca客户端的Home文件夹\"><a href=\"#设置Fabric-Ca客户端的Home文件夹\" class=\"headerlink\" title=\"设置Fabric Ca客户端的Home文件夹\"></a>设置Fabric Ca客户端的<code>Home</code>文件夹</h3><p>与服务器相同，客户端也具有自己的主文件夹，用来保存客户端的证书秘钥等等。<br>可以通过以下几种方式设置Fabric-Ca客户端的主文件夹，优先级由高到低排序：</p>\n<ol>\n<li>通过命令行设置参数<code>--home</code>设置。</li>\n<li>如果设置了<code>FABRIC_CA_CLIENT_HOME</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果设置了<code>FABRIC_CA_HOME</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果设置了<code>CA_CFG_PATH</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果以上方法都没有设置，则<code>$HOME/.fabric-ca-client</code>将作为主文件夹。</li>\n</ol>\n<p>官方例子：<code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin</code></p>\n<h3 id=\"用户登陆\"><a href=\"#用户登陆\" class=\"headerlink\" title=\"用户登陆\"></a>用户登陆</h3><p>设置完之后，我们使用命令行工具登陆管理员用户:</p>\n<pre><code>fabric-ca-client enroll -u http://admin:adminpw@localhost:7054</code></pre><p>在<code>admin</code>目录下会产生以下文件：</p>\n<pre><code>.\n├── fabric-ca-client-config.yaml\n└── msp\n    ├── IssuerPublicKey\n    ├── IssuerRevocationPublicKey\n    ├── cacerts\n    │   └── localhost-7054.pem     #CA服务器的根证书，只不过换了一个名字\n    ├── keystore\n    │   └── 7ec84cbc25c20600ba98bf2bafb9c695ad57e392a57fb9f33b51fc493601a432_sk  #当前用户的秘钥信息\n    ├── signcerts\n    │   └── cert.pem   #当前用户的证书\n    └── user</code></pre><h3 id=\"注册一个身份\"><a href=\"#注册一个身份\" class=\"headerlink\" title=\"注册一个身份\"></a>注册一个身份</h3><p>通过<code>Fabric-CA</code>注册新的身份时，将由<code>Fabric-CA</code>服务器进行三个部分的权限检查确定当前用户是否具有权限进行身份的注册。</p>\n<ol>\n<li>注册者必须含有<code>hf.Registrar.Roles</code>属性，并且需要注册的身份类型必须在该属性对应的值的列表中存在。比如注册者的<code>hf.Registrar.Roles</code>属性中对应的值只有一个<code>peer</code>，那么注册者使能注册类型为<code>peer</code>的身份，而不能注册<code>client</code>,<code>admin</code>,<code>orderer</code>.如果注册者的<code>hf.Registrar.Roles</code>属性对应的值为<code>*</code>，则说明可以注册任何类型的身份。</li>\n<li>简单说就是上下级关系，比如注册者所处的部门为<code>a.b</code>,那么他只能注册处于<code>a.b</code>以及<code>a.b.*</code>部门的身份，而不能注册处于<code>a.c</code>部门的身份。如果需要注册一个最上级的部门的身份，那么需要将需要将需要注册的身份的<code>hf.affiliation</code>指定为<code>.</code>，并且注册者所处的部门也需要是最上级的部门。如果在注册身份时没有指定所属的部门，则默认被注册的身份所处的部门与注册者部门相同。</li>\n<li>如果注册者满足以下条件则可以注册带有属性的身份：<ul>\n<li>对于<code>Fabric CA</code>中的保留属性(前缀以<code>hf</code>开头的)：只有注册者具有这个属性并且是<code>hf.Registrar.Attributes</code>属性中的值得一部分。也就是说如果需要注册一个带有<code>hf.a</code>属性的身份，那么注册者自己也需要有这个属性，并且在注册者的<code>hf.Registrar.Attributes</code>属性对应的值中需要包含<code>hf.a</code>这个属性。并且<code>hf.a</code>这个属性的值是一个列表，那么被注册的身份具有的<code>hf.a</code>属性只能等于或者等于列表中的一个子集。另外，如果<code>hf.a</code>这个属性的值对应的是一个布尔值，那么需要注册者<code>hf.a</code>属性的值为<code>true</code>。</li>\n<li>对于注册者自定义的属性(不是<code>Fabric Ca</code>中的保留属性)：注册者<code>hf.Registrar.Attributes</code>对应的值需要包括这个属性，或者是已经注册过的模式。唯一支持的模式是以<code>*</code>结尾的字符串。比如注册者<code>hf.Registar.Attributes</code>对应的值为<code>a.b.*</code>，那么他可以注册的属性需要以<code>a.b.</code>开头。如果注册者<code>hf.Registar.Attributes</code>对应的值为<code>orgAdmin</code>,那么注册者只可以对一个身份进行添加或者删除<code>orgAdmin</code>属性.</li>\n<li>对于<code>hr.Registrar.Attributes</code>属性：一个额外的检查是该属性对应的值需要等于注册者具有的该属性对应的值，或者是注册者具有的该属性对应的值的子集。</li>\n</ul>\n</li>\n</ol>\n<p>接下来使用<code>admin</code>的身份注册一个身份：</p>\n<ul>\n<li><code>enrollment id</code>为<code>admin2</code></li>\n<li>部门为<code>org1.department1</code></li>\n<li>属性名字为<code>hf.Revoker</code>,对应的值为<code>true</code></li>\n<li>属性名字为<code>admin</code>,对应的值为<code>true</code></li>\n</ul>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name admin2     \\\n    --id.affiliation org1.department1 \\\n    --id.attrs &#39;hf.Revoker=true,admin=true:ecert&#39;</code></pre><p>其中对于属性<code>admin=true</code>,后缀为<code>ecert</code>表示这条属性将会添加到证书中，可以用来进行做访问控制的决定。</p>\n<p>对于多个属性，可以使用<code>--id.attrs</code>参数标记，并使用单引号括起来，每个属性使用逗号分隔开：</p>\n<pre><code>fabric-ca-client register -d \\\n    --id.name admin2 \\\n    --id.affiliation org1.department1 \\\n    --id.attrs &#39;&quot;hf.Registrar.Roles=peer,client&quot;,hf.Revoker=true&#39;</code></pre><p>或者是：</p>\n<pre><code>fabric-ca-client register -d \\\n    --id.name admin2  \\\n    --id.affiliation org1.department1 \\\n    --id.attrs &#39;&quot;hf.Registrar.Roles=peer,client&quot;&#39; \\\n    --id.attrs hf.Revoker=true</code></pre><p>或者是通过客户端配置文件<code>fabric-ca-client-config.yaml</code>：</p>\n<pre><code>id:\n  name:\n  type: client\n  affiliation: org1.department1\n  maxenrollments: -1\n  attributes:\n    - name: hf.Revoker\n      value: true\n    - name: anotherAttrName\n      value: anotherAttrValue</code></pre><p>接下来的命令是通过以上的配置文件注册一个身份：</p>\n<ul>\n<li><code>enrollment id</code>为<code>admin3</code></li>\n<li>身份类型为<code>client</code></li>\n<li>部门为<code>org1.department1</code></li>\n<li>属性名字为<code>hf.Revoker</code>,对应的值为<code>true</code></li>\n<li>属性名字为<code>anotherAttrName</code>,对应的值为<code>anotherAttrValue</code></li>\n</ul>\n<p>设置<code>maxenrollments</code>为0或者是不设置将导致该身份可以使用<code>CA</code>的最大<code>enrollment</code>次数。并且一个身份的<code>maxenrollments</code>不能超过<code>CA</code>的<code>enrollments</code>最大值。例如，如果<code>CA</code>的<code>enrollment</code>最大值为5，则任何新的身份必须含有一个小于等于5的值。并且也不能设置为<code>-1</code>(-1表示无限制).</p>\n<p>接下来注册一个<code>peer</code>类型的身份。在这里我们选择自定义的密码而不是由服务器自动生成：</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name peer1 \\\n    --id.type peer \\\n    --id.affiliation org1.department1 \\\n    --id.secret peer1pw</code></pre><p>注意，部门信息区分大小写，但服务器配置文件中指定的<strong>非叶子</strong>部门关系始终以小写形式存储。 例如，如果服务器配置文件的部门关系部分如下所示：</p>\n<pre><code>affiliations:\n  BU1:\n    Department1:\n      - Team1\n  BU2:\n    - Department2\n    - Department3</code></pre><p><code>BU1</code>,<code>Department1</code>,<code>BU2</code>使用小写进行存储。这是因为<code>Fabric CA</code>使用<code>Viper</code>读取配置。<code>Viper</code>对于大小写不敏感，如果需要注册一个身份部门为<code>Team1</code>,则需要通过<code>--id.affiliation</code>参数这样配置：<code>bu1.department1.Team1</code></p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name client1 \\\n    --id.type client \\\n    --id.affiliation bu1.department1.Team1</code></pre><h3 id=\"登录一个peer身份的用户\"><a href=\"#登录一个peer身份的用户\" class=\"headerlink\" title=\"登录一个peer身份的用户\"></a>登录一个<code>peer</code>身份的用户</h3><p>之前已经成功注册了一个<code>peer</code>身份的用户，可以通过指定<code>id</code>和<code>secret</code>进行登录，与之前不同的是需要通过<code>-M</code>参数指定<code>Hyperledger Fabric MSP</code>(成员关系服务提供者)文件夹结构。</p>\n<p>接下来的命令将会登录<code>peer1</code>,确保使用<code>-M</code>参数指定了<code>peer</code>的MSP文件夹路径，该路径也是<code>peer</code>的<code>core.yaml</code>文件内<code>mspConfigPath</code>参数的设置值。或者也可以通过环境变量<code>FABRIC_CA_CLIENT_HOME</code>指定<code>peer</code>的主目录。</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/peer1\nfabric-ca-client enroll \\\n    -u http://peer1:peer1pw@localhost:7054 \\\n    -M $FABRIC_CA_CLIENT_HOME/msp</code></pre><p>登录一个<code>orderer</code>也是相同的，除了<code>MSP</code>文件夹是在<code>orderer</code>的<code>orderer.yaml</code>文件中通过参数<code>LocalMSPDir</code>进行设置。</p>\n<p>由<code>fabric-ca-server</code>颁发的所有注册证书均具有以下组织单位（或简称为“ OU”）：</p>\n<ol>\n<li>OU层次结构的根等于身份类型。</li>\n<li>OU被添加到身份部门关系的每个组成部分。</li>\n</ol>\n<p>例如，如果一个身份类型为<code>peer</code>,部门为<code>department.team1</code>,则身份的OU分层(从根部开始)：<code>OU=team1,OU=department1,OU=peer</code>.</p>\n<h3 id=\"获取身份混合器证书\"><a href=\"#获取身份混合器证书\" class=\"headerlink\" title=\"获取身份混合器证书\"></a>获取身份混合器证书</h3><p>…</p>\n<h3 id=\"获取身份混合器证书撤销信息\"><a href=\"#获取身份混合器证书撤销信息\" class=\"headerlink\" title=\"获取身份混合器证书撤销信息\"></a>获取身份混合器证书撤销信息</h3><h3 id=\"重新登录一个身份\"><a href=\"#重新登录一个身份\" class=\"headerlink\" title=\"重新登录一个身份\"></a>重新登录一个身份</h3><p>假如你的登录证书过期了或者被恶意操作，需要通过以下命令重新创建一个登录证书：</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/peer1\nfabric-ca-client reenroll</code></pre><h3 id=\"撤销一个证书或者身份\"><a href=\"#撤销一个证书或者身份\" class=\"headerlink\" title=\"撤销一个证书或者身份\"></a>撤销一个证书或者身份</h3><p>身份或者证书是可以被撤销的。撤销一个身份将会撤销所有属于这个身份的证书同时也会阻止该身份去获取新的证书。撤销一个证书只会使单个证书无效。</p>\n<p>为了撤销一个证书或者是身份。撤销者必须含有<code>hf.Revoker</code>和<code>hf.Registrar.Roles</code>两个属性。撤销一个身份只可以撤销从属于自己下级或者相同级别部门的证书或者是身份。进一步，撤销者只能撤销在撤销者<code>hf.Registrar.Roles</code>属性列表中存在的身份类型的身份。</p>\n<p>例如，部门为<code>orgs.org1</code>并且<code>hf.Registrar.Roles=peer,client</code>的撤销者可以撤销从属于<code>orgs.org1</code>部门或者是<code>orgs.org1.department1</code>并且身份类型为<code>peer</code>或者是<code>client</code>的身份。不能撤销从属于<code>orgs.org2</code>部门或者是其他类型的身份。</p>\n<p>下面的命令将会使一个身份与该身份下的所有证书失效，该身份未来对<code>fabric CA</code>服务器的所有请求将会被拒绝。</p>\n<pre><code>fabric-ca-client revoke -e &lt;enrollment_id&gt; -r &lt;reason&gt;</code></pre><p>下面是<code>-r</code>参数支持的具体的原因：</p>\n<ol>\n<li>unspecified</li>\n<li>keycompromise</li>\n<li>cacompromise</li>\n<li>affiliationchange</li>\n<li>superseded</li>\n<li>cessationofoperation</li>\n<li>certificatehold</li>\n<li>removefromcrl</li>\n<li>privilegewithdrawn</li>\n<li>aacompromise</li>\n</ol>\n<p>例如，引导启动的<code>admin</code>属于部门的最上级可以撤销<code>peer1</code>的身份信息：</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client revoke -e peer1</code></pre><p>属于一个身份的登录证书可以通过具体的AKI(权限密钥标识符)和序列号进行撤销：</p>\n<pre><code>fabric-ca-client revoke -a xxx -s yyy -r &lt;reason&gt;</code></pre><p>例如，可以通过使用<code>openssl</code>命令获取一个证书的AKI和序列号并通过<code>revoke</code>命令撤销证书：</p>\n<pre><code>serial=$(openssl x509 -in userecert.pem -serial -noout | cut -d &quot;=&quot; -f 2)\naki=$(openssl x509 -in userecert.pem -text | awk &#39;/keyid/ {gsub(/ *keyid:|:/,&quot;&quot;,$1);print tolower($0)}&#39;)\nfabric-ca-client revoke -s $serial -a $aki -r affiliationchange</code></pre><p><code>--gencrl</code>参数可以用来生成<code>CRL</code>(证书撤销列表)，<code>CRL</code>包含所有被撤销的证书。例如，以下命令可以撤销<code>peer1</code>的身份。生成一个<code>CRL</code>并存储到<code>&lt;msp 文件夹&gt;/crls/crl.pem</code>文件。</p>\n<pre><code>fabric-ca-client revoke -e peer1 --gencrl</code></pre><p><code>CRL</code>可以使用<code>gencrl</code>命令生成，参考<a href=\"\">生成CRL</a>部分获取关于<code>gencrl</code>命令的更多信息。</p>\n<h3 id=\"生成CRL-证书撤销列表\"><a href=\"#生成CRL-证书撤销列表\" class=\"headerlink\" title=\"生成CRL(证书撤销列表)\"></a>生成CRL(证书撤销列表)</h3><p>通过<code>Fabric CA SERVER</code>撤销一个证书后，在<code>Hyperledger Fabric</code>中合适的<code>MSP</code>文件需要进行更新。包括本地的<code>peer</code>节点的<code>MSP</code>与合适的通道配置区块中的<code>MSP</code>.为了做到这一点，<code>PEM</code>编码的<code>CRL</code>需要放置到<code>MSP</code>文件夹内的<code>crls</code>文件夹。<code>fabric-ca-client gencrl</code>命令可以生成<code>CRL</code>。任何带有<code>hf.GenCRL</code>属性的身份都可以生成包含所有在一个确定的时间被撤销的证书的序列号的<code>CRL</code>。生成的<code>CRL</code>存储在<code>&lt;msp 文件夹&gt;/crls/crl.pem</code>文件。</p>\n<p>以下的命令将会创建包含所有被撤销的(超时和未超时)证书的<code>CRL</code>存储在<code>&lt;msp 文件夹&gt;/crls/crl.pem</code>文件。</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=~/clientconfig\nfabric-ca-client gencrl -M ~/msp</code></pre><p>…</p>\n<h3 id=\"开启TLS\"><a href=\"#开启TLS\" class=\"headerlink\" title=\"开启TLS\"></a>开启TLS</h3><p>这一部分描述如何为<code>Fabric CA</code>客户端配置TLS的更多细节。</p>\n<p>以下部分可以在<code>fabric-ca-client-config.yaml</code>文件中进行配置：</p>\n<pre><code>tls:\n  enabled: true\n  certfiles:\n    - root.pem\n  client:\n    certfile: tls_client-cert.pem\n    keyfile: tls_client-key.pem</code></pre><p><code>certfiles</code>选项设置为被客户端信任的根证书。典型的就是<code>Fabric CA</code>根服务器的证书，<code>ca-cert.pem</code>可以在服务器的主目录发现.</p>\n<p><code>client</code>选项要求只能手动在服务器进行TLS配置。</p>\n<h3 id=\"基于属性的访问控制\"><a href=\"#基于属性的访问控制\" class=\"headerlink\" title=\"基于属性的访问控制\"></a>基于属性的访问控制</h3><p>…未完待续</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Fabric—Ca的概念不再解释了，这里只说明使用方法:</p>\n<h2 id=\"前置条件\"><a href=\"#前置条件\" class=\"headerlink\" title=\"前置条件\"></a>前置条件</h2><ul>\n<li>Go语言1.10+版本</li>\n<li>GOPATH环境变量正确设置</li>\n<li>已安装<code>libtool</code>和<code>libtdhl-dev</code>包</li>\n</ul>\n<h4 id=\"Ubuntu系统\"><a href=\"#Ubuntu系统\" class=\"headerlink\" title=\"Ubuntu系统\"></a>Ubuntu系统</h4><p>通过以下命令安装<code>libtool</code>和<code>libtdhl-dev</code>包：</p>\n<pre><code>sudo apt install libtool libltdl-dev</code></pre><h4 id=\"MacOs-系统\"><a href=\"#MacOs-系统\" class=\"headerlink\" title=\"MacOs 系统\"></a>MacOs 系统</h4><p>Mac系统通过以下命令安装：</p>\n<pre><code>brew install libtool</code></pre><h2 id=\"Fabric-Ca安装\"><a href=\"#Fabric-Ca安装\" class=\"headerlink\" title=\"Fabric-Ca安装\"></a>Fabric-Ca安装</h2><p>可以通过以下两种途径进行安装：</p>\n<ol>\n<li>直接下载二进制文件：<pre><code>go get -u github.com/hyperledger/fabric-ca/cmd/...</code></pre>如果使用这种方式安装，安装成功的话直接在命令行输入(前提是GOPATH正确配置):<pre><code>fabric-ca-server version</code></pre>即可打印出安装的Ca版本。</li>\n<li>从源码编译安装：<br>首先在系统中建立以下路径:<pre><code>mkdir -p $GOPATH/src/github.com/hyperledger/\ncd $GOPATH/src/github.com/hyperledger/</code></pre>从Github上面将Fabric-Ca仓库克隆到本地：<pre><code>git clone https://github.com/hyperledger/fabric-ca.git\ncd fabric-ca</code></pre>进行源码编译：<pre><code>make fabric-ca-server\nmake fabric-ca-client</code></pre>如果没有报错的话，当前文件下会编译出一个<code>bin</code>文件夹，最后一步将该文件夹添加到环境变量，安装完成！</li>\n</ol>\n<h4 id=\"编译Ca的Docker镜像\"><a href=\"#编译Ca的Docker镜像\" class=\"headerlink\" title=\"编译Ca的Docker镜像\"></a>编译Ca的Docker镜像</h4><p>直接在<code>fabric-ca</code>文件夹内执行以下命令：</p>\n<pre><code>make docker</code></pre><h2 id=\"Fabric-Ca服务器简单使用\"><a href=\"#Fabric-Ca服务器简单使用\" class=\"headerlink\" title=\"Fabric-Ca服务器简单使用\"></a>Fabric-Ca服务器简单使用</h2><hr>\n<h3 id=\"设置Fabric-Ca服务器的Home文件夹\"><a href=\"#设置Fabric-Ca服务器的Home文件夹\" class=\"headerlink\" title=\"设置Fabric Ca服务器的Home文件夹\"></a>设置Fabric Ca服务器的<code>Home</code>文件夹</h3><p>启动Fabric Ca 服务器的第一步是需要对Fabric Ca服务器进行初始化操作，初始化操作将会生成一些默认的配置文件，所以我们首先需要指定一个文件夹作为服务器的主文件夹用来放生成的配置文件。<br>可以通过以下几种方式设置Fabric-Ca服务器的主文件夹，优先级由高到低排序：</p>\n<ol>\n<li>通过命令行设置参数<code>--home</code>设置。</li>\n<li>如果设置了<code>FABRIC_CA_SERVER_HOME</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果设置了<code>FABRIC_CA_HOME</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果设置了<code>CA_CFG_PATH</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果以上方法都没有设置，则将当前工作目录作为主文件夹。</li>\n</ol>\n<p>官方建议是通过设置<code>FABRIC_CA_HOME</code>为<code>$HOME/fabric-ca/server</code>作为服务器的主文件夹。</p>\n<h3 id=\"初始化服务器\"><a href=\"#初始化服务器\" class=\"headerlink\" title=\"初始化服务器\"></a>初始化服务器</h3><p>上一步骤完成后，就可以对Fabric Ca进行初始化了，执行以下命令：</p>\n<pre><code>fabric-ca-server init -b admin:adminpw</code></pre><p>通过<code>-b</code>参数指定管理员的账号和密码对服务器进行初始化。将会生成一个自签名的证书。</p>\n<ul>\n<li>admin:相当于管理员账号</li>\n<li>adminpw:相当于管理员密码</li>\n</ul>\n<p><code>admin:adminpw</code>可以自行设置。<br>或者服务器的初始化也可以通过<code>-u</code>参数指定服务器的上一级服务器，也就是父服务器。格式为:<code>-u &lt;parent-fabric-ca-server-URL</code>,其中这里的<code>URL</code>必须使用<code>&lt;协议&gt;://&lt;enrollmentId&gt;:&lt;secret&gt;@&lt;host&gt;:&lt;port&gt;</code>的格式。<br>初始化之后将会生成几个文件：</p>\n<pre><code>IssuerPublicKey      #与零知识证明相关文件，暂不解释\nIssuerRevocationPublicKey #与零知识证明相关文件，暂不解释\nca-cert.pem             #CA服务器的根证书文件,只有持有该证书，用户才可以进行证书的颁发\nfabric-ca-server-config.yaml   #默认配置文件,对Ca服务器进行配置时可以用到\nfabric-ca-server.db  #Ca服务器数据库，存储注册的用户，组织，证书等信息。可以通过sqlite3 命令进去查看\nmsp/</code></pre><h3 id=\"启动服务器\"><a href=\"#启动服务器\" class=\"headerlink\" title=\"启动服务器\"></a>启动服务器</h3><p>初始化之后可以直接启动服务器：</p>\n<pre><code>fabric-ca-server start -b &lt;admin&gt;:&lt;adminpw&gt;</code></pre><p>服务器将会监听在7054端口。如果需要服务器监听在<code>https</code>上而不是<code>http</code>上，需要将<code>tls.enabled</code>设置为<code>true</code>。</p>\n<p>启动完之后，即可以通过<code>fabric-ca-client</code>工具或者是SDK对Ca服务器进行操作了。</p>\n<h2 id=\"Fabric-Ca-客户端\"><a href=\"#Fabric-Ca-客户端\" class=\"headerlink\" title=\"Fabric Ca 客户端\"></a>Fabric Ca 客户端</h2><p>这一部分说明命令行工具<code>fabric-ca-client</code>的简单使用。</p>\n<h3 id=\"设置Fabric-Ca客户端的Home文件夹\"><a href=\"#设置Fabric-Ca客户端的Home文件夹\" class=\"headerlink\" title=\"设置Fabric Ca客户端的Home文件夹\"></a>设置Fabric Ca客户端的<code>Home</code>文件夹</h3><p>与服务器相同，客户端也具有自己的主文件夹，用来保存客户端的证书秘钥等等。<br>可以通过以下几种方式设置Fabric-Ca客户端的主文件夹，优先级由高到低排序：</p>\n<ol>\n<li>通过命令行设置参数<code>--home</code>设置。</li>\n<li>如果设置了<code>FABRIC_CA_CLIENT_HOME</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果设置了<code>FABRIC_CA_HOME</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果设置了<code>CA_CFG_PATH</code>环境变量,则使用该环境变量作为主文件夹。</li>\n<li>如果以上方法都没有设置，则<code>$HOME/.fabric-ca-client</code>将作为主文件夹。</li>\n</ol>\n<p>官方例子：<code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin</code></p>\n<h3 id=\"用户登陆\"><a href=\"#用户登陆\" class=\"headerlink\" title=\"用户登陆\"></a>用户登陆</h3><p>设置完之后，我们使用命令行工具登陆管理员用户:</p>\n<pre><code>fabric-ca-client enroll -u http://admin:adminpw@localhost:7054</code></pre><p>在<code>admin</code>目录下会产生以下文件：</p>\n<pre><code>.\n├── fabric-ca-client-config.yaml\n└── msp\n    ├── IssuerPublicKey\n    ├── IssuerRevocationPublicKey\n    ├── cacerts\n    │   └── localhost-7054.pem     #CA服务器的根证书，只不过换了一个名字\n    ├── keystore\n    │   └── 7ec84cbc25c20600ba98bf2bafb9c695ad57e392a57fb9f33b51fc493601a432_sk  #当前用户的秘钥信息\n    ├── signcerts\n    │   └── cert.pem   #当前用户的证书\n    └── user</code></pre><h3 id=\"注册一个身份\"><a href=\"#注册一个身份\" class=\"headerlink\" title=\"注册一个身份\"></a>注册一个身份</h3><p>通过<code>Fabric-CA</code>注册新的身份时，将由<code>Fabric-CA</code>服务器进行三个部分的权限检查确定当前用户是否具有权限进行身份的注册。</p>\n<ol>\n<li>注册者必须含有<code>hf.Registrar.Roles</code>属性，并且需要注册的身份类型必须在该属性对应的值的列表中存在。比如注册者的<code>hf.Registrar.Roles</code>属性中对应的值只有一个<code>peer</code>，那么注册者使能注册类型为<code>peer</code>的身份，而不能注册<code>client</code>,<code>admin</code>,<code>orderer</code>.如果注册者的<code>hf.Registrar.Roles</code>属性对应的值为<code>*</code>，则说明可以注册任何类型的身份。</li>\n<li>简单说就是上下级关系，比如注册者所处的部门为<code>a.b</code>,那么他只能注册处于<code>a.b</code>以及<code>a.b.*</code>部门的身份，而不能注册处于<code>a.c</code>部门的身份。如果需要注册一个最上级的部门的身份，那么需要将需要将需要注册的身份的<code>hf.affiliation</code>指定为<code>.</code>，并且注册者所处的部门也需要是最上级的部门。如果在注册身份时没有指定所属的部门，则默认被注册的身份所处的部门与注册者部门相同。</li>\n<li>如果注册者满足以下条件则可以注册带有属性的身份：<ul>\n<li>对于<code>Fabric CA</code>中的保留属性(前缀以<code>hf</code>开头的)：只有注册者具有这个属性并且是<code>hf.Registrar.Attributes</code>属性中的值得一部分。也就是说如果需要注册一个带有<code>hf.a</code>属性的身份，那么注册者自己也需要有这个属性，并且在注册者的<code>hf.Registrar.Attributes</code>属性对应的值中需要包含<code>hf.a</code>这个属性。并且<code>hf.a</code>这个属性的值是一个列表，那么被注册的身份具有的<code>hf.a</code>属性只能等于或者等于列表中的一个子集。另外，如果<code>hf.a</code>这个属性的值对应的是一个布尔值，那么需要注册者<code>hf.a</code>属性的值为<code>true</code>。</li>\n<li>对于注册者自定义的属性(不是<code>Fabric Ca</code>中的保留属性)：注册者<code>hf.Registrar.Attributes</code>对应的值需要包括这个属性，或者是已经注册过的模式。唯一支持的模式是以<code>*</code>结尾的字符串。比如注册者<code>hf.Registar.Attributes</code>对应的值为<code>a.b.*</code>，那么他可以注册的属性需要以<code>a.b.</code>开头。如果注册者<code>hf.Registar.Attributes</code>对应的值为<code>orgAdmin</code>,那么注册者只可以对一个身份进行添加或者删除<code>orgAdmin</code>属性.</li>\n<li>对于<code>hr.Registrar.Attributes</code>属性：一个额外的检查是该属性对应的值需要等于注册者具有的该属性对应的值，或者是注册者具有的该属性对应的值的子集。</li>\n</ul>\n</li>\n</ol>\n<p>接下来使用<code>admin</code>的身份注册一个身份：</p>\n<ul>\n<li><code>enrollment id</code>为<code>admin2</code></li>\n<li>部门为<code>org1.department1</code></li>\n<li>属性名字为<code>hf.Revoker</code>,对应的值为<code>true</code></li>\n<li>属性名字为<code>admin</code>,对应的值为<code>true</code></li>\n</ul>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name admin2     \\\n    --id.affiliation org1.department1 \\\n    --id.attrs &#39;hf.Revoker=true,admin=true:ecert&#39;</code></pre><p>其中对于属性<code>admin=true</code>,后缀为<code>ecert</code>表示这条属性将会添加到证书中，可以用来进行做访问控制的决定。</p>\n<p>对于多个属性，可以使用<code>--id.attrs</code>参数标记，并使用单引号括起来，每个属性使用逗号分隔开：</p>\n<pre><code>fabric-ca-client register -d \\\n    --id.name admin2 \\\n    --id.affiliation org1.department1 \\\n    --id.attrs &#39;&quot;hf.Registrar.Roles=peer,client&quot;,hf.Revoker=true&#39;</code></pre><p>或者是：</p>\n<pre><code>fabric-ca-client register -d \\\n    --id.name admin2  \\\n    --id.affiliation org1.department1 \\\n    --id.attrs &#39;&quot;hf.Registrar.Roles=peer,client&quot;&#39; \\\n    --id.attrs hf.Revoker=true</code></pre><p>或者是通过客户端配置文件<code>fabric-ca-client-config.yaml</code>：</p>\n<pre><code>id:\n  name:\n  type: client\n  affiliation: org1.department1\n  maxenrollments: -1\n  attributes:\n    - name: hf.Revoker\n      value: true\n    - name: anotherAttrName\n      value: anotherAttrValue</code></pre><p>接下来的命令是通过以上的配置文件注册一个身份：</p>\n<ul>\n<li><code>enrollment id</code>为<code>admin3</code></li>\n<li>身份类型为<code>client</code></li>\n<li>部门为<code>org1.department1</code></li>\n<li>属性名字为<code>hf.Revoker</code>,对应的值为<code>true</code></li>\n<li>属性名字为<code>anotherAttrName</code>,对应的值为<code>anotherAttrValue</code></li>\n</ul>\n<p>设置<code>maxenrollments</code>为0或者是不设置将导致该身份可以使用<code>CA</code>的最大<code>enrollment</code>次数。并且一个身份的<code>maxenrollments</code>不能超过<code>CA</code>的<code>enrollments</code>最大值。例如，如果<code>CA</code>的<code>enrollment</code>最大值为5，则任何新的身份必须含有一个小于等于5的值。并且也不能设置为<code>-1</code>(-1表示无限制).</p>\n<p>接下来注册一个<code>peer</code>类型的身份。在这里我们选择自定义的密码而不是由服务器自动生成：</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name peer1 \\\n    --id.type peer \\\n    --id.affiliation org1.department1 \\\n    --id.secret peer1pw</code></pre><p>注意，部门信息区分大小写，但服务器配置文件中指定的<strong>非叶子</strong>部门关系始终以小写形式存储。 例如，如果服务器配置文件的部门关系部分如下所示：</p>\n<pre><code>affiliations:\n  BU1:\n    Department1:\n      - Team1\n  BU2:\n    - Department2\n    - Department3</code></pre><p><code>BU1</code>,<code>Department1</code>,<code>BU2</code>使用小写进行存储。这是因为<code>Fabric CA</code>使用<code>Viper</code>读取配置。<code>Viper</code>对于大小写不敏感，如果需要注册一个身份部门为<code>Team1</code>,则需要通过<code>--id.affiliation</code>参数这样配置：<code>bu1.department1.Team1</code></p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client register \\\n    --id.name client1 \\\n    --id.type client \\\n    --id.affiliation bu1.department1.Team1</code></pre><h3 id=\"登录一个peer身份的用户\"><a href=\"#登录一个peer身份的用户\" class=\"headerlink\" title=\"登录一个peer身份的用户\"></a>登录一个<code>peer</code>身份的用户</h3><p>之前已经成功注册了一个<code>peer</code>身份的用户，可以通过指定<code>id</code>和<code>secret</code>进行登录，与之前不同的是需要通过<code>-M</code>参数指定<code>Hyperledger Fabric MSP</code>(成员关系服务提供者)文件夹结构。</p>\n<p>接下来的命令将会登录<code>peer1</code>,确保使用<code>-M</code>参数指定了<code>peer</code>的MSP文件夹路径，该路径也是<code>peer</code>的<code>core.yaml</code>文件内<code>mspConfigPath</code>参数的设置值。或者也可以通过环境变量<code>FABRIC_CA_CLIENT_HOME</code>指定<code>peer</code>的主目录。</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/peer1\nfabric-ca-client enroll \\\n    -u http://peer1:peer1pw@localhost:7054 \\\n    -M $FABRIC_CA_CLIENT_HOME/msp</code></pre><p>登录一个<code>orderer</code>也是相同的，除了<code>MSP</code>文件夹是在<code>orderer</code>的<code>orderer.yaml</code>文件中通过参数<code>LocalMSPDir</code>进行设置。</p>\n<p>由<code>fabric-ca-server</code>颁发的所有注册证书均具有以下组织单位（或简称为“ OU”）：</p>\n<ol>\n<li>OU层次结构的根等于身份类型。</li>\n<li>OU被添加到身份部门关系的每个组成部分。</li>\n</ol>\n<p>例如，如果一个身份类型为<code>peer</code>,部门为<code>department.team1</code>,则身份的OU分层(从根部开始)：<code>OU=team1,OU=department1,OU=peer</code>.</p>\n<h3 id=\"获取身份混合器证书\"><a href=\"#获取身份混合器证书\" class=\"headerlink\" title=\"获取身份混合器证书\"></a>获取身份混合器证书</h3><p>…</p>\n<h3 id=\"获取身份混合器证书撤销信息\"><a href=\"#获取身份混合器证书撤销信息\" class=\"headerlink\" title=\"获取身份混合器证书撤销信息\"></a>获取身份混合器证书撤销信息</h3><h3 id=\"重新登录一个身份\"><a href=\"#重新登录一个身份\" class=\"headerlink\" title=\"重新登录一个身份\"></a>重新登录一个身份</h3><p>假如你的登录证书过期了或者被恶意操作，需要通过以下命令重新创建一个登录证书：</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/peer1\nfabric-ca-client reenroll</code></pre><h3 id=\"撤销一个证书或者身份\"><a href=\"#撤销一个证书或者身份\" class=\"headerlink\" title=\"撤销一个证书或者身份\"></a>撤销一个证书或者身份</h3><p>身份或者证书是可以被撤销的。撤销一个身份将会撤销所有属于这个身份的证书同时也会阻止该身份去获取新的证书。撤销一个证书只会使单个证书无效。</p>\n<p>为了撤销一个证书或者是身份。撤销者必须含有<code>hf.Revoker</code>和<code>hf.Registrar.Roles</code>两个属性。撤销一个身份只可以撤销从属于自己下级或者相同级别部门的证书或者是身份。进一步，撤销者只能撤销在撤销者<code>hf.Registrar.Roles</code>属性列表中存在的身份类型的身份。</p>\n<p>例如，部门为<code>orgs.org1</code>并且<code>hf.Registrar.Roles=peer,client</code>的撤销者可以撤销从属于<code>orgs.org1</code>部门或者是<code>orgs.org1.department1</code>并且身份类型为<code>peer</code>或者是<code>client</code>的身份。不能撤销从属于<code>orgs.org2</code>部门或者是其他类型的身份。</p>\n<p>下面的命令将会使一个身份与该身份下的所有证书失效，该身份未来对<code>fabric CA</code>服务器的所有请求将会被拒绝。</p>\n<pre><code>fabric-ca-client revoke -e &lt;enrollment_id&gt; -r &lt;reason&gt;</code></pre><p>下面是<code>-r</code>参数支持的具体的原因：</p>\n<ol>\n<li>unspecified</li>\n<li>keycompromise</li>\n<li>cacompromise</li>\n<li>affiliationchange</li>\n<li>superseded</li>\n<li>cessationofoperation</li>\n<li>certificatehold</li>\n<li>removefromcrl</li>\n<li>privilegewithdrawn</li>\n<li>aacompromise</li>\n</ol>\n<p>例如，引导启动的<code>admin</code>属于部门的最上级可以撤销<code>peer1</code>的身份信息：</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=$HOME/fabric-ca/clients/admin\nfabric-ca-client revoke -e peer1</code></pre><p>属于一个身份的登录证书可以通过具体的AKI(权限密钥标识符)和序列号进行撤销：</p>\n<pre><code>fabric-ca-client revoke -a xxx -s yyy -r &lt;reason&gt;</code></pre><p>例如，可以通过使用<code>openssl</code>命令获取一个证书的AKI和序列号并通过<code>revoke</code>命令撤销证书：</p>\n<pre><code>serial=$(openssl x509 -in userecert.pem -serial -noout | cut -d &quot;=&quot; -f 2)\naki=$(openssl x509 -in userecert.pem -text | awk &#39;/keyid/ {gsub(/ *keyid:|:/,&quot;&quot;,$1);print tolower($0)}&#39;)\nfabric-ca-client revoke -s $serial -a $aki -r affiliationchange</code></pre><p><code>--gencrl</code>参数可以用来生成<code>CRL</code>(证书撤销列表)，<code>CRL</code>包含所有被撤销的证书。例如，以下命令可以撤销<code>peer1</code>的身份。生成一个<code>CRL</code>并存储到<code>&lt;msp 文件夹&gt;/crls/crl.pem</code>文件。</p>\n<pre><code>fabric-ca-client revoke -e peer1 --gencrl</code></pre><p><code>CRL</code>可以使用<code>gencrl</code>命令生成，参考<a href=\"\">生成CRL</a>部分获取关于<code>gencrl</code>命令的更多信息。</p>\n<h3 id=\"生成CRL-证书撤销列表\"><a href=\"#生成CRL-证书撤销列表\" class=\"headerlink\" title=\"生成CRL(证书撤销列表)\"></a>生成CRL(证书撤销列表)</h3><p>通过<code>Fabric CA SERVER</code>撤销一个证书后，在<code>Hyperledger Fabric</code>中合适的<code>MSP</code>文件需要进行更新。包括本地的<code>peer</code>节点的<code>MSP</code>与合适的通道配置区块中的<code>MSP</code>.为了做到这一点，<code>PEM</code>编码的<code>CRL</code>需要放置到<code>MSP</code>文件夹内的<code>crls</code>文件夹。<code>fabric-ca-client gencrl</code>命令可以生成<code>CRL</code>。任何带有<code>hf.GenCRL</code>属性的身份都可以生成包含所有在一个确定的时间被撤销的证书的序列号的<code>CRL</code>。生成的<code>CRL</code>存储在<code>&lt;msp 文件夹&gt;/crls/crl.pem</code>文件。</p>\n<p>以下的命令将会创建包含所有被撤销的(超时和未超时)证书的<code>CRL</code>存储在<code>&lt;msp 文件夹&gt;/crls/crl.pem</code>文件。</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=~/clientconfig\nfabric-ca-client gencrl -M ~/msp</code></pre><p>…</p>\n<h3 id=\"开启TLS\"><a href=\"#开启TLS\" class=\"headerlink\" title=\"开启TLS\"></a>开启TLS</h3><p>这一部分描述如何为<code>Fabric CA</code>客户端配置TLS的更多细节。</p>\n<p>以下部分可以在<code>fabric-ca-client-config.yaml</code>文件中进行配置：</p>\n<pre><code>tls:\n  enabled: true\n  certfiles:\n    - root.pem\n  client:\n    certfile: tls_client-cert.pem\n    keyfile: tls_client-key.pem</code></pre><p><code>certfiles</code>选项设置为被客户端信任的根证书。典型的就是<code>Fabric CA</code>根服务器的证书，<code>ca-cert.pem</code>可以在服务器的主目录发现.</p>\n<p><code>client</code>选项要求只能手动在服务器进行TLS配置。</p>\n<h3 id=\"基于属性的访问控制\"><a href=\"#基于属性的访问控制\" class=\"headerlink\" title=\"基于属性的访问控制\"></a>基于属性的访问控制</h3><p>…未完待续</p>\n"},{"title":"Hyperledger Fabric动态配置Raft节点","date":"2019-12-31T04:48:21.000Z","_content":"# Hyperledger Fabric动态配置Raft节点\n最近看官方文档发现新的共识算法etcdRaft允许动态添加或删除排序节点，所以也花了一天时间操作了以下，写篇文章把整个过程记录一下。\n初始网络本文设置了4个Orderer节点，1个Peer节点(用于更新配置文件以及测试用),然后动态添加第五个Orderer节点。\n本文分成两个部分:\n\n1. 第一部分是手动通过Fabric-CA生成每一个节点的证书文件\n2. 第二部分是更新Fabric网络配置添加新的Orderer节点。\n\n本文基于**Fabric v2.0.0-beta**版本。版本号只要高于1.4.1就行\n## 1 搭建定制化的Fabric网络\n前提条件是成功跑起来Fabric的示例网络，可以看这里->[Hyperledger Fabric环境搭建](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n首先在`$GOPATH`下(本文路径地址为`$GOPATH/src/github.com/hyperledger/fab`)建立如下几个文件夹用于之后的操作:\n```\n.  # 这里是根目录fab\n├── ca    # 用于生成CA证书的ca配置文件的文件夹\n│   ├── org1\n│   │   └── fabric-ca-server-config.yaml\n│   └── server\n│       └── fabric-ca-server-config.yaml\n├── channel-artifacts    #用于保存创世区块以及通道配置文件\n├── configtx.yaml      #配置文件：用于生成创世区块以及通道配置文件\n├── crypto-config     #存储生成的证书文件\n├── docker      # Fabric网络节点通过Docker启动，用于启动节点的Docker文件\n│   ├── base.yaml\n│   ├── docker-compose-addOrderer5.yaml\n│   ├── docker-compose-ca.yaml\n│   ├── docker-compose-orderers.yaml\n│   └── docker-compose-peer.yaml\n└── store    #存储区块等信息\n```\n**以下所有操作默认都在根目录文件夹内！**\n### 1.1CA配置文件\n直接在这里贴出来:`org1/fabric-ca-server-config.yaml`:\n```\n\nversion: 1.2.0\n\n# Server's listening port (default: 7054)\nport: 7054\n\n# Enables debug logging (default: false)\ndebug: false\n\ncrlsizelimit: 512000\n\ntls:\n  # Enable TLS (default: false)\n  enabled: true\n  certfile:\n  keyfile:\n  clientauth:\n    type: noclientcert\n    certfiles:\n\nca:\n  # Name of this CA\n  name: Org1CA\n  keyfile:\n  certfile:\n  chainfile:\n\ncrl:\n  expiry: 24h\n\nregistry:\n\n  maxenrollments: -1\n\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: \"\"\n       attrs:\n          hf.Registrar.Roles: \"*\"\n          hf.Registrar.DelegateRoles: \"*\"\n          hf.Revoker: true\n          hf.IntermediateCA: true\n          hf.GenCRL: true\n          hf.Registrar.Attributes: \"*\"\n          hf.AffiliationMgr: true\n\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\n  tls:\n      enabled: false\n      certfiles:\n      client:\n        certfile:\n        keyfile:\n\nldap:\n\n   enabled: false\n   url: ldap://<adminDN>:<adminPassword>@<host>:<port>/<base>\n   tls:\n      certfiles:\n      client:\n         certfile:\n         keyfile:\n   attribute:\n      names: ['uid','member']\n      converters:\n         - name:\n           value:\n      maps:\n         groups:\n            - name:\n              value:\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n\nsigning:\n    default:\n      usage:\n        - digital signature\n      expiry: 8760h\n    profiles:\n      ca:\n         usage:\n           - cert sign\n           - crl sign\n         expiry: 43800h\n         caconstraint:\n           isca: true\n           maxpathlen: 0\n      tls:\n         usage:\n            - signing\n            - key encipherment\n            - server auth\n            - client auth\n            - key agreement\n         expiry: 8760h\n\ncsr:\n   cn: ca.org1.example.com\n   names:\n      - C: US\n        ST: \"North Carolina\"\n        L: \"Durham\"\n        O: org1.example.com\n        OU:\n   hosts:\n     - localhost\n     - org1.example.com\n   ca:\n      expiry: 131400h\n      pathlength: 1\n\nbccsp:\n    default: SW\n    sw:\n        hash: SHA2\n        security: 256\n        filekeystore:\n            keystore: msp/keystore\n\ncacount:\n\ncafiles:\n\nintermediate:\n  parentserver:\n    url:\n    caname:\n\n  enrollment:\n    hosts:\n    profile:\n    label:\n\n  tls:\n    certfiles:\n    client:\n      certfile:\n      keyfile:\n      \n```\n以及`server/fabric-ca-server-config.yaml:`:\n```\n# Version of config file\nversion: 1.2.0\n# Server's listening port (default: 7054)\nport: 7054\n# Enables debug logging (default: false)\ndebug: false\n# Size limit of an acceptable CRL in bytes (default: 512000)\ncrlsizelimit: 512000\ntls:\n  # Enable TLS (default: false)\n  enabled: true\n  # TLS for the server's listening port\n  certfile:\n  keyfile:\n  clientauth:\n    type: noclientcert\n    certfiles:\n\nca:\n  # Name of this CA\n  name: OrdererCA\n  keyfile:\n  certfile:\n  chainfile:\n\ncrl:\n  expiry: 24h\n\nregistry:\n  maxenrollments: -1\n\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: \"\"\n       attrs:\n          hf.Registrar.Roles: \"*\"\n          hf.Registrar.DelegateRoles: \"*\"\n          hf.Revoker: true\n          hf.IntermediateCA: true\n          hf.GenCRL: true\n          hf.Registrar.Attributes: \"*\"\n          hf.AffiliationMgr: true\n\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\n  tls:\n      enabled: false\n      certfiles:\n      client:\n        certfile:\n        keyfile:\n\nldap:\n   enabled: false\n   url: ldap://<adminDN>:<adminPassword>@<host>:<port>/<base>\n   tls:\n      certfiles:\n      client:\n         certfile:\n         keyfile:\n   attribute:\n      names: ['uid','member']\n      converters:\n         - name:\n           value:\n      maps:\n         groups:\n            - name:\n              value:\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n\nsigning:\n    default:\n      usage:\n        - digital signature\n      expiry: 8760h\n    profiles:\n      ca:\n         usage:\n           - cert sign\n           - crl sign\n         expiry: 43800h\n         caconstraint:\n           isca: true\n           maxpathlen: 0\n      tls:\n         usage:\n            - signing\n            - key encipherment\n            - server auth\n            - client auth\n            - key agreement\n         expiry: 8760h\n\ncsr:\n   cn: ca.example.com\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: example.com\n        OU:\n   hosts:\n     - localhost\n     - example.com\n   ca:\n      expiry: 131400h\n      pathlength: 1\n\nbccsp:\n    default: SW\n    sw:\n        hash: SHA2\n        security: 256\n        filekeystore:\n            keystore: msp/keystore\n\ncacount:\ncafiles:\n\nintermediate:\n  parentserver:\n    url:\n    caname:\n\n  enrollment:\n    hosts:\n    profile:\n    label:\n\n  tls:\n    certfiles:\n    client:\n      certfile:\n      keyfile:\n```\n`docker-compose-ca.yaml`文件:\n```\nversion: '2'\n\nservices:\n  ca:\n    image: hyperledger/fabric-ca:1.4.4\n    environment:\n      - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server\n      - FABRIC_CA_SERVER_CA_NAME=ca-orderer\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_PORT=9054\n    ports:\n      - \"9054:9054\"\n    command: sh -c 'fabric-ca-server start -b admin:adminpw -d'\n    volumes:\n      - ../ca/server:/etc/hyperledger/fabric-ca-server\n    container_name: ca_orderer\n\n  ca0:\n    image: hyperledger/fabric-ca:1.4.4\n    environment:\n      - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server\n      - FABRIC_CA_SERVER_CA_NAME=ca-org1\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_PORT=7054\n    ports:\n      - \"7054:7054\"\n    command: sh -c 'fabric-ca-server start -b admin:adminpw -d'\n    volumes:\n      - ../ca/org1:/etc/hyperledger/fabric-ca-server\n    container_name: ca_org1\n```\n将以上三个文件保存到指定的路径，然后使用以下命令启动CA服务器：\n```\ndocker-compose -f docker/docker-compose-ca.yaml up -d\n```\n服务器会自动读取上面的两个配置文件，并初始化CA服务器。\n当然，服务器配置文件将自动生成在`ca/server/`子文件夹内，其中最主要使用到的是`tls-cert.pem`文件。\n\n### 1.2 注册Orderer节点\n首先配置环境变量并登陆管理员账号:\n```\n#创建存储Order节点证书的子文件夹。\nmkdir -p crypto-config/orderOrganization/example.com\nexport FABRIC_CA_CLIENT_HOME=${PWD}/crypto-config/orderOrganization/example.com\nfabric-ca-client enroll -u https://admin:adminpw@localhost:9054 --caname ca-orderer --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n生成节点类型分类配置文件(不知道这个文件应该称作什么，暂且使用这个名字称呼好了):\n```\n  echo 'NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: orderer' > ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml\n```\n之后注册网络中初始的4个Orderer节点:\n```\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer1 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer2 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer3 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer4 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n注册`Admin`节点:\n```\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name ordererAdmin --id.secret ordererAdminpw --id.type admin --id.attrs '\"hf.Registrar.Roles=admin\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n\n### 1.3 获取Orderer证书文件\n为刚刚创建的几个用户创建各自的文件夹用于存储证书文件:\n```\nmkdir -p crypto-config/orderOrganization/example.com/orderers\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer1.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer2.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer3.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer4.example.com\n```\n接下来获取每一个Orderer节点的`MSP`证书文件:\n```\nfabric-ca-client enroll -u https://orderer1:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp --csr.hosts orderer1.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer2:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp --csr.hosts orderer2.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer3:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp --csr.hosts orderer3.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer4:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp --csr.hosts orderer4.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n还有每一个节点的`TLS`证书:\n```\nfabric-ca-client enroll -u https://orderer1:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls --enrollment.profile tls --csr.hosts orderer1.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer2:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls --enrollment.profile tls --csr.hosts orderer2.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer3:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls --enrollment.profile tls --csr.hosts orderer3.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer4:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls --enrollment.profile tls --csr.hosts orderer4.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n将之前生成的节点类型分类配置文件拷贝到每一个节点的`MSP`文件夹:\n```\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/config.yaml\n```\n然后为每一个节点的`TLS`证书以及秘钥文件修改名字，方便之后的使用:\n```\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.key\n```\n然后在`MSP`文件夹内创建`tlscacerts`文件夹，并将`TLS`文件拷贝过去:\n```\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/tlscacerts\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n```\n复制TLS根证书:\n```\nmkdir -p ${PWD}/crypto-config/orderOrganization/example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n```\n最后是`Admin`节点的证书文件:\n```\n#首先也是创建文件夹\nmkdir -p crypto-config/orderOrganization/example.com/users\nmkdir -p crypto-config/orderOrganization/example.com/users/Admin@example.com\n#获取证书文件\nfabric-ca-client enroll -u https://ordererAdmin:ordererAdminpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/users/Admin@example.com/msp --tls.certfiles ${PWD}/ca/server/tls-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/users/Admin@example.com/msp/config.yaml\n```\n到这里Orderer节点证书已经生成完毕(可以根据实际需要修改Orderer节点数量，最少不能低于3个)，接下来是网络中唯一的`peer`节点的配置文件生成:\n\n### 1.4 注册Peer节点\n和上面步骤相同，首先创建子文件夹用于存储证书文件:\n```\nmkdir -p crypto-config/peerOrganizations/org1.example.com/\n```\n配置环境变量并登陆管理员身份:\n```\nexport FABRIC_CA_CLIENT_HOME=${PWD}/crypto-config/peerOrganizations/org1.example.com/\nfabric-ca-client enroll -u https://admin:adminpw@localhost:7054 --caname ca-org1 --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n```\n生成节点类型分类配置文件:\n```\necho 'NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: orderer' > ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml\n```\n虽然网络中只有一个peer节点，但是我们需要注册三个用户:`peer0,user1,org1admin`，其中第一个是必需的，第二个是用于测试的，第三个为`Admin`用户，安装和实例化链码需要`Admin`用户的证书:\n```\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name peer0 --id.secret peer0pw --id.type peer --id.attrs '\"hf.Registrar.Roles=peer\"' --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name user1 --id.secret user1pw --id.type client --id.attrs '\"hf.Registrar.Roles=client\"' --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name org1admin --id.secret org1adminpw --id.type admin --id.attrs '\"hf.Registrar.Roles=admin\"' --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n```\n\n### 1.5 获取Peer节点证书文件\n节点注册完毕，获取他们的证书文件:\n创建子文件夹:\n```\nmkdir -p crypto-config/peerOrganizations/org1.example.com/peers\nmkdir -p crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.co\n```\n获取证书文件:\n```\n#MSP文件\nfabric-ca-client enroll -u https://peer0:peer0pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp --csr.hosts peer0.org1.example.com --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n#TLS证书\nfabric-ca-client enroll -u https://peer0:peer0pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls --enrollment.profile tls --csr.hosts peer0.org1.example.com --csr.hosts localhost --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n```\n拷贝节点分类配置文件:\n```\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/config.yaml\n```\n修改证书以及秘钥文件，方便之后使用:\n```\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/signcerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/keystore/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n```\n将TLS相关证书复制一份:\n```\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/tlscacerts/ca.crt\n\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/tlsca\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem\n\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/ca\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/cacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/ca/ca.org1.example.com-cert.pem\n```\n获取`user`与`Admin`用户证书文件:\n```\n#创建子文件夹\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users/User1@org1.example.com\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com\n#获取证书文件\nfabric-ca-client enroll -u https://user1:user1pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/User1@org1.example.com/msp --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client enroll -u https://org1admin:org1adminpw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp/config.yaml\n```\n\n### 1.6 启动网络之前的准备\n到这里我们已经生成了所有需要的证书文件，接下来是生成用于启动网络的创世区块,生成创世区块需要一个文件`configtx.yaml`,直接复制过来:\n```\n\nOrganizations:\n    - &OrdererOrg\n        Name: OrdererOrg\n        ID: OrdererMSP\n        MSPDir: ./crypto-config/orderOrganization/example.com/msp   #这里路径需要对应！！！\n        Policies:\n            Readers:\n                Type: Signature\n                Rule: \"OR('OrdererMSP.member')\"\n            Writers:\n                Type: Signature\n                Rule: \"OR('OrdererMSP.member')\"\n            Admins:\n                Type: Signature\n                Rule: \"OR('OrdererMSP.admin')\"\n\n    - &Org1  #如果需要更多组织节点，可以按照该模板在下面添加\n        Name: Org1MSP\n        ID: Org1MSP\n        MSPDir: ./crypto-config/peerOrganizations/org1.example.com/msp  #这里路径需要对应！！！\n        Policies:\n            Readers:\n                Type: Signature\n                Rule: \"OR('Org1MSP.admin', 'Org1MSP.peer', 'Org1MSP.client')\"\n            Writers:\n                Type: Signature\n                Rule: \"OR('Org1MSP.admin', 'Org1MSP.client')\"\n            Admins:\n                Type: Signature\n                Rule: \"OR('Org1MSP.admin')\"\n            Endorsement:\n                Type: Signature\n                Rule: \"OR('Org1MSP.peer')\"\n        AnchorPeers:\n              Port: 7051\n\nCapabilities:\n    Channel: &ChannelCapabilities\n        V2_0: true\n\n    Orderer: &OrdererCapabilities\n        V2_0: true\n\n    Application: &ApplicationCapabilities\n        V2_0: true\n\nApplication: &ApplicationDefaults\n    Organizations:\n    Policies:\n        Readers:\n            Type: ImplicitMeta\n            Rule: \"ANY Readers\"\n        Writers:\n            Type: ImplicitMeta\n            Rule: \"ANY Writers\"\n        Admins:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Admins\"\n        LifecycleEndorsement:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Endorsement\"\n        Endorsement:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Endorsement\"\n    Capabilities:\n        <<: *ApplicationCapabilities\n\nOrderer: &OrdererDefaults\n    OrdererType: etcdraft\n    \n    Addresses:\n        - orderer1.example.com:7050\n    BatchTimeout: 2s\n    BatchSize:\n        MaxMessageCount: 10\n        AbsoluteMaxBytes: 99 MB\n        PreferredMaxBytes: 512 KB\n    Organizations:\n    Policies:\n        Readers:\n            Type: ImplicitMeta\n            Rule: \"ANY Readers\"\n        Writers:\n            Type: ImplicitMeta\n            Rule: \"ANY Writers\"\n        Admins:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Admins\"\n        BlockValidation:\n            Type: ImplicitMeta\n            Rule: \"ANY Writers\"\nChannel: &ChannelDefaults\n    Policies:\n        # Who may invoke the 'Deliver' API\n        Readers:\n            Type: ImplicitMeta\n            Rule: \"ANY Readers\"\n        # Who may invoke the 'Broadcast' API\n        Writers:\n            Type: ImplicitMeta\n            Rule: \"ANY Writers\"\n        # By default, who may modify elements at this config level\n        Admins:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Admins\"\n    Capabilities:\n        <<: *ChannelCapabilities\n\nProfiles:\n    \n    TwoOrgsChannel:   #用于生成通道配置文件\n        Consortium: SampleConsortium\n        <<: *ChannelDefaults\n        Application:\n            <<: *ApplicationDefaults\n            Organizations:\n                - *Org1\n            Capabilities:\n                <<: *ApplicationCapabilities\n\n    SampleMultiNodeEtcdRaft:   #用于生成系统通道创世区块\n        <<: *ChannelDefaults\n        Capabilities:\n            <<: *ChannelCapabilities\n        Orderer:\n            <<: *OrdererDefaults\n            OrdererType: etcdraft   #指定使用etcdraft共识算法\n            EtcdRaft:\n                Consenters:\n                - Host: orderer1.example.com\n                  Port: 7050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n                - Host: orderer2.example.com\n                  Port: 8050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\n                - Host: orderer3.example.com\n                  Port: 9050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\n                - Host: orderer4.example.com\n                  Port: 10050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\n#                    - Host: orderer5.example.com\n#                      Port: 11050\n#                      ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n#                      ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n            Addresses:\n                - orderer1.example.com:7050\n                - orderer2.example.com:8050\n                - orderer3.example.com:9050\n                - orderer4.example.com:10050\n#                - orderer5.example.com:11050\n         \n            Organizations:\n            - *OrdererOrg\n            Capabilities:\n                <<: *OrdererCapabilities\n        Application:\n            <<: *ApplicationDefaults\n            Organizations:\n            - <<: *OrdererOrg\n        Consortiums:\n            SampleConsortium:\n                Organizations:\n                - *Org1\n```\n将该文件保存到指定位置，接下来生成创世区块:\n```\nexport FABRIC_CFG_PATH=$PWD\nconfigtxgen -profile SampleMultiNodeEtcdRaft -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n# 生成通道配置文件\nexport CHANNEL_NAME=mychannel\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/mychannel.tx -channelID $CHANNEL_NAME\n```\n\n### 1.7 启动网络\n首先写包含所有节点的Docker文件,这里直接贴出来:\n`base.yaml`:\n```\nversion: '2'\n\nservices:\n  orderer-base:\n    image: hyperledger/fabric-orderer:2.0.0-beta\n    environment:\n      - FABRIC_LOGGING_SPEC=INFO\n#      - FABRIC_LOGGING_SPEC=DEBUG\n      - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0\n      - ORDERER_GENERAL_BOOTSTRAPMETHOD=file\n      - ORDERER_GENERAL_BOOTSTRAPFILE=/var/hyperledger/orderer/orderer.genesis.block\n      - ORDERER_GENERAL_LOCALMSPID=OrdererMSP\n      - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp\n      # enabled TLS\n      - ORDERER_GENERAL_TLS_ENABLED=true\n      - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key\n      - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt\n      - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt]\n      - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt\n      - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key\n      - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt]\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric\n    command: orderer\n```\n然后是Orderer节点的Docker文件`docker-compose-orderers.yaml`:\n```\n# Copyright IBM Corp. All Rights Reserved.\n#\n# SPDX-License-Identifier: Apache-2.0\n#\n\nversion: '2'\n\nvolumes:\n  orderer1.example.com:\n  orderer2.example.com:\n  orderer3.example.com:\n  orderer4.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n  \n  orderer1.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=7050\n    container_name: orderer1.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o1:/var/hyperledger/production/orderer\n    ports:\n      - 7050:7050\n\n\n\n  orderer2.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=8050\n    container_name: orderer2.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o2:/var/hyperledger/production/orderer\n    ports:\n      - 8050:8050\n  \n  orderer3.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=9050\n    container_name: orderer3.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o3:/var/hyperledger/production/orderer\n    ports:\n      - 9050:9050\n  \n  orderer4.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=10050\n    container_name: orderer4.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o4:/var/hyperledger/production/orderer\n    ports:\n      - 10050:10050\n```\n最后一个是peer节点的Docker文件`docker-compose-peer.yaml`：\n```\nversion: '2'\n\nvolumes:\n  peer0.org1.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    image: hyperledger/fabric-peer:2.0.0-beta\n    environment:\n      #Generic peer variables\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      # the following setting starts chaincode containers on the same\n      # bridge network as the peers\n      # https://docs.docker.com/compose/networking/\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=${COMPOSE_PROJECT_NAME}_byfn\n      - FABRIC_LOGGING_SPEC=INFO\n      #- FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_PROFILE_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt\n      # Peer specific variabes\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:7051\n      - CORE_PEER_CHAINCODEADDRESS=peer0.org1.example.com:7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org1.example.com:7051\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_LEDGER_STATE_STATEDATABASE=CouchDB\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984\n      # The CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME and CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD\n      # provide the credentials for ledger to connect to CouchDB.  The username and password must\n      # match the username and password set for the associated CouchDB.\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME=\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD=\n    volumes:\n      - /var/run/:/host/var/run/\n      - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp:/etc/hyperledger/fabric/msp\n      - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls:/etc/hyperledger/fabric/tls\n      - ../store/p1:/var/hyperledger/production\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: peer node start\n    depends_on:\n      - couchdb0\n    ports:\n      - 7051:7051\n    networks:\n      - byfn\n\n  couchdb0:\n    container_name: couchdb0\n    image: couchdb:2.3\n    # Populate the COUCHDB_USER and COUCHDB_PASSWORD to set an admin user and password\n    # for CouchDB.  This will prevent CouchDB from operating in an \"Admin Party\" mode.\n    environment:\n      - COUCHDB_USER=\n      - COUCHDB_PASSWORD=\n    # Comment/Uncomment the port mapping if you want to hide/expose the CouchDB service,\n    # for example map it to utilize Fauxton User Interface in dev environments.\n    ports:\n      - \"5984:5984\"\n    networks:\n      - byfn\n\n  cli:\n    container_name: cli\n    image: hyperledger/fabric-tools:2.0.0-beta\n    tty: true\n    stdin_open: true\n    environment:\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n#      - FABRIC_LOGGING_SPEC=DEBUG\n      - FABRIC_LOGGING_SPEC=INFO\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\n      - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: /bin/bash\n    volumes:\n      - /var/run/:/host/var/run/\n      - ./../../chaincode/:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - ../crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/\n      - ../channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts\n    depends_on:\n      - peer0.org1.example.com\n    networks:\n      - byfn\n```\n将以上文件保存到指定位置后，使用以下命令直接启动:\n```\ndocker-compose -f docker/docker-compose-orderers.yaml -f docker/docker-compose-peer.yaml up -d\n```\n启动完成后可以查看每个节点的日志确认节点成功运行:\n```\ndocker logs orderer1.example.com\n...\ndocker logs peer0.org1.example.com\n```\n如果没有错误的话就可以进行第二部分了，如果出现错误则要回去检查是不是哪里漏掉了。\n\n### 1.8 简单测试\n先进行第一部分的测试，看一下创建通道，加入通道是否成功:\n```\n#进入CLI容器\ndocker exec -it cli bash\n#配置环境变量\nexport CHANNEL_NAME=mychannel\nexport ORDERER_CA=${PWD}/crypto/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport PEER0_ORG1_CA=${PWD}/crypto/peerOrganization/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\nexport CORE_PEER_LOCALMSPID=\"Org1MSP\"\nexport CORE_PEER_TLS_ROOTCERT_FILE=$PEER0_ORG1_CA\nexport CORE_PEER_MSPCONFIGPATH=${PWD}/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n```\n创建通道:\n```\npeer channel create -o orderer1.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/mychannel.tx --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA --outputBlock ./channel-artifacts/${CHANNEL_NAME}.block\n```\n加入通道:\n```\npeer channel join -b ./channel-artifacts/$CHANNEL_NAME.block\n```\n如果一切顺利的话，网络就成功搭建起来了，至于链码就不再测试了。\n直接到第二部分，动态添加一个Orderer节点。\n## 2 动态添加Raft节点\n主要步骤如下：\n\n1. 为该节点生成证书文件\n2. 获取当前网络的配置文件\n3. 将证书文件添加到配置文件中\n4. 更新配置文件\n5. 启动新的Orderer节点\n\n### 2.1 生成证书文件\n\n#### 2.1.1 注册该节点身份\n```\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer5 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n为该节点创建存储证书的文件夹:\n```\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer5.example.com\n```\n#### 2.1.2 获取该节点证书\n```\n#MSP\nfabric-ca-client enroll -u https://orderer5:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp --csr.hosts orderer5.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n#TLS\nfabric-ca-client enroll -u https://orderer5:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls --enrollment.profile tls --csr.hosts orderer5.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n复制节点分类配置文件:\n```\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/config.yaml\n```\n修改证书与秘钥文件名称:\n```\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/server.key\n```\n创建文件夹并拷贝TLS证书文件:\n```\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n```\n\n### 2.2 获取网络配置文件\n将节点添加进网络，首先需要将该节点添加到系统通道内，所以先获取系统通道的配置文件:\n进入`cli`容器:\n```\ndocker exec -it cli bash\n```\n配置环境变量，需要使用Orderer节点的身份信息:\n```\nexport CORE_PEER_LOCALMSPID=\"OrdererMSP\"\nexport ORDERER_CA=${PWD}/crypto/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport CORE_PEER_TLS_ROOTCERT_FILE=${PWD}/crypto/ordererOrganization/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/orderOrganization/example.com/users/Admin@example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n```\n获取系统通道配置文件:\n```\npeer channel fetch config channel-artifacts/config_block.pb -o orderer1.example.com:7050 -c byfn-sys-channel --tls --cafile $ORDERER_CA\n```\n解码该配置文件:\n```\nconfigtxlator proto_decode --input channel-artifacts/config_block.pb --type common.Block | jq .data.data[0].payload.data.config > channel-artifacts/config.json\n```\n\n### 2.3将证书文件添加到配置文件中\n退出容器，可以在`channel-artifacts`文件内找到`config.json`文件。将该文件复制一份并在`channel-artifacts`文件夹下保存为`update_config.json`,使用编辑工具打开，并搜索`.example.com`字段如下：\n字段一部分：\n```\n  {\n    \"client_tls_cert\": \"一连串的字符串\",\n    \"host\": \"orderer1.example.com\",\n    \"port\": 7050,\n    \"server_tls_cert\": \"一连串的字符串\"\n  }\n```\n以及匹配到的第二部分的字段:\n```\n      \"OrdererAddresses\": {\n        \"mod_policy\": \"/Channel/Orderer/Admins\",\n        \"value\": {\n          \"addresses\": [\n            \"orderer1.example.com:7050\",\n            \"orderer2.example.com:8050\",\n            \"orderer3.example.com:9050\",\n            \"orderer4.example.com:10050\"\n          ]\n        },\n        \"version\": \"0\"\n    }\n```\n在字段一部分，需要将我们生成的新的节点的证书添加上去，其中证书文件地址为:\n```\ncrypto-config/ordererOrganizations/example.com/orderers/orderer5.example.com/tls/server.crt\n```\n使用`BASE64`转码:\n```\ncat crypto-config/ordererOrganizations/example.com/orderers/orderer5.example.com/tls/server.crt | base64 > cert.txt\n```\n在`update_config.json`文件中字段一的部分下面按照字段一的格式添加相同的代码块，并进行修改：\n将`cert.txt`文件中的内容复制到字段一的`client_tls_cert,server_tls_cert`对应部分，并修改`host`对应部分为`orderer5.example.com`，`port`为`11050`.\n\n### 2.4更新配置文件\n接下来进入`cli`容器:\n```\ndocker exec -it cli bash\n```\n对原有的配置文件与更新的配置文件进行编码:\n```\nconfigtxlator proto_encode --input channel-artifacts/config.json --type common.Config > channel-artifacts/config.pb\nconfigtxlator proto_encode --input channel-artifacts/update_config.json --type common.Config > channel-artifacts/config_update.pb\n```\n计算出两个文件的差异:\n```\nconfigtxlator compute_update --channel_id byfn-sys-channel --original channel-artifacts/config.pb --updated channel-artifacts/config_update.pb > channel-artifacts/updated.pb\n```\n对该文件进行解码，并添加用于更新配置的头部信息:\n```\nconfigtxlator proto_decode --input channel-artifacts/updated.pb --type common.ConfigUpdate > channel-artifacts/updated.json\necho '{\"payload\":{\"header\":{\"channel_header\":{\"channel_id\":\"byfn-sys-channel\", \"type\":2}},\"data\":{\"config_update\":'$(cat channel-artifacts/updated.json)'}}}' | jq . > channel-artifacts/updated_envelope.json\n```\n编码为`Envelope`格式的文件:\n```\nconfigtxlator proto_encode --input channel-artifacts/updated_envelope.json --type common.Envelope > channel-artifacts/updated_envelope.pb\n```\n对该文件进行签名操作，用于更新配置:\n```\npeer channel signconfigtx -f channel-artifacts/updated_envelope.pb\n```\n提交更新通道配置交易:\n```\npeer channel update -f channel-artifacts/updated_envelope.pb -c byfn-sys-channel -o orderer1.example.com:7050 --tls true --cafile $ORDERER_CA\n```\n如果没有错误的话，新的Orderer节点证书已经成功添加到网络配置中，接下来可以启动新的节点了:\n\n### 2.5 启动新的Orderer节点\n写一下新的Orderer节点的Docker文件`docker-compose-addOrderer5.yaml`:\n```\n\nversion: '2'\n\nvolumes:\n  orderer5.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n  orderer5.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=11050\n    container_name: orderer5.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o5:/var/hyperledger/production/orderer\n    ports:\n      - 11050:11050\n```\n直接通过命令启动它:\n```\ndocker-compose -f docker-compose-addOrderer5.yaml up -d\n```\n可以查看新节点的日志确认新的节点已经成功加入了网络。\n\n到这里，本文成功把新的Orderer节点添加进了网络，但是只将该节点添加到了系统通道内，对于应用通道`mychannel`来说，新的节点并没有添加进来，将新的节点添加进`mychannel`通道和以上步骤相同，只需要将通道名称由系统通道修改为`mychannel`即可。本文便不再说明了。\n而动态删除节点的过程与添加相似，只不过是从配置文件中删除节点证书。\n\n","source":"_posts/blog/fabric/动态配置Raft节点.md","raw":"---\ntitle: Hyperledger Fabric动态配置Raft节点\ndate: 2019-12-31 12:48:21\ntags: fabric\ncategories: fabric应用\n---\n# Hyperledger Fabric动态配置Raft节点\n最近看官方文档发现新的共识算法etcdRaft允许动态添加或删除排序节点，所以也花了一天时间操作了以下，写篇文章把整个过程记录一下。\n初始网络本文设置了4个Orderer节点，1个Peer节点(用于更新配置文件以及测试用),然后动态添加第五个Orderer节点。\n本文分成两个部分:\n\n1. 第一部分是手动通过Fabric-CA生成每一个节点的证书文件\n2. 第二部分是更新Fabric网络配置添加新的Orderer节点。\n\n本文基于**Fabric v2.0.0-beta**版本。版本号只要高于1.4.1就行\n## 1 搭建定制化的Fabric网络\n前提条件是成功跑起来Fabric的示例网络，可以看这里->[Hyperledger Fabric环境搭建](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n首先在`$GOPATH`下(本文路径地址为`$GOPATH/src/github.com/hyperledger/fab`)建立如下几个文件夹用于之后的操作:\n```\n.  # 这里是根目录fab\n├── ca    # 用于生成CA证书的ca配置文件的文件夹\n│   ├── org1\n│   │   └── fabric-ca-server-config.yaml\n│   └── server\n│       └── fabric-ca-server-config.yaml\n├── channel-artifacts    #用于保存创世区块以及通道配置文件\n├── configtx.yaml      #配置文件：用于生成创世区块以及通道配置文件\n├── crypto-config     #存储生成的证书文件\n├── docker      # Fabric网络节点通过Docker启动，用于启动节点的Docker文件\n│   ├── base.yaml\n│   ├── docker-compose-addOrderer5.yaml\n│   ├── docker-compose-ca.yaml\n│   ├── docker-compose-orderers.yaml\n│   └── docker-compose-peer.yaml\n└── store    #存储区块等信息\n```\n**以下所有操作默认都在根目录文件夹内！**\n### 1.1CA配置文件\n直接在这里贴出来:`org1/fabric-ca-server-config.yaml`:\n```\n\nversion: 1.2.0\n\n# Server's listening port (default: 7054)\nport: 7054\n\n# Enables debug logging (default: false)\ndebug: false\n\ncrlsizelimit: 512000\n\ntls:\n  # Enable TLS (default: false)\n  enabled: true\n  certfile:\n  keyfile:\n  clientauth:\n    type: noclientcert\n    certfiles:\n\nca:\n  # Name of this CA\n  name: Org1CA\n  keyfile:\n  certfile:\n  chainfile:\n\ncrl:\n  expiry: 24h\n\nregistry:\n\n  maxenrollments: -1\n\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: \"\"\n       attrs:\n          hf.Registrar.Roles: \"*\"\n          hf.Registrar.DelegateRoles: \"*\"\n          hf.Revoker: true\n          hf.IntermediateCA: true\n          hf.GenCRL: true\n          hf.Registrar.Attributes: \"*\"\n          hf.AffiliationMgr: true\n\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\n  tls:\n      enabled: false\n      certfiles:\n      client:\n        certfile:\n        keyfile:\n\nldap:\n\n   enabled: false\n   url: ldap://<adminDN>:<adminPassword>@<host>:<port>/<base>\n   tls:\n      certfiles:\n      client:\n         certfile:\n         keyfile:\n   attribute:\n      names: ['uid','member']\n      converters:\n         - name:\n           value:\n      maps:\n         groups:\n            - name:\n              value:\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n\nsigning:\n    default:\n      usage:\n        - digital signature\n      expiry: 8760h\n    profiles:\n      ca:\n         usage:\n           - cert sign\n           - crl sign\n         expiry: 43800h\n         caconstraint:\n           isca: true\n           maxpathlen: 0\n      tls:\n         usage:\n            - signing\n            - key encipherment\n            - server auth\n            - client auth\n            - key agreement\n         expiry: 8760h\n\ncsr:\n   cn: ca.org1.example.com\n   names:\n      - C: US\n        ST: \"North Carolina\"\n        L: \"Durham\"\n        O: org1.example.com\n        OU:\n   hosts:\n     - localhost\n     - org1.example.com\n   ca:\n      expiry: 131400h\n      pathlength: 1\n\nbccsp:\n    default: SW\n    sw:\n        hash: SHA2\n        security: 256\n        filekeystore:\n            keystore: msp/keystore\n\ncacount:\n\ncafiles:\n\nintermediate:\n  parentserver:\n    url:\n    caname:\n\n  enrollment:\n    hosts:\n    profile:\n    label:\n\n  tls:\n    certfiles:\n    client:\n      certfile:\n      keyfile:\n      \n```\n以及`server/fabric-ca-server-config.yaml:`:\n```\n# Version of config file\nversion: 1.2.0\n# Server's listening port (default: 7054)\nport: 7054\n# Enables debug logging (default: false)\ndebug: false\n# Size limit of an acceptable CRL in bytes (default: 512000)\ncrlsizelimit: 512000\ntls:\n  # Enable TLS (default: false)\n  enabled: true\n  # TLS for the server's listening port\n  certfile:\n  keyfile:\n  clientauth:\n    type: noclientcert\n    certfiles:\n\nca:\n  # Name of this CA\n  name: OrdererCA\n  keyfile:\n  certfile:\n  chainfile:\n\ncrl:\n  expiry: 24h\n\nregistry:\n  maxenrollments: -1\n\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: \"\"\n       attrs:\n          hf.Registrar.Roles: \"*\"\n          hf.Registrar.DelegateRoles: \"*\"\n          hf.Revoker: true\n          hf.IntermediateCA: true\n          hf.GenCRL: true\n          hf.Registrar.Attributes: \"*\"\n          hf.AffiliationMgr: true\n\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\n  tls:\n      enabled: false\n      certfiles:\n      client:\n        certfile:\n        keyfile:\n\nldap:\n   enabled: false\n   url: ldap://<adminDN>:<adminPassword>@<host>:<port>/<base>\n   tls:\n      certfiles:\n      client:\n         certfile:\n         keyfile:\n   attribute:\n      names: ['uid','member']\n      converters:\n         - name:\n           value:\n      maps:\n         groups:\n            - name:\n              value:\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n\nsigning:\n    default:\n      usage:\n        - digital signature\n      expiry: 8760h\n    profiles:\n      ca:\n         usage:\n           - cert sign\n           - crl sign\n         expiry: 43800h\n         caconstraint:\n           isca: true\n           maxpathlen: 0\n      tls:\n         usage:\n            - signing\n            - key encipherment\n            - server auth\n            - client auth\n            - key agreement\n         expiry: 8760h\n\ncsr:\n   cn: ca.example.com\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: example.com\n        OU:\n   hosts:\n     - localhost\n     - example.com\n   ca:\n      expiry: 131400h\n      pathlength: 1\n\nbccsp:\n    default: SW\n    sw:\n        hash: SHA2\n        security: 256\n        filekeystore:\n            keystore: msp/keystore\n\ncacount:\ncafiles:\n\nintermediate:\n  parentserver:\n    url:\n    caname:\n\n  enrollment:\n    hosts:\n    profile:\n    label:\n\n  tls:\n    certfiles:\n    client:\n      certfile:\n      keyfile:\n```\n`docker-compose-ca.yaml`文件:\n```\nversion: '2'\n\nservices:\n  ca:\n    image: hyperledger/fabric-ca:1.4.4\n    environment:\n      - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server\n      - FABRIC_CA_SERVER_CA_NAME=ca-orderer\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_PORT=9054\n    ports:\n      - \"9054:9054\"\n    command: sh -c 'fabric-ca-server start -b admin:adminpw -d'\n    volumes:\n      - ../ca/server:/etc/hyperledger/fabric-ca-server\n    container_name: ca_orderer\n\n  ca0:\n    image: hyperledger/fabric-ca:1.4.4\n    environment:\n      - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server\n      - FABRIC_CA_SERVER_CA_NAME=ca-org1\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_PORT=7054\n    ports:\n      - \"7054:7054\"\n    command: sh -c 'fabric-ca-server start -b admin:adminpw -d'\n    volumes:\n      - ../ca/org1:/etc/hyperledger/fabric-ca-server\n    container_name: ca_org1\n```\n将以上三个文件保存到指定的路径，然后使用以下命令启动CA服务器：\n```\ndocker-compose -f docker/docker-compose-ca.yaml up -d\n```\n服务器会自动读取上面的两个配置文件，并初始化CA服务器。\n当然，服务器配置文件将自动生成在`ca/server/`子文件夹内，其中最主要使用到的是`tls-cert.pem`文件。\n\n### 1.2 注册Orderer节点\n首先配置环境变量并登陆管理员账号:\n```\n#创建存储Order节点证书的子文件夹。\nmkdir -p crypto-config/orderOrganization/example.com\nexport FABRIC_CA_CLIENT_HOME=${PWD}/crypto-config/orderOrganization/example.com\nfabric-ca-client enroll -u https://admin:adminpw@localhost:9054 --caname ca-orderer --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n生成节点类型分类配置文件(不知道这个文件应该称作什么，暂且使用这个名字称呼好了):\n```\n  echo 'NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: orderer' > ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml\n```\n之后注册网络中初始的4个Orderer节点:\n```\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer1 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer2 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer3 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer4 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n注册`Admin`节点:\n```\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name ordererAdmin --id.secret ordererAdminpw --id.type admin --id.attrs '\"hf.Registrar.Roles=admin\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n\n### 1.3 获取Orderer证书文件\n为刚刚创建的几个用户创建各自的文件夹用于存储证书文件:\n```\nmkdir -p crypto-config/orderOrganization/example.com/orderers\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer1.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer2.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer3.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer4.example.com\n```\n接下来获取每一个Orderer节点的`MSP`证书文件:\n```\nfabric-ca-client enroll -u https://orderer1:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp --csr.hosts orderer1.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer2:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp --csr.hosts orderer2.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer3:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp --csr.hosts orderer3.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer4:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp --csr.hosts orderer4.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n还有每一个节点的`TLS`证书:\n```\nfabric-ca-client enroll -u https://orderer1:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls --enrollment.profile tls --csr.hosts orderer1.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer2:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls --enrollment.profile tls --csr.hosts orderer2.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer3:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls --enrollment.profile tls --csr.hosts orderer3.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer4:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls --enrollment.profile tls --csr.hosts orderer4.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n将之前生成的节点类型分类配置文件拷贝到每一个节点的`MSP`文件夹:\n```\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/config.yaml\n```\n然后为每一个节点的`TLS`证书以及秘钥文件修改名字，方便之后的使用:\n```\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.key\n```\n然后在`MSP`文件夹内创建`tlscacerts`文件夹，并将`TLS`文件拷贝过去:\n```\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/tlscacerts\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n```\n复制TLS根证书:\n```\nmkdir -p ${PWD}/crypto-config/orderOrganization/example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n```\n最后是`Admin`节点的证书文件:\n```\n#首先也是创建文件夹\nmkdir -p crypto-config/orderOrganization/example.com/users\nmkdir -p crypto-config/orderOrganization/example.com/users/Admin@example.com\n#获取证书文件\nfabric-ca-client enroll -u https://ordererAdmin:ordererAdminpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/users/Admin@example.com/msp --tls.certfiles ${PWD}/ca/server/tls-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/users/Admin@example.com/msp/config.yaml\n```\n到这里Orderer节点证书已经生成完毕(可以根据实际需要修改Orderer节点数量，最少不能低于3个)，接下来是网络中唯一的`peer`节点的配置文件生成:\n\n### 1.4 注册Peer节点\n和上面步骤相同，首先创建子文件夹用于存储证书文件:\n```\nmkdir -p crypto-config/peerOrganizations/org1.example.com/\n```\n配置环境变量并登陆管理员身份:\n```\nexport FABRIC_CA_CLIENT_HOME=${PWD}/crypto-config/peerOrganizations/org1.example.com/\nfabric-ca-client enroll -u https://admin:adminpw@localhost:7054 --caname ca-org1 --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n```\n生成节点类型分类配置文件:\n```\necho 'NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: orderer' > ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml\n```\n虽然网络中只有一个peer节点，但是我们需要注册三个用户:`peer0,user1,org1admin`，其中第一个是必需的，第二个是用于测试的，第三个为`Admin`用户，安装和实例化链码需要`Admin`用户的证书:\n```\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name peer0 --id.secret peer0pw --id.type peer --id.attrs '\"hf.Registrar.Roles=peer\"' --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name user1 --id.secret user1pw --id.type client --id.attrs '\"hf.Registrar.Roles=client\"' --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name org1admin --id.secret org1adminpw --id.type admin --id.attrs '\"hf.Registrar.Roles=admin\"' --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n```\n\n### 1.5 获取Peer节点证书文件\n节点注册完毕，获取他们的证书文件:\n创建子文件夹:\n```\nmkdir -p crypto-config/peerOrganizations/org1.example.com/peers\nmkdir -p crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.co\n```\n获取证书文件:\n```\n#MSP文件\nfabric-ca-client enroll -u https://peer0:peer0pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp --csr.hosts peer0.org1.example.com --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n#TLS证书\nfabric-ca-client enroll -u https://peer0:peer0pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls --enrollment.profile tls --csr.hosts peer0.org1.example.com --csr.hosts localhost --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n```\n拷贝节点分类配置文件:\n```\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/config.yaml\n```\n修改证书以及秘钥文件，方便之后使用:\n```\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/signcerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/keystore/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n```\n将TLS相关证书复制一份:\n```\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/tlscacerts/ca.crt\n\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/tlsca\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem\n\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/ca\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/cacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/ca/ca.org1.example.com-cert.pem\n```\n获取`user`与`Admin`用户证书文件:\n```\n#创建子文件夹\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users/User1@org1.example.com\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com\n#获取证书文件\nfabric-ca-client enroll -u https://user1:user1pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/User1@org1.example.com/msp --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client enroll -u https://org1admin:org1adminpw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp/config.yaml\n```\n\n### 1.6 启动网络之前的准备\n到这里我们已经生成了所有需要的证书文件，接下来是生成用于启动网络的创世区块,生成创世区块需要一个文件`configtx.yaml`,直接复制过来:\n```\n\nOrganizations:\n    - &OrdererOrg\n        Name: OrdererOrg\n        ID: OrdererMSP\n        MSPDir: ./crypto-config/orderOrganization/example.com/msp   #这里路径需要对应！！！\n        Policies:\n            Readers:\n                Type: Signature\n                Rule: \"OR('OrdererMSP.member')\"\n            Writers:\n                Type: Signature\n                Rule: \"OR('OrdererMSP.member')\"\n            Admins:\n                Type: Signature\n                Rule: \"OR('OrdererMSP.admin')\"\n\n    - &Org1  #如果需要更多组织节点，可以按照该模板在下面添加\n        Name: Org1MSP\n        ID: Org1MSP\n        MSPDir: ./crypto-config/peerOrganizations/org1.example.com/msp  #这里路径需要对应！！！\n        Policies:\n            Readers:\n                Type: Signature\n                Rule: \"OR('Org1MSP.admin', 'Org1MSP.peer', 'Org1MSP.client')\"\n            Writers:\n                Type: Signature\n                Rule: \"OR('Org1MSP.admin', 'Org1MSP.client')\"\n            Admins:\n                Type: Signature\n                Rule: \"OR('Org1MSP.admin')\"\n            Endorsement:\n                Type: Signature\n                Rule: \"OR('Org1MSP.peer')\"\n        AnchorPeers:\n              Port: 7051\n\nCapabilities:\n    Channel: &ChannelCapabilities\n        V2_0: true\n\n    Orderer: &OrdererCapabilities\n        V2_0: true\n\n    Application: &ApplicationCapabilities\n        V2_0: true\n\nApplication: &ApplicationDefaults\n    Organizations:\n    Policies:\n        Readers:\n            Type: ImplicitMeta\n            Rule: \"ANY Readers\"\n        Writers:\n            Type: ImplicitMeta\n            Rule: \"ANY Writers\"\n        Admins:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Admins\"\n        LifecycleEndorsement:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Endorsement\"\n        Endorsement:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Endorsement\"\n    Capabilities:\n        <<: *ApplicationCapabilities\n\nOrderer: &OrdererDefaults\n    OrdererType: etcdraft\n    \n    Addresses:\n        - orderer1.example.com:7050\n    BatchTimeout: 2s\n    BatchSize:\n        MaxMessageCount: 10\n        AbsoluteMaxBytes: 99 MB\n        PreferredMaxBytes: 512 KB\n    Organizations:\n    Policies:\n        Readers:\n            Type: ImplicitMeta\n            Rule: \"ANY Readers\"\n        Writers:\n            Type: ImplicitMeta\n            Rule: \"ANY Writers\"\n        Admins:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Admins\"\n        BlockValidation:\n            Type: ImplicitMeta\n            Rule: \"ANY Writers\"\nChannel: &ChannelDefaults\n    Policies:\n        # Who may invoke the 'Deliver' API\n        Readers:\n            Type: ImplicitMeta\n            Rule: \"ANY Readers\"\n        # Who may invoke the 'Broadcast' API\n        Writers:\n            Type: ImplicitMeta\n            Rule: \"ANY Writers\"\n        # By default, who may modify elements at this config level\n        Admins:\n            Type: ImplicitMeta\n            Rule: \"MAJORITY Admins\"\n    Capabilities:\n        <<: *ChannelCapabilities\n\nProfiles:\n    \n    TwoOrgsChannel:   #用于生成通道配置文件\n        Consortium: SampleConsortium\n        <<: *ChannelDefaults\n        Application:\n            <<: *ApplicationDefaults\n            Organizations:\n                - *Org1\n            Capabilities:\n                <<: *ApplicationCapabilities\n\n    SampleMultiNodeEtcdRaft:   #用于生成系统通道创世区块\n        <<: *ChannelDefaults\n        Capabilities:\n            <<: *ChannelCapabilities\n        Orderer:\n            <<: *OrdererDefaults\n            OrdererType: etcdraft   #指定使用etcdraft共识算法\n            EtcdRaft:\n                Consenters:\n                - Host: orderer1.example.com\n                  Port: 7050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n                - Host: orderer2.example.com\n                  Port: 8050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\n                - Host: orderer3.example.com\n                  Port: 9050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\n                - Host: orderer4.example.com\n                  Port: 10050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\n#                    - Host: orderer5.example.com\n#                      Port: 11050\n#                      ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n#                      ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n            Addresses:\n                - orderer1.example.com:7050\n                - orderer2.example.com:8050\n                - orderer3.example.com:9050\n                - orderer4.example.com:10050\n#                - orderer5.example.com:11050\n         \n            Organizations:\n            - *OrdererOrg\n            Capabilities:\n                <<: *OrdererCapabilities\n        Application:\n            <<: *ApplicationDefaults\n            Organizations:\n            - <<: *OrdererOrg\n        Consortiums:\n            SampleConsortium:\n                Organizations:\n                - *Org1\n```\n将该文件保存到指定位置，接下来生成创世区块:\n```\nexport FABRIC_CFG_PATH=$PWD\nconfigtxgen -profile SampleMultiNodeEtcdRaft -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n# 生成通道配置文件\nexport CHANNEL_NAME=mychannel\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/mychannel.tx -channelID $CHANNEL_NAME\n```\n\n### 1.7 启动网络\n首先写包含所有节点的Docker文件,这里直接贴出来:\n`base.yaml`:\n```\nversion: '2'\n\nservices:\n  orderer-base:\n    image: hyperledger/fabric-orderer:2.0.0-beta\n    environment:\n      - FABRIC_LOGGING_SPEC=INFO\n#      - FABRIC_LOGGING_SPEC=DEBUG\n      - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0\n      - ORDERER_GENERAL_BOOTSTRAPMETHOD=file\n      - ORDERER_GENERAL_BOOTSTRAPFILE=/var/hyperledger/orderer/orderer.genesis.block\n      - ORDERER_GENERAL_LOCALMSPID=OrdererMSP\n      - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp\n      # enabled TLS\n      - ORDERER_GENERAL_TLS_ENABLED=true\n      - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key\n      - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt\n      - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt]\n      - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt\n      - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key\n      - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt]\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric\n    command: orderer\n```\n然后是Orderer节点的Docker文件`docker-compose-orderers.yaml`:\n```\n# Copyright IBM Corp. All Rights Reserved.\n#\n# SPDX-License-Identifier: Apache-2.0\n#\n\nversion: '2'\n\nvolumes:\n  orderer1.example.com:\n  orderer2.example.com:\n  orderer3.example.com:\n  orderer4.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n  \n  orderer1.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=7050\n    container_name: orderer1.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o1:/var/hyperledger/production/orderer\n    ports:\n      - 7050:7050\n\n\n\n  orderer2.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=8050\n    container_name: orderer2.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o2:/var/hyperledger/production/orderer\n    ports:\n      - 8050:8050\n  \n  orderer3.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=9050\n    container_name: orderer3.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o3:/var/hyperledger/production/orderer\n    ports:\n      - 9050:9050\n  \n  orderer4.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=10050\n    container_name: orderer4.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o4:/var/hyperledger/production/orderer\n    ports:\n      - 10050:10050\n```\n最后一个是peer节点的Docker文件`docker-compose-peer.yaml`：\n```\nversion: '2'\n\nvolumes:\n  peer0.org1.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    image: hyperledger/fabric-peer:2.0.0-beta\n    environment:\n      #Generic peer variables\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      # the following setting starts chaincode containers on the same\n      # bridge network as the peers\n      # https://docs.docker.com/compose/networking/\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=${COMPOSE_PROJECT_NAME}_byfn\n      - FABRIC_LOGGING_SPEC=INFO\n      #- FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_PROFILE_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt\n      # Peer specific variabes\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:7051\n      - CORE_PEER_CHAINCODEADDRESS=peer0.org1.example.com:7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org1.example.com:7051\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_LEDGER_STATE_STATEDATABASE=CouchDB\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984\n      # The CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME and CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD\n      # provide the credentials for ledger to connect to CouchDB.  The username and password must\n      # match the username and password set for the associated CouchDB.\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME=\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD=\n    volumes:\n      - /var/run/:/host/var/run/\n      - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp:/etc/hyperledger/fabric/msp\n      - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls:/etc/hyperledger/fabric/tls\n      - ../store/p1:/var/hyperledger/production\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: peer node start\n    depends_on:\n      - couchdb0\n    ports:\n      - 7051:7051\n    networks:\n      - byfn\n\n  couchdb0:\n    container_name: couchdb0\n    image: couchdb:2.3\n    # Populate the COUCHDB_USER and COUCHDB_PASSWORD to set an admin user and password\n    # for CouchDB.  This will prevent CouchDB from operating in an \"Admin Party\" mode.\n    environment:\n      - COUCHDB_USER=\n      - COUCHDB_PASSWORD=\n    # Comment/Uncomment the port mapping if you want to hide/expose the CouchDB service,\n    # for example map it to utilize Fauxton User Interface in dev environments.\n    ports:\n      - \"5984:5984\"\n    networks:\n      - byfn\n\n  cli:\n    container_name: cli\n    image: hyperledger/fabric-tools:2.0.0-beta\n    tty: true\n    stdin_open: true\n    environment:\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n#      - FABRIC_LOGGING_SPEC=DEBUG\n      - FABRIC_LOGGING_SPEC=INFO\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\n      - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: /bin/bash\n    volumes:\n      - /var/run/:/host/var/run/\n      - ./../../chaincode/:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - ../crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/\n      - ../channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts\n    depends_on:\n      - peer0.org1.example.com\n    networks:\n      - byfn\n```\n将以上文件保存到指定位置后，使用以下命令直接启动:\n```\ndocker-compose -f docker/docker-compose-orderers.yaml -f docker/docker-compose-peer.yaml up -d\n```\n启动完成后可以查看每个节点的日志确认节点成功运行:\n```\ndocker logs orderer1.example.com\n...\ndocker logs peer0.org1.example.com\n```\n如果没有错误的话就可以进行第二部分了，如果出现错误则要回去检查是不是哪里漏掉了。\n\n### 1.8 简单测试\n先进行第一部分的测试，看一下创建通道，加入通道是否成功:\n```\n#进入CLI容器\ndocker exec -it cli bash\n#配置环境变量\nexport CHANNEL_NAME=mychannel\nexport ORDERER_CA=${PWD}/crypto/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport PEER0_ORG1_CA=${PWD}/crypto/peerOrganization/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\nexport CORE_PEER_LOCALMSPID=\"Org1MSP\"\nexport CORE_PEER_TLS_ROOTCERT_FILE=$PEER0_ORG1_CA\nexport CORE_PEER_MSPCONFIGPATH=${PWD}/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n```\n创建通道:\n```\npeer channel create -o orderer1.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/mychannel.tx --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA --outputBlock ./channel-artifacts/${CHANNEL_NAME}.block\n```\n加入通道:\n```\npeer channel join -b ./channel-artifacts/$CHANNEL_NAME.block\n```\n如果一切顺利的话，网络就成功搭建起来了，至于链码就不再测试了。\n直接到第二部分，动态添加一个Orderer节点。\n## 2 动态添加Raft节点\n主要步骤如下：\n\n1. 为该节点生成证书文件\n2. 获取当前网络的配置文件\n3. 将证书文件添加到配置文件中\n4. 更新配置文件\n5. 启动新的Orderer节点\n\n### 2.1 生成证书文件\n\n#### 2.1.1 注册该节点身份\n```\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer5 --id.secret ordererpw --id.type orderer --id.attrs '\"hf.Registrar.Roles=orderer\"' --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n为该节点创建存储证书的文件夹:\n```\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer5.example.com\n```\n#### 2.1.2 获取该节点证书\n```\n#MSP\nfabric-ca-client enroll -u https://orderer5:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp --csr.hosts orderer5.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n#TLS\nfabric-ca-client enroll -u https://orderer5:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls --enrollment.profile tls --csr.hosts orderer5.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n```\n复制节点分类配置文件:\n```\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/config.yaml\n```\n修改证书与秘钥文件名称:\n```\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/server.key\n```\n创建文件夹并拷贝TLS证书文件:\n```\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n```\n\n### 2.2 获取网络配置文件\n将节点添加进网络，首先需要将该节点添加到系统通道内，所以先获取系统通道的配置文件:\n进入`cli`容器:\n```\ndocker exec -it cli bash\n```\n配置环境变量，需要使用Orderer节点的身份信息:\n```\nexport CORE_PEER_LOCALMSPID=\"OrdererMSP\"\nexport ORDERER_CA=${PWD}/crypto/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport CORE_PEER_TLS_ROOTCERT_FILE=${PWD}/crypto/ordererOrganization/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/orderOrganization/example.com/users/Admin@example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n```\n获取系统通道配置文件:\n```\npeer channel fetch config channel-artifacts/config_block.pb -o orderer1.example.com:7050 -c byfn-sys-channel --tls --cafile $ORDERER_CA\n```\n解码该配置文件:\n```\nconfigtxlator proto_decode --input channel-artifacts/config_block.pb --type common.Block | jq .data.data[0].payload.data.config > channel-artifacts/config.json\n```\n\n### 2.3将证书文件添加到配置文件中\n退出容器，可以在`channel-artifacts`文件内找到`config.json`文件。将该文件复制一份并在`channel-artifacts`文件夹下保存为`update_config.json`,使用编辑工具打开，并搜索`.example.com`字段如下：\n字段一部分：\n```\n  {\n    \"client_tls_cert\": \"一连串的字符串\",\n    \"host\": \"orderer1.example.com\",\n    \"port\": 7050,\n    \"server_tls_cert\": \"一连串的字符串\"\n  }\n```\n以及匹配到的第二部分的字段:\n```\n      \"OrdererAddresses\": {\n        \"mod_policy\": \"/Channel/Orderer/Admins\",\n        \"value\": {\n          \"addresses\": [\n            \"orderer1.example.com:7050\",\n            \"orderer2.example.com:8050\",\n            \"orderer3.example.com:9050\",\n            \"orderer4.example.com:10050\"\n          ]\n        },\n        \"version\": \"0\"\n    }\n```\n在字段一部分，需要将我们生成的新的节点的证书添加上去，其中证书文件地址为:\n```\ncrypto-config/ordererOrganizations/example.com/orderers/orderer5.example.com/tls/server.crt\n```\n使用`BASE64`转码:\n```\ncat crypto-config/ordererOrganizations/example.com/orderers/orderer5.example.com/tls/server.crt | base64 > cert.txt\n```\n在`update_config.json`文件中字段一的部分下面按照字段一的格式添加相同的代码块，并进行修改：\n将`cert.txt`文件中的内容复制到字段一的`client_tls_cert,server_tls_cert`对应部分，并修改`host`对应部分为`orderer5.example.com`，`port`为`11050`.\n\n### 2.4更新配置文件\n接下来进入`cli`容器:\n```\ndocker exec -it cli bash\n```\n对原有的配置文件与更新的配置文件进行编码:\n```\nconfigtxlator proto_encode --input channel-artifacts/config.json --type common.Config > channel-artifacts/config.pb\nconfigtxlator proto_encode --input channel-artifacts/update_config.json --type common.Config > channel-artifacts/config_update.pb\n```\n计算出两个文件的差异:\n```\nconfigtxlator compute_update --channel_id byfn-sys-channel --original channel-artifacts/config.pb --updated channel-artifacts/config_update.pb > channel-artifacts/updated.pb\n```\n对该文件进行解码，并添加用于更新配置的头部信息:\n```\nconfigtxlator proto_decode --input channel-artifacts/updated.pb --type common.ConfigUpdate > channel-artifacts/updated.json\necho '{\"payload\":{\"header\":{\"channel_header\":{\"channel_id\":\"byfn-sys-channel\", \"type\":2}},\"data\":{\"config_update\":'$(cat channel-artifacts/updated.json)'}}}' | jq . > channel-artifacts/updated_envelope.json\n```\n编码为`Envelope`格式的文件:\n```\nconfigtxlator proto_encode --input channel-artifacts/updated_envelope.json --type common.Envelope > channel-artifacts/updated_envelope.pb\n```\n对该文件进行签名操作，用于更新配置:\n```\npeer channel signconfigtx -f channel-artifacts/updated_envelope.pb\n```\n提交更新通道配置交易:\n```\npeer channel update -f channel-artifacts/updated_envelope.pb -c byfn-sys-channel -o orderer1.example.com:7050 --tls true --cafile $ORDERER_CA\n```\n如果没有错误的话，新的Orderer节点证书已经成功添加到网络配置中，接下来可以启动新的节点了:\n\n### 2.5 启动新的Orderer节点\n写一下新的Orderer节点的Docker文件`docker-compose-addOrderer5.yaml`:\n```\n\nversion: '2'\n\nvolumes:\n  orderer5.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n  orderer5.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=11050\n    container_name: orderer5.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o5:/var/hyperledger/production/orderer\n    ports:\n      - 11050:11050\n```\n直接通过命令启动它:\n```\ndocker-compose -f docker-compose-addOrderer5.yaml up -d\n```\n可以查看新节点的日志确认新的节点已经成功加入了网络。\n\n到这里，本文成功把新的Orderer节点添加进了网络，但是只将该节点添加到了系统通道内，对于应用通道`mychannel`来说，新的节点并没有添加进来，将新的节点添加进`mychannel`通道和以上步骤相同，只需要将通道名称由系统通道修改为`mychannel`即可。本文便不再说明了。\n而动态删除节点的过程与添加相似，只不过是从配置文件中删除节点证书。\n\n","slug":"blog/fabric/动态配置Raft节点","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyjp003uk0vqbl1t19kj","content":"<h1 id=\"Hyperledger-Fabric动态配置Raft节点\"><a href=\"#Hyperledger-Fabric动态配置Raft节点\" class=\"headerlink\" title=\"Hyperledger Fabric动态配置Raft节点\"></a>Hyperledger Fabric动态配置Raft节点</h1><p>最近看官方文档发现新的共识算法etcdRaft允许动态添加或删除排序节点，所以也花了一天时间操作了以下，写篇文章把整个过程记录一下。<br>初始网络本文设置了4个Orderer节点，1个Peer节点(用于更新配置文件以及测试用),然后动态添加第五个Orderer节点。<br>本文分成两个部分:</p>\n<ol>\n<li>第一部分是手动通过Fabric-CA生成每一个节点的证书文件</li>\n<li>第二部分是更新Fabric网络配置添加新的Orderer节点。</li>\n</ol>\n<p>本文基于<strong>Fabric v2.0.0-beta</strong>版本。版本号只要高于1.4.1就行</p>\n<h2 id=\"1-搭建定制化的Fabric网络\"><a href=\"#1-搭建定制化的Fabric网络\" class=\"headerlink\" title=\"1 搭建定制化的Fabric网络\"></a>1 搭建定制化的Fabric网络</h2><p>前提条件是成功跑起来Fabric的示例网络，可以看这里-&gt;<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric环境搭建</a></p>\n<p>首先在<code>$GOPATH</code>下(本文路径地址为<code>$GOPATH/src/github.com/hyperledger/fab</code>)建立如下几个文件夹用于之后的操作:</p>\n<pre><code>.  # 这里是根目录fab\n├── ca    # 用于生成CA证书的ca配置文件的文件夹\n│   ├── org1\n│   │   └── fabric-ca-server-config.yaml\n│   └── server\n│       └── fabric-ca-server-config.yaml\n├── channel-artifacts    #用于保存创世区块以及通道配置文件\n├── configtx.yaml      #配置文件：用于生成创世区块以及通道配置文件\n├── crypto-config     #存储生成的证书文件\n├── docker      # Fabric网络节点通过Docker启动，用于启动节点的Docker文件\n│   ├── base.yaml\n│   ├── docker-compose-addOrderer5.yaml\n│   ├── docker-compose-ca.yaml\n│   ├── docker-compose-orderers.yaml\n│   └── docker-compose-peer.yaml\n└── store    #存储区块等信息</code></pre><p><strong>以下所有操作默认都在根目录文件夹内！</strong></p>\n<h3 id=\"1-1CA配置文件\"><a href=\"#1-1CA配置文件\" class=\"headerlink\" title=\"1.1CA配置文件\"></a>1.1CA配置文件</h3><p>直接在这里贴出来:<code>org1/fabric-ca-server-config.yaml</code>:</p>\n<pre><code>\nversion: 1.2.0\n\n# Server&#39;s listening port (default: 7054)\nport: 7054\n\n# Enables debug logging (default: false)\ndebug: false\n\ncrlsizelimit: 512000\n\ntls:\n  # Enable TLS (default: false)\n  enabled: true\n  certfile:\n  keyfile:\n  clientauth:\n    type: noclientcert\n    certfiles:\n\nca:\n  # Name of this CA\n  name: Org1CA\n  keyfile:\n  certfile:\n  chainfile:\n\ncrl:\n  expiry: 24h\n\nregistry:\n\n  maxenrollments: -1\n\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: &quot;&quot;\n       attrs:\n          hf.Registrar.Roles: &quot;*&quot;\n          hf.Registrar.DelegateRoles: &quot;*&quot;\n          hf.Revoker: true\n          hf.IntermediateCA: true\n          hf.GenCRL: true\n          hf.Registrar.Attributes: &quot;*&quot;\n          hf.AffiliationMgr: true\n\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\n  tls:\n      enabled: false\n      certfiles:\n      client:\n        certfile:\n        keyfile:\n\nldap:\n\n   enabled: false\n   url: ldap://&lt;adminDN&gt;:&lt;adminPassword&gt;@&lt;host&gt;:&lt;port&gt;/&lt;base&gt;\n   tls:\n      certfiles:\n      client:\n         certfile:\n         keyfile:\n   attribute:\n      names: [&#39;uid&#39;,&#39;member&#39;]\n      converters:\n         - name:\n           value:\n      maps:\n         groups:\n            - name:\n              value:\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n\nsigning:\n    default:\n      usage:\n        - digital signature\n      expiry: 8760h\n    profiles:\n      ca:\n         usage:\n           - cert sign\n           - crl sign\n         expiry: 43800h\n         caconstraint:\n           isca: true\n           maxpathlen: 0\n      tls:\n         usage:\n            - signing\n            - key encipherment\n            - server auth\n            - client auth\n            - key agreement\n         expiry: 8760h\n\ncsr:\n   cn: ca.org1.example.com\n   names:\n      - C: US\n        ST: &quot;North Carolina&quot;\n        L: &quot;Durham&quot;\n        O: org1.example.com\n        OU:\n   hosts:\n     - localhost\n     - org1.example.com\n   ca:\n      expiry: 131400h\n      pathlength: 1\n\nbccsp:\n    default: SW\n    sw:\n        hash: SHA2\n        security: 256\n        filekeystore:\n            keystore: msp/keystore\n\ncacount:\n\ncafiles:\n\nintermediate:\n  parentserver:\n    url:\n    caname:\n\n  enrollment:\n    hosts:\n    profile:\n    label:\n\n  tls:\n    certfiles:\n    client:\n      certfile:\n      keyfile:\n</code></pre><p>以及<code>server/fabric-ca-server-config.yaml:</code>:</p>\n<pre><code># Version of config file\nversion: 1.2.0\n# Server&#39;s listening port (default: 7054)\nport: 7054\n# Enables debug logging (default: false)\ndebug: false\n# Size limit of an acceptable CRL in bytes (default: 512000)\ncrlsizelimit: 512000\ntls:\n  # Enable TLS (default: false)\n  enabled: true\n  # TLS for the server&#39;s listening port\n  certfile:\n  keyfile:\n  clientauth:\n    type: noclientcert\n    certfiles:\n\nca:\n  # Name of this CA\n  name: OrdererCA\n  keyfile:\n  certfile:\n  chainfile:\n\ncrl:\n  expiry: 24h\n\nregistry:\n  maxenrollments: -1\n\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: &quot;&quot;\n       attrs:\n          hf.Registrar.Roles: &quot;*&quot;\n          hf.Registrar.DelegateRoles: &quot;*&quot;\n          hf.Revoker: true\n          hf.IntermediateCA: true\n          hf.GenCRL: true\n          hf.Registrar.Attributes: &quot;*&quot;\n          hf.AffiliationMgr: true\n\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\n  tls:\n      enabled: false\n      certfiles:\n      client:\n        certfile:\n        keyfile:\n\nldap:\n   enabled: false\n   url: ldap://&lt;adminDN&gt;:&lt;adminPassword&gt;@&lt;host&gt;:&lt;port&gt;/&lt;base&gt;\n   tls:\n      certfiles:\n      client:\n         certfile:\n         keyfile:\n   attribute:\n      names: [&#39;uid&#39;,&#39;member&#39;]\n      converters:\n         - name:\n           value:\n      maps:\n         groups:\n            - name:\n              value:\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n\nsigning:\n    default:\n      usage:\n        - digital signature\n      expiry: 8760h\n    profiles:\n      ca:\n         usage:\n           - cert sign\n           - crl sign\n         expiry: 43800h\n         caconstraint:\n           isca: true\n           maxpathlen: 0\n      tls:\n         usage:\n            - signing\n            - key encipherment\n            - server auth\n            - client auth\n            - key agreement\n         expiry: 8760h\n\ncsr:\n   cn: ca.example.com\n   names:\n      - C: US\n        ST: &quot;New York&quot;\n        L: &quot;New York&quot;\n        O: example.com\n        OU:\n   hosts:\n     - localhost\n     - example.com\n   ca:\n      expiry: 131400h\n      pathlength: 1\n\nbccsp:\n    default: SW\n    sw:\n        hash: SHA2\n        security: 256\n        filekeystore:\n            keystore: msp/keystore\n\ncacount:\ncafiles:\n\nintermediate:\n  parentserver:\n    url:\n    caname:\n\n  enrollment:\n    hosts:\n    profile:\n    label:\n\n  tls:\n    certfiles:\n    client:\n      certfile:\n      keyfile:</code></pre><p><code>docker-compose-ca.yaml</code>文件:</p>\n<pre><code>version: &#39;2&#39;\n\nservices:\n  ca:\n    image: hyperledger/fabric-ca:1.4.4\n    environment:\n      - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server\n      - FABRIC_CA_SERVER_CA_NAME=ca-orderer\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_PORT=9054\n    ports:\n      - &quot;9054:9054&quot;\n    command: sh -c &#39;fabric-ca-server start -b admin:adminpw -d&#39;\n    volumes:\n      - ../ca/server:/etc/hyperledger/fabric-ca-server\n    container_name: ca_orderer\n\n  ca0:\n    image: hyperledger/fabric-ca:1.4.4\n    environment:\n      - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server\n      - FABRIC_CA_SERVER_CA_NAME=ca-org1\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_PORT=7054\n    ports:\n      - &quot;7054:7054&quot;\n    command: sh -c &#39;fabric-ca-server start -b admin:adminpw -d&#39;\n    volumes:\n      - ../ca/org1:/etc/hyperledger/fabric-ca-server\n    container_name: ca_org1</code></pre><p>将以上三个文件保存到指定的路径，然后使用以下命令启动CA服务器：</p>\n<pre><code>docker-compose -f docker/docker-compose-ca.yaml up -d</code></pre><p>服务器会自动读取上面的两个配置文件，并初始化CA服务器。<br>当然，服务器配置文件将自动生成在<code>ca/server/</code>子文件夹内，其中最主要使用到的是<code>tls-cert.pem</code>文件。</p>\n<h3 id=\"1-2-注册Orderer节点\"><a href=\"#1-2-注册Orderer节点\" class=\"headerlink\" title=\"1.2 注册Orderer节点\"></a>1.2 注册Orderer节点</h3><p>首先配置环境变量并登陆管理员账号:</p>\n<pre><code>#创建存储Order节点证书的子文件夹。\nmkdir -p crypto-config/orderOrganization/example.com\nexport FABRIC_CA_CLIENT_HOME=${PWD}/crypto-config/orderOrganization/example.com\nfabric-ca-client enroll -u https://admin:adminpw@localhost:9054 --caname ca-orderer --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>生成节点类型分类配置文件(不知道这个文件应该称作什么，暂且使用这个名字称呼好了):</p>\n<pre><code>  echo &#39;NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: orderer&#39; &gt; ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml</code></pre><p>之后注册网络中初始的4个Orderer节点:</p>\n<pre><code>fabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer1 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer2 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer3 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer4 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>注册<code>Admin</code>节点:</p>\n<pre><code>fabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name ordererAdmin --id.secret ordererAdminpw --id.type admin --id.attrs &#39;&quot;hf.Registrar.Roles=admin&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><h3 id=\"1-3-获取Orderer证书文件\"><a href=\"#1-3-获取Orderer证书文件\" class=\"headerlink\" title=\"1.3 获取Orderer证书文件\"></a>1.3 获取Orderer证书文件</h3><p>为刚刚创建的几个用户创建各自的文件夹用于存储证书文件:</p>\n<pre><code>mkdir -p crypto-config/orderOrganization/example.com/orderers\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer1.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer2.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer3.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer4.example.com</code></pre><p>接下来获取每一个Orderer节点的<code>MSP</code>证书文件:</p>\n<pre><code>fabric-ca-client enroll -u https://orderer1:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp --csr.hosts orderer1.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer2:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp --csr.hosts orderer2.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer3:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp --csr.hosts orderer3.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer4:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp --csr.hosts orderer4.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>还有每一个节点的<code>TLS</code>证书:</p>\n<pre><code>fabric-ca-client enroll -u https://orderer1:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls --enrollment.profile tls --csr.hosts orderer1.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer2:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls --enrollment.profile tls --csr.hosts orderer2.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer3:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls --enrollment.profile tls --csr.hosts orderer3.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer4:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls --enrollment.profile tls --csr.hosts orderer4.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>将之前生成的节点类型分类配置文件拷贝到每一个节点的<code>MSP</code>文件夹:</p>\n<pre><code>cp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/config.yaml</code></pre><p>然后为每一个节点的<code>TLS</code>证书以及秘钥文件修改名字，方便之后的使用:</p>\n<pre><code>cp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.key</code></pre><p>然后在<code>MSP</code>文件夹内创建<code>tlscacerts</code>文件夹，并将<code>TLS</code>文件拷贝过去:</p>\n<pre><code>mkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/tlscacerts\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/tlscacerts/tlsca.example.com-cert.pem</code></pre><p>复制TLS根证书:</p>\n<pre><code>mkdir -p ${PWD}/crypto-config/orderOrganization/example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/msp/tlscacerts/tlsca.example.com-cert.pem</code></pre><p>最后是<code>Admin</code>节点的证书文件:</p>\n<pre><code>#首先也是创建文件夹\nmkdir -p crypto-config/orderOrganization/example.com/users\nmkdir -p crypto-config/orderOrganization/example.com/users/Admin@example.com\n#获取证书文件\nfabric-ca-client enroll -u https://ordererAdmin:ordererAdminpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/users/Admin@example.com/msp --tls.certfiles ${PWD}/ca/server/tls-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/users/Admin@example.com/msp/config.yaml</code></pre><p>到这里Orderer节点证书已经生成完毕(可以根据实际需要修改Orderer节点数量，最少不能低于3个)，接下来是网络中唯一的<code>peer</code>节点的配置文件生成:</p>\n<h3 id=\"1-4-注册Peer节点\"><a href=\"#1-4-注册Peer节点\" class=\"headerlink\" title=\"1.4 注册Peer节点\"></a>1.4 注册Peer节点</h3><p>和上面步骤相同，首先创建子文件夹用于存储证书文件:</p>\n<pre><code>mkdir -p crypto-config/peerOrganizations/org1.example.com/</code></pre><p>配置环境变量并登陆管理员身份:</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=${PWD}/crypto-config/peerOrganizations/org1.example.com/\nfabric-ca-client enroll -u https://admin:adminpw@localhost:7054 --caname ca-org1 --tls.certfiles ${PWD}/ca/org1/tls-cert.pem</code></pre><p>生成节点类型分类配置文件:</p>\n<pre><code>echo &#39;NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: orderer&#39; &gt; ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml</code></pre><p>虽然网络中只有一个peer节点，但是我们需要注册三个用户:<code>peer0,user1,org1admin</code>，其中第一个是必需的，第二个是用于测试的，第三个为<code>Admin</code>用户，安装和实例化链码需要<code>Admin</code>用户的证书:</p>\n<pre><code>fabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name peer0 --id.secret peer0pw --id.type peer --id.attrs &#39;&quot;hf.Registrar.Roles=peer&quot;&#39; --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name user1 --id.secret user1pw --id.type client --id.attrs &#39;&quot;hf.Registrar.Roles=client&quot;&#39; --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name org1admin --id.secret org1adminpw --id.type admin --id.attrs &#39;&quot;hf.Registrar.Roles=admin&quot;&#39; --tls.certfiles ${PWD}/ca/org1/tls-cert.pem</code></pre><h3 id=\"1-5-获取Peer节点证书文件\"><a href=\"#1-5-获取Peer节点证书文件\" class=\"headerlink\" title=\"1.5 获取Peer节点证书文件\"></a>1.5 获取Peer节点证书文件</h3><p>节点注册完毕，获取他们的证书文件:<br>创建子文件夹:</p>\n<pre><code>mkdir -p crypto-config/peerOrganizations/org1.example.com/peers\nmkdir -p crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.co</code></pre><p>获取证书文件:</p>\n<pre><code>#MSP文件\nfabric-ca-client enroll -u https://peer0:peer0pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp --csr.hosts peer0.org1.example.com --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n#TLS证书\nfabric-ca-client enroll -u https://peer0:peer0pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls --enrollment.profile tls --csr.hosts peer0.org1.example.com --csr.hosts localhost --tls.certfiles ${PWD}/ca/org1/tls-cert.pem</code></pre><p>拷贝节点分类配置文件:</p>\n<pre><code>cp ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/config.yaml</code></pre><p>修改证书以及秘钥文件，方便之后使用:</p>\n<pre><code>cp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/signcerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/keystore/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key</code></pre><p>将TLS相关证书复制一份:</p>\n<pre><code>mkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/tlscacerts/ca.crt\n\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/tlsca\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem\n\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/ca\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/cacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/ca/ca.org1.example.com-cert.pem</code></pre><p>获取<code>user</code>与<code>Admin</code>用户证书文件:</p>\n<pre><code>#创建子文件夹\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users/User1@org1.example.com\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com\n#获取证书文件\nfabric-ca-client enroll -u https://user1:user1pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/User1@org1.example.com/msp --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client enroll -u https://org1admin:org1adminpw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp/config.yaml</code></pre><h3 id=\"1-6-启动网络之前的准备\"><a href=\"#1-6-启动网络之前的准备\" class=\"headerlink\" title=\"1.6 启动网络之前的准备\"></a>1.6 启动网络之前的准备</h3><p>到这里我们已经生成了所有需要的证书文件，接下来是生成用于启动网络的创世区块,生成创世区块需要一个文件<code>configtx.yaml</code>,直接复制过来:</p>\n<pre><code>\nOrganizations:\n    - &amp;OrdererOrg\n        Name: OrdererOrg\n        ID: OrdererMSP\n        MSPDir: ./crypto-config/orderOrganization/example.com/msp   #这里路径需要对应！！！\n        Policies:\n            Readers:\n                Type: Signature\n                Rule: &quot;OR(&#39;OrdererMSP.member&#39;)&quot;\n            Writers:\n                Type: Signature\n                Rule: &quot;OR(&#39;OrdererMSP.member&#39;)&quot;\n            Admins:\n                Type: Signature\n                Rule: &quot;OR(&#39;OrdererMSP.admin&#39;)&quot;\n\n    - &amp;Org1  #如果需要更多组织节点，可以按照该模板在下面添加\n        Name: Org1MSP\n        ID: Org1MSP\n        MSPDir: ./crypto-config/peerOrganizations/org1.example.com/msp  #这里路径需要对应！！！\n        Policies:\n            Readers:\n                Type: Signature\n                Rule: &quot;OR(&#39;Org1MSP.admin&#39;, &#39;Org1MSP.peer&#39;, &#39;Org1MSP.client&#39;)&quot;\n            Writers:\n                Type: Signature\n                Rule: &quot;OR(&#39;Org1MSP.admin&#39;, &#39;Org1MSP.client&#39;)&quot;\n            Admins:\n                Type: Signature\n                Rule: &quot;OR(&#39;Org1MSP.admin&#39;)&quot;\n            Endorsement:\n                Type: Signature\n                Rule: &quot;OR(&#39;Org1MSP.peer&#39;)&quot;\n        AnchorPeers:\n              Port: 7051\n\nCapabilities:\n    Channel: &amp;ChannelCapabilities\n        V2_0: true\n\n    Orderer: &amp;OrdererCapabilities\n        V2_0: true\n\n    Application: &amp;ApplicationCapabilities\n        V2_0: true\n\nApplication: &amp;ApplicationDefaults\n    Organizations:\n    Policies:\n        Readers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Readers&quot;\n        Writers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Writers&quot;\n        Admins:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Admins&quot;\n        LifecycleEndorsement:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Endorsement&quot;\n        Endorsement:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Endorsement&quot;\n    Capabilities:\n        &lt;&lt;: *ApplicationCapabilities\n\nOrderer: &amp;OrdererDefaults\n    OrdererType: etcdraft\n\n    Addresses:\n        - orderer1.example.com:7050\n    BatchTimeout: 2s\n    BatchSize:\n        MaxMessageCount: 10\n        AbsoluteMaxBytes: 99 MB\n        PreferredMaxBytes: 512 KB\n    Organizations:\n    Policies:\n        Readers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Readers&quot;\n        Writers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Writers&quot;\n        Admins:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Admins&quot;\n        BlockValidation:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Writers&quot;\nChannel: &amp;ChannelDefaults\n    Policies:\n        # Who may invoke the &#39;Deliver&#39; API\n        Readers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Readers&quot;\n        # Who may invoke the &#39;Broadcast&#39; API\n        Writers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Writers&quot;\n        # By default, who may modify elements at this config level\n        Admins:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Admins&quot;\n    Capabilities:\n        &lt;&lt;: *ChannelCapabilities\n\nProfiles:\n\n    TwoOrgsChannel:   #用于生成通道配置文件\n        Consortium: SampleConsortium\n        &lt;&lt;: *ChannelDefaults\n        Application:\n            &lt;&lt;: *ApplicationDefaults\n            Organizations:\n                - *Org1\n            Capabilities:\n                &lt;&lt;: *ApplicationCapabilities\n\n    SampleMultiNodeEtcdRaft:   #用于生成系统通道创世区块\n        &lt;&lt;: *ChannelDefaults\n        Capabilities:\n            &lt;&lt;: *ChannelCapabilities\n        Orderer:\n            &lt;&lt;: *OrdererDefaults\n            OrdererType: etcdraft   #指定使用etcdraft共识算法\n            EtcdRaft:\n                Consenters:\n                - Host: orderer1.example.com\n                  Port: 7050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n                - Host: orderer2.example.com\n                  Port: 8050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\n                - Host: orderer3.example.com\n                  Port: 9050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\n                - Host: orderer4.example.com\n                  Port: 10050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\n#                    - Host: orderer5.example.com\n#                      Port: 11050\n#                      ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n#                      ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n            Addresses:\n                - orderer1.example.com:7050\n                - orderer2.example.com:8050\n                - orderer3.example.com:9050\n                - orderer4.example.com:10050\n#                - orderer5.example.com:11050\n\n            Organizations:\n            - *OrdererOrg\n            Capabilities:\n                &lt;&lt;: *OrdererCapabilities\n        Application:\n            &lt;&lt;: *ApplicationDefaults\n            Organizations:\n            - &lt;&lt;: *OrdererOrg\n        Consortiums:\n            SampleConsortium:\n                Organizations:\n                - *Org1</code></pre><p>将该文件保存到指定位置，接下来生成创世区块:</p>\n<pre><code>export FABRIC_CFG_PATH=$PWD\nconfigtxgen -profile SampleMultiNodeEtcdRaft -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n# 生成通道配置文件\nexport CHANNEL_NAME=mychannel\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/mychannel.tx -channelID $CHANNEL_NAME</code></pre><h3 id=\"1-7-启动网络\"><a href=\"#1-7-启动网络\" class=\"headerlink\" title=\"1.7 启动网络\"></a>1.7 启动网络</h3><p>首先写包含所有节点的Docker文件,这里直接贴出来:<br><code>base.yaml</code>:</p>\n<pre><code>version: &#39;2&#39;\n\nservices:\n  orderer-base:\n    image: hyperledger/fabric-orderer:2.0.0-beta\n    environment:\n      - FABRIC_LOGGING_SPEC=INFO\n#      - FABRIC_LOGGING_SPEC=DEBUG\n      - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0\n      - ORDERER_GENERAL_BOOTSTRAPMETHOD=file\n      - ORDERER_GENERAL_BOOTSTRAPFILE=/var/hyperledger/orderer/orderer.genesis.block\n      - ORDERER_GENERAL_LOCALMSPID=OrdererMSP\n      - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp\n      # enabled TLS\n      - ORDERER_GENERAL_TLS_ENABLED=true\n      - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key\n      - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt\n      - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt]\n      - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt\n      - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key\n      - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt]\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric\n    command: orderer</code></pre><p>然后是Orderer节点的Docker文件<code>docker-compose-orderers.yaml</code>:</p>\n<pre><code># Copyright IBM Corp. All Rights Reserved.\n#\n# SPDX-License-Identifier: Apache-2.0\n#\n\nversion: &#39;2&#39;\n\nvolumes:\n  orderer1.example.com:\n  orderer2.example.com:\n  orderer3.example.com:\n  orderer4.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  orderer1.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=7050\n    container_name: orderer1.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o1:/var/hyperledger/production/orderer\n    ports:\n      - 7050:7050\n\n\n\n  orderer2.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=8050\n    container_name: orderer2.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o2:/var/hyperledger/production/orderer\n    ports:\n      - 8050:8050\n\n  orderer3.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=9050\n    container_name: orderer3.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o3:/var/hyperledger/production/orderer\n    ports:\n      - 9050:9050\n\n  orderer4.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=10050\n    container_name: orderer4.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o4:/var/hyperledger/production/orderer\n    ports:\n      - 10050:10050</code></pre><p>最后一个是peer节点的Docker文件<code>docker-compose-peer.yaml</code>：</p>\n<pre><code>version: &#39;2&#39;\n\nvolumes:\n  peer0.org1.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    image: hyperledger/fabric-peer:2.0.0-beta\n    environment:\n      #Generic peer variables\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      # the following setting starts chaincode containers on the same\n      # bridge network as the peers\n      # https://docs.docker.com/compose/networking/\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=${COMPOSE_PROJECT_NAME}_byfn\n      - FABRIC_LOGGING_SPEC=INFO\n      #- FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_PROFILE_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt\n      # Peer specific variabes\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:7051\n      - CORE_PEER_CHAINCODEADDRESS=peer0.org1.example.com:7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org1.example.com:7051\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_LEDGER_STATE_STATEDATABASE=CouchDB\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984\n      # The CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME and CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD\n      # provide the credentials for ledger to connect to CouchDB.  The username and password must\n      # match the username and password set for the associated CouchDB.\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME=\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD=\n    volumes:\n      - /var/run/:/host/var/run/\n      - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp:/etc/hyperledger/fabric/msp\n      - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls:/etc/hyperledger/fabric/tls\n      - ../store/p1:/var/hyperledger/production\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: peer node start\n    depends_on:\n      - couchdb0\n    ports:\n      - 7051:7051\n    networks:\n      - byfn\n\n  couchdb0:\n    container_name: couchdb0\n    image: couchdb:2.3\n    # Populate the COUCHDB_USER and COUCHDB_PASSWORD to set an admin user and password\n    # for CouchDB.  This will prevent CouchDB from operating in an &quot;Admin Party&quot; mode.\n    environment:\n      - COUCHDB_USER=\n      - COUCHDB_PASSWORD=\n    # Comment/Uncomment the port mapping if you want to hide/expose the CouchDB service,\n    # for example map it to utilize Fauxton User Interface in dev environments.\n    ports:\n      - &quot;5984:5984&quot;\n    networks:\n      - byfn\n\n  cli:\n    container_name: cli\n    image: hyperledger/fabric-tools:2.0.0-beta\n    tty: true\n    stdin_open: true\n    environment:\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n#      - FABRIC_LOGGING_SPEC=DEBUG\n      - FABRIC_LOGGING_SPEC=INFO\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\n      - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: /bin/bash\n    volumes:\n      - /var/run/:/host/var/run/\n      - ./../../chaincode/:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - ../crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/\n      - ../channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts\n    depends_on:\n      - peer0.org1.example.com\n    networks:\n      - byfn</code></pre><p>将以上文件保存到指定位置后，使用以下命令直接启动:</p>\n<pre><code>docker-compose -f docker/docker-compose-orderers.yaml -f docker/docker-compose-peer.yaml up -d</code></pre><p>启动完成后可以查看每个节点的日志确认节点成功运行:</p>\n<pre><code>docker logs orderer1.example.com\n...\ndocker logs peer0.org1.example.com</code></pre><p>如果没有错误的话就可以进行第二部分了，如果出现错误则要回去检查是不是哪里漏掉了。</p>\n<h3 id=\"1-8-简单测试\"><a href=\"#1-8-简单测试\" class=\"headerlink\" title=\"1.8 简单测试\"></a>1.8 简单测试</h3><p>先进行第一部分的测试，看一下创建通道，加入通道是否成功:</p>\n<pre><code>#进入CLI容器\ndocker exec -it cli bash\n#配置环境变量\nexport CHANNEL_NAME=mychannel\nexport ORDERER_CA=${PWD}/crypto/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport PEER0_ORG1_CA=${PWD}/crypto/peerOrganization/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\nexport CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;\nexport CORE_PEER_TLS_ROOTCERT_FILE=$PEER0_ORG1_CA\nexport CORE_PEER_MSPCONFIGPATH=${PWD}/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051</code></pre><p>创建通道:</p>\n<pre><code>peer channel create -o orderer1.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/mychannel.tx --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA --outputBlock ./channel-artifacts/${CHANNEL_NAME}.block</code></pre><p>加入通道:</p>\n<pre><code>peer channel join -b ./channel-artifacts/$CHANNEL_NAME.block</code></pre><p>如果一切顺利的话，网络就成功搭建起来了，至于链码就不再测试了。<br>直接到第二部分，动态添加一个Orderer节点。</p>\n<h2 id=\"2-动态添加Raft节点\"><a href=\"#2-动态添加Raft节点\" class=\"headerlink\" title=\"2 动态添加Raft节点\"></a>2 动态添加Raft节点</h2><p>主要步骤如下：</p>\n<ol>\n<li>为该节点生成证书文件</li>\n<li>获取当前网络的配置文件</li>\n<li>将证书文件添加到配置文件中</li>\n<li>更新配置文件</li>\n<li>启动新的Orderer节点</li>\n</ol>\n<h3 id=\"2-1-生成证书文件\"><a href=\"#2-1-生成证书文件\" class=\"headerlink\" title=\"2.1 生成证书文件\"></a>2.1 生成证书文件</h3><h4 id=\"2-1-1-注册该节点身份\"><a href=\"#2-1-1-注册该节点身份\" class=\"headerlink\" title=\"2.1.1 注册该节点身份\"></a>2.1.1 注册该节点身份</h4><pre><code>fabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer5 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>为该节点创建存储证书的文件夹:</p>\n<pre><code>mkdir -p crypto-config/orderOrganization/example.com/orderers/orderer5.example.com</code></pre><h4 id=\"2-1-2-获取该节点证书\"><a href=\"#2-1-2-获取该节点证书\" class=\"headerlink\" title=\"2.1.2 获取该节点证书\"></a>2.1.2 获取该节点证书</h4><pre><code>#MSP\nfabric-ca-client enroll -u https://orderer5:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp --csr.hosts orderer5.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n#TLS\nfabric-ca-client enroll -u https://orderer5:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls --enrollment.profile tls --csr.hosts orderer5.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>复制节点分类配置文件:</p>\n<pre><code>cp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/config.yaml</code></pre><p>修改证书与秘钥文件名称:</p>\n<pre><code>cp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/server.key</code></pre><p>创建文件夹并拷贝TLS证书文件:</p>\n<pre><code>mkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/tlscacerts/tlsca.example.com-cert.pem</code></pre><h3 id=\"2-2-获取网络配置文件\"><a href=\"#2-2-获取网络配置文件\" class=\"headerlink\" title=\"2.2 获取网络配置文件\"></a>2.2 获取网络配置文件</h3><p>将节点添加进网络，首先需要将该节点添加到系统通道内，所以先获取系统通道的配置文件:<br>进入<code>cli</code>容器:</p>\n<pre><code>docker exec -it cli bash</code></pre><p>配置环境变量，需要使用Orderer节点的身份信息:</p>\n<pre><code>export CORE_PEER_LOCALMSPID=&quot;OrdererMSP&quot;\nexport ORDERER_CA=${PWD}/crypto/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport CORE_PEER_TLS_ROOTCERT_FILE=${PWD}/crypto/ordererOrganization/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/orderOrganization/example.com/users/Admin@example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051</code></pre><p>获取系统通道配置文件:</p>\n<pre><code>peer channel fetch config channel-artifacts/config_block.pb -o orderer1.example.com:7050 -c byfn-sys-channel --tls --cafile $ORDERER_CA</code></pre><p>解码该配置文件:</p>\n<pre><code>configtxlator proto_decode --input channel-artifacts/config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; channel-artifacts/config.json</code></pre><h3 id=\"2-3将证书文件添加到配置文件中\"><a href=\"#2-3将证书文件添加到配置文件中\" class=\"headerlink\" title=\"2.3将证书文件添加到配置文件中\"></a>2.3将证书文件添加到配置文件中</h3><p>退出容器，可以在<code>channel-artifacts</code>文件内找到<code>config.json</code>文件。将该文件复制一份并在<code>channel-artifacts</code>文件夹下保存为<code>update_config.json</code>,使用编辑工具打开，并搜索<code>.example.com</code>字段如下：<br>字段一部分：</p>\n<pre><code>  {\n    &quot;client_tls_cert&quot;: &quot;一连串的字符串&quot;,\n    &quot;host&quot;: &quot;orderer1.example.com&quot;,\n    &quot;port&quot;: 7050,\n    &quot;server_tls_cert&quot;: &quot;一连串的字符串&quot;\n  }</code></pre><p>以及匹配到的第二部分的字段:</p>\n<pre><code>      &quot;OrdererAddresses&quot;: {\n        &quot;mod_policy&quot;: &quot;/Channel/Orderer/Admins&quot;,\n        &quot;value&quot;: {\n          &quot;addresses&quot;: [\n            &quot;orderer1.example.com:7050&quot;,\n            &quot;orderer2.example.com:8050&quot;,\n            &quot;orderer3.example.com:9050&quot;,\n            &quot;orderer4.example.com:10050&quot;\n          ]\n        },\n        &quot;version&quot;: &quot;0&quot;\n    }</code></pre><p>在字段一部分，需要将我们生成的新的节点的证书添加上去，其中证书文件地址为:</p>\n<pre><code>crypto-config/ordererOrganizations/example.com/orderers/orderer5.example.com/tls/server.crt</code></pre><p>使用<code>BASE64</code>转码:</p>\n<pre><code>cat crypto-config/ordererOrganizations/example.com/orderers/orderer5.example.com/tls/server.crt | base64 &gt; cert.txt</code></pre><p>在<code>update_config.json</code>文件中字段一的部分下面按照字段一的格式添加相同的代码块，并进行修改：<br>将<code>cert.txt</code>文件中的内容复制到字段一的<code>client_tls_cert,server_tls_cert</code>对应部分，并修改<code>host</code>对应部分为<code>orderer5.example.com</code>，<code>port</code>为<code>11050</code>.</p>\n<h3 id=\"2-4更新配置文件\"><a href=\"#2-4更新配置文件\" class=\"headerlink\" title=\"2.4更新配置文件\"></a>2.4更新配置文件</h3><p>接下来进入<code>cli</code>容器:</p>\n<pre><code>docker exec -it cli bash</code></pre><p>对原有的配置文件与更新的配置文件进行编码:</p>\n<pre><code>configtxlator proto_encode --input channel-artifacts/config.json --type common.Config &gt; channel-artifacts/config.pb\nconfigtxlator proto_encode --input channel-artifacts/update_config.json --type common.Config &gt; channel-artifacts/config_update.pb</code></pre><p>计算出两个文件的差异:</p>\n<pre><code>configtxlator compute_update --channel_id byfn-sys-channel --original channel-artifacts/config.pb --updated channel-artifacts/config_update.pb &gt; channel-artifacts/updated.pb</code></pre><p>对该文件进行解码，并添加用于更新配置的头部信息:</p>\n<pre><code>configtxlator proto_decode --input channel-artifacts/updated.pb --type common.ConfigUpdate &gt; channel-artifacts/updated.json\necho &#39;{&quot;payload&quot;:{&quot;header&quot;:{&quot;channel_header&quot;:{&quot;channel_id&quot;:&quot;byfn-sys-channel&quot;, &quot;type&quot;:2}},&quot;data&quot;:{&quot;config_update&quot;:&#39;$(cat channel-artifacts/updated.json)&#39;}}}&#39; | jq . &gt; channel-artifacts/updated_envelope.json</code></pre><p>编码为<code>Envelope</code>格式的文件:</p>\n<pre><code>configtxlator proto_encode --input channel-artifacts/updated_envelope.json --type common.Envelope &gt; channel-artifacts/updated_envelope.pb</code></pre><p>对该文件进行签名操作，用于更新配置:</p>\n<pre><code>peer channel signconfigtx -f channel-artifacts/updated_envelope.pb</code></pre><p>提交更新通道配置交易:</p>\n<pre><code>peer channel update -f channel-artifacts/updated_envelope.pb -c byfn-sys-channel -o orderer1.example.com:7050 --tls true --cafile $ORDERER_CA</code></pre><p>如果没有错误的话，新的Orderer节点证书已经成功添加到网络配置中，接下来可以启动新的节点了:</p>\n<h3 id=\"2-5-启动新的Orderer节点\"><a href=\"#2-5-启动新的Orderer节点\" class=\"headerlink\" title=\"2.5 启动新的Orderer节点\"></a>2.5 启动新的Orderer节点</h3><p>写一下新的Orderer节点的Docker文件<code>docker-compose-addOrderer5.yaml</code>:</p>\n<pre><code>\nversion: &#39;2&#39;\n\nvolumes:\n  orderer5.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n  orderer5.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=11050\n    container_name: orderer5.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o5:/var/hyperledger/production/orderer\n    ports:\n      - 11050:11050</code></pre><p>直接通过命令启动它:</p>\n<pre><code>docker-compose -f docker-compose-addOrderer5.yaml up -d</code></pre><p>可以查看新节点的日志确认新的节点已经成功加入了网络。</p>\n<p>到这里，本文成功把新的Orderer节点添加进了网络，但是只将该节点添加到了系统通道内，对于应用通道<code>mychannel</code>来说，新的节点并没有添加进来，将新的节点添加进<code>mychannel</code>通道和以上步骤相同，只需要将通道名称由系统通道修改为<code>mychannel</code>即可。本文便不再说明了。<br>而动态删除节点的过程与添加相似，只不过是从配置文件中删除节点证书。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Hyperledger-Fabric动态配置Raft节点\"><a href=\"#Hyperledger-Fabric动态配置Raft节点\" class=\"headerlink\" title=\"Hyperledger Fabric动态配置Raft节点\"></a>Hyperledger Fabric动态配置Raft节点</h1><p>最近看官方文档发现新的共识算法etcdRaft允许动态添加或删除排序节点，所以也花了一天时间操作了以下，写篇文章把整个过程记录一下。<br>初始网络本文设置了4个Orderer节点，1个Peer节点(用于更新配置文件以及测试用),然后动态添加第五个Orderer节点。<br>本文分成两个部分:</p>\n<ol>\n<li>第一部分是手动通过Fabric-CA生成每一个节点的证书文件</li>\n<li>第二部分是更新Fabric网络配置添加新的Orderer节点。</li>\n</ol>\n<p>本文基于<strong>Fabric v2.0.0-beta</strong>版本。版本号只要高于1.4.1就行</p>\n<h2 id=\"1-搭建定制化的Fabric网络\"><a href=\"#1-搭建定制化的Fabric网络\" class=\"headerlink\" title=\"1 搭建定制化的Fabric网络\"></a>1 搭建定制化的Fabric网络</h2><p>前提条件是成功跑起来Fabric的示例网络，可以看这里-&gt;<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric环境搭建</a></p>\n<p>首先在<code>$GOPATH</code>下(本文路径地址为<code>$GOPATH/src/github.com/hyperledger/fab</code>)建立如下几个文件夹用于之后的操作:</p>\n<pre><code>.  # 这里是根目录fab\n├── ca    # 用于生成CA证书的ca配置文件的文件夹\n│   ├── org1\n│   │   └── fabric-ca-server-config.yaml\n│   └── server\n│       └── fabric-ca-server-config.yaml\n├── channel-artifacts    #用于保存创世区块以及通道配置文件\n├── configtx.yaml      #配置文件：用于生成创世区块以及通道配置文件\n├── crypto-config     #存储生成的证书文件\n├── docker      # Fabric网络节点通过Docker启动，用于启动节点的Docker文件\n│   ├── base.yaml\n│   ├── docker-compose-addOrderer5.yaml\n│   ├── docker-compose-ca.yaml\n│   ├── docker-compose-orderers.yaml\n│   └── docker-compose-peer.yaml\n└── store    #存储区块等信息</code></pre><p><strong>以下所有操作默认都在根目录文件夹内！</strong></p>\n<h3 id=\"1-1CA配置文件\"><a href=\"#1-1CA配置文件\" class=\"headerlink\" title=\"1.1CA配置文件\"></a>1.1CA配置文件</h3><p>直接在这里贴出来:<code>org1/fabric-ca-server-config.yaml</code>:</p>\n<pre><code>\nversion: 1.2.0\n\n# Server&#39;s listening port (default: 7054)\nport: 7054\n\n# Enables debug logging (default: false)\ndebug: false\n\ncrlsizelimit: 512000\n\ntls:\n  # Enable TLS (default: false)\n  enabled: true\n  certfile:\n  keyfile:\n  clientauth:\n    type: noclientcert\n    certfiles:\n\nca:\n  # Name of this CA\n  name: Org1CA\n  keyfile:\n  certfile:\n  chainfile:\n\ncrl:\n  expiry: 24h\n\nregistry:\n\n  maxenrollments: -1\n\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: &quot;&quot;\n       attrs:\n          hf.Registrar.Roles: &quot;*&quot;\n          hf.Registrar.DelegateRoles: &quot;*&quot;\n          hf.Revoker: true\n          hf.IntermediateCA: true\n          hf.GenCRL: true\n          hf.Registrar.Attributes: &quot;*&quot;\n          hf.AffiliationMgr: true\n\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\n  tls:\n      enabled: false\n      certfiles:\n      client:\n        certfile:\n        keyfile:\n\nldap:\n\n   enabled: false\n   url: ldap://&lt;adminDN&gt;:&lt;adminPassword&gt;@&lt;host&gt;:&lt;port&gt;/&lt;base&gt;\n   tls:\n      certfiles:\n      client:\n         certfile:\n         keyfile:\n   attribute:\n      names: [&#39;uid&#39;,&#39;member&#39;]\n      converters:\n         - name:\n           value:\n      maps:\n         groups:\n            - name:\n              value:\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n\nsigning:\n    default:\n      usage:\n        - digital signature\n      expiry: 8760h\n    profiles:\n      ca:\n         usage:\n           - cert sign\n           - crl sign\n         expiry: 43800h\n         caconstraint:\n           isca: true\n           maxpathlen: 0\n      tls:\n         usage:\n            - signing\n            - key encipherment\n            - server auth\n            - client auth\n            - key agreement\n         expiry: 8760h\n\ncsr:\n   cn: ca.org1.example.com\n   names:\n      - C: US\n        ST: &quot;North Carolina&quot;\n        L: &quot;Durham&quot;\n        O: org1.example.com\n        OU:\n   hosts:\n     - localhost\n     - org1.example.com\n   ca:\n      expiry: 131400h\n      pathlength: 1\n\nbccsp:\n    default: SW\n    sw:\n        hash: SHA2\n        security: 256\n        filekeystore:\n            keystore: msp/keystore\n\ncacount:\n\ncafiles:\n\nintermediate:\n  parentserver:\n    url:\n    caname:\n\n  enrollment:\n    hosts:\n    profile:\n    label:\n\n  tls:\n    certfiles:\n    client:\n      certfile:\n      keyfile:\n</code></pre><p>以及<code>server/fabric-ca-server-config.yaml:</code>:</p>\n<pre><code># Version of config file\nversion: 1.2.0\n# Server&#39;s listening port (default: 7054)\nport: 7054\n# Enables debug logging (default: false)\ndebug: false\n# Size limit of an acceptable CRL in bytes (default: 512000)\ncrlsizelimit: 512000\ntls:\n  # Enable TLS (default: false)\n  enabled: true\n  # TLS for the server&#39;s listening port\n  certfile:\n  keyfile:\n  clientauth:\n    type: noclientcert\n    certfiles:\n\nca:\n  # Name of this CA\n  name: OrdererCA\n  keyfile:\n  certfile:\n  chainfile:\n\ncrl:\n  expiry: 24h\n\nregistry:\n  maxenrollments: -1\n\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: &quot;&quot;\n       attrs:\n          hf.Registrar.Roles: &quot;*&quot;\n          hf.Registrar.DelegateRoles: &quot;*&quot;\n          hf.Revoker: true\n          hf.IntermediateCA: true\n          hf.GenCRL: true\n          hf.Registrar.Attributes: &quot;*&quot;\n          hf.AffiliationMgr: true\n\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\n  tls:\n      enabled: false\n      certfiles:\n      client:\n        certfile:\n        keyfile:\n\nldap:\n   enabled: false\n   url: ldap://&lt;adminDN&gt;:&lt;adminPassword&gt;@&lt;host&gt;:&lt;port&gt;/&lt;base&gt;\n   tls:\n      certfiles:\n      client:\n         certfile:\n         keyfile:\n   attribute:\n      names: [&#39;uid&#39;,&#39;member&#39;]\n      converters:\n         - name:\n           value:\n      maps:\n         groups:\n            - name:\n              value:\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n\nsigning:\n    default:\n      usage:\n        - digital signature\n      expiry: 8760h\n    profiles:\n      ca:\n         usage:\n           - cert sign\n           - crl sign\n         expiry: 43800h\n         caconstraint:\n           isca: true\n           maxpathlen: 0\n      tls:\n         usage:\n            - signing\n            - key encipherment\n            - server auth\n            - client auth\n            - key agreement\n         expiry: 8760h\n\ncsr:\n   cn: ca.example.com\n   names:\n      - C: US\n        ST: &quot;New York&quot;\n        L: &quot;New York&quot;\n        O: example.com\n        OU:\n   hosts:\n     - localhost\n     - example.com\n   ca:\n      expiry: 131400h\n      pathlength: 1\n\nbccsp:\n    default: SW\n    sw:\n        hash: SHA2\n        security: 256\n        filekeystore:\n            keystore: msp/keystore\n\ncacount:\ncafiles:\n\nintermediate:\n  parentserver:\n    url:\n    caname:\n\n  enrollment:\n    hosts:\n    profile:\n    label:\n\n  tls:\n    certfiles:\n    client:\n      certfile:\n      keyfile:</code></pre><p><code>docker-compose-ca.yaml</code>文件:</p>\n<pre><code>version: &#39;2&#39;\n\nservices:\n  ca:\n    image: hyperledger/fabric-ca:1.4.4\n    environment:\n      - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server\n      - FABRIC_CA_SERVER_CA_NAME=ca-orderer\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_PORT=9054\n    ports:\n      - &quot;9054:9054&quot;\n    command: sh -c &#39;fabric-ca-server start -b admin:adminpw -d&#39;\n    volumes:\n      - ../ca/server:/etc/hyperledger/fabric-ca-server\n    container_name: ca_orderer\n\n  ca0:\n    image: hyperledger/fabric-ca:1.4.4\n    environment:\n      - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server\n      - FABRIC_CA_SERVER_CA_NAME=ca-org1\n      - FABRIC_CA_SERVER_TLS_ENABLED=true\n      - FABRIC_CA_SERVER_PORT=7054\n    ports:\n      - &quot;7054:7054&quot;\n    command: sh -c &#39;fabric-ca-server start -b admin:adminpw -d&#39;\n    volumes:\n      - ../ca/org1:/etc/hyperledger/fabric-ca-server\n    container_name: ca_org1</code></pre><p>将以上三个文件保存到指定的路径，然后使用以下命令启动CA服务器：</p>\n<pre><code>docker-compose -f docker/docker-compose-ca.yaml up -d</code></pre><p>服务器会自动读取上面的两个配置文件，并初始化CA服务器。<br>当然，服务器配置文件将自动生成在<code>ca/server/</code>子文件夹内，其中最主要使用到的是<code>tls-cert.pem</code>文件。</p>\n<h3 id=\"1-2-注册Orderer节点\"><a href=\"#1-2-注册Orderer节点\" class=\"headerlink\" title=\"1.2 注册Orderer节点\"></a>1.2 注册Orderer节点</h3><p>首先配置环境变量并登陆管理员账号:</p>\n<pre><code>#创建存储Order节点证书的子文件夹。\nmkdir -p crypto-config/orderOrganization/example.com\nexport FABRIC_CA_CLIENT_HOME=${PWD}/crypto-config/orderOrganization/example.com\nfabric-ca-client enroll -u https://admin:adminpw@localhost:9054 --caname ca-orderer --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>生成节点类型分类配置文件(不知道这个文件应该称作什么，暂且使用这个名字称呼好了):</p>\n<pre><code>  echo &#39;NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/localhost-9054-ca-orderer.pem\n    OrganizationalUnitIdentifier: orderer&#39; &gt; ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml</code></pre><p>之后注册网络中初始的4个Orderer节点:</p>\n<pre><code>fabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer1 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer2 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer3 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer4 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>注册<code>Admin</code>节点:</p>\n<pre><code>fabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name ordererAdmin --id.secret ordererAdminpw --id.type admin --id.attrs &#39;&quot;hf.Registrar.Roles=admin&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><h3 id=\"1-3-获取Orderer证书文件\"><a href=\"#1-3-获取Orderer证书文件\" class=\"headerlink\" title=\"1.3 获取Orderer证书文件\"></a>1.3 获取Orderer证书文件</h3><p>为刚刚创建的几个用户创建各自的文件夹用于存储证书文件:</p>\n<pre><code>mkdir -p crypto-config/orderOrganization/example.com/orderers\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer1.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer2.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer3.example.com\nmkdir -p crypto-config/orderOrganization/example.com/orderers/orderer4.example.com</code></pre><p>接下来获取每一个Orderer节点的<code>MSP</code>证书文件:</p>\n<pre><code>fabric-ca-client enroll -u https://orderer1:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp --csr.hosts orderer1.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer2:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp --csr.hosts orderer2.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer3:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp --csr.hosts orderer3.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer4:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp --csr.hosts orderer4.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>还有每一个节点的<code>TLS</code>证书:</p>\n<pre><code>fabric-ca-client enroll -u https://orderer1:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls --enrollment.profile tls --csr.hosts orderer1.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer2:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls --enrollment.profile tls --csr.hosts orderer2.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer3:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls --enrollment.profile tls --csr.hosts orderer3.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\nfabric-ca-client enroll -u https://orderer4:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls --enrollment.profile tls --csr.hosts orderer4.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>将之前生成的节点类型分类配置文件拷贝到每一个节点的<code>MSP</code>文件夹:</p>\n<pre><code>cp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/config.yaml\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/config.yaml</code></pre><p>然后为每一个节点的<code>TLS</code>证书以及秘钥文件修改名字，方便之后的使用:</p>\n<pre><code>cp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.key\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.key</code></pre><p>然后在<code>MSP</code>文件夹内创建<code>tlscacerts</code>文件夹，并将<code>TLS</code>文件拷贝过去:</p>\n<pre><code>mkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/tlscacerts\nmkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/tlscacerts\n\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp/tlscacerts/tlsca.example.com-cert.pem</code></pre><p>复制TLS根证书:</p>\n<pre><code>mkdir -p ${PWD}/crypto-config/orderOrganization/example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/msp/tlscacerts/tlsca.example.com-cert.pem</code></pre><p>最后是<code>Admin</code>节点的证书文件:</p>\n<pre><code>#首先也是创建文件夹\nmkdir -p crypto-config/orderOrganization/example.com/users\nmkdir -p crypto-config/orderOrganization/example.com/users/Admin@example.com\n#获取证书文件\nfabric-ca-client enroll -u https://ordererAdmin:ordererAdminpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/users/Admin@example.com/msp --tls.certfiles ${PWD}/ca/server/tls-cert.pem\ncp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/users/Admin@example.com/msp/config.yaml</code></pre><p>到这里Orderer节点证书已经生成完毕(可以根据实际需要修改Orderer节点数量，最少不能低于3个)，接下来是网络中唯一的<code>peer</code>节点的配置文件生成:</p>\n<h3 id=\"1-4-注册Peer节点\"><a href=\"#1-4-注册Peer节点\" class=\"headerlink\" title=\"1.4 注册Peer节点\"></a>1.4 注册Peer节点</h3><p>和上面步骤相同，首先创建子文件夹用于存储证书文件:</p>\n<pre><code>mkdir -p crypto-config/peerOrganizations/org1.example.com/</code></pre><p>配置环境变量并登陆管理员身份:</p>\n<pre><code>export FABRIC_CA_CLIENT_HOME=${PWD}/crypto-config/peerOrganizations/org1.example.com/\nfabric-ca-client enroll -u https://admin:adminpw@localhost:7054 --caname ca-org1 --tls.certfiles ${PWD}/ca/org1/tls-cert.pem</code></pre><p>生成节点类型分类配置文件:</p>\n<pre><code>echo &#39;NodeOUs:\n  Enable: true\n  ClientOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: client\n  PeerOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: peer\n  AdminOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: admin\n  OrdererOUIdentifier:\n    Certificate: cacerts/localhost-7054-ca-org1.pem\n    OrganizationalUnitIdentifier: orderer&#39; &gt; ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml</code></pre><p>虽然网络中只有一个peer节点，但是我们需要注册三个用户:<code>peer0,user1,org1admin</code>，其中第一个是必需的，第二个是用于测试的，第三个为<code>Admin</code>用户，安装和实例化链码需要<code>Admin</code>用户的证书:</p>\n<pre><code>fabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name peer0 --id.secret peer0pw --id.type peer --id.attrs &#39;&quot;hf.Registrar.Roles=peer&quot;&#39; --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name user1 --id.secret user1pw --id.type client --id.attrs &#39;&quot;hf.Registrar.Roles=client&quot;&#39; --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client register -u https://admin:adminpw@localhost:7054 --caname ca-org1 --id.name org1admin --id.secret org1adminpw --id.type admin --id.attrs &#39;&quot;hf.Registrar.Roles=admin&quot;&#39; --tls.certfiles ${PWD}/ca/org1/tls-cert.pem</code></pre><h3 id=\"1-5-获取Peer节点证书文件\"><a href=\"#1-5-获取Peer节点证书文件\" class=\"headerlink\" title=\"1.5 获取Peer节点证书文件\"></a>1.5 获取Peer节点证书文件</h3><p>节点注册完毕，获取他们的证书文件:<br>创建子文件夹:</p>\n<pre><code>mkdir -p crypto-config/peerOrganizations/org1.example.com/peers\nmkdir -p crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.co</code></pre><p>获取证书文件:</p>\n<pre><code>#MSP文件\nfabric-ca-client enroll -u https://peer0:peer0pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp --csr.hosts peer0.org1.example.com --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\n#TLS证书\nfabric-ca-client enroll -u https://peer0:peer0pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls --enrollment.profile tls --csr.hosts peer0.org1.example.com --csr.hosts localhost --tls.certfiles ${PWD}/ca/org1/tls-cert.pem</code></pre><p>拷贝节点分类配置文件:</p>\n<pre><code>cp ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/config.yaml</code></pre><p>修改证书以及秘钥文件，方便之后使用:</p>\n<pre><code>cp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/signcerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/keystore/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key</code></pre><p>将TLS相关证书复制一份:</p>\n<pre><code>mkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/tlscacerts/ca.crt\n\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/tlsca\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/tlscacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem\n\nmkdir ${PWD}/crypto-config/peerOrganizations/org1.example.com/ca\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/cacerts/* ${PWD}/crypto-config/peerOrganizations/org1.example.com/ca/ca.org1.example.com-cert.pem</code></pre><p>获取<code>user</code>与<code>Admin</code>用户证书文件:</p>\n<pre><code>#创建子文件夹\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users/User1@org1.example.com\nmkdir -p crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com\n#获取证书文件\nfabric-ca-client enroll -u https://user1:user1pw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/User1@org1.example.com/msp --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\nfabric-ca-client enroll -u https://org1admin:org1adminpw@localhost:7054 --caname ca-org1 -M ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp --tls.certfiles ${PWD}/ca/org1/tls-cert.pem\ncp ${PWD}/crypto-config/peerOrganizations/org1.example.com/msp/config.yaml ${PWD}/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp/config.yaml</code></pre><h3 id=\"1-6-启动网络之前的准备\"><a href=\"#1-6-启动网络之前的准备\" class=\"headerlink\" title=\"1.6 启动网络之前的准备\"></a>1.6 启动网络之前的准备</h3><p>到这里我们已经生成了所有需要的证书文件，接下来是生成用于启动网络的创世区块,生成创世区块需要一个文件<code>configtx.yaml</code>,直接复制过来:</p>\n<pre><code>\nOrganizations:\n    - &amp;OrdererOrg\n        Name: OrdererOrg\n        ID: OrdererMSP\n        MSPDir: ./crypto-config/orderOrganization/example.com/msp   #这里路径需要对应！！！\n        Policies:\n            Readers:\n                Type: Signature\n                Rule: &quot;OR(&#39;OrdererMSP.member&#39;)&quot;\n            Writers:\n                Type: Signature\n                Rule: &quot;OR(&#39;OrdererMSP.member&#39;)&quot;\n            Admins:\n                Type: Signature\n                Rule: &quot;OR(&#39;OrdererMSP.admin&#39;)&quot;\n\n    - &amp;Org1  #如果需要更多组织节点，可以按照该模板在下面添加\n        Name: Org1MSP\n        ID: Org1MSP\n        MSPDir: ./crypto-config/peerOrganizations/org1.example.com/msp  #这里路径需要对应！！！\n        Policies:\n            Readers:\n                Type: Signature\n                Rule: &quot;OR(&#39;Org1MSP.admin&#39;, &#39;Org1MSP.peer&#39;, &#39;Org1MSP.client&#39;)&quot;\n            Writers:\n                Type: Signature\n                Rule: &quot;OR(&#39;Org1MSP.admin&#39;, &#39;Org1MSP.client&#39;)&quot;\n            Admins:\n                Type: Signature\n                Rule: &quot;OR(&#39;Org1MSP.admin&#39;)&quot;\n            Endorsement:\n                Type: Signature\n                Rule: &quot;OR(&#39;Org1MSP.peer&#39;)&quot;\n        AnchorPeers:\n              Port: 7051\n\nCapabilities:\n    Channel: &amp;ChannelCapabilities\n        V2_0: true\n\n    Orderer: &amp;OrdererCapabilities\n        V2_0: true\n\n    Application: &amp;ApplicationCapabilities\n        V2_0: true\n\nApplication: &amp;ApplicationDefaults\n    Organizations:\n    Policies:\n        Readers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Readers&quot;\n        Writers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Writers&quot;\n        Admins:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Admins&quot;\n        LifecycleEndorsement:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Endorsement&quot;\n        Endorsement:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Endorsement&quot;\n    Capabilities:\n        &lt;&lt;: *ApplicationCapabilities\n\nOrderer: &amp;OrdererDefaults\n    OrdererType: etcdraft\n\n    Addresses:\n        - orderer1.example.com:7050\n    BatchTimeout: 2s\n    BatchSize:\n        MaxMessageCount: 10\n        AbsoluteMaxBytes: 99 MB\n        PreferredMaxBytes: 512 KB\n    Organizations:\n    Policies:\n        Readers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Readers&quot;\n        Writers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Writers&quot;\n        Admins:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Admins&quot;\n        BlockValidation:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Writers&quot;\nChannel: &amp;ChannelDefaults\n    Policies:\n        # Who may invoke the &#39;Deliver&#39; API\n        Readers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Readers&quot;\n        # Who may invoke the &#39;Broadcast&#39; API\n        Writers:\n            Type: ImplicitMeta\n            Rule: &quot;ANY Writers&quot;\n        # By default, who may modify elements at this config level\n        Admins:\n            Type: ImplicitMeta\n            Rule: &quot;MAJORITY Admins&quot;\n    Capabilities:\n        &lt;&lt;: *ChannelCapabilities\n\nProfiles:\n\n    TwoOrgsChannel:   #用于生成通道配置文件\n        Consortium: SampleConsortium\n        &lt;&lt;: *ChannelDefaults\n        Application:\n            &lt;&lt;: *ApplicationDefaults\n            Organizations:\n                - *Org1\n            Capabilities:\n                &lt;&lt;: *ApplicationCapabilities\n\n    SampleMultiNodeEtcdRaft:   #用于生成系统通道创世区块\n        &lt;&lt;: *ChannelDefaults\n        Capabilities:\n            &lt;&lt;: *ChannelCapabilities\n        Orderer:\n            &lt;&lt;: *OrdererDefaults\n            OrdererType: etcdraft   #指定使用etcdraft共识算法\n            EtcdRaft:\n                Consenters:\n                - Host: orderer1.example.com\n                  Port: 7050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n                - Host: orderer2.example.com\n                  Port: 8050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/server.crt\n                - Host: orderer3.example.com\n                  Port: 9050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/server.crt\n                - Host: orderer4.example.com\n                  Port: 10050\n                  ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\n                  ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/server.crt\n#                    - Host: orderer5.example.com\n#                      Port: 11050\n#                      ClientTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n#                      ServerTLSCert: ./crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/server.crt\n            Addresses:\n                - orderer1.example.com:7050\n                - orderer2.example.com:8050\n                - orderer3.example.com:9050\n                - orderer4.example.com:10050\n#                - orderer5.example.com:11050\n\n            Organizations:\n            - *OrdererOrg\n            Capabilities:\n                &lt;&lt;: *OrdererCapabilities\n        Application:\n            &lt;&lt;: *ApplicationDefaults\n            Organizations:\n            - &lt;&lt;: *OrdererOrg\n        Consortiums:\n            SampleConsortium:\n                Organizations:\n                - *Org1</code></pre><p>将该文件保存到指定位置，接下来生成创世区块:</p>\n<pre><code>export FABRIC_CFG_PATH=$PWD\nconfigtxgen -profile SampleMultiNodeEtcdRaft -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n# 生成通道配置文件\nexport CHANNEL_NAME=mychannel\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/mychannel.tx -channelID $CHANNEL_NAME</code></pre><h3 id=\"1-7-启动网络\"><a href=\"#1-7-启动网络\" class=\"headerlink\" title=\"1.7 启动网络\"></a>1.7 启动网络</h3><p>首先写包含所有节点的Docker文件,这里直接贴出来:<br><code>base.yaml</code>:</p>\n<pre><code>version: &#39;2&#39;\n\nservices:\n  orderer-base:\n    image: hyperledger/fabric-orderer:2.0.0-beta\n    environment:\n      - FABRIC_LOGGING_SPEC=INFO\n#      - FABRIC_LOGGING_SPEC=DEBUG\n      - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0\n      - ORDERER_GENERAL_BOOTSTRAPMETHOD=file\n      - ORDERER_GENERAL_BOOTSTRAPFILE=/var/hyperledger/orderer/orderer.genesis.block\n      - ORDERER_GENERAL_LOCALMSPID=OrdererMSP\n      - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp\n      # enabled TLS\n      - ORDERER_GENERAL_TLS_ENABLED=true\n      - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key\n      - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt\n      - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt]\n      - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt\n      - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key\n      - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt]\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric\n    command: orderer</code></pre><p>然后是Orderer节点的Docker文件<code>docker-compose-orderers.yaml</code>:</p>\n<pre><code># Copyright IBM Corp. All Rights Reserved.\n#\n# SPDX-License-Identifier: Apache-2.0\n#\n\nversion: &#39;2&#39;\n\nvolumes:\n  orderer1.example.com:\n  orderer2.example.com:\n  orderer3.example.com:\n  orderer4.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  orderer1.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=7050\n    container_name: orderer1.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer1.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o1:/var/hyperledger/production/orderer\n    ports:\n      - 7050:7050\n\n\n\n  orderer2.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=8050\n    container_name: orderer2.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer2.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o2:/var/hyperledger/production/orderer\n    ports:\n      - 8050:8050\n\n  orderer3.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=9050\n    container_name: orderer3.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer3.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o3:/var/hyperledger/production/orderer\n    ports:\n      - 9050:9050\n\n  orderer4.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=10050\n    container_name: orderer4.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer4.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o4:/var/hyperledger/production/orderer\n    ports:\n      - 10050:10050</code></pre><p>最后一个是peer节点的Docker文件<code>docker-compose-peer.yaml</code>：</p>\n<pre><code>version: &#39;2&#39;\n\nvolumes:\n  peer0.org1.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n\n  peer0.org1.example.com:\n    container_name: peer0.org1.example.com\n    image: hyperledger/fabric-peer:2.0.0-beta\n    environment:\n      #Generic peer variables\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n      # the following setting starts chaincode containers on the same\n      # bridge network as the peers\n      # https://docs.docker.com/compose/networking/\n      - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=${COMPOSE_PROJECT_NAME}_byfn\n      - FABRIC_LOGGING_SPEC=INFO\n      #- FABRIC_LOGGING_SPEC=DEBUG\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_GOSSIP_USELEADERELECTION=true\n      - CORE_PEER_GOSSIP_ORGLEADER=false\n      - CORE_PEER_PROFILE_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt\n      # Peer specific variabes\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LISTENADDRESS=0.0.0.0:7051\n      - CORE_PEER_CHAINCODEADDRESS=peer0.org1.example.com:7052\n      - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n      - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org1.example.com:7051\n      - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_LEDGER_STATE_STATEDATABASE=CouchDB\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984\n      # The CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME and CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD\n      # provide the credentials for ledger to connect to CouchDB.  The username and password must\n      # match the username and password set for the associated CouchDB.\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME=\n      - CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD=\n    volumes:\n      - /var/run/:/host/var/run/\n      - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp:/etc/hyperledger/fabric/msp\n      - ../crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls:/etc/hyperledger/fabric/tls\n      - ../store/p1:/var/hyperledger/production\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: peer node start\n    depends_on:\n      - couchdb0\n    ports:\n      - 7051:7051\n    networks:\n      - byfn\n\n  couchdb0:\n    container_name: couchdb0\n    image: couchdb:2.3\n    # Populate the COUCHDB_USER and COUCHDB_PASSWORD to set an admin user and password\n    # for CouchDB.  This will prevent CouchDB from operating in an &quot;Admin Party&quot; mode.\n    environment:\n      - COUCHDB_USER=\n      - COUCHDB_PASSWORD=\n    # Comment/Uncomment the port mapping if you want to hide/expose the CouchDB service,\n    # for example map it to utilize Fauxton User Interface in dev environments.\n    ports:\n      - &quot;5984:5984&quot;\n    networks:\n      - byfn\n\n  cli:\n    container_name: cli\n    image: hyperledger/fabric-tools:2.0.0-beta\n    tty: true\n    stdin_open: true\n    environment:\n      - GOPATH=/opt/gopath\n      - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock\n#      - FABRIC_LOGGING_SPEC=DEBUG\n      - FABRIC_LOGGING_SPEC=INFO\n      - CORE_PEER_ID=peer0.org1.example.com\n      - CORE_PEER_ADDRESS=peer0.org1.example.com:7051\n      - CORE_PEER_LOCALMSPID=Org1MSP\n      - CORE_PEER_TLS_ENABLED=true\n      - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt\n      - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key\n      - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\n      - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n    working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer\n    command: /bin/bash\n    volumes:\n      - /var/run/:/host/var/run/\n      - ./../../chaincode/:/opt/gopath/src/github.com/hyperledger/fabric-samples/chaincode\n      - ../crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/\n      - ../channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts\n    depends_on:\n      - peer0.org1.example.com\n    networks:\n      - byfn</code></pre><p>将以上文件保存到指定位置后，使用以下命令直接启动:</p>\n<pre><code>docker-compose -f docker/docker-compose-orderers.yaml -f docker/docker-compose-peer.yaml up -d</code></pre><p>启动完成后可以查看每个节点的日志确认节点成功运行:</p>\n<pre><code>docker logs orderer1.example.com\n...\ndocker logs peer0.org1.example.com</code></pre><p>如果没有错误的话就可以进行第二部分了，如果出现错误则要回去检查是不是哪里漏掉了。</p>\n<h3 id=\"1-8-简单测试\"><a href=\"#1-8-简单测试\" class=\"headerlink\" title=\"1.8 简单测试\"></a>1.8 简单测试</h3><p>先进行第一部分的测试，看一下创建通道，加入通道是否成功:</p>\n<pre><code>#进入CLI容器\ndocker exec -it cli bash\n#配置环境变量\nexport CHANNEL_NAME=mychannel\nexport ORDERER_CA=${PWD}/crypto/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport PEER0_ORG1_CA=${PWD}/crypto/peerOrganization/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt\nexport CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;\nexport CORE_PEER_TLS_ROOTCERT_FILE=$PEER0_ORG1_CA\nexport CORE_PEER_MSPCONFIGPATH=${PWD}/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051</code></pre><p>创建通道:</p>\n<pre><code>peer channel create -o orderer1.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/mychannel.tx --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA --outputBlock ./channel-artifacts/${CHANNEL_NAME}.block</code></pre><p>加入通道:</p>\n<pre><code>peer channel join -b ./channel-artifacts/$CHANNEL_NAME.block</code></pre><p>如果一切顺利的话，网络就成功搭建起来了，至于链码就不再测试了。<br>直接到第二部分，动态添加一个Orderer节点。</p>\n<h2 id=\"2-动态添加Raft节点\"><a href=\"#2-动态添加Raft节点\" class=\"headerlink\" title=\"2 动态添加Raft节点\"></a>2 动态添加Raft节点</h2><p>主要步骤如下：</p>\n<ol>\n<li>为该节点生成证书文件</li>\n<li>获取当前网络的配置文件</li>\n<li>将证书文件添加到配置文件中</li>\n<li>更新配置文件</li>\n<li>启动新的Orderer节点</li>\n</ol>\n<h3 id=\"2-1-生成证书文件\"><a href=\"#2-1-生成证书文件\" class=\"headerlink\" title=\"2.1 生成证书文件\"></a>2.1 生成证书文件</h3><h4 id=\"2-1-1-注册该节点身份\"><a href=\"#2-1-1-注册该节点身份\" class=\"headerlink\" title=\"2.1.1 注册该节点身份\"></a>2.1.1 注册该节点身份</h4><pre><code>fabric-ca-client register -u https://admin:adminpw@localhost:9054 --caname ca-orderer --id.name orderer5 --id.secret ordererpw --id.type orderer --id.attrs &#39;&quot;hf.Registrar.Roles=orderer&quot;&#39; --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>为该节点创建存储证书的文件夹:</p>\n<pre><code>mkdir -p crypto-config/orderOrganization/example.com/orderers/orderer5.example.com</code></pre><h4 id=\"2-1-2-获取该节点证书\"><a href=\"#2-1-2-获取该节点证书\" class=\"headerlink\" title=\"2.1.2 获取该节点证书\"></a>2.1.2 获取该节点证书</h4><pre><code>#MSP\nfabric-ca-client enroll -u https://orderer5:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp --csr.hosts orderer5.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem\n#TLS\nfabric-ca-client enroll -u https://orderer5:ordererpw@localhost:9054 --caname ca-orderer -M ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls --enrollment.profile tls --csr.hosts orderer5.example.com --tls.certfiles ${PWD}/ca/server/tls-cert.pem</code></pre><p>复制节点分类配置文件:</p>\n<pre><code>cp ${PWD}/crypto-config/orderOrganization/example.com/msp/config.yaml ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/config.yaml</code></pre><p>修改证书与秘钥文件名称:</p>\n<pre><code>cp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/ca.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/signcerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/server.crt\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/keystore/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/server.key</code></pre><p>创建文件夹并拷贝TLS证书文件:</p>\n<pre><code>mkdir ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/tlscacerts\ncp ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/tlscacerts/* ${PWD}/crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp/tlscacerts/tlsca.example.com-cert.pem</code></pre><h3 id=\"2-2-获取网络配置文件\"><a href=\"#2-2-获取网络配置文件\" class=\"headerlink\" title=\"2.2 获取网络配置文件\"></a>2.2 获取网络配置文件</h3><p>将节点添加进网络，首先需要将该节点添加到系统通道内，所以先获取系统通道的配置文件:<br>进入<code>cli</code>容器:</p>\n<pre><code>docker exec -it cli bash</code></pre><p>配置环境变量，需要使用Orderer节点的身份信息:</p>\n<pre><code>export CORE_PEER_LOCALMSPID=&quot;OrdererMSP&quot;\nexport ORDERER_CA=${PWD}/crypto/orderOrganization/example.com/orderers/orderer1.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport CORE_PEER_TLS_ROOTCERT_FILE=${PWD}/crypto/ordererOrganization/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\nexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/orderOrganization/example.com/users/Admin@example.com/msp\nexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051</code></pre><p>获取系统通道配置文件:</p>\n<pre><code>peer channel fetch config channel-artifacts/config_block.pb -o orderer1.example.com:7050 -c byfn-sys-channel --tls --cafile $ORDERER_CA</code></pre><p>解码该配置文件:</p>\n<pre><code>configtxlator proto_decode --input channel-artifacts/config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; channel-artifacts/config.json</code></pre><h3 id=\"2-3将证书文件添加到配置文件中\"><a href=\"#2-3将证书文件添加到配置文件中\" class=\"headerlink\" title=\"2.3将证书文件添加到配置文件中\"></a>2.3将证书文件添加到配置文件中</h3><p>退出容器，可以在<code>channel-artifacts</code>文件内找到<code>config.json</code>文件。将该文件复制一份并在<code>channel-artifacts</code>文件夹下保存为<code>update_config.json</code>,使用编辑工具打开，并搜索<code>.example.com</code>字段如下：<br>字段一部分：</p>\n<pre><code>  {\n    &quot;client_tls_cert&quot;: &quot;一连串的字符串&quot;,\n    &quot;host&quot;: &quot;orderer1.example.com&quot;,\n    &quot;port&quot;: 7050,\n    &quot;server_tls_cert&quot;: &quot;一连串的字符串&quot;\n  }</code></pre><p>以及匹配到的第二部分的字段:</p>\n<pre><code>      &quot;OrdererAddresses&quot;: {\n        &quot;mod_policy&quot;: &quot;/Channel/Orderer/Admins&quot;,\n        &quot;value&quot;: {\n          &quot;addresses&quot;: [\n            &quot;orderer1.example.com:7050&quot;,\n            &quot;orderer2.example.com:8050&quot;,\n            &quot;orderer3.example.com:9050&quot;,\n            &quot;orderer4.example.com:10050&quot;\n          ]\n        },\n        &quot;version&quot;: &quot;0&quot;\n    }</code></pre><p>在字段一部分，需要将我们生成的新的节点的证书添加上去，其中证书文件地址为:</p>\n<pre><code>crypto-config/ordererOrganizations/example.com/orderers/orderer5.example.com/tls/server.crt</code></pre><p>使用<code>BASE64</code>转码:</p>\n<pre><code>cat crypto-config/ordererOrganizations/example.com/orderers/orderer5.example.com/tls/server.crt | base64 &gt; cert.txt</code></pre><p>在<code>update_config.json</code>文件中字段一的部分下面按照字段一的格式添加相同的代码块，并进行修改：<br>将<code>cert.txt</code>文件中的内容复制到字段一的<code>client_tls_cert,server_tls_cert</code>对应部分，并修改<code>host</code>对应部分为<code>orderer5.example.com</code>，<code>port</code>为<code>11050</code>.</p>\n<h3 id=\"2-4更新配置文件\"><a href=\"#2-4更新配置文件\" class=\"headerlink\" title=\"2.4更新配置文件\"></a>2.4更新配置文件</h3><p>接下来进入<code>cli</code>容器:</p>\n<pre><code>docker exec -it cli bash</code></pre><p>对原有的配置文件与更新的配置文件进行编码:</p>\n<pre><code>configtxlator proto_encode --input channel-artifacts/config.json --type common.Config &gt; channel-artifacts/config.pb\nconfigtxlator proto_encode --input channel-artifacts/update_config.json --type common.Config &gt; channel-artifacts/config_update.pb</code></pre><p>计算出两个文件的差异:</p>\n<pre><code>configtxlator compute_update --channel_id byfn-sys-channel --original channel-artifacts/config.pb --updated channel-artifacts/config_update.pb &gt; channel-artifacts/updated.pb</code></pre><p>对该文件进行解码，并添加用于更新配置的头部信息:</p>\n<pre><code>configtxlator proto_decode --input channel-artifacts/updated.pb --type common.ConfigUpdate &gt; channel-artifacts/updated.json\necho &#39;{&quot;payload&quot;:{&quot;header&quot;:{&quot;channel_header&quot;:{&quot;channel_id&quot;:&quot;byfn-sys-channel&quot;, &quot;type&quot;:2}},&quot;data&quot;:{&quot;config_update&quot;:&#39;$(cat channel-artifacts/updated.json)&#39;}}}&#39; | jq . &gt; channel-artifacts/updated_envelope.json</code></pre><p>编码为<code>Envelope</code>格式的文件:</p>\n<pre><code>configtxlator proto_encode --input channel-artifacts/updated_envelope.json --type common.Envelope &gt; channel-artifacts/updated_envelope.pb</code></pre><p>对该文件进行签名操作，用于更新配置:</p>\n<pre><code>peer channel signconfigtx -f channel-artifacts/updated_envelope.pb</code></pre><p>提交更新通道配置交易:</p>\n<pre><code>peer channel update -f channel-artifacts/updated_envelope.pb -c byfn-sys-channel -o orderer1.example.com:7050 --tls true --cafile $ORDERER_CA</code></pre><p>如果没有错误的话，新的Orderer节点证书已经成功添加到网络配置中，接下来可以启动新的节点了:</p>\n<h3 id=\"2-5-启动新的Orderer节点\"><a href=\"#2-5-启动新的Orderer节点\" class=\"headerlink\" title=\"2.5 启动新的Orderer节点\"></a>2.5 启动新的Orderer节点</h3><p>写一下新的Orderer节点的Docker文件<code>docker-compose-addOrderer5.yaml</code>:</p>\n<pre><code>\nversion: &#39;2&#39;\n\nvolumes:\n  orderer5.example.com:\n\nnetworks:\n  byfn:\n\nservices:\n  orderer5.example.com:\n    extends:\n      file: base.yaml\n      service: orderer-base\n    environment:\n      - ORDERER_GENERAL_LISTENPORT=11050\n    container_name: orderer5.example.com\n    networks:\n      - byfn\n    volumes:\n      - ../channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/msp:/var/hyperledger/orderer/msp\n      - ../crypto-config/orderOrganization/example.com/orderers/orderer5.example.com/tls/:/var/hyperledger/orderer/tls\n      - ../store/o5:/var/hyperledger/production/orderer\n    ports:\n      - 11050:11050</code></pre><p>直接通过命令启动它:</p>\n<pre><code>docker-compose -f docker-compose-addOrderer5.yaml up -d</code></pre><p>可以查看新节点的日志确认新的节点已经成功加入了网络。</p>\n<p>到这里，本文成功把新的Orderer节点添加进了网络，但是只将该节点添加到了系统通道内，对于应用通道<code>mychannel</code>来说，新的节点并没有添加进来，将新的节点添加进<code>mychannel</code>通道和以上步骤相同，只需要将通道名称由系统通道修改为<code>mychannel</code>即可。本文便不再说明了。<br>而动态删除节点的过程与添加相似，只不过是从配置文件中删除节点证书。</p>\n"},{"title":"Hyperledger Fabric外部链码构建与运行","date":"2019-12-24T11:42:58.000Z","_content":"# 外部链码构建与运行\n\n* * *\n\n[官方文档](https://hyperledger-fabric.readthedocs.io/en/latest/cc_launcher.html)\n在Hyperledger Fabric 2.0版本之前，链码的构建和运行是节点实现的一部分，并且定制化是困难的。所有链码在节点上实例化是通过”构建“即根据语言指定的逻辑在节点上硬编码。构建过程将生成`Docker`容器镜像作为客户端连接节点用来运行可执行的链码。\n这种方法将链代码实现限制为只能使用几种语言实现，要求`Docker`成为部署环境的一部分，并阻止将链代码作为长时间运行的服务器进程运行。\n\n# 外部构建模式\n\nHyperledger Fabric外部构建器和启动器大致基于Heroku [Buildpacks](https://devcenter.heroku.com/articles/buildpack-api)。`buildpack`实现只是将程序归档转换为可以运行的程序或脚本的集合。`buildpack`模型已针对链码包进行了调整，并扩展为支持链码执行和发现。\n\n## 外部构建和运行API\n\n外部构建和运行由4个脚本程序组成：\n\n* `bin/detect`:确定是否应使用此`buildpack`来构建`chaincode`程序包并启动它\n* `bin/build`:转换链码包为可执行的链码\n* `bin/release(可选)`:为`peer`节点提供关于链码的元数据\n* `bin/run(可选)`:运行链码\n\n### `bin/detect`\n\n`bin/detect`脚本决定是否应使用此`buildpack`来构建`chaincode`程序包并启动它,`peer`节点通过两个参数调用`detect`:\n```\nbin/detect CHAINCOD_SOURCE_DIR CHAINCODE_METADATA_DIR\n```\n\n当`detect`被调用，`CHAINCOD_SOURCE_DIR`包含的链码资源以及`CHAINCODE_METADATA_DIR`包含的`metadata.json`文件将从链码包中安装到节点。`CHAINCOD_SOURCE_DIR`和`CHAINCODE_METADATA_DIR`应该被作为只读输入。如果将`buildpack`应用于`chaincode`源程序包，`detect`必须返回退出码0；否则，其他任何退出代码都将指示`buildpack`不应用内于`chaincode`源程序包。\n下面是一个简单的用于`go`语言链码的`detect`脚本例子：\n```\n#!/bin/bash\n\nCHAINCODE_METADATA_DIR=\"$2\"\n\n# 使用jq工具从metadata.json中获取链码类型，如果链码类型为golang，则成功退出\nif [ \"$(jq -r .type \"$CHAINCODE_METADATA_DIR/metadata.json\" | tr '[:upper:]' '[:lower:]')\" = \"golang\" ]; then\n    exit 0\nfi\n\nexit 1\n```\n\n### `bin/build`\n\n`bin/build`脚本用于构建，编译，或者转换链码包的内容到可以被`release`和`run`使用的类型。节点通过三个参数调用`build`:\n```\nbin/build CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR BUILD_OUTPUT_DIR\n```\n\n当`build`被调用，`CHAINCOD_SOURCE_DIR`包含的链码资源以及`CHAINCODE_METADATA_DIR`包含的`metadata.json`文件将从链码包中安装到节点。`BUILD_OUTPUT_DIR`是一个文件夹用于存放`release`和`run`需要的文件。`build`脚本应该将`CHAINCOD_SOURCE_DIR`和`CHAINCODE_METADATA_DIR`作为只读输入，`BUILD_OUTPUT_DIR`作为可写输出。\n\n下面是一个简单的用于`go`语言链码的`build`脚本例子：\n```\n#!/bin/bash\n\nCHAINCODE_SOURCE_DIR=\"$1\"\nCHAINCODE_METADATA_DIR=\"$2\"\nBUILD_OUTPUT_DIR=\"$3\"\n\n# 从 metadata.json获取包内容\nGO_PACKAGE_PATH=\"$(jq -r .path \"$CHAINCODE_METADATA_DIR/metadata.json\")\"\nif [ -f \"$CHAINCODE_SOURCE_DIR/src/go.mod\" ]; then\n    cd \"$CHAINCODE_SOURCE_DIR/src\"\n    go build -v -mod=readonly -o \"$BUILD_OUTPUT_DIR/chaincode\" \"$GO_PACKAGE_PATH\"\nelse\n    GO111MODULE=off go build -v  -o \"$BUILD_OUTPUT_DIR/chaincode\" \"$GO_PACKAGE_PATH\"\nfi\n\n# 存储状态数据库索引元数据提供给release\nif [ -d \"$CHAINCODE_SOURCE_DIR/META-INF\" ]; then\n    cp -a \"$CHAINCODE_SOURCE_DIR/META-INF\" \"$BUILD_OUTPUT_DIR/\"\nfi\n```\n\n### `bin/release`\n\n`bin/release`脚本为节点提供链码元数据。该脚本是可选的。如果没有提供，这一步将会跳过。节点通过两个参数调用`release`：\n```\nbin/release BUILD_OUTPUT_DIR RELEASE_OUTPUT_DIR\n```\n\n调用`release`时，`BUILD_OUTPUT_DIR`包含构建程序填充的归档，应将其视为只读输入。`RELEASE_OUTPUT_DIR`是`release`必须放置归档以供节点使用的目录。\n\n当`release`执行完成，节点将会从`RELEASE_OUTPUT_DIR`消费两种类型的元数据:\n\n* CouchDB定义的状态数据库索引。\n* 外部链码服务连接信息(`chaincode/server/connection.json`)\n\n如果链码要求CouchDB索引定义，`release`需要将索引放置在`RELEASE_OUTPUT_DIR`下的`state/couchdb/indexes`文件夹内。索引必须含有`.json`扩展名。\n\n在使用链码服务器实现的情况下，`release`负责使用链码服务器的地址以及与链码通信所需的任何TLS资产来填充`chaincode/server/connection.json`。将服务器连接信息提供给节点时，将不会调用`run`。\n\n下面是一个简单的用于`go`语言链码的`release`脚本例子：\n\n```\n#!/bin/bash\n\nBUILD_OUTPUT_DIR=\"$1\"\nRELEASE_OUTPUT_DIR=\"$2\"\n\n# 从 META-INF/* 拷贝索引文件到输出文件夹\nif [ -d \"$BUILD_OUTPUT_DIR/META-INF\" ] ; then\n   cp -a \"$BUILD_OUTPUT_DIR/META-INF/\"* \"$RELEASE_OUTPUT_DIR/\"\nfi\n```\n\n\n### `bin/run`\n\n`bin/run`脚本用于链码的运行。节点通过两个参数调用`run`：\n```\nbin/run BUILD_OUTPUT_DIR RUN_METADATA_DIR\n```\n\n当`BUILD_OUTPUT_DIR`包含`build`程序填充的归档，而`RUN_METADATA_DIR`包含有一个名为`chaincode.json`的文件，该文件包含链码连接和注册到节点所需的信息，`run`将被调用。`bin/run`脚本对于`BUILD_OUTPUT_DIR`以及`RUN_METADATA_DIR`文件夹应为只读输入。`chaincode.json`文件包含的关键信息有：\n\n* `chaincode_id:`连接到链码包的唯一ID\n* `peer_address:``peer`节点的`ChaincodeSupport`中的gRPC服务端点主机地址，格式为`host:port`.\n* `client_cert:`由`peer`生成的`PEM`编码的TLS客户端证书。当链码与节点建立连接时将会被使用。\n* `client_key:`由`peer`生成的`PEM`编码的客户端秘钥。当链码与节点建立连接时将会被使用。\n* `root_cert:`由`peer`节点的`ChaincodeSupport`中的gRPC服务端点主机使用的`PEM`编码的`TLS`根证书。\n\n当`run`停止时，与`peer`连接的链码也会终止。如果另一个请求访问链码，节点将会尝试通过调用`run`启动链码的另一个实例。在调用链码时，`chaincode.json`文件内容不能够被缓存。\n\n下面是一个简单的用于`go`语言链码的`run`脚本例子：\n\n```\n#!/bin/bash\n\nBUILD_OUTPUT_DIR=\"$1\"\nRUN_METADATA_DIR=\"$2\"\n\n# 为go语言链码shim包配置环境变量\nexport CORE_CHAINCODE_ID_NAME=\"$(jq -r .chaincode_id \"$RUN_METADATA_DIR/chaincode.json\")\"\nexport CORE_PEER_TLS_ENABLED=\"true\"\nexport CORE_TLS_CLIENT_CERT_FILE=\"$RUN_METADATA_DIR/client.crt\"\nexport CORE_TLS_CLIENT_KEY_FILE=\"$RUN_METADATA_DIR/client.key\"\nexport CORE_PEER_TLS_ROOTCERT_FILE=\"$RUN_METADATA_DIR/root.crt\"\n\n# 为go语言链码shim包获取秘钥和证书材料\njq -r .client_cert \"$RUN_METADATA_DIR/chaincode.json\" > \"$CORE_TLS_CLIENT_CERT_FILE\"\njq -r .client_key  \"$RUN_METADATA_DIR/chaincode.json\" > \"$CORE_TLS_CLIENT_KEY_FILE\"\njq -r .root_cert   \"$RUN_METADATA_DIR/chaincode.json\" > \"$CORE_PEER_TLS_ROOTCERT_FILE\"\nif [ -z \"$(jq -r .client_cert \"$RUN_METADATA_DIR/chaincode.json\")\" ]; then\n    export CORE_PEER_TLS_ENABLED=\"false\"\nfi\n\n# 执行链码并使用链码进程替代脚本\nexec \"$BUILD_OUTPUT_DIR/chaincode\" -peer.address=\"$(jq -r .peer_address \"$ARTIFACTS/chaincode.json\")\"\n```\n\n## 外部构建和运行的配置\n\n在`core.yaml`的`chaincode`配置区域下添加一个`externalBuilder`元素配置节点以使用外部构建器.每一个外部构建器的定义必须包含名字(用于日志)和包含构建器脚本的`bin`文件夹的上一级路径。\n\n调用外部构建器脚本时还可以从节点获取环境变量名称的可选列表。\n\n下面的示例定义了两个外部构建器：\n```\nchaincode:\n  externalBuilders:\n  - name: my-golang-builder\n    path: /builders/golang\n    environmentWhitelist:\n    - GOPROXY\n    - GONOPROXY\n    - GOSUMDB\n    - GONOSUMDB\n  - name: noop-builder\n    path: /builders/binary\n```\n\n在这个示例中，实现的构建器`my-golang-builder`被包含在`/builders/golang`文件夹内，它的脚本文件位于`/builders/golang/bin`.当节点调用任何与`my-golang-builder`相关的构建脚本时，将只会传播白名单内的环境变量的值。\n\n这些环境变量总是传播到外部构建器：\n\n* LD_LIBRARY_PATH\n* LIBPATH\n* PATH\n* TMPDIR\n\n当`externalBuilder`配置存在时，节点将会迭代按顺序排列的构建器的列表。调用`/bin/detect`直到其中的一个成功执行。\n如果没有构建器成功执行`detect`脚本，节点将会回滚使用初始的`Docker`通过节点实现构建进程。这说明外部的构建器是完全可选的。\n\n在上面的示例中，节点将试图使用`my-golang-builder`，如果无效的话则使用`noop-builder`，还是无效的话最后使用节点内部构建进程。\n\n## 链码包\nFabric 2.0引入了新的生命周期链码。链码包的格式从序列号协议缓冲消息变为了由`gzip`压缩的`POSIX tape`归档。链码包通过使用`peer lifecycle chaincode package`创建新的格式。\n\n### `Lifecycle`链码包内容\n\n`lifecycle`链码包包含两个文件，第一个文件`code.tar.gz`是一个使用`gzip`压缩的`POSIX tape`归档。这个文件包括链码的源归档。由节点`CLI`创建并将链码的实现源码放置在`src`文件夹下，链码的元数据(如CouchDB索引文件)放置在`META-INF`文件夹。\n\n第二个文件`metadata.json`是一个`JSON`格式的文档包含三个键：\n\n* `type`:链码的类型(例如`GOLANG`,`JAVA`,`NODE`)\n* `path`:对于go语言链码，则是`GOPATH`或者`GOMOD`到主链码包的相对路径，其他类型的链码未定义。\n* `label`:用于生成包ID的链码标签，在新的链码`lifecycle`过程中用于标识包的身份。\n\n`type`和`path`字段仅由`Docker`平台构建使用。\n\n### 链码包以及外部构建器\n\n当链码包安装在节点上后，`code.tar.gz`和`metadata.json`的内容将不能调用外部构建器处理。除了`label`字段用于新的`lifecycle`对包ID进行计算。为用户提供了很大的灵活性，使他们可以打包将由外部构建者和启动者处理的源和元数据。\n\n例如，可以构造一个自定义的链码包，该代码包在`code.tar.gz`中包含一个预编译的链码实现，并带有一个`metadata.json`文件，允许二进制构建包检测该自定义包，验证哈希值并作为链码运行。\n\n另一个示例是chaincode程序包，其中仅包含状态数据库索引定义以及外部启动程序连接到正在运行的`chaincode`服务器所需的数据。在这种情况下，`build`过程将仅从过程中提取元数据，然后将其`release`给节点。\n\n唯一的要求是`code.tar.gz`只能包含常规文件和目录条目，并且这些条目不能包含会导致文件写入链码包根路径逻辑外。","source":"_posts/blog/fabric/外部链码构建和运行.md","raw":"---\ntitle: Hyperledger Fabric外部链码构建与运行\ndate: 2019-12-24 19:42:58\ntags: fabric\ncategories: fabric应用\n---\n# 外部链码构建与运行\n\n* * *\n\n[官方文档](https://hyperledger-fabric.readthedocs.io/en/latest/cc_launcher.html)\n在Hyperledger Fabric 2.0版本之前，链码的构建和运行是节点实现的一部分，并且定制化是困难的。所有链码在节点上实例化是通过”构建“即根据语言指定的逻辑在节点上硬编码。构建过程将生成`Docker`容器镜像作为客户端连接节点用来运行可执行的链码。\n这种方法将链代码实现限制为只能使用几种语言实现，要求`Docker`成为部署环境的一部分，并阻止将链代码作为长时间运行的服务器进程运行。\n\n# 外部构建模式\n\nHyperledger Fabric外部构建器和启动器大致基于Heroku [Buildpacks](https://devcenter.heroku.com/articles/buildpack-api)。`buildpack`实现只是将程序归档转换为可以运行的程序或脚本的集合。`buildpack`模型已针对链码包进行了调整，并扩展为支持链码执行和发现。\n\n## 外部构建和运行API\n\n外部构建和运行由4个脚本程序组成：\n\n* `bin/detect`:确定是否应使用此`buildpack`来构建`chaincode`程序包并启动它\n* `bin/build`:转换链码包为可执行的链码\n* `bin/release(可选)`:为`peer`节点提供关于链码的元数据\n* `bin/run(可选)`:运行链码\n\n### `bin/detect`\n\n`bin/detect`脚本决定是否应使用此`buildpack`来构建`chaincode`程序包并启动它,`peer`节点通过两个参数调用`detect`:\n```\nbin/detect CHAINCOD_SOURCE_DIR CHAINCODE_METADATA_DIR\n```\n\n当`detect`被调用，`CHAINCOD_SOURCE_DIR`包含的链码资源以及`CHAINCODE_METADATA_DIR`包含的`metadata.json`文件将从链码包中安装到节点。`CHAINCOD_SOURCE_DIR`和`CHAINCODE_METADATA_DIR`应该被作为只读输入。如果将`buildpack`应用于`chaincode`源程序包，`detect`必须返回退出码0；否则，其他任何退出代码都将指示`buildpack`不应用内于`chaincode`源程序包。\n下面是一个简单的用于`go`语言链码的`detect`脚本例子：\n```\n#!/bin/bash\n\nCHAINCODE_METADATA_DIR=\"$2\"\n\n# 使用jq工具从metadata.json中获取链码类型，如果链码类型为golang，则成功退出\nif [ \"$(jq -r .type \"$CHAINCODE_METADATA_DIR/metadata.json\" | tr '[:upper:]' '[:lower:]')\" = \"golang\" ]; then\n    exit 0\nfi\n\nexit 1\n```\n\n### `bin/build`\n\n`bin/build`脚本用于构建，编译，或者转换链码包的内容到可以被`release`和`run`使用的类型。节点通过三个参数调用`build`:\n```\nbin/build CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR BUILD_OUTPUT_DIR\n```\n\n当`build`被调用，`CHAINCOD_SOURCE_DIR`包含的链码资源以及`CHAINCODE_METADATA_DIR`包含的`metadata.json`文件将从链码包中安装到节点。`BUILD_OUTPUT_DIR`是一个文件夹用于存放`release`和`run`需要的文件。`build`脚本应该将`CHAINCOD_SOURCE_DIR`和`CHAINCODE_METADATA_DIR`作为只读输入，`BUILD_OUTPUT_DIR`作为可写输出。\n\n下面是一个简单的用于`go`语言链码的`build`脚本例子：\n```\n#!/bin/bash\n\nCHAINCODE_SOURCE_DIR=\"$1\"\nCHAINCODE_METADATA_DIR=\"$2\"\nBUILD_OUTPUT_DIR=\"$3\"\n\n# 从 metadata.json获取包内容\nGO_PACKAGE_PATH=\"$(jq -r .path \"$CHAINCODE_METADATA_DIR/metadata.json\")\"\nif [ -f \"$CHAINCODE_SOURCE_DIR/src/go.mod\" ]; then\n    cd \"$CHAINCODE_SOURCE_DIR/src\"\n    go build -v -mod=readonly -o \"$BUILD_OUTPUT_DIR/chaincode\" \"$GO_PACKAGE_PATH\"\nelse\n    GO111MODULE=off go build -v  -o \"$BUILD_OUTPUT_DIR/chaincode\" \"$GO_PACKAGE_PATH\"\nfi\n\n# 存储状态数据库索引元数据提供给release\nif [ -d \"$CHAINCODE_SOURCE_DIR/META-INF\" ]; then\n    cp -a \"$CHAINCODE_SOURCE_DIR/META-INF\" \"$BUILD_OUTPUT_DIR/\"\nfi\n```\n\n### `bin/release`\n\n`bin/release`脚本为节点提供链码元数据。该脚本是可选的。如果没有提供，这一步将会跳过。节点通过两个参数调用`release`：\n```\nbin/release BUILD_OUTPUT_DIR RELEASE_OUTPUT_DIR\n```\n\n调用`release`时，`BUILD_OUTPUT_DIR`包含构建程序填充的归档，应将其视为只读输入。`RELEASE_OUTPUT_DIR`是`release`必须放置归档以供节点使用的目录。\n\n当`release`执行完成，节点将会从`RELEASE_OUTPUT_DIR`消费两种类型的元数据:\n\n* CouchDB定义的状态数据库索引。\n* 外部链码服务连接信息(`chaincode/server/connection.json`)\n\n如果链码要求CouchDB索引定义，`release`需要将索引放置在`RELEASE_OUTPUT_DIR`下的`state/couchdb/indexes`文件夹内。索引必须含有`.json`扩展名。\n\n在使用链码服务器实现的情况下，`release`负责使用链码服务器的地址以及与链码通信所需的任何TLS资产来填充`chaincode/server/connection.json`。将服务器连接信息提供给节点时，将不会调用`run`。\n\n下面是一个简单的用于`go`语言链码的`release`脚本例子：\n\n```\n#!/bin/bash\n\nBUILD_OUTPUT_DIR=\"$1\"\nRELEASE_OUTPUT_DIR=\"$2\"\n\n# 从 META-INF/* 拷贝索引文件到输出文件夹\nif [ -d \"$BUILD_OUTPUT_DIR/META-INF\" ] ; then\n   cp -a \"$BUILD_OUTPUT_DIR/META-INF/\"* \"$RELEASE_OUTPUT_DIR/\"\nfi\n```\n\n\n### `bin/run`\n\n`bin/run`脚本用于链码的运行。节点通过两个参数调用`run`：\n```\nbin/run BUILD_OUTPUT_DIR RUN_METADATA_DIR\n```\n\n当`BUILD_OUTPUT_DIR`包含`build`程序填充的归档，而`RUN_METADATA_DIR`包含有一个名为`chaincode.json`的文件，该文件包含链码连接和注册到节点所需的信息，`run`将被调用。`bin/run`脚本对于`BUILD_OUTPUT_DIR`以及`RUN_METADATA_DIR`文件夹应为只读输入。`chaincode.json`文件包含的关键信息有：\n\n* `chaincode_id:`连接到链码包的唯一ID\n* `peer_address:``peer`节点的`ChaincodeSupport`中的gRPC服务端点主机地址，格式为`host:port`.\n* `client_cert:`由`peer`生成的`PEM`编码的TLS客户端证书。当链码与节点建立连接时将会被使用。\n* `client_key:`由`peer`生成的`PEM`编码的客户端秘钥。当链码与节点建立连接时将会被使用。\n* `root_cert:`由`peer`节点的`ChaincodeSupport`中的gRPC服务端点主机使用的`PEM`编码的`TLS`根证书。\n\n当`run`停止时，与`peer`连接的链码也会终止。如果另一个请求访问链码，节点将会尝试通过调用`run`启动链码的另一个实例。在调用链码时，`chaincode.json`文件内容不能够被缓存。\n\n下面是一个简单的用于`go`语言链码的`run`脚本例子：\n\n```\n#!/bin/bash\n\nBUILD_OUTPUT_DIR=\"$1\"\nRUN_METADATA_DIR=\"$2\"\n\n# 为go语言链码shim包配置环境变量\nexport CORE_CHAINCODE_ID_NAME=\"$(jq -r .chaincode_id \"$RUN_METADATA_DIR/chaincode.json\")\"\nexport CORE_PEER_TLS_ENABLED=\"true\"\nexport CORE_TLS_CLIENT_CERT_FILE=\"$RUN_METADATA_DIR/client.crt\"\nexport CORE_TLS_CLIENT_KEY_FILE=\"$RUN_METADATA_DIR/client.key\"\nexport CORE_PEER_TLS_ROOTCERT_FILE=\"$RUN_METADATA_DIR/root.crt\"\n\n# 为go语言链码shim包获取秘钥和证书材料\njq -r .client_cert \"$RUN_METADATA_DIR/chaincode.json\" > \"$CORE_TLS_CLIENT_CERT_FILE\"\njq -r .client_key  \"$RUN_METADATA_DIR/chaincode.json\" > \"$CORE_TLS_CLIENT_KEY_FILE\"\njq -r .root_cert   \"$RUN_METADATA_DIR/chaincode.json\" > \"$CORE_PEER_TLS_ROOTCERT_FILE\"\nif [ -z \"$(jq -r .client_cert \"$RUN_METADATA_DIR/chaincode.json\")\" ]; then\n    export CORE_PEER_TLS_ENABLED=\"false\"\nfi\n\n# 执行链码并使用链码进程替代脚本\nexec \"$BUILD_OUTPUT_DIR/chaincode\" -peer.address=\"$(jq -r .peer_address \"$ARTIFACTS/chaincode.json\")\"\n```\n\n## 外部构建和运行的配置\n\n在`core.yaml`的`chaincode`配置区域下添加一个`externalBuilder`元素配置节点以使用外部构建器.每一个外部构建器的定义必须包含名字(用于日志)和包含构建器脚本的`bin`文件夹的上一级路径。\n\n调用外部构建器脚本时还可以从节点获取环境变量名称的可选列表。\n\n下面的示例定义了两个外部构建器：\n```\nchaincode:\n  externalBuilders:\n  - name: my-golang-builder\n    path: /builders/golang\n    environmentWhitelist:\n    - GOPROXY\n    - GONOPROXY\n    - GOSUMDB\n    - GONOSUMDB\n  - name: noop-builder\n    path: /builders/binary\n```\n\n在这个示例中，实现的构建器`my-golang-builder`被包含在`/builders/golang`文件夹内，它的脚本文件位于`/builders/golang/bin`.当节点调用任何与`my-golang-builder`相关的构建脚本时，将只会传播白名单内的环境变量的值。\n\n这些环境变量总是传播到外部构建器：\n\n* LD_LIBRARY_PATH\n* LIBPATH\n* PATH\n* TMPDIR\n\n当`externalBuilder`配置存在时，节点将会迭代按顺序排列的构建器的列表。调用`/bin/detect`直到其中的一个成功执行。\n如果没有构建器成功执行`detect`脚本，节点将会回滚使用初始的`Docker`通过节点实现构建进程。这说明外部的构建器是完全可选的。\n\n在上面的示例中，节点将试图使用`my-golang-builder`，如果无效的话则使用`noop-builder`，还是无效的话最后使用节点内部构建进程。\n\n## 链码包\nFabric 2.0引入了新的生命周期链码。链码包的格式从序列号协议缓冲消息变为了由`gzip`压缩的`POSIX tape`归档。链码包通过使用`peer lifecycle chaincode package`创建新的格式。\n\n### `Lifecycle`链码包内容\n\n`lifecycle`链码包包含两个文件，第一个文件`code.tar.gz`是一个使用`gzip`压缩的`POSIX tape`归档。这个文件包括链码的源归档。由节点`CLI`创建并将链码的实现源码放置在`src`文件夹下，链码的元数据(如CouchDB索引文件)放置在`META-INF`文件夹。\n\n第二个文件`metadata.json`是一个`JSON`格式的文档包含三个键：\n\n* `type`:链码的类型(例如`GOLANG`,`JAVA`,`NODE`)\n* `path`:对于go语言链码，则是`GOPATH`或者`GOMOD`到主链码包的相对路径，其他类型的链码未定义。\n* `label`:用于生成包ID的链码标签，在新的链码`lifecycle`过程中用于标识包的身份。\n\n`type`和`path`字段仅由`Docker`平台构建使用。\n\n### 链码包以及外部构建器\n\n当链码包安装在节点上后，`code.tar.gz`和`metadata.json`的内容将不能调用外部构建器处理。除了`label`字段用于新的`lifecycle`对包ID进行计算。为用户提供了很大的灵活性，使他们可以打包将由外部构建者和启动者处理的源和元数据。\n\n例如，可以构造一个自定义的链码包，该代码包在`code.tar.gz`中包含一个预编译的链码实现，并带有一个`metadata.json`文件，允许二进制构建包检测该自定义包，验证哈希值并作为链码运行。\n\n另一个示例是chaincode程序包，其中仅包含状态数据库索引定义以及外部启动程序连接到正在运行的`chaincode`服务器所需的数据。在这种情况下，`build`过程将仅从过程中提取元数据，然后将其`release`给节点。\n\n唯一的要求是`code.tar.gz`只能包含常规文件和目录条目，并且这些条目不能包含会导致文件写入链码包根路径逻辑外。","slug":"blog/fabric/外部链码构建和运行","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyjs003yk0vq038d3uci","content":"<h1 id=\"外部链码构建与运行\"><a href=\"#外部链码构建与运行\" class=\"headerlink\" title=\"外部链码构建与运行\"></a>外部链码构建与运行</h1><hr>\n<p><a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/cc_launcher.html\" target=\"_blank\" rel=\"noopener\">官方文档</a><br>在Hyperledger Fabric 2.0版本之前，链码的构建和运行是节点实现的一部分，并且定制化是困难的。所有链码在节点上实例化是通过”构建“即根据语言指定的逻辑在节点上硬编码。构建过程将生成<code>Docker</code>容器镜像作为客户端连接节点用来运行可执行的链码。<br>这种方法将链代码实现限制为只能使用几种语言实现，要求<code>Docker</code>成为部署环境的一部分，并阻止将链代码作为长时间运行的服务器进程运行。</p>\n<h1 id=\"外部构建模式\"><a href=\"#外部构建模式\" class=\"headerlink\" title=\"外部构建模式\"></a>外部构建模式</h1><p>Hyperledger Fabric外部构建器和启动器大致基于Heroku <a href=\"https://devcenter.heroku.com/articles/buildpack-api\" target=\"_blank\" rel=\"noopener\">Buildpacks</a>。<code>buildpack</code>实现只是将程序归档转换为可以运行的程序或脚本的集合。<code>buildpack</code>模型已针对链码包进行了调整，并扩展为支持链码执行和发现。</p>\n<h2 id=\"外部构建和运行API\"><a href=\"#外部构建和运行API\" class=\"headerlink\" title=\"外部构建和运行API\"></a>外部构建和运行API</h2><p>外部构建和运行由4个脚本程序组成：</p>\n<ul>\n<li><code>bin/detect</code>:确定是否应使用此<code>buildpack</code>来构建<code>chaincode</code>程序包并启动它</li>\n<li><code>bin/build</code>:转换链码包为可执行的链码</li>\n<li><code>bin/release(可选)</code>:为<code>peer</code>节点提供关于链码的元数据</li>\n<li><code>bin/run(可选)</code>:运行链码</li>\n</ul>\n<h3 id=\"bin-detect\"><a href=\"#bin-detect\" class=\"headerlink\" title=\"bin/detect\"></a><code>bin/detect</code></h3><p><code>bin/detect</code>脚本决定是否应使用此<code>buildpack</code>来构建<code>chaincode</code>程序包并启动它,<code>peer</code>节点通过两个参数调用<code>detect</code>:</p>\n<pre><code>bin/detect CHAINCOD_SOURCE_DIR CHAINCODE_METADATA_DIR</code></pre><p>当<code>detect</code>被调用，<code>CHAINCOD_SOURCE_DIR</code>包含的链码资源以及<code>CHAINCODE_METADATA_DIR</code>包含的<code>metadata.json</code>文件将从链码包中安装到节点。<code>CHAINCOD_SOURCE_DIR</code>和<code>CHAINCODE_METADATA_DIR</code>应该被作为只读输入。如果将<code>buildpack</code>应用于<code>chaincode</code>源程序包，<code>detect</code>必须返回退出码0；否则，其他任何退出代码都将指示<code>buildpack</code>不应用内于<code>chaincode</code>源程序包。<br>下面是一个简单的用于<code>go</code>语言链码的<code>detect</code>脚本例子：</p>\n<pre><code>#!/bin/bash\n\nCHAINCODE_METADATA_DIR=&quot;$2&quot;\n\n# 使用jq工具从metadata.json中获取链码类型，如果链码类型为golang，则成功退出\nif [ &quot;$(jq -r .type &quot;$CHAINCODE_METADATA_DIR/metadata.json&quot; | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39;)&quot; = &quot;golang&quot; ]; then\n    exit 0\nfi\n\nexit 1</code></pre><h3 id=\"bin-build\"><a href=\"#bin-build\" class=\"headerlink\" title=\"bin/build\"></a><code>bin/build</code></h3><p><code>bin/build</code>脚本用于构建，编译，或者转换链码包的内容到可以被<code>release</code>和<code>run</code>使用的类型。节点通过三个参数调用<code>build</code>:</p>\n<pre><code>bin/build CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR BUILD_OUTPUT_DIR</code></pre><p>当<code>build</code>被调用，<code>CHAINCOD_SOURCE_DIR</code>包含的链码资源以及<code>CHAINCODE_METADATA_DIR</code>包含的<code>metadata.json</code>文件将从链码包中安装到节点。<code>BUILD_OUTPUT_DIR</code>是一个文件夹用于存放<code>release</code>和<code>run</code>需要的文件。<code>build</code>脚本应该将<code>CHAINCOD_SOURCE_DIR</code>和<code>CHAINCODE_METADATA_DIR</code>作为只读输入，<code>BUILD_OUTPUT_DIR</code>作为可写输出。</p>\n<p>下面是一个简单的用于<code>go</code>语言链码的<code>build</code>脚本例子：</p>\n<pre><code>#!/bin/bash\n\nCHAINCODE_SOURCE_DIR=&quot;$1&quot;\nCHAINCODE_METADATA_DIR=&quot;$2&quot;\nBUILD_OUTPUT_DIR=&quot;$3&quot;\n\n# 从 metadata.json获取包内容\nGO_PACKAGE_PATH=&quot;$(jq -r .path &quot;$CHAINCODE_METADATA_DIR/metadata.json&quot;)&quot;\nif [ -f &quot;$CHAINCODE_SOURCE_DIR/src/go.mod&quot; ]; then\n    cd &quot;$CHAINCODE_SOURCE_DIR/src&quot;\n    go build -v -mod=readonly -o &quot;$BUILD_OUTPUT_DIR/chaincode&quot; &quot;$GO_PACKAGE_PATH&quot;\nelse\n    GO111MODULE=off go build -v  -o &quot;$BUILD_OUTPUT_DIR/chaincode&quot; &quot;$GO_PACKAGE_PATH&quot;\nfi\n\n# 存储状态数据库索引元数据提供给release\nif [ -d &quot;$CHAINCODE_SOURCE_DIR/META-INF&quot; ]; then\n    cp -a &quot;$CHAINCODE_SOURCE_DIR/META-INF&quot; &quot;$BUILD_OUTPUT_DIR/&quot;\nfi</code></pre><h3 id=\"bin-release\"><a href=\"#bin-release\" class=\"headerlink\" title=\"bin/release\"></a><code>bin/release</code></h3><p><code>bin/release</code>脚本为节点提供链码元数据。该脚本是可选的。如果没有提供，这一步将会跳过。节点通过两个参数调用<code>release</code>：</p>\n<pre><code>bin/release BUILD_OUTPUT_DIR RELEASE_OUTPUT_DIR</code></pre><p>调用<code>release</code>时，<code>BUILD_OUTPUT_DIR</code>包含构建程序填充的归档，应将其视为只读输入。<code>RELEASE_OUTPUT_DIR</code>是<code>release</code>必须放置归档以供节点使用的目录。</p>\n<p>当<code>release</code>执行完成，节点将会从<code>RELEASE_OUTPUT_DIR</code>消费两种类型的元数据:</p>\n<ul>\n<li>CouchDB定义的状态数据库索引。</li>\n<li>外部链码服务连接信息(<code>chaincode/server/connection.json</code>)</li>\n</ul>\n<p>如果链码要求CouchDB索引定义，<code>release</code>需要将索引放置在<code>RELEASE_OUTPUT_DIR</code>下的<code>state/couchdb/indexes</code>文件夹内。索引必须含有<code>.json</code>扩展名。</p>\n<p>在使用链码服务器实现的情况下，<code>release</code>负责使用链码服务器的地址以及与链码通信所需的任何TLS资产来填充<code>chaincode/server/connection.json</code>。将服务器连接信息提供给节点时，将不会调用<code>run</code>。</p>\n<p>下面是一个简单的用于<code>go</code>语言链码的<code>release</code>脚本例子：</p>\n<pre><code>#!/bin/bash\n\nBUILD_OUTPUT_DIR=&quot;$1&quot;\nRELEASE_OUTPUT_DIR=&quot;$2&quot;\n\n# 从 META-INF/* 拷贝索引文件到输出文件夹\nif [ -d &quot;$BUILD_OUTPUT_DIR/META-INF&quot; ] ; then\n   cp -a &quot;$BUILD_OUTPUT_DIR/META-INF/&quot;* &quot;$RELEASE_OUTPUT_DIR/&quot;\nfi</code></pre><h3 id=\"bin-run\"><a href=\"#bin-run\" class=\"headerlink\" title=\"bin/run\"></a><code>bin/run</code></h3><p><code>bin/run</code>脚本用于链码的运行。节点通过两个参数调用<code>run</code>：</p>\n<pre><code>bin/run BUILD_OUTPUT_DIR RUN_METADATA_DIR</code></pre><p>当<code>BUILD_OUTPUT_DIR</code>包含<code>build</code>程序填充的归档，而<code>RUN_METADATA_DIR</code>包含有一个名为<code>chaincode.json</code>的文件，该文件包含链码连接和注册到节点所需的信息，<code>run</code>将被调用。<code>bin/run</code>脚本对于<code>BUILD_OUTPUT_DIR</code>以及<code>RUN_METADATA_DIR</code>文件夹应为只读输入。<code>chaincode.json</code>文件包含的关键信息有：</p>\n<ul>\n<li><code>chaincode_id:</code>连接到链码包的唯一ID</li>\n<li><code>peer_address:``peer</code>节点的<code>ChaincodeSupport</code>中的gRPC服务端点主机地址，格式为<code>host:port</code>.</li>\n<li><code>client_cert:</code>由<code>peer</code>生成的<code>PEM</code>编码的TLS客户端证书。当链码与节点建立连接时将会被使用。</li>\n<li><code>client_key:</code>由<code>peer</code>生成的<code>PEM</code>编码的客户端秘钥。当链码与节点建立连接时将会被使用。</li>\n<li><code>root_cert:</code>由<code>peer</code>节点的<code>ChaincodeSupport</code>中的gRPC服务端点主机使用的<code>PEM</code>编码的<code>TLS</code>根证书。</li>\n</ul>\n<p>当<code>run</code>停止时，与<code>peer</code>连接的链码也会终止。如果另一个请求访问链码，节点将会尝试通过调用<code>run</code>启动链码的另一个实例。在调用链码时，<code>chaincode.json</code>文件内容不能够被缓存。</p>\n<p>下面是一个简单的用于<code>go</code>语言链码的<code>run</code>脚本例子：</p>\n<pre><code>#!/bin/bash\n\nBUILD_OUTPUT_DIR=&quot;$1&quot;\nRUN_METADATA_DIR=&quot;$2&quot;\n\n# 为go语言链码shim包配置环境变量\nexport CORE_CHAINCODE_ID_NAME=&quot;$(jq -r .chaincode_id &quot;$RUN_METADATA_DIR/chaincode.json&quot;)&quot;\nexport CORE_PEER_TLS_ENABLED=&quot;true&quot;\nexport CORE_TLS_CLIENT_CERT_FILE=&quot;$RUN_METADATA_DIR/client.crt&quot;\nexport CORE_TLS_CLIENT_KEY_FILE=&quot;$RUN_METADATA_DIR/client.key&quot;\nexport CORE_PEER_TLS_ROOTCERT_FILE=&quot;$RUN_METADATA_DIR/root.crt&quot;\n\n# 为go语言链码shim包获取秘钥和证书材料\njq -r .client_cert &quot;$RUN_METADATA_DIR/chaincode.json&quot; &gt; &quot;$CORE_TLS_CLIENT_CERT_FILE&quot;\njq -r .client_key  &quot;$RUN_METADATA_DIR/chaincode.json&quot; &gt; &quot;$CORE_TLS_CLIENT_KEY_FILE&quot;\njq -r .root_cert   &quot;$RUN_METADATA_DIR/chaincode.json&quot; &gt; &quot;$CORE_PEER_TLS_ROOTCERT_FILE&quot;\nif [ -z &quot;$(jq -r .client_cert &quot;$RUN_METADATA_DIR/chaincode.json&quot;)&quot; ]; then\n    export CORE_PEER_TLS_ENABLED=&quot;false&quot;\nfi\n\n# 执行链码并使用链码进程替代脚本\nexec &quot;$BUILD_OUTPUT_DIR/chaincode&quot; -peer.address=&quot;$(jq -r .peer_address &quot;$ARTIFACTS/chaincode.json&quot;)&quot;</code></pre><h2 id=\"外部构建和运行的配置\"><a href=\"#外部构建和运行的配置\" class=\"headerlink\" title=\"外部构建和运行的配置\"></a>外部构建和运行的配置</h2><p>在<code>core.yaml</code>的<code>chaincode</code>配置区域下添加一个<code>externalBuilder</code>元素配置节点以使用外部构建器.每一个外部构建器的定义必须包含名字(用于日志)和包含构建器脚本的<code>bin</code>文件夹的上一级路径。</p>\n<p>调用外部构建器脚本时还可以从节点获取环境变量名称的可选列表。</p>\n<p>下面的示例定义了两个外部构建器：</p>\n<pre><code>chaincode:\n  externalBuilders:\n  - name: my-golang-builder\n    path: /builders/golang\n    environmentWhitelist:\n    - GOPROXY\n    - GONOPROXY\n    - GOSUMDB\n    - GONOSUMDB\n  - name: noop-builder\n    path: /builders/binary</code></pre><p>在这个示例中，实现的构建器<code>my-golang-builder</code>被包含在<code>/builders/golang</code>文件夹内，它的脚本文件位于<code>/builders/golang/bin</code>.当节点调用任何与<code>my-golang-builder</code>相关的构建脚本时，将只会传播白名单内的环境变量的值。</p>\n<p>这些环境变量总是传播到外部构建器：</p>\n<ul>\n<li>LD_LIBRARY_PATH</li>\n<li>LIBPATH</li>\n<li>PATH</li>\n<li>TMPDIR</li>\n</ul>\n<p>当<code>externalBuilder</code>配置存在时，节点将会迭代按顺序排列的构建器的列表。调用<code>/bin/detect</code>直到其中的一个成功执行。<br>如果没有构建器成功执行<code>detect</code>脚本，节点将会回滚使用初始的<code>Docker</code>通过节点实现构建进程。这说明外部的构建器是完全可选的。</p>\n<p>在上面的示例中，节点将试图使用<code>my-golang-builder</code>，如果无效的话则使用<code>noop-builder</code>，还是无效的话最后使用节点内部构建进程。</p>\n<h2 id=\"链码包\"><a href=\"#链码包\" class=\"headerlink\" title=\"链码包\"></a>链码包</h2><p>Fabric 2.0引入了新的生命周期链码。链码包的格式从序列号协议缓冲消息变为了由<code>gzip</code>压缩的<code>POSIX tape</code>归档。链码包通过使用<code>peer lifecycle chaincode package</code>创建新的格式。</p>\n<h3 id=\"Lifecycle链码包内容\"><a href=\"#Lifecycle链码包内容\" class=\"headerlink\" title=\"Lifecycle链码包内容\"></a><code>Lifecycle</code>链码包内容</h3><p><code>lifecycle</code>链码包包含两个文件，第一个文件<code>code.tar.gz</code>是一个使用<code>gzip</code>压缩的<code>POSIX tape</code>归档。这个文件包括链码的源归档。由节点<code>CLI</code>创建并将链码的实现源码放置在<code>src</code>文件夹下，链码的元数据(如CouchDB索引文件)放置在<code>META-INF</code>文件夹。</p>\n<p>第二个文件<code>metadata.json</code>是一个<code>JSON</code>格式的文档包含三个键：</p>\n<ul>\n<li><code>type</code>:链码的类型(例如<code>GOLANG</code>,<code>JAVA</code>,<code>NODE</code>)</li>\n<li><code>path</code>:对于go语言链码，则是<code>GOPATH</code>或者<code>GOMOD</code>到主链码包的相对路径，其他类型的链码未定义。</li>\n<li><code>label</code>:用于生成包ID的链码标签，在新的链码<code>lifecycle</code>过程中用于标识包的身份。</li>\n</ul>\n<p><code>type</code>和<code>path</code>字段仅由<code>Docker</code>平台构建使用。</p>\n<h3 id=\"链码包以及外部构建器\"><a href=\"#链码包以及外部构建器\" class=\"headerlink\" title=\"链码包以及外部构建器\"></a>链码包以及外部构建器</h3><p>当链码包安装在节点上后，<code>code.tar.gz</code>和<code>metadata.json</code>的内容将不能调用外部构建器处理。除了<code>label</code>字段用于新的<code>lifecycle</code>对包ID进行计算。为用户提供了很大的灵活性，使他们可以打包将由外部构建者和启动者处理的源和元数据。</p>\n<p>例如，可以构造一个自定义的链码包，该代码包在<code>code.tar.gz</code>中包含一个预编译的链码实现，并带有一个<code>metadata.json</code>文件，允许二进制构建包检测该自定义包，验证哈希值并作为链码运行。</p>\n<p>另一个示例是chaincode程序包，其中仅包含状态数据库索引定义以及外部启动程序连接到正在运行的<code>chaincode</code>服务器所需的数据。在这种情况下，<code>build</code>过程将仅从过程中提取元数据，然后将其<code>release</code>给节点。</p>\n<p>唯一的要求是<code>code.tar.gz</code>只能包含常规文件和目录条目，并且这些条目不能包含会导致文件写入链码包根路径逻辑外。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"外部链码构建与运行\"><a href=\"#外部链码构建与运行\" class=\"headerlink\" title=\"外部链码构建与运行\"></a>外部链码构建与运行</h1><hr>\n<p><a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/cc_launcher.html\" target=\"_blank\" rel=\"noopener\">官方文档</a><br>在Hyperledger Fabric 2.0版本之前，链码的构建和运行是节点实现的一部分，并且定制化是困难的。所有链码在节点上实例化是通过”构建“即根据语言指定的逻辑在节点上硬编码。构建过程将生成<code>Docker</code>容器镜像作为客户端连接节点用来运行可执行的链码。<br>这种方法将链代码实现限制为只能使用几种语言实现，要求<code>Docker</code>成为部署环境的一部分，并阻止将链代码作为长时间运行的服务器进程运行。</p>\n<h1 id=\"外部构建模式\"><a href=\"#外部构建模式\" class=\"headerlink\" title=\"外部构建模式\"></a>外部构建模式</h1><p>Hyperledger Fabric外部构建器和启动器大致基于Heroku <a href=\"https://devcenter.heroku.com/articles/buildpack-api\" target=\"_blank\" rel=\"noopener\">Buildpacks</a>。<code>buildpack</code>实现只是将程序归档转换为可以运行的程序或脚本的集合。<code>buildpack</code>模型已针对链码包进行了调整，并扩展为支持链码执行和发现。</p>\n<h2 id=\"外部构建和运行API\"><a href=\"#外部构建和运行API\" class=\"headerlink\" title=\"外部构建和运行API\"></a>外部构建和运行API</h2><p>外部构建和运行由4个脚本程序组成：</p>\n<ul>\n<li><code>bin/detect</code>:确定是否应使用此<code>buildpack</code>来构建<code>chaincode</code>程序包并启动它</li>\n<li><code>bin/build</code>:转换链码包为可执行的链码</li>\n<li><code>bin/release(可选)</code>:为<code>peer</code>节点提供关于链码的元数据</li>\n<li><code>bin/run(可选)</code>:运行链码</li>\n</ul>\n<h3 id=\"bin-detect\"><a href=\"#bin-detect\" class=\"headerlink\" title=\"bin/detect\"></a><code>bin/detect</code></h3><p><code>bin/detect</code>脚本决定是否应使用此<code>buildpack</code>来构建<code>chaincode</code>程序包并启动它,<code>peer</code>节点通过两个参数调用<code>detect</code>:</p>\n<pre><code>bin/detect CHAINCOD_SOURCE_DIR CHAINCODE_METADATA_DIR</code></pre><p>当<code>detect</code>被调用，<code>CHAINCOD_SOURCE_DIR</code>包含的链码资源以及<code>CHAINCODE_METADATA_DIR</code>包含的<code>metadata.json</code>文件将从链码包中安装到节点。<code>CHAINCOD_SOURCE_DIR</code>和<code>CHAINCODE_METADATA_DIR</code>应该被作为只读输入。如果将<code>buildpack</code>应用于<code>chaincode</code>源程序包，<code>detect</code>必须返回退出码0；否则，其他任何退出代码都将指示<code>buildpack</code>不应用内于<code>chaincode</code>源程序包。<br>下面是一个简单的用于<code>go</code>语言链码的<code>detect</code>脚本例子：</p>\n<pre><code>#!/bin/bash\n\nCHAINCODE_METADATA_DIR=&quot;$2&quot;\n\n# 使用jq工具从metadata.json中获取链码类型，如果链码类型为golang，则成功退出\nif [ &quot;$(jq -r .type &quot;$CHAINCODE_METADATA_DIR/metadata.json&quot; | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39;)&quot; = &quot;golang&quot; ]; then\n    exit 0\nfi\n\nexit 1</code></pre><h3 id=\"bin-build\"><a href=\"#bin-build\" class=\"headerlink\" title=\"bin/build\"></a><code>bin/build</code></h3><p><code>bin/build</code>脚本用于构建，编译，或者转换链码包的内容到可以被<code>release</code>和<code>run</code>使用的类型。节点通过三个参数调用<code>build</code>:</p>\n<pre><code>bin/build CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR BUILD_OUTPUT_DIR</code></pre><p>当<code>build</code>被调用，<code>CHAINCOD_SOURCE_DIR</code>包含的链码资源以及<code>CHAINCODE_METADATA_DIR</code>包含的<code>metadata.json</code>文件将从链码包中安装到节点。<code>BUILD_OUTPUT_DIR</code>是一个文件夹用于存放<code>release</code>和<code>run</code>需要的文件。<code>build</code>脚本应该将<code>CHAINCOD_SOURCE_DIR</code>和<code>CHAINCODE_METADATA_DIR</code>作为只读输入，<code>BUILD_OUTPUT_DIR</code>作为可写输出。</p>\n<p>下面是一个简单的用于<code>go</code>语言链码的<code>build</code>脚本例子：</p>\n<pre><code>#!/bin/bash\n\nCHAINCODE_SOURCE_DIR=&quot;$1&quot;\nCHAINCODE_METADATA_DIR=&quot;$2&quot;\nBUILD_OUTPUT_DIR=&quot;$3&quot;\n\n# 从 metadata.json获取包内容\nGO_PACKAGE_PATH=&quot;$(jq -r .path &quot;$CHAINCODE_METADATA_DIR/metadata.json&quot;)&quot;\nif [ -f &quot;$CHAINCODE_SOURCE_DIR/src/go.mod&quot; ]; then\n    cd &quot;$CHAINCODE_SOURCE_DIR/src&quot;\n    go build -v -mod=readonly -o &quot;$BUILD_OUTPUT_DIR/chaincode&quot; &quot;$GO_PACKAGE_PATH&quot;\nelse\n    GO111MODULE=off go build -v  -o &quot;$BUILD_OUTPUT_DIR/chaincode&quot; &quot;$GO_PACKAGE_PATH&quot;\nfi\n\n# 存储状态数据库索引元数据提供给release\nif [ -d &quot;$CHAINCODE_SOURCE_DIR/META-INF&quot; ]; then\n    cp -a &quot;$CHAINCODE_SOURCE_DIR/META-INF&quot; &quot;$BUILD_OUTPUT_DIR/&quot;\nfi</code></pre><h3 id=\"bin-release\"><a href=\"#bin-release\" class=\"headerlink\" title=\"bin/release\"></a><code>bin/release</code></h3><p><code>bin/release</code>脚本为节点提供链码元数据。该脚本是可选的。如果没有提供，这一步将会跳过。节点通过两个参数调用<code>release</code>：</p>\n<pre><code>bin/release BUILD_OUTPUT_DIR RELEASE_OUTPUT_DIR</code></pre><p>调用<code>release</code>时，<code>BUILD_OUTPUT_DIR</code>包含构建程序填充的归档，应将其视为只读输入。<code>RELEASE_OUTPUT_DIR</code>是<code>release</code>必须放置归档以供节点使用的目录。</p>\n<p>当<code>release</code>执行完成，节点将会从<code>RELEASE_OUTPUT_DIR</code>消费两种类型的元数据:</p>\n<ul>\n<li>CouchDB定义的状态数据库索引。</li>\n<li>外部链码服务连接信息(<code>chaincode/server/connection.json</code>)</li>\n</ul>\n<p>如果链码要求CouchDB索引定义，<code>release</code>需要将索引放置在<code>RELEASE_OUTPUT_DIR</code>下的<code>state/couchdb/indexes</code>文件夹内。索引必须含有<code>.json</code>扩展名。</p>\n<p>在使用链码服务器实现的情况下，<code>release</code>负责使用链码服务器的地址以及与链码通信所需的任何TLS资产来填充<code>chaincode/server/connection.json</code>。将服务器连接信息提供给节点时，将不会调用<code>run</code>。</p>\n<p>下面是一个简单的用于<code>go</code>语言链码的<code>release</code>脚本例子：</p>\n<pre><code>#!/bin/bash\n\nBUILD_OUTPUT_DIR=&quot;$1&quot;\nRELEASE_OUTPUT_DIR=&quot;$2&quot;\n\n# 从 META-INF/* 拷贝索引文件到输出文件夹\nif [ -d &quot;$BUILD_OUTPUT_DIR/META-INF&quot; ] ; then\n   cp -a &quot;$BUILD_OUTPUT_DIR/META-INF/&quot;* &quot;$RELEASE_OUTPUT_DIR/&quot;\nfi</code></pre><h3 id=\"bin-run\"><a href=\"#bin-run\" class=\"headerlink\" title=\"bin/run\"></a><code>bin/run</code></h3><p><code>bin/run</code>脚本用于链码的运行。节点通过两个参数调用<code>run</code>：</p>\n<pre><code>bin/run BUILD_OUTPUT_DIR RUN_METADATA_DIR</code></pre><p>当<code>BUILD_OUTPUT_DIR</code>包含<code>build</code>程序填充的归档，而<code>RUN_METADATA_DIR</code>包含有一个名为<code>chaincode.json</code>的文件，该文件包含链码连接和注册到节点所需的信息，<code>run</code>将被调用。<code>bin/run</code>脚本对于<code>BUILD_OUTPUT_DIR</code>以及<code>RUN_METADATA_DIR</code>文件夹应为只读输入。<code>chaincode.json</code>文件包含的关键信息有：</p>\n<ul>\n<li><code>chaincode_id:</code>连接到链码包的唯一ID</li>\n<li><code>peer_address:``peer</code>节点的<code>ChaincodeSupport</code>中的gRPC服务端点主机地址，格式为<code>host:port</code>.</li>\n<li><code>client_cert:</code>由<code>peer</code>生成的<code>PEM</code>编码的TLS客户端证书。当链码与节点建立连接时将会被使用。</li>\n<li><code>client_key:</code>由<code>peer</code>生成的<code>PEM</code>编码的客户端秘钥。当链码与节点建立连接时将会被使用。</li>\n<li><code>root_cert:</code>由<code>peer</code>节点的<code>ChaincodeSupport</code>中的gRPC服务端点主机使用的<code>PEM</code>编码的<code>TLS</code>根证书。</li>\n</ul>\n<p>当<code>run</code>停止时，与<code>peer</code>连接的链码也会终止。如果另一个请求访问链码，节点将会尝试通过调用<code>run</code>启动链码的另一个实例。在调用链码时，<code>chaincode.json</code>文件内容不能够被缓存。</p>\n<p>下面是一个简单的用于<code>go</code>语言链码的<code>run</code>脚本例子：</p>\n<pre><code>#!/bin/bash\n\nBUILD_OUTPUT_DIR=&quot;$1&quot;\nRUN_METADATA_DIR=&quot;$2&quot;\n\n# 为go语言链码shim包配置环境变量\nexport CORE_CHAINCODE_ID_NAME=&quot;$(jq -r .chaincode_id &quot;$RUN_METADATA_DIR/chaincode.json&quot;)&quot;\nexport CORE_PEER_TLS_ENABLED=&quot;true&quot;\nexport CORE_TLS_CLIENT_CERT_FILE=&quot;$RUN_METADATA_DIR/client.crt&quot;\nexport CORE_TLS_CLIENT_KEY_FILE=&quot;$RUN_METADATA_DIR/client.key&quot;\nexport CORE_PEER_TLS_ROOTCERT_FILE=&quot;$RUN_METADATA_DIR/root.crt&quot;\n\n# 为go语言链码shim包获取秘钥和证书材料\njq -r .client_cert &quot;$RUN_METADATA_DIR/chaincode.json&quot; &gt; &quot;$CORE_TLS_CLIENT_CERT_FILE&quot;\njq -r .client_key  &quot;$RUN_METADATA_DIR/chaincode.json&quot; &gt; &quot;$CORE_TLS_CLIENT_KEY_FILE&quot;\njq -r .root_cert   &quot;$RUN_METADATA_DIR/chaincode.json&quot; &gt; &quot;$CORE_PEER_TLS_ROOTCERT_FILE&quot;\nif [ -z &quot;$(jq -r .client_cert &quot;$RUN_METADATA_DIR/chaincode.json&quot;)&quot; ]; then\n    export CORE_PEER_TLS_ENABLED=&quot;false&quot;\nfi\n\n# 执行链码并使用链码进程替代脚本\nexec &quot;$BUILD_OUTPUT_DIR/chaincode&quot; -peer.address=&quot;$(jq -r .peer_address &quot;$ARTIFACTS/chaincode.json&quot;)&quot;</code></pre><h2 id=\"外部构建和运行的配置\"><a href=\"#外部构建和运行的配置\" class=\"headerlink\" title=\"外部构建和运行的配置\"></a>外部构建和运行的配置</h2><p>在<code>core.yaml</code>的<code>chaincode</code>配置区域下添加一个<code>externalBuilder</code>元素配置节点以使用外部构建器.每一个外部构建器的定义必须包含名字(用于日志)和包含构建器脚本的<code>bin</code>文件夹的上一级路径。</p>\n<p>调用外部构建器脚本时还可以从节点获取环境变量名称的可选列表。</p>\n<p>下面的示例定义了两个外部构建器：</p>\n<pre><code>chaincode:\n  externalBuilders:\n  - name: my-golang-builder\n    path: /builders/golang\n    environmentWhitelist:\n    - GOPROXY\n    - GONOPROXY\n    - GOSUMDB\n    - GONOSUMDB\n  - name: noop-builder\n    path: /builders/binary</code></pre><p>在这个示例中，实现的构建器<code>my-golang-builder</code>被包含在<code>/builders/golang</code>文件夹内，它的脚本文件位于<code>/builders/golang/bin</code>.当节点调用任何与<code>my-golang-builder</code>相关的构建脚本时，将只会传播白名单内的环境变量的值。</p>\n<p>这些环境变量总是传播到外部构建器：</p>\n<ul>\n<li>LD_LIBRARY_PATH</li>\n<li>LIBPATH</li>\n<li>PATH</li>\n<li>TMPDIR</li>\n</ul>\n<p>当<code>externalBuilder</code>配置存在时，节点将会迭代按顺序排列的构建器的列表。调用<code>/bin/detect</code>直到其中的一个成功执行。<br>如果没有构建器成功执行<code>detect</code>脚本，节点将会回滚使用初始的<code>Docker</code>通过节点实现构建进程。这说明外部的构建器是完全可选的。</p>\n<p>在上面的示例中，节点将试图使用<code>my-golang-builder</code>，如果无效的话则使用<code>noop-builder</code>，还是无效的话最后使用节点内部构建进程。</p>\n<h2 id=\"链码包\"><a href=\"#链码包\" class=\"headerlink\" title=\"链码包\"></a>链码包</h2><p>Fabric 2.0引入了新的生命周期链码。链码包的格式从序列号协议缓冲消息变为了由<code>gzip</code>压缩的<code>POSIX tape</code>归档。链码包通过使用<code>peer lifecycle chaincode package</code>创建新的格式。</p>\n<h3 id=\"Lifecycle链码包内容\"><a href=\"#Lifecycle链码包内容\" class=\"headerlink\" title=\"Lifecycle链码包内容\"></a><code>Lifecycle</code>链码包内容</h3><p><code>lifecycle</code>链码包包含两个文件，第一个文件<code>code.tar.gz</code>是一个使用<code>gzip</code>压缩的<code>POSIX tape</code>归档。这个文件包括链码的源归档。由节点<code>CLI</code>创建并将链码的实现源码放置在<code>src</code>文件夹下，链码的元数据(如CouchDB索引文件)放置在<code>META-INF</code>文件夹。</p>\n<p>第二个文件<code>metadata.json</code>是一个<code>JSON</code>格式的文档包含三个键：</p>\n<ul>\n<li><code>type</code>:链码的类型(例如<code>GOLANG</code>,<code>JAVA</code>,<code>NODE</code>)</li>\n<li><code>path</code>:对于go语言链码，则是<code>GOPATH</code>或者<code>GOMOD</code>到主链码包的相对路径，其他类型的链码未定义。</li>\n<li><code>label</code>:用于生成包ID的链码标签，在新的链码<code>lifecycle</code>过程中用于标识包的身份。</li>\n</ul>\n<p><code>type</code>和<code>path</code>字段仅由<code>Docker</code>平台构建使用。</p>\n<h3 id=\"链码包以及外部构建器\"><a href=\"#链码包以及外部构建器\" class=\"headerlink\" title=\"链码包以及外部构建器\"></a>链码包以及外部构建器</h3><p>当链码包安装在节点上后，<code>code.tar.gz</code>和<code>metadata.json</code>的内容将不能调用外部构建器处理。除了<code>label</code>字段用于新的<code>lifecycle</code>对包ID进行计算。为用户提供了很大的灵活性，使他们可以打包将由外部构建者和启动者处理的源和元数据。</p>\n<p>例如，可以构造一个自定义的链码包，该代码包在<code>code.tar.gz</code>中包含一个预编译的链码实现，并带有一个<code>metadata.json</code>文件，允许二进制构建包检测该自定义包，验证哈希值并作为链码运行。</p>\n<p>另一个示例是chaincode程序包，其中仅包含状态数据库索引定义以及外部启动程序连接到正在运行的<code>chaincode</code>服务器所需的数据。在这种情况下，<code>build</code>过程将仅从过程中提取元数据，然后将其<code>release</code>给节点。</p>\n<p>唯一的要求是<code>code.tar.gz</code>只能包含常规文件和目录条目，并且这些条目不能包含会导致文件写入链码包根路径逻辑外。</p>\n"},{"title":"深入解析Hyperledger Fabric搭建的全过程","date":"2019-11-23T10:46:49.000Z","_content":"在这篇文章中，使用``fabric-samples/first-network``中的文件进行fabric网络(solo类型的网络)搭建全过程的解析。如有错误欢迎批评指正。\n至于Fabric网络的搭建这里不再介绍，可以参考这一篇文章[Hyperledger Fabric环境搭建过程](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\nfabric网络：单机，solo类型，两个组织，分别有两个节点\n首先看一下该文件夹内有哪些文件：\n```\nbase                  connection-org2.json    docker-compose-cli.yaml           docker-compose-org3.yaml\nbyfn.sh               connection-org2.yaml    docker-compose-couch-org3.yaml    eyfn.sh\nchannel-artifacts     connection-org3.json    docker-compose-couch.yaml         org3-artifacts\nconfigtx.yaml         connection-org3.yaml    docker-compose-e2e-template.yaml  README.md\nconnection-org1.json  crypto-config.yaml      docker-compose-etcdraft2.yaml     scripts\nconnection-org1.yaml  docker-compose-ca.yaml  docker-compose-kafka.yaml\n```\n将本次用不到的文件删除，剩余的文件：\n```\n.\n├── base\n│   ├── docker-compose-base.yaml\n│   └── peer-base.yaml\n├── channel-artifacts\n├── configtx.yaml\n├── crypto-config.yaml\n├── docker-compose-cli.yaml\n├── docker-compose-couch.yaml\n├── docker-compose-e2e-template.yaml\n\n```\n## 1.证书的生成\n在Fabric网络环境中，第一步需要生成各个节点的证书文件，所用到的配置文件为``crypto-config.yaml``，说明一下文件内各字段的意义：\n```\nOrdererOrgs:    #定义一个Order组织\n  - Name: Orderer    #order节点的名称,当前网络模式为solo类型，所以只定义了一个Order节点\n    Domain: example.com    #order节点的域\n    Specs:      #暂时用不到\n      - Hostname: orderer\n      - Hostname: orderer2\n      - Hostname: orderer3\n      - Hostname: orderer4\n      - Hostname: orderer5\n\nPeerOrgs:      #定义Peer组织\n  - Name: Org1      #声明Peer组织名称为Org1\n    Domain: org1.example.com    #Org1组织的域\n    EnableNodeOUs: true    #暂时没搞清楚该字段的意义\n    Template:       #在这里可以定义所生成的Org1组织中的Peer节点证书数量，不包括Admin\n      Count: 2      #表明需要生成两个Peer节点的证书，如果需要其他数量的Peer节点，只需要更改这里的数量。\n    Users:        #在这里可以定义所生成的Org1组织中类型为User的证书数量，不包括Admin\n      Count: 1    #生成用户的证书的数量\n\n  - Name: Org2   #声明第二个Peer组织名称为Org2，如果需要更多的Peer组织证书，只需要按该模板添加即可。\n    Domain: org2.example.com  #与以上相同 \n    EnableNodeOUs: true\n    Template:\n      Count: 2\n    Users:\n      Count: 1\n```\n我们这里就使用两个组织，每个组织分别有两个节点和一个User。接下来我们使用该文件生成对应数量的证书：\n```\n#路径需要更改为自己的路径\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/  \n#在这里可能会报错，通常是权限问题，可以添加sudo重新执行\ncryptogen generate --config=./crypto-config.yaml\n#执行完毕后，当前文件夹下会出现一个新的文件夹：crypto-config，在该文件夹下就是刚刚生成的证书.\n```\n文件夹内证书不再详解，会在另一篇文章中专门解释Fabric-ca的内容。\n## 2 生成创世区块，通道配置，锚节点配置文件\n在这里需要用到``configtxgen``这个二进制文件。\n#### 2.1生成创世区块 \n```\n#首先进入文件夹\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/  \n#执行命令生成创世区块 \nconfigtxgen -profile TwoOrgsOrdererGenesis -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n#如果没有channel-artifacts这个文件夹，则需要手动去创建\n```\n如果没有出现错误的话，在``channel-artifacts``文件夹中可以看至生成的``genesis.block``文件。\n#### 2.2生成通道配置信息\n```\n#执行命令生成通道配置信息\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel\n```\n同样，在``channel-artifacts``文件夹中可以看至生成的``channel.tx``文件。\n#### 2.3生成锚节点配置文件 \n```\n#首先生成Org1的锚节点配置文件\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID mychannel -asOrg Org1MSP\n#生成Org2的锚节点配置文件\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID mychannel -asOrg Org2MSP\n```\n所有需要的配置文件全部建立完成，在``channel-artifacts``中应该有以下几个文件:\n```\nchannel.tx  genesis.block  Org1MSPanchors.tx  Org2MSPanchors.tx\n```\n[启动网络]:##3启动网络\n## 3启动网络\n到了这一步，可以启动网络了。\n```\n#首先进入``fabric-samples/first-network``文件夹。\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/\n#启动容器\nsudo docker-compose -f docker-compose-cli.yaml up -d\n```\n执行以下命令查看容器是否启动成功:\n```\nsudo docker ps\n#如果可以看到如下信息说明启动成功\nCONTAINER ID        IMAGE                               COMMAND             CREATED             STATUS              PORTS                      NAMES\n17d79586b1b7        hyperledger/fabric-tools:latest     \"/bin/bash\"         30 seconds ago      Up 28 seconds                                  cli\n0f4adb6b578e        hyperledger/fabric-orderer:latest   \"orderer\"           57 seconds ago      Up 29 seconds       0.0.0.0:7050->7050/tcp     orderer.example.com\ne2795ea9d43b        hyperledger/fabric-peer:latest      \"peer node start\"   57 seconds ago      Up 30 seconds       0.0.0.0:10051->10051/tcp   peer1.org2.example.com\n247a6e4fdd62        hyperledger/fabric-peer:latest      \"peer node start\"   57 seconds ago      Up 30 seconds       0.0.0.0:9051->9051/tcp     peer0.org2.example.com\nad4af3309e8c        hyperledger/fabric-peer:latest      \"peer node start\"   57 seconds ago      Up 31 seconds       0.0.0.0:8051->8051/tcp     peer1.org1.example.com\nf6d25896b517        hyperledger/fabric-peer:latest      \"peer node start\"   58 seconds ago      Up 40 seconds       0.0.0.0:7051->7051/tcp     peer0.org1.example.com\n```\n#### 3.1创建通道\n创建通道需要进入cli容器：\n```\nsudo docker exec -it cli bash\n#看到光标前的信息变为\nroot@17d79586b1b7:/opt/gopath/src/github.com/hyperledger/fabric/peer# \n#则成功进入容器\n```\n首先配置环境变量：\n```\n#当前cli容器默认配置是节点peer0,所以不需要其他配置信息\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#创建通道信息\npeer channel create -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/channel.tx --tls true --cafile $ORDERER_CA\n#看到如下信息说明创建通道成功\n2019-06-20 13:05:55.829 UTC [channelCmd] InitCmdFactory -> INFO 001 Endorser and orderer connections initialized\n2019-06-20 13:05:55.926 UTC [cli.common] readBlock -> INFO 002 Received block: 0\n#将生成的文件移动到channel-artifacts文件夹中\nmv mychannel.block channel-artifacts/\n```\n#### 3.2加入通道\n```\n#因为当前cli容器使用的是peer0的配置，所以可以直接将peer0加入通道 \n peer channel join -b channel-artifacts/mychannel.block\n#更新环境变量使其他节点也加入通道\n#=========peer1.org1===========  注意这里端口要与上面文件中配置的端口号相同\nCORE_PEER_ADDRESS=peer1.org1.example.com:8051  \n#=========peer0.org2============\nCORE_PEER_LOCALMSPID=\"Org2MSP\"\nCORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nCORE_PEER_ADDRESS=peer0.org2.example.com:9051\npeer channel join -b channel-artifacts/mychannel.block \n#=========peer1.org2=============\nCORE_PEER_ADDRESS=peer1.org2.example.com:10051\npeer channel join -b channel-artifacts/mychannel.block\n#退出容器\nexit\n```\n#### 3.3更新锚节点 \n```\n#重新进入容器\nsudo docker exec -it cli bash\n#更新环境变量\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#========Org1================\npeer channel update -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/Org1MSPanchors.tx --tls true --cafile $ORDERER_CA\n#========Org2================\nCORE_PEER_LOCALMSPID=\"Org2MSP\"\nCORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nCORE_PEER_ADDRESS=peer0.org2.example.com:9051\npeer channel update -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/Org2MSPanchors.tx --tls true --cafile $ORDERER_CA\n#退出容器\nexit\n```\n#### 3.4安装链码\n```\n#链码的安装仍然需要在所有节点上进行操作\n#进入容器\nsudo docker exec -it cli bash\n#更新环境变量\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#=========peer0.org1=========== \n#这里很有可能会出现路径不存在的错误，解决方法是在容器内找到对应的链码所在位置，然后替换当前链码路径\n##比如本文中链码路径为/opt/gopath/src/github.com/chaincode/chaincode_example02/go\n##则可以将以下命令的链码路径更改为github.com/chaincode/chaincode_example02\n\npeer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02\n#实例化链码 该步骤创建了a,b两个账户，其中a账户余额定义为100，b账户余额定义为200\npeer chaincode instantiate -o orderer.example.com:7050 --tls true --cafile $ORDERER_CA -C mychannel -n mycc -v 1.0 -c '{\"Args\":[\"init\",\"a\",\"100\",\"b\",\"200\"]}' -P \"OR      ('Org1MSP.member','Org2MSP.member')\"\n#这一步执行完毕后可以在其他节点上也安装链码，具体环境变量配置见本文中4.2\n```\n#### 3.5调用链码\n```\n#以peer0.org1为例\n#首先进入cli容器\nsudo docker exec -it cli bash\n#执行以下命令进行查询a账户余额\npeer chaincode query -C mychannel -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n#如果命令行输出100说明链码成功调用.\n\n#接下来我们发起一笔交易：通过peer0.org1节点将a账户余额转账给b20\npeer chaincode invoke -o orderer.example.com:7050  --tls true --cafile $ORDERER_CA -C mychannel -n mycc -c '{\"Args\":[\"invoke\",\"a\",\"b\",\"10\"]}'\n#然后登陆peer1.org1节点进行查询\nCORE_PEER_ADDRESS=peer1.org1.example.com:8051 \npeer chaincode query -C mychannel -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n#如果输出结果为:80\n说明Fabric网络手动搭建成功\n#退出容器\nexit\n```\n最后关闭网络：\n```\nsudo docker-compose -f docker-compose-cli.yaml down --volumes \n#删除生成的文件，下次启动网络需要重新生成\nsudo rm -r channel-artifacts crypto-config\n```\n## 4总结\n本文并没有使用CouchDb作为fabric网络的数据库，准备放到下一篇多机搭建Fabric网络中一起讲解。到这里，整个网络的手动搭建过程已经完成，希望大家能够有所收获。","source":"_posts/blog/fabric/深入解析Fabric搭建的全过程.md","raw":"---\ntitle: 深入解析Hyperledger Fabric搭建的全过程\ndate: 2019-11-23 18:46:49\ntags: fabric\ncategories: fabric应用\n---\n在这篇文章中，使用``fabric-samples/first-network``中的文件进行fabric网络(solo类型的网络)搭建全过程的解析。如有错误欢迎批评指正。\n至于Fabric网络的搭建这里不再介绍，可以参考这一篇文章[Hyperledger Fabric环境搭建过程](https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\nfabric网络：单机，solo类型，两个组织，分别有两个节点\n首先看一下该文件夹内有哪些文件：\n```\nbase                  connection-org2.json    docker-compose-cli.yaml           docker-compose-org3.yaml\nbyfn.sh               connection-org2.yaml    docker-compose-couch-org3.yaml    eyfn.sh\nchannel-artifacts     connection-org3.json    docker-compose-couch.yaml         org3-artifacts\nconfigtx.yaml         connection-org3.yaml    docker-compose-e2e-template.yaml  README.md\nconnection-org1.json  crypto-config.yaml      docker-compose-etcdraft2.yaml     scripts\nconnection-org1.yaml  docker-compose-ca.yaml  docker-compose-kafka.yaml\n```\n将本次用不到的文件删除，剩余的文件：\n```\n.\n├── base\n│   ├── docker-compose-base.yaml\n│   └── peer-base.yaml\n├── channel-artifacts\n├── configtx.yaml\n├── crypto-config.yaml\n├── docker-compose-cli.yaml\n├── docker-compose-couch.yaml\n├── docker-compose-e2e-template.yaml\n\n```\n## 1.证书的生成\n在Fabric网络环境中，第一步需要生成各个节点的证书文件，所用到的配置文件为``crypto-config.yaml``，说明一下文件内各字段的意义：\n```\nOrdererOrgs:    #定义一个Order组织\n  - Name: Orderer    #order节点的名称,当前网络模式为solo类型，所以只定义了一个Order节点\n    Domain: example.com    #order节点的域\n    Specs:      #暂时用不到\n      - Hostname: orderer\n      - Hostname: orderer2\n      - Hostname: orderer3\n      - Hostname: orderer4\n      - Hostname: orderer5\n\nPeerOrgs:      #定义Peer组织\n  - Name: Org1      #声明Peer组织名称为Org1\n    Domain: org1.example.com    #Org1组织的域\n    EnableNodeOUs: true    #暂时没搞清楚该字段的意义\n    Template:       #在这里可以定义所生成的Org1组织中的Peer节点证书数量，不包括Admin\n      Count: 2      #表明需要生成两个Peer节点的证书，如果需要其他数量的Peer节点，只需要更改这里的数量。\n    Users:        #在这里可以定义所生成的Org1组织中类型为User的证书数量，不包括Admin\n      Count: 1    #生成用户的证书的数量\n\n  - Name: Org2   #声明第二个Peer组织名称为Org2，如果需要更多的Peer组织证书，只需要按该模板添加即可。\n    Domain: org2.example.com  #与以上相同 \n    EnableNodeOUs: true\n    Template:\n      Count: 2\n    Users:\n      Count: 1\n```\n我们这里就使用两个组织，每个组织分别有两个节点和一个User。接下来我们使用该文件生成对应数量的证书：\n```\n#路径需要更改为自己的路径\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/  \n#在这里可能会报错，通常是权限问题，可以添加sudo重新执行\ncryptogen generate --config=./crypto-config.yaml\n#执行完毕后，当前文件夹下会出现一个新的文件夹：crypto-config，在该文件夹下就是刚刚生成的证书.\n```\n文件夹内证书不再详解，会在另一篇文章中专门解释Fabric-ca的内容。\n## 2 生成创世区块，通道配置，锚节点配置文件\n在这里需要用到``configtxgen``这个二进制文件。\n#### 2.1生成创世区块 \n```\n#首先进入文件夹\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/  \n#执行命令生成创世区块 \nconfigtxgen -profile TwoOrgsOrdererGenesis -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n#如果没有channel-artifacts这个文件夹，则需要手动去创建\n```\n如果没有出现错误的话，在``channel-artifacts``文件夹中可以看至生成的``genesis.block``文件。\n#### 2.2生成通道配置信息\n```\n#执行命令生成通道配置信息\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel\n```\n同样，在``channel-artifacts``文件夹中可以看至生成的``channel.tx``文件。\n#### 2.3生成锚节点配置文件 \n```\n#首先生成Org1的锚节点配置文件\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID mychannel -asOrg Org1MSP\n#生成Org2的锚节点配置文件\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID mychannel -asOrg Org2MSP\n```\n所有需要的配置文件全部建立完成，在``channel-artifacts``中应该有以下几个文件:\n```\nchannel.tx  genesis.block  Org1MSPanchors.tx  Org2MSPanchors.tx\n```\n[启动网络]:##3启动网络\n## 3启动网络\n到了这一步，可以启动网络了。\n```\n#首先进入``fabric-samples/first-network``文件夹。\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/\n#启动容器\nsudo docker-compose -f docker-compose-cli.yaml up -d\n```\n执行以下命令查看容器是否启动成功:\n```\nsudo docker ps\n#如果可以看到如下信息说明启动成功\nCONTAINER ID        IMAGE                               COMMAND             CREATED             STATUS              PORTS                      NAMES\n17d79586b1b7        hyperledger/fabric-tools:latest     \"/bin/bash\"         30 seconds ago      Up 28 seconds                                  cli\n0f4adb6b578e        hyperledger/fabric-orderer:latest   \"orderer\"           57 seconds ago      Up 29 seconds       0.0.0.0:7050->7050/tcp     orderer.example.com\ne2795ea9d43b        hyperledger/fabric-peer:latest      \"peer node start\"   57 seconds ago      Up 30 seconds       0.0.0.0:10051->10051/tcp   peer1.org2.example.com\n247a6e4fdd62        hyperledger/fabric-peer:latest      \"peer node start\"   57 seconds ago      Up 30 seconds       0.0.0.0:9051->9051/tcp     peer0.org2.example.com\nad4af3309e8c        hyperledger/fabric-peer:latest      \"peer node start\"   57 seconds ago      Up 31 seconds       0.0.0.0:8051->8051/tcp     peer1.org1.example.com\nf6d25896b517        hyperledger/fabric-peer:latest      \"peer node start\"   58 seconds ago      Up 40 seconds       0.0.0.0:7051->7051/tcp     peer0.org1.example.com\n```\n#### 3.1创建通道\n创建通道需要进入cli容器：\n```\nsudo docker exec -it cli bash\n#看到光标前的信息变为\nroot@17d79586b1b7:/opt/gopath/src/github.com/hyperledger/fabric/peer# \n#则成功进入容器\n```\n首先配置环境变量：\n```\n#当前cli容器默认配置是节点peer0,所以不需要其他配置信息\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#创建通道信息\npeer channel create -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/channel.tx --tls true --cafile $ORDERER_CA\n#看到如下信息说明创建通道成功\n2019-06-20 13:05:55.829 UTC [channelCmd] InitCmdFactory -> INFO 001 Endorser and orderer connections initialized\n2019-06-20 13:05:55.926 UTC [cli.common] readBlock -> INFO 002 Received block: 0\n#将生成的文件移动到channel-artifacts文件夹中\nmv mychannel.block channel-artifacts/\n```\n#### 3.2加入通道\n```\n#因为当前cli容器使用的是peer0的配置，所以可以直接将peer0加入通道 \n peer channel join -b channel-artifacts/mychannel.block\n#更新环境变量使其他节点也加入通道\n#=========peer1.org1===========  注意这里端口要与上面文件中配置的端口号相同\nCORE_PEER_ADDRESS=peer1.org1.example.com:8051  \n#=========peer0.org2============\nCORE_PEER_LOCALMSPID=\"Org2MSP\"\nCORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nCORE_PEER_ADDRESS=peer0.org2.example.com:9051\npeer channel join -b channel-artifacts/mychannel.block \n#=========peer1.org2=============\nCORE_PEER_ADDRESS=peer1.org2.example.com:10051\npeer channel join -b channel-artifacts/mychannel.block\n#退出容器\nexit\n```\n#### 3.3更新锚节点 \n```\n#重新进入容器\nsudo docker exec -it cli bash\n#更新环境变量\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#========Org1================\npeer channel update -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/Org1MSPanchors.tx --tls true --cafile $ORDERER_CA\n#========Org2================\nCORE_PEER_LOCALMSPID=\"Org2MSP\"\nCORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nCORE_PEER_ADDRESS=peer0.org2.example.com:9051\npeer channel update -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/Org2MSPanchors.tx --tls true --cafile $ORDERER_CA\n#退出容器\nexit\n```\n#### 3.4安装链码\n```\n#链码的安装仍然需要在所有节点上进行操作\n#进入容器\nsudo docker exec -it cli bash\n#更新环境变量\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#=========peer0.org1=========== \n#这里很有可能会出现路径不存在的错误，解决方法是在容器内找到对应的链码所在位置，然后替换当前链码路径\n##比如本文中链码路径为/opt/gopath/src/github.com/chaincode/chaincode_example02/go\n##则可以将以下命令的链码路径更改为github.com/chaincode/chaincode_example02\n\npeer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02\n#实例化链码 该步骤创建了a,b两个账户，其中a账户余额定义为100，b账户余额定义为200\npeer chaincode instantiate -o orderer.example.com:7050 --tls true --cafile $ORDERER_CA -C mychannel -n mycc -v 1.0 -c '{\"Args\":[\"init\",\"a\",\"100\",\"b\",\"200\"]}' -P \"OR      ('Org1MSP.member','Org2MSP.member')\"\n#这一步执行完毕后可以在其他节点上也安装链码，具体环境变量配置见本文中4.2\n```\n#### 3.5调用链码\n```\n#以peer0.org1为例\n#首先进入cli容器\nsudo docker exec -it cli bash\n#执行以下命令进行查询a账户余额\npeer chaincode query -C mychannel -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n#如果命令行输出100说明链码成功调用.\n\n#接下来我们发起一笔交易：通过peer0.org1节点将a账户余额转账给b20\npeer chaincode invoke -o orderer.example.com:7050  --tls true --cafile $ORDERER_CA -C mychannel -n mycc -c '{\"Args\":[\"invoke\",\"a\",\"b\",\"10\"]}'\n#然后登陆peer1.org1节点进行查询\nCORE_PEER_ADDRESS=peer1.org1.example.com:8051 \npeer chaincode query -C mychannel -n mycc -c '{\"Args\":[\"query\",\"a\"]}'\n#如果输出结果为:80\n说明Fabric网络手动搭建成功\n#退出容器\nexit\n```\n最后关闭网络：\n```\nsudo docker-compose -f docker-compose-cli.yaml down --volumes \n#删除生成的文件，下次启动网络需要重新生成\nsudo rm -r channel-artifacts crypto-config\n```\n## 4总结\n本文并没有使用CouchDb作为fabric网络的数据库，准备放到下一篇多机搭建Fabric网络中一起讲解。到这里，整个网络的手动搭建过程已经完成，希望大家能够有所收获。","slug":"blog/fabric/深入解析Fabric搭建的全过程","published":1,"updated":"2020-05-13T08:36:12.347Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyjt0041k0vq64wo4q43","content":"<p>在这篇文章中，使用<code>fabric-samples/first-network</code>中的文件进行fabric网络(solo类型的网络)搭建全过程的解析。如有错误欢迎批评指正。<br>至于Fabric网络的搭建这里不再介绍，可以参考这一篇文章<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric环境搭建过程</a><br>fabric网络：单机，solo类型，两个组织，分别有两个节点<br>首先看一下该文件夹内有哪些文件：</p>\n<pre><code>base                  connection-org2.json    docker-compose-cli.yaml           docker-compose-org3.yaml\nbyfn.sh               connection-org2.yaml    docker-compose-couch-org3.yaml    eyfn.sh\nchannel-artifacts     connection-org3.json    docker-compose-couch.yaml         org3-artifacts\nconfigtx.yaml         connection-org3.yaml    docker-compose-e2e-template.yaml  README.md\nconnection-org1.json  crypto-config.yaml      docker-compose-etcdraft2.yaml     scripts\nconnection-org1.yaml  docker-compose-ca.yaml  docker-compose-kafka.yaml</code></pre><p>将本次用不到的文件删除，剩余的文件：</p>\n<pre><code>.\n├── base\n│   ├── docker-compose-base.yaml\n│   └── peer-base.yaml\n├── channel-artifacts\n├── configtx.yaml\n├── crypto-config.yaml\n├── docker-compose-cli.yaml\n├── docker-compose-couch.yaml\n├── docker-compose-e2e-template.yaml\n</code></pre><h2 id=\"1-证书的生成\"><a href=\"#1-证书的生成\" class=\"headerlink\" title=\"1.证书的生成\"></a>1.证书的生成</h2><p>在Fabric网络环境中，第一步需要生成各个节点的证书文件，所用到的配置文件为<code>crypto-config.yaml</code>，说明一下文件内各字段的意义：</p>\n<pre><code>OrdererOrgs:    #定义一个Order组织\n  - Name: Orderer    #order节点的名称,当前网络模式为solo类型，所以只定义了一个Order节点\n    Domain: example.com    #order节点的域\n    Specs:      #暂时用不到\n      - Hostname: orderer\n      - Hostname: orderer2\n      - Hostname: orderer3\n      - Hostname: orderer4\n      - Hostname: orderer5\n\nPeerOrgs:      #定义Peer组织\n  - Name: Org1      #声明Peer组织名称为Org1\n    Domain: org1.example.com    #Org1组织的域\n    EnableNodeOUs: true    #暂时没搞清楚该字段的意义\n    Template:       #在这里可以定义所生成的Org1组织中的Peer节点证书数量，不包括Admin\n      Count: 2      #表明需要生成两个Peer节点的证书，如果需要其他数量的Peer节点，只需要更改这里的数量。\n    Users:        #在这里可以定义所生成的Org1组织中类型为User的证书数量，不包括Admin\n      Count: 1    #生成用户的证书的数量\n\n  - Name: Org2   #声明第二个Peer组织名称为Org2，如果需要更多的Peer组织证书，只需要按该模板添加即可。\n    Domain: org2.example.com  #与以上相同 \n    EnableNodeOUs: true\n    Template:\n      Count: 2\n    Users:\n      Count: 1</code></pre><p>我们这里就使用两个组织，每个组织分别有两个节点和一个User。接下来我们使用该文件生成对应数量的证书：</p>\n<pre><code>#路径需要更改为自己的路径\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/  \n#在这里可能会报错，通常是权限问题，可以添加sudo重新执行\ncryptogen generate --config=./crypto-config.yaml\n#执行完毕后，当前文件夹下会出现一个新的文件夹：crypto-config，在该文件夹下就是刚刚生成的证书.</code></pre><p>文件夹内证书不再详解，会在另一篇文章中专门解释Fabric-ca的内容。</p>\n<h2 id=\"2-生成创世区块，通道配置，锚节点配置文件\"><a href=\"#2-生成创世区块，通道配置，锚节点配置文件\" class=\"headerlink\" title=\"2 生成创世区块，通道配置，锚节点配置文件\"></a>2 生成创世区块，通道配置，锚节点配置文件</h2><p>在这里需要用到<code>configtxgen</code>这个二进制文件。</p>\n<h4 id=\"2-1生成创世区块\"><a href=\"#2-1生成创世区块\" class=\"headerlink\" title=\"2.1生成创世区块\"></a>2.1生成创世区块</h4><pre><code>#首先进入文件夹\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/  \n#执行命令生成创世区块 \nconfigtxgen -profile TwoOrgsOrdererGenesis -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n#如果没有channel-artifacts这个文件夹，则需要手动去创建</code></pre><p>如果没有出现错误的话，在<code>channel-artifacts</code>文件夹中可以看至生成的<code>genesis.block</code>文件。</p>\n<h4 id=\"2-2生成通道配置信息\"><a href=\"#2-2生成通道配置信息\" class=\"headerlink\" title=\"2.2生成通道配置信息\"></a>2.2生成通道配置信息</h4><pre><code>#执行命令生成通道配置信息\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel</code></pre><p>同样，在<code>channel-artifacts</code>文件夹中可以看至生成的<code>channel.tx</code>文件。</p>\n<h4 id=\"2-3生成锚节点配置文件\"><a href=\"#2-3生成锚节点配置文件\" class=\"headerlink\" title=\"2.3生成锚节点配置文件\"></a>2.3生成锚节点配置文件</h4><pre><code>#首先生成Org1的锚节点配置文件\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID mychannel -asOrg Org1MSP\n#生成Org2的锚节点配置文件\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID mychannel -asOrg Org2MSP</code></pre><p>所有需要的配置文件全部建立完成，在<code>channel-artifacts</code>中应该有以下几个文件:</p>\n<pre><code>channel.tx  genesis.block  Org1MSPanchors.tx  Org2MSPanchors.tx</code></pre><h2 id=\"3启动网络\"><a href=\"#3启动网络\" class=\"headerlink\" title=\"3启动网络\"></a>3启动网络</h2><p>到了这一步，可以启动网络了。</p>\n<pre><code>#首先进入``fabric-samples/first-network``文件夹。\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/\n#启动容器\nsudo docker-compose -f docker-compose-cli.yaml up -d</code></pre><p>执行以下命令查看容器是否启动成功:</p>\n<pre><code>sudo docker ps\n#如果可以看到如下信息说明启动成功\nCONTAINER ID        IMAGE                               COMMAND             CREATED             STATUS              PORTS                      NAMES\n17d79586b1b7        hyperledger/fabric-tools:latest     &quot;/bin/bash&quot;         30 seconds ago      Up 28 seconds                                  cli\n0f4adb6b578e        hyperledger/fabric-orderer:latest   &quot;orderer&quot;           57 seconds ago      Up 29 seconds       0.0.0.0:7050-&gt;7050/tcp     orderer.example.com\ne2795ea9d43b        hyperledger/fabric-peer:latest      &quot;peer node start&quot;   57 seconds ago      Up 30 seconds       0.0.0.0:10051-&gt;10051/tcp   peer1.org2.example.com\n247a6e4fdd62        hyperledger/fabric-peer:latest      &quot;peer node start&quot;   57 seconds ago      Up 30 seconds       0.0.0.0:9051-&gt;9051/tcp     peer0.org2.example.com\nad4af3309e8c        hyperledger/fabric-peer:latest      &quot;peer node start&quot;   57 seconds ago      Up 31 seconds       0.0.0.0:8051-&gt;8051/tcp     peer1.org1.example.com\nf6d25896b517        hyperledger/fabric-peer:latest      &quot;peer node start&quot;   58 seconds ago      Up 40 seconds       0.0.0.0:7051-&gt;7051/tcp     peer0.org1.example.com</code></pre><h4 id=\"3-1创建通道\"><a href=\"#3-1创建通道\" class=\"headerlink\" title=\"3.1创建通道\"></a>3.1创建通道</h4><p>创建通道需要进入cli容器：</p>\n<pre><code>sudo docker exec -it cli bash\n#看到光标前的信息变为\nroot@17d79586b1b7:/opt/gopath/src/github.com/hyperledger/fabric/peer# \n#则成功进入容器</code></pre><p>首先配置环境变量：</p>\n<pre><code>#当前cli容器默认配置是节点peer0,所以不需要其他配置信息\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#创建通道信息\npeer channel create -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/channel.tx --tls true --cafile $ORDERER_CA\n#看到如下信息说明创建通道成功\n2019-06-20 13:05:55.829 UTC [channelCmd] InitCmdFactory -&gt; INFO 001 Endorser and orderer connections initialized\n2019-06-20 13:05:55.926 UTC [cli.common] readBlock -&gt; INFO 002 Received block: 0\n#将生成的文件移动到channel-artifacts文件夹中\nmv mychannel.block channel-artifacts/</code></pre><h4 id=\"3-2加入通道\"><a href=\"#3-2加入通道\" class=\"headerlink\" title=\"3.2加入通道\"></a>3.2加入通道</h4><pre><code>#因为当前cli容器使用的是peer0的配置，所以可以直接将peer0加入通道 \n peer channel join -b channel-artifacts/mychannel.block\n#更新环境变量使其他节点也加入通道\n#=========peer1.org1===========  注意这里端口要与上面文件中配置的端口号相同\nCORE_PEER_ADDRESS=peer1.org1.example.com:8051  \n#=========peer0.org2============\nCORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;\nCORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nCORE_PEER_ADDRESS=peer0.org2.example.com:9051\npeer channel join -b channel-artifacts/mychannel.block \n#=========peer1.org2=============\nCORE_PEER_ADDRESS=peer1.org2.example.com:10051\npeer channel join -b channel-artifacts/mychannel.block\n#退出容器\nexit</code></pre><h4 id=\"3-3更新锚节点\"><a href=\"#3-3更新锚节点\" class=\"headerlink\" title=\"3.3更新锚节点\"></a>3.3更新锚节点</h4><pre><code>#重新进入容器\nsudo docker exec -it cli bash\n#更新环境变量\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#========Org1================\npeer channel update -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/Org1MSPanchors.tx --tls true --cafile $ORDERER_CA\n#========Org2================\nCORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;\nCORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nCORE_PEER_ADDRESS=peer0.org2.example.com:9051\npeer channel update -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/Org2MSPanchors.tx --tls true --cafile $ORDERER_CA\n#退出容器\nexit</code></pre><h4 id=\"3-4安装链码\"><a href=\"#3-4安装链码\" class=\"headerlink\" title=\"3.4安装链码\"></a>3.4安装链码</h4><pre><code>#链码的安装仍然需要在所有节点上进行操作\n#进入容器\nsudo docker exec -it cli bash\n#更新环境变量\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#=========peer0.org1=========== \n#这里很有可能会出现路径不存在的错误，解决方法是在容器内找到对应的链码所在位置，然后替换当前链码路径\n##比如本文中链码路径为/opt/gopath/src/github.com/chaincode/chaincode_example02/go\n##则可以将以下命令的链码路径更改为github.com/chaincode/chaincode_example02\n\npeer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02\n#实例化链码 该步骤创建了a,b两个账户，其中a账户余额定义为100，b账户余额定义为200\npeer chaincode instantiate -o orderer.example.com:7050 --tls true --cafile $ORDERER_CA -C mychannel -n mycc -v 1.0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;100&quot;,&quot;b&quot;,&quot;200&quot;]}&#39; -P &quot;OR      (&#39;Org1MSP.member&#39;,&#39;Org2MSP.member&#39;)&quot;\n#这一步执行完毕后可以在其他节点上也安装链码，具体环境变量配置见本文中4.2</code></pre><h4 id=\"3-5调用链码\"><a href=\"#3-5调用链码\" class=\"headerlink\" title=\"3.5调用链码\"></a>3.5调用链码</h4><pre><code>#以peer0.org1为例\n#首先进入cli容器\nsudo docker exec -it cli bash\n#执行以下命令进行查询a账户余额\npeer chaincode query -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n#如果命令行输出100说明链码成功调用.\n\n#接下来我们发起一笔交易：通过peer0.org1节点将a账户余额转账给b20\npeer chaincode invoke -o orderer.example.com:7050  --tls true --cafile $ORDERER_CA -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]}&#39;\n#然后登陆peer1.org1节点进行查询\nCORE_PEER_ADDRESS=peer1.org1.example.com:8051 \npeer chaincode query -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n#如果输出结果为:80\n说明Fabric网络手动搭建成功\n#退出容器\nexit</code></pre><p>最后关闭网络：</p>\n<pre><code>sudo docker-compose -f docker-compose-cli.yaml down --volumes \n#删除生成的文件，下次启动网络需要重新生成\nsudo rm -r channel-artifacts crypto-config</code></pre><h2 id=\"4总结\"><a href=\"#4总结\" class=\"headerlink\" title=\"4总结\"></a>4总结</h2><p>本文并没有使用CouchDb作为fabric网络的数据库，准备放到下一篇多机搭建Fabric网络中一起讲解。到这里，整个网络的手动搭建过程已经完成，希望大家能够有所收获。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在这篇文章中，使用<code>fabric-samples/first-network</code>中的文件进行fabric网络(solo类型的网络)搭建全过程的解析。如有错误欢迎批评指正。<br>至于Fabric网络的搭建这里不再介绍，可以参考这一篇文章<a href=\"https://ifican.top/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\" target=\"_blank\" rel=\"noopener\">Hyperledger Fabric环境搭建过程</a><br>fabric网络：单机，solo类型，两个组织，分别有两个节点<br>首先看一下该文件夹内有哪些文件：</p>\n<pre><code>base                  connection-org2.json    docker-compose-cli.yaml           docker-compose-org3.yaml\nbyfn.sh               connection-org2.yaml    docker-compose-couch-org3.yaml    eyfn.sh\nchannel-artifacts     connection-org3.json    docker-compose-couch.yaml         org3-artifacts\nconfigtx.yaml         connection-org3.yaml    docker-compose-e2e-template.yaml  README.md\nconnection-org1.json  crypto-config.yaml      docker-compose-etcdraft2.yaml     scripts\nconnection-org1.yaml  docker-compose-ca.yaml  docker-compose-kafka.yaml</code></pre><p>将本次用不到的文件删除，剩余的文件：</p>\n<pre><code>.\n├── base\n│   ├── docker-compose-base.yaml\n│   └── peer-base.yaml\n├── channel-artifacts\n├── configtx.yaml\n├── crypto-config.yaml\n├── docker-compose-cli.yaml\n├── docker-compose-couch.yaml\n├── docker-compose-e2e-template.yaml\n</code></pre><h2 id=\"1-证书的生成\"><a href=\"#1-证书的生成\" class=\"headerlink\" title=\"1.证书的生成\"></a>1.证书的生成</h2><p>在Fabric网络环境中，第一步需要生成各个节点的证书文件，所用到的配置文件为<code>crypto-config.yaml</code>，说明一下文件内各字段的意义：</p>\n<pre><code>OrdererOrgs:    #定义一个Order组织\n  - Name: Orderer    #order节点的名称,当前网络模式为solo类型，所以只定义了一个Order节点\n    Domain: example.com    #order节点的域\n    Specs:      #暂时用不到\n      - Hostname: orderer\n      - Hostname: orderer2\n      - Hostname: orderer3\n      - Hostname: orderer4\n      - Hostname: orderer5\n\nPeerOrgs:      #定义Peer组织\n  - Name: Org1      #声明Peer组织名称为Org1\n    Domain: org1.example.com    #Org1组织的域\n    EnableNodeOUs: true    #暂时没搞清楚该字段的意义\n    Template:       #在这里可以定义所生成的Org1组织中的Peer节点证书数量，不包括Admin\n      Count: 2      #表明需要生成两个Peer节点的证书，如果需要其他数量的Peer节点，只需要更改这里的数量。\n    Users:        #在这里可以定义所生成的Org1组织中类型为User的证书数量，不包括Admin\n      Count: 1    #生成用户的证书的数量\n\n  - Name: Org2   #声明第二个Peer组织名称为Org2，如果需要更多的Peer组织证书，只需要按该模板添加即可。\n    Domain: org2.example.com  #与以上相同 \n    EnableNodeOUs: true\n    Template:\n      Count: 2\n    Users:\n      Count: 1</code></pre><p>我们这里就使用两个组织，每个组织分别有两个节点和一个User。接下来我们使用该文件生成对应数量的证书：</p>\n<pre><code>#路径需要更改为自己的路径\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/  \n#在这里可能会报错，通常是权限问题，可以添加sudo重新执行\ncryptogen generate --config=./crypto-config.yaml\n#执行完毕后，当前文件夹下会出现一个新的文件夹：crypto-config，在该文件夹下就是刚刚生成的证书.</code></pre><p>文件夹内证书不再详解，会在另一篇文章中专门解释Fabric-ca的内容。</p>\n<h2 id=\"2-生成创世区块，通道配置，锚节点配置文件\"><a href=\"#2-生成创世区块，通道配置，锚节点配置文件\" class=\"headerlink\" title=\"2 生成创世区块，通道配置，锚节点配置文件\"></a>2 生成创世区块，通道配置，锚节点配置文件</h2><p>在这里需要用到<code>configtxgen</code>这个二进制文件。</p>\n<h4 id=\"2-1生成创世区块\"><a href=\"#2-1生成创世区块\" class=\"headerlink\" title=\"2.1生成创世区块\"></a>2.1生成创世区块</h4><pre><code>#首先进入文件夹\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/  \n#执行命令生成创世区块 \nconfigtxgen -profile TwoOrgsOrdererGenesis -channelID byfn-sys-channel -outputBlock ./channel-artifacts/genesis.block\n#如果没有channel-artifacts这个文件夹，则需要手动去创建</code></pre><p>如果没有出现错误的话，在<code>channel-artifacts</code>文件夹中可以看至生成的<code>genesis.block</code>文件。</p>\n<h4 id=\"2-2生成通道配置信息\"><a href=\"#2-2生成通道配置信息\" class=\"headerlink\" title=\"2.2生成通道配置信息\"></a>2.2生成通道配置信息</h4><pre><code>#执行命令生成通道配置信息\nconfigtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel</code></pre><p>同样，在<code>channel-artifacts</code>文件夹中可以看至生成的<code>channel.tx</code>文件。</p>\n<h4 id=\"2-3生成锚节点配置文件\"><a href=\"#2-3生成锚节点配置文件\" class=\"headerlink\" title=\"2.3生成锚节点配置文件\"></a>2.3生成锚节点配置文件</h4><pre><code>#首先生成Org1的锚节点配置文件\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID mychannel -asOrg Org1MSP\n#生成Org2的锚节点配置文件\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID mychannel -asOrg Org2MSP</code></pre><p>所有需要的配置文件全部建立完成，在<code>channel-artifacts</code>中应该有以下几个文件:</p>\n<pre><code>channel.tx  genesis.block  Org1MSPanchors.tx  Org2MSPanchors.tx</code></pre><h2 id=\"3启动网络\"><a href=\"#3启动网络\" class=\"headerlink\" title=\"3启动网络\"></a>3启动网络</h2><p>到了这一步，可以启动网络了。</p>\n<pre><code>#首先进入``fabric-samples/first-network``文件夹。\ncd ~/go/src/github.com/hyperledger/fabric/scripts/fabric-samples/first-network/\n#启动容器\nsudo docker-compose -f docker-compose-cli.yaml up -d</code></pre><p>执行以下命令查看容器是否启动成功:</p>\n<pre><code>sudo docker ps\n#如果可以看到如下信息说明启动成功\nCONTAINER ID        IMAGE                               COMMAND             CREATED             STATUS              PORTS                      NAMES\n17d79586b1b7        hyperledger/fabric-tools:latest     &quot;/bin/bash&quot;         30 seconds ago      Up 28 seconds                                  cli\n0f4adb6b578e        hyperledger/fabric-orderer:latest   &quot;orderer&quot;           57 seconds ago      Up 29 seconds       0.0.0.0:7050-&gt;7050/tcp     orderer.example.com\ne2795ea9d43b        hyperledger/fabric-peer:latest      &quot;peer node start&quot;   57 seconds ago      Up 30 seconds       0.0.0.0:10051-&gt;10051/tcp   peer1.org2.example.com\n247a6e4fdd62        hyperledger/fabric-peer:latest      &quot;peer node start&quot;   57 seconds ago      Up 30 seconds       0.0.0.0:9051-&gt;9051/tcp     peer0.org2.example.com\nad4af3309e8c        hyperledger/fabric-peer:latest      &quot;peer node start&quot;   57 seconds ago      Up 31 seconds       0.0.0.0:8051-&gt;8051/tcp     peer1.org1.example.com\nf6d25896b517        hyperledger/fabric-peer:latest      &quot;peer node start&quot;   58 seconds ago      Up 40 seconds       0.0.0.0:7051-&gt;7051/tcp     peer0.org1.example.com</code></pre><h4 id=\"3-1创建通道\"><a href=\"#3-1创建通道\" class=\"headerlink\" title=\"3.1创建通道\"></a>3.1创建通道</h4><p>创建通道需要进入cli容器：</p>\n<pre><code>sudo docker exec -it cli bash\n#看到光标前的信息变为\nroot@17d79586b1b7:/opt/gopath/src/github.com/hyperledger/fabric/peer# \n#则成功进入容器</code></pre><p>首先配置环境变量：</p>\n<pre><code>#当前cli容器默认配置是节点peer0,所以不需要其他配置信息\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#创建通道信息\npeer channel create -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/channel.tx --tls true --cafile $ORDERER_CA\n#看到如下信息说明创建通道成功\n2019-06-20 13:05:55.829 UTC [channelCmd] InitCmdFactory -&gt; INFO 001 Endorser and orderer connections initialized\n2019-06-20 13:05:55.926 UTC [cli.common] readBlock -&gt; INFO 002 Received block: 0\n#将生成的文件移动到channel-artifacts文件夹中\nmv mychannel.block channel-artifacts/</code></pre><h4 id=\"3-2加入通道\"><a href=\"#3-2加入通道\" class=\"headerlink\" title=\"3.2加入通道\"></a>3.2加入通道</h4><pre><code>#因为当前cli容器使用的是peer0的配置，所以可以直接将peer0加入通道 \n peer channel join -b channel-artifacts/mychannel.block\n#更新环境变量使其他节点也加入通道\n#=========peer1.org1===========  注意这里端口要与上面文件中配置的端口号相同\nCORE_PEER_ADDRESS=peer1.org1.example.com:8051  \n#=========peer0.org2============\nCORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;\nCORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nCORE_PEER_ADDRESS=peer0.org2.example.com:9051\npeer channel join -b channel-artifacts/mychannel.block \n#=========peer1.org2=============\nCORE_PEER_ADDRESS=peer1.org2.example.com:10051\npeer channel join -b channel-artifacts/mychannel.block\n#退出容器\nexit</code></pre><h4 id=\"3-3更新锚节点\"><a href=\"#3-3更新锚节点\" class=\"headerlink\" title=\"3.3更新锚节点\"></a>3.3更新锚节点</h4><pre><code>#重新进入容器\nsudo docker exec -it cli bash\n#更新环境变量\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#========Org1================\npeer channel update -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/Org1MSPanchors.tx --tls true --cafile $ORDERER_CA\n#========Org2================\nCORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;\nCORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt\nCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\nCORE_PEER_ADDRESS=peer0.org2.example.com:9051\npeer channel update -o orderer.example.com:7050 -c mychannel -f ./channel-artifacts/Org2MSPanchors.tx --tls true --cafile $ORDERER_CA\n#退出容器\nexit</code></pre><h4 id=\"3-4安装链码\"><a href=\"#3-4安装链码\" class=\"headerlink\" title=\"3.4安装链码\"></a>3.4安装链码</h4><pre><code>#链码的安装仍然需要在所有节点上进行操作\n#进入容器\nsudo docker exec -it cli bash\n#更新环境变量\nORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem\n#=========peer0.org1=========== \n#这里很有可能会出现路径不存在的错误，解决方法是在容器内找到对应的链码所在位置，然后替换当前链码路径\n##比如本文中链码路径为/opt/gopath/src/github.com/chaincode/chaincode_example02/go\n##则可以将以下命令的链码路径更改为github.com/chaincode/chaincode_example02\n\npeer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02\n#实例化链码 该步骤创建了a,b两个账户，其中a账户余额定义为100，b账户余额定义为200\npeer chaincode instantiate -o orderer.example.com:7050 --tls true --cafile $ORDERER_CA -C mychannel -n mycc -v 1.0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;100&quot;,&quot;b&quot;,&quot;200&quot;]}&#39; -P &quot;OR      (&#39;Org1MSP.member&#39;,&#39;Org2MSP.member&#39;)&quot;\n#这一步执行完毕后可以在其他节点上也安装链码，具体环境变量配置见本文中4.2</code></pre><h4 id=\"3-5调用链码\"><a href=\"#3-5调用链码\" class=\"headerlink\" title=\"3.5调用链码\"></a>3.5调用链码</h4><pre><code>#以peer0.org1为例\n#首先进入cli容器\nsudo docker exec -it cli bash\n#执行以下命令进行查询a账户余额\npeer chaincode query -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n#如果命令行输出100说明链码成功调用.\n\n#接下来我们发起一笔交易：通过peer0.org1节点将a账户余额转账给b20\npeer chaincode invoke -o orderer.example.com:7050  --tls true --cafile $ORDERER_CA -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]}&#39;\n#然后登陆peer1.org1节点进行查询\nCORE_PEER_ADDRESS=peer1.org1.example.com:8051 \npeer chaincode query -C mychannel -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;\n#如果输出结果为:80\n说明Fabric网络手动搭建成功\n#退出容器\nexit</code></pre><p>最后关闭网络：</p>\n<pre><code>sudo docker-compose -f docker-compose-cli.yaml down --volumes \n#删除生成的文件，下次启动网络需要重新生成\nsudo rm -r channel-artifacts crypto-config</code></pre><h2 id=\"4总结\"><a href=\"#4总结\" class=\"headerlink\" title=\"4总结\"></a>4总结</h2><p>本文并没有使用CouchDb作为fabric网络的数据库，准备放到下一篇多机搭建Fabric网络中一起讲解。到这里，整个网络的手动搭建过程已经完成，希望大家能够有所收获。</p>\n"},{"title":"Hyperledger Fabric私有数据","date":"2019-12-04T12:26:39.000Z","_content":"官方文档:[点这里](https://hyperledger-fabric.readthedocs.io/en/latest/private-data/private-data.html)\n* * *\n\n## 1简介\n在**同一个通道**中，允许某一组织在对同一通道内其他组织保持部分的数据私有。也就是说有一部分被标识为私有的数据只能具有权限的组织查看和操作，而其余组织不具备查看和操作私有数据的权限。\n通常如果需要保持数据私有可以另外创建一个通道只为私有数据服务，但是如果涉及到多个业务方时，为每一个组织另外创建通道会增加额外的管理类开销。并且也不能在进行私有数据操作的时候，让其余没有权限的组织节点都知道有这么一笔交易发生。\n从Fabric 1.2开始，引入了一个**私有数据集合**的概念，它允许通道内的指定的某一个组织中的部分成员可以对私有数据进行操作，而其他没有权限的节点只能知道有这么一笔交易发生而不能了解交易的细节。\n### 1.1私有数据集合\n* * *\n私有数据集合包括两个部分：\n\n1. **私有数据实体**：通过Gossip协议在具有权限的节点之间传输，并且只有具有权限的节点可以看到。这部分私有数据存储在具有权限的节点的私有的状态数据库中。可以通过链码API在具有权限的节点上进行访问。并且私有数据不涉及排序服务，因为是通过Peer节点间的Gossip协议进行传输的。所以要求每一个节点都需要设置参数`CORE_PEER_GOSSIP_EXTERNALENDPOINT`，并在通道内设置锚节点用于跨组织通信。\n\n2. **私有数据的哈希值**：这一部分数据用于背书，排序以及写账本到通道内的每一个Peer节点。哈希值作为交易的证明用于状态验证还可以用于审计。\n\n### 1.2什么时候使用私有数据\n* * *\n\n* 当所有的数据都需要在通道内的成员之间保密的时候，使用通道比较合适。\n* 当交易要在所有组织之间传播，并且要求只有通道内的部分组织成员可以查看或操作交易内的某一部分数据时，需要使用私有数据集合。并且部分数据需要对排序节点进行保密时，使用私有数据集合。\n\n### 1.3私有数据的交易流程\n\n    1. 当客户端提交一个调用链码的功能(读或写私有数据)提案请求到具有该私有数据集合操作权限的背书节点时,私有数据或者是用于通过链码生成私有数据的数据时，通过提案中的`transient`字段进行发送。\n    2. 背书节点模拟交易并将私有数据存储到`peer`节点上的`transient data store`一个临时的数据存储区，并基于私有数据定义的策略，通过`Gossip`协议发送到其他具有权限的节点。\n    3. 背书节点将提案响应发送给客户端。提案响应包括已经背书的读写集。读写集包括公共数据和私有数据的哈希值。发送给客户端的不包括任何的私有数据。\n    4. 客户端应用提交交易(包括带有私有数据哈希值的提案响应)到排序节点。带有私有数据哈希值得交易将和正常交易一样包括在区块中。带有私有数据哈希值得区块分发到所有节点上。用这种方式，通道中所有的`peer`节点可以通过私有数据的哈希值对交易进行验证而不需要知道任何的私有数据信息。\n    5、 在区块提交时，具有权限的节点通过集合策略确定是否具有访问私有数据的权限。如果具有权限，他们将会检查本地的`transient data store`确定他们是否已经在进行链码背书的时候接收到私有数据。如果没有，将试图从其他具有权限的节点处拉取私有数据。他们将验证公共区块中私有数据的哈希值并提交交易。当验证与提交结束后，私有数据将移动到他们的私有数据库和私有读写副本中。最后从`transient data store`中删除私有数据。\n    \n    \n","source":"_posts/blog/fabric/私有数据.md","raw":"---\ntitle: Hyperledger Fabric私有数据\ndate: 2019-12-04 20:26:39\ntags: fabric\ncategories: fabric应用\n---\n官方文档:[点这里](https://hyperledger-fabric.readthedocs.io/en/latest/private-data/private-data.html)\n* * *\n\n## 1简介\n在**同一个通道**中，允许某一组织在对同一通道内其他组织保持部分的数据私有。也就是说有一部分被标识为私有的数据只能具有权限的组织查看和操作，而其余组织不具备查看和操作私有数据的权限。\n通常如果需要保持数据私有可以另外创建一个通道只为私有数据服务，但是如果涉及到多个业务方时，为每一个组织另外创建通道会增加额外的管理类开销。并且也不能在进行私有数据操作的时候，让其余没有权限的组织节点都知道有这么一笔交易发生。\n从Fabric 1.2开始，引入了一个**私有数据集合**的概念，它允许通道内的指定的某一个组织中的部分成员可以对私有数据进行操作，而其他没有权限的节点只能知道有这么一笔交易发生而不能了解交易的细节。\n### 1.1私有数据集合\n* * *\n私有数据集合包括两个部分：\n\n1. **私有数据实体**：通过Gossip协议在具有权限的节点之间传输，并且只有具有权限的节点可以看到。这部分私有数据存储在具有权限的节点的私有的状态数据库中。可以通过链码API在具有权限的节点上进行访问。并且私有数据不涉及排序服务，因为是通过Peer节点间的Gossip协议进行传输的。所以要求每一个节点都需要设置参数`CORE_PEER_GOSSIP_EXTERNALENDPOINT`，并在通道内设置锚节点用于跨组织通信。\n\n2. **私有数据的哈希值**：这一部分数据用于背书，排序以及写账本到通道内的每一个Peer节点。哈希值作为交易的证明用于状态验证还可以用于审计。\n\n### 1.2什么时候使用私有数据\n* * *\n\n* 当所有的数据都需要在通道内的成员之间保密的时候，使用通道比较合适。\n* 当交易要在所有组织之间传播，并且要求只有通道内的部分组织成员可以查看或操作交易内的某一部分数据时，需要使用私有数据集合。并且部分数据需要对排序节点进行保密时，使用私有数据集合。\n\n### 1.3私有数据的交易流程\n\n    1. 当客户端提交一个调用链码的功能(读或写私有数据)提案请求到具有该私有数据集合操作权限的背书节点时,私有数据或者是用于通过链码生成私有数据的数据时，通过提案中的`transient`字段进行发送。\n    2. 背书节点模拟交易并将私有数据存储到`peer`节点上的`transient data store`一个临时的数据存储区，并基于私有数据定义的策略，通过`Gossip`协议发送到其他具有权限的节点。\n    3. 背书节点将提案响应发送给客户端。提案响应包括已经背书的读写集。读写集包括公共数据和私有数据的哈希值。发送给客户端的不包括任何的私有数据。\n    4. 客户端应用提交交易(包括带有私有数据哈希值的提案响应)到排序节点。带有私有数据哈希值得交易将和正常交易一样包括在区块中。带有私有数据哈希值得区块分发到所有节点上。用这种方式，通道中所有的`peer`节点可以通过私有数据的哈希值对交易进行验证而不需要知道任何的私有数据信息。\n    5、 在区块提交时，具有权限的节点通过集合策略确定是否具有访问私有数据的权限。如果具有权限，他们将会检查本地的`transient data store`确定他们是否已经在进行链码背书的时候接收到私有数据。如果没有，将试图从其他具有权限的节点处拉取私有数据。他们将验证公共区块中私有数据的哈希值并提交交易。当验证与提交结束后，私有数据将移动到他们的私有数据库和私有读写副本中。最后从`transient data store`中删除私有数据。\n    \n    \n","slug":"blog/fabric/私有数据","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyjv0044k0vqcrllci97","content":"<p>官方文档:<a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/private-data/private-data.html\" target=\"_blank\" rel=\"noopener\">点这里</a></p>\n<hr>\n<h2 id=\"1简介\"><a href=\"#1简介\" class=\"headerlink\" title=\"1简介\"></a>1简介</h2><p>在<strong>同一个通道</strong>中，允许某一组织在对同一通道内其他组织保持部分的数据私有。也就是说有一部分被标识为私有的数据只能具有权限的组织查看和操作，而其余组织不具备查看和操作私有数据的权限。<br>通常如果需要保持数据私有可以另外创建一个通道只为私有数据服务，但是如果涉及到多个业务方时，为每一个组织另外创建通道会增加额外的管理类开销。并且也不能在进行私有数据操作的时候，让其余没有权限的组织节点都知道有这么一笔交易发生。<br>从Fabric 1.2开始，引入了一个<strong>私有数据集合</strong>的概念，它允许通道内的指定的某一个组织中的部分成员可以对私有数据进行操作，而其他没有权限的节点只能知道有这么一笔交易发生而不能了解交易的细节。</p>\n<h3 id=\"1-1私有数据集合\"><a href=\"#1-1私有数据集合\" class=\"headerlink\" title=\"1.1私有数据集合\"></a>1.1私有数据集合</h3><hr>\n<p>私有数据集合包括两个部分：</p>\n<ol>\n<li><p><strong>私有数据实体</strong>：通过Gossip协议在具有权限的节点之间传输，并且只有具有权限的节点可以看到。这部分私有数据存储在具有权限的节点的私有的状态数据库中。可以通过链码API在具有权限的节点上进行访问。并且私有数据不涉及排序服务，因为是通过Peer节点间的Gossip协议进行传输的。所以要求每一个节点都需要设置参数<code>CORE_PEER_GOSSIP_EXTERNALENDPOINT</code>，并在通道内设置锚节点用于跨组织通信。</p>\n</li>\n<li><p><strong>私有数据的哈希值</strong>：这一部分数据用于背书，排序以及写账本到通道内的每一个Peer节点。哈希值作为交易的证明用于状态验证还可以用于审计。</p>\n</li>\n</ol>\n<h3 id=\"1-2什么时候使用私有数据\"><a href=\"#1-2什么时候使用私有数据\" class=\"headerlink\" title=\"1.2什么时候使用私有数据\"></a>1.2什么时候使用私有数据</h3><hr>\n<ul>\n<li>当所有的数据都需要在通道内的成员之间保密的时候，使用通道比较合适。</li>\n<li>当交易要在所有组织之间传播，并且要求只有通道内的部分组织成员可以查看或操作交易内的某一部分数据时，需要使用私有数据集合。并且部分数据需要对排序节点进行保密时，使用私有数据集合。</li>\n</ul>\n<h3 id=\"1-3私有数据的交易流程\"><a href=\"#1-3私有数据的交易流程\" class=\"headerlink\" title=\"1.3私有数据的交易流程\"></a>1.3私有数据的交易流程</h3><pre><code>1. 当客户端提交一个调用链码的功能(读或写私有数据)提案请求到具有该私有数据集合操作权限的背书节点时,私有数据或者是用于通过链码生成私有数据的数据时，通过提案中的`transient`字段进行发送。\n2. 背书节点模拟交易并将私有数据存储到`peer`节点上的`transient data store`一个临时的数据存储区，并基于私有数据定义的策略，通过`Gossip`协议发送到其他具有权限的节点。\n3. 背书节点将提案响应发送给客户端。提案响应包括已经背书的读写集。读写集包括公共数据和私有数据的哈希值。发送给客户端的不包括任何的私有数据。\n4. 客户端应用提交交易(包括带有私有数据哈希值的提案响应)到排序节点。带有私有数据哈希值得交易将和正常交易一样包括在区块中。带有私有数据哈希值得区块分发到所有节点上。用这种方式，通道中所有的`peer`节点可以通过私有数据的哈希值对交易进行验证而不需要知道任何的私有数据信息。\n5、 在区块提交时，具有权限的节点通过集合策略确定是否具有访问私有数据的权限。如果具有权限，他们将会检查本地的`transient data store`确定他们是否已经在进行链码背书的时候接收到私有数据。如果没有，将试图从其他具有权限的节点处拉取私有数据。他们将验证公共区块中私有数据的哈希值并提交交易。当验证与提交结束后，私有数据将移动到他们的私有数据库和私有读写副本中。最后从`transient data store`中删除私有数据。</code></pre>","site":{"data":{}},"excerpt":"","more":"<p>官方文档:<a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/private-data/private-data.html\" target=\"_blank\" rel=\"noopener\">点这里</a></p>\n<hr>\n<h2 id=\"1简介\"><a href=\"#1简介\" class=\"headerlink\" title=\"1简介\"></a>1简介</h2><p>在<strong>同一个通道</strong>中，允许某一组织在对同一通道内其他组织保持部分的数据私有。也就是说有一部分被标识为私有的数据只能具有权限的组织查看和操作，而其余组织不具备查看和操作私有数据的权限。<br>通常如果需要保持数据私有可以另外创建一个通道只为私有数据服务，但是如果涉及到多个业务方时，为每一个组织另外创建通道会增加额外的管理类开销。并且也不能在进行私有数据操作的时候，让其余没有权限的组织节点都知道有这么一笔交易发生。<br>从Fabric 1.2开始，引入了一个<strong>私有数据集合</strong>的概念，它允许通道内的指定的某一个组织中的部分成员可以对私有数据进行操作，而其他没有权限的节点只能知道有这么一笔交易发生而不能了解交易的细节。</p>\n<h3 id=\"1-1私有数据集合\"><a href=\"#1-1私有数据集合\" class=\"headerlink\" title=\"1.1私有数据集合\"></a>1.1私有数据集合</h3><hr>\n<p>私有数据集合包括两个部分：</p>\n<ol>\n<li><p><strong>私有数据实体</strong>：通过Gossip协议在具有权限的节点之间传输，并且只有具有权限的节点可以看到。这部分私有数据存储在具有权限的节点的私有的状态数据库中。可以通过链码API在具有权限的节点上进行访问。并且私有数据不涉及排序服务，因为是通过Peer节点间的Gossip协议进行传输的。所以要求每一个节点都需要设置参数<code>CORE_PEER_GOSSIP_EXTERNALENDPOINT</code>，并在通道内设置锚节点用于跨组织通信。</p>\n</li>\n<li><p><strong>私有数据的哈希值</strong>：这一部分数据用于背书，排序以及写账本到通道内的每一个Peer节点。哈希值作为交易的证明用于状态验证还可以用于审计。</p>\n</li>\n</ol>\n<h3 id=\"1-2什么时候使用私有数据\"><a href=\"#1-2什么时候使用私有数据\" class=\"headerlink\" title=\"1.2什么时候使用私有数据\"></a>1.2什么时候使用私有数据</h3><hr>\n<ul>\n<li>当所有的数据都需要在通道内的成员之间保密的时候，使用通道比较合适。</li>\n<li>当交易要在所有组织之间传播，并且要求只有通道内的部分组织成员可以查看或操作交易内的某一部分数据时，需要使用私有数据集合。并且部分数据需要对排序节点进行保密时，使用私有数据集合。</li>\n</ul>\n<h3 id=\"1-3私有数据的交易流程\"><a href=\"#1-3私有数据的交易流程\" class=\"headerlink\" title=\"1.3私有数据的交易流程\"></a>1.3私有数据的交易流程</h3><pre><code>1. 当客户端提交一个调用链码的功能(读或写私有数据)提案请求到具有该私有数据集合操作权限的背书节点时,私有数据或者是用于通过链码生成私有数据的数据时，通过提案中的`transient`字段进行发送。\n2. 背书节点模拟交易并将私有数据存储到`peer`节点上的`transient data store`一个临时的数据存储区，并基于私有数据定义的策略，通过`Gossip`协议发送到其他具有权限的节点。\n3. 背书节点将提案响应发送给客户端。提案响应包括已经背书的读写集。读写集包括公共数据和私有数据的哈希值。发送给客户端的不包括任何的私有数据。\n4. 客户端应用提交交易(包括带有私有数据哈希值的提案响应)到排序节点。带有私有数据哈希值得交易将和正常交易一样包括在区块中。带有私有数据哈希值得区块分发到所有节点上。用这种方式，通道中所有的`peer`节点可以通过私有数据的哈希值对交易进行验证而不需要知道任何的私有数据信息。\n5、 在区块提交时，具有权限的节点通过集合策略确定是否具有访问私有数据的权限。如果具有权限，他们将会检查本地的`transient data store`确定他们是否已经在进行链码背书的时候接收到私有数据。如果没有，将试图从其他具有权限的节点处拉取私有数据。他们将验证公共区块中私有数据的哈希值并提交交易。当验证与提交结束后，私有数据将移动到他们的私有数据库和私有读写副本中。最后从`transient data store`中删除私有数据。</code></pre>"},{"title":"Hyperledger Fabric链码作为外部服务","date":"2019-12-26T18:17:12.000Z","_content":"\n# 链码作为外部服务\n\nFabric v2.0支持链码在Fabric环境外部署和执行。允许用户管理与节点保持独立的链码运行。这种方案激励了Fabric中的链码云部署，例如`Kubernetes`。代替了在每一个节点上面构建与运行链码。链码可以作为一个服务运行，它的生命周期将可以在Fabric环境外进行管理。这种措施利用Fabirc v2.0的外部构建和运行功能。其功能具有允许操作者通过程序构建，运行，发现链码对节点进行扩展。在读取本文内容之前，应该对[外部构建与扩展](https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/)较为熟悉。\n\n在外部构建功能可以使用之前，链码包内容要求指定的编程语言的源代码进行构建并作为链码二进制文件运行。新的外部构建和运行功能允许用户有选择地定制化构建过程。将链码作为外部服务运行。构建过程允许指定链码运行服务的端点信息。因此包内容可以简单地由外部链码运行服务端点信息和用于安全通信的TLS归档组成。TLS是可选的，但是除了简单的测试环境以外，强烈建议所有环境都使用TLS。\n\n接下来的部分将描述如何将链码配置为外部服务：\n\n* [打包链码](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#打包链码)\n* [配置节点对外部链码进行处理](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#配置节点对外部链码进行处理)\n* [外部构建和运行的简单脚本文件](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#外部构建和运行的脚本文件模板)\n* [编写链码作为外部服务运行](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#编写链码作为外部服务运行)\n* [部署链码](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#部署链码)\n* [将链码作为外部服务运行](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#将链码作为外部服务运行)\n\n\n## 打包链码\n\n通过Fabric V2.0版本的`chaincode lifecycle`，链码可以被打包并以`.tar.gz`格式进行安装。下面的`myccpackage.tgz`归档说明了要求的结构：\n```\ntar xvfz myccpackage.tgz\ncode.tar.gz\nmetadata.json\n```\n\n### `code.tar.gz`归档要求\n\n`code.tar.gz`归档必须包含链码端点的连接信息。该信息将在`/bin/release`步骤处打包进`connection.json`(见下面)。在这里直接将`connection.json`打包进`code.tar.gz`，所以`release`步骤可以直接从这里复制。\n\n* `address - `可以被`peer`节点访问的链码服务端点，必须指定以`:`格式。\n* `dial_timeout - `等待连接完成的间隔时间，字符串类型并需要指定单位，如`\"10s\",\"500ms\",\"1m\"`,默认为`\"3s\"`.\n* `tls_required - `是否使用`TLS`加密。如果为`false`则不要求使用下面四个属性\n* `client_auth_required - `如果为`true`则需要是定客户端权限认证的`key_path`,`cert_path`.默认为`false`.\n* `key_path - `秘钥文件的路径\n* `cert_path - `证书文件的路径\n* `root_cert_path - `根证书文件路径。\n\n例如：\n```\n{\n  \"address\": \"your.chaincode.host.com:9999\",\n  \"dial_timeout\": \"10s\",\n  \"tls_required\": true,\n  \"client_auth_required\": \"true\",\n  \"key_path\": \"path/rooted/in/release/directory/key.pem\",\n  \"cert_path\": \"path/rooted/in/release/directory/cert.pem\",\n  \"root_cert_path\": \"path/rooted/in/release/directory/rootcert.pem\"\n}\n```\n\n`TLS`文件可以放在`code.tar.gz`归档的任何地方，因为`.tar.gz`文件夹内的文件内容将会提供给外部链码构建脚本。`bin/release`脚本，将会将文件移动到合适的位置。\n\n### `metadata.json`文件要求\n\n当使用链码作为外部服务时，需要在`metadata.json`文件中设置`type`字段，为了指定使用的是外部服务，例如：\n```\n{\"path\":\"\",\"type\":\"external\",\"label\":\"mycc\"}\n```\n\n## 配置节点对外部链码进行处理\n这个过程和[外部构建与扩展](https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/)介绍的内容是相似的。利用这些脚本来定义外部链码信息。这些脚本位于`peer`节点的文件系统并且可以访问并处理`peer`节点处的`core.yaml`文件中`chaincode`部分定义的`externalBuilders`元素。\n\n### 创建`peer`节点上的外部构建器和运行器脚本\n\n 为了配置链码作为外部服务，必须使用以下脚本文件:\n\n* `detect - `检测`metadata.json`文件中`type`是否设置为`external`并接受链码包。\n* `build - `构建链码并将构建的归档放置在`BUILD_OUTPUT_DIR`位置。脚本提取`connection.json`文件中的链码端点信息并将`code.tar.gz`文件中的其他归档文件放置在指定位置。\n* `release - `拷贝被构建的归档(在`connection.sjon`文件中)到指定位置。\n\n注意到对于链码作为外部服务，没有要求外部构建器和运行器`bin/run`脚本。\n脚本文件要求存在`peer`节点的文件夹内:\n```\n    <peer的环境下完全正确的路径>\n    └── bin\n        ├── build\n        ├── detect\n        └── release\n```\n\n### 使`peer`节点的`core.yaml`文件包括`externalBuilder`\n\n最后，为了让`peer`节点能够使用外部构建器和运行器脚本，需要修改位于`peer`节点的`core.yaml`文件中的`chaincode`部分，使它包括`externalBuilder`配置元素。\n```\nexternalBuilders:\n     - name: myexternal\n       path: <peer的环境下完全正确的路径> #就是上面的那个路径\n```\n## 外部构建和运行的脚本文件模板\n为了帮助理解在链码作为外部服务时，每一个脚本需要包含哪些工作，这一部分包含`bin/detect,bin/build,bin/release`脚本示例。\n这些例子使用`jq`命令对`json`个数数据进行转换，可以通过运行`jq --version`检查是否安装该工具。否则，需要安装`jq`或者对脚本文件进行适当的修改。\n\n### **/bin/detect**\n`bin/detect`脚本的职责是决定是否使用`buildpack`对链码包进行构建和运行。对于链码作为外部服务，脚本需要检测`metadata.json`文件中的`type`是否被设置为`external`。`peer`节点通过两个参数调用该脚本:\n```\nbin/detect CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR\n```\n一个典型的`detect`脚本应该包含:\n```\n#!/bin/bash\n\nset -euo pipefail\n\nif [ \"$#\" -ne 2 ]; then\n    >&2 echo \"Expected 2 directories got $#\"\n    exit 2\nfi\n\n#检测`type`是否被设置为`external`\nif [ \"$(jq -r .type \"$2/metadata.json\")\" == \"external\" ]; then\n    exit 0\nfi\n\nexit 1\n```\n\n`metadata.json`文件应该包含以下关键点:\n```\n{\"path\":\"\",\"type\":\"external\",\"label\":\"mycc\"}\n```\n\n### **/bin/build**\n\n`bin/build`脚本的职责是构建，编译，以及转换链码包内容到可以被`release`和`run`使用的归档中。对于链码作为外部服务，该脚本拷贝`connection.json`文件到`BUILD_OUTPUT_DIR`.`peer`节点通过三个参数调用该脚本:\n```\nbin/build CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR BUILD_OUTPUT_DIR\n```\n一个典型的`build`脚本应该包含:\n```\n#!/bin/bash\n\nset -euo pipefail\n\nif [ \"$#\" -ne 3 ]; then\n    >&2 echo \"Expected 3 directories got $#\"\n    exit 1\nfi\n\nSOURCE=$1\nOUTPUT=$3\n\n#检查connection.json文件是否存在\nif [ ! -f \"$SOURCE/connection.json\" ]  ; then\n    >&2 echo \"$SOURCE/connection.json not found\"\n    exit 1\nfi\n\n#如果需要的话在这里做更多验证\n\n#简单拷贝端点信息到指定的输出位置\ncp $SOURCE/connection.json $OUTPUT/connection.json\n\nexit 0\n```\n### **/bin/release**\n`bin/release`脚本的职责是为`peer`节点提供链码元数据。对于链码作为外部服务，`bin/release`脚本作用是为`peer`提供放置在`RELEASE_OUTPUT_DIR`位置的`connection.json`文件。`peer`节点通过两个参数调用该脚本:\n```\nbin/release BUILD_OUTPUT_DIR RELEASE_OUTPUT_DIR\n```\n一个典型的`release`脚本应该包含:\n```\n#!/bin/bash\n\nset -euo pipefail\n\nset -x\n\nif [ \"$#\" -ne 2 ]; then\n    >&2 echo \"Expected 2 directories got $#\"\n    exit 2\nfi\n\nBLD=\"$1\"\nRELEASE=\"$2\"\n\n#外部链码期望归档被放置在\"$RELEASE\"/chaincode/server路径下\nif [ -f $BLD/connection.json ]; then\n   mkdir -p \"$RELEASE\"/chaincode/server\n   cp $BLD/connection.json \"$RELEASE\"/chaincode/server\n   exit 0\nfi\n\nexit 1\n```\n\n## 编写链码作为外部服务运行\n当前，将链码作为外部服务运行模板只支持GO语言链码shim.在Fabric v2.0，Go shim API添加了`ChaincodeServer`类型。开发者可以使用它创建链码服务。`Invoke`和`Query`API不受影响。开发者需要写`shim.ChaincodeServer`API，然后选择构建链码并在外部环境中运行。这里有一个简单的链码程序模板用来说明这种模式:\n```\npackage main\n\nimport (\n        \"fmt\"\n\n        \"github.com/hyperledger/fabric-chaincode-go/shim\"\n        pb \"github.com/hyperledger/fabric-protos-go/peer\"\n)\n\n// SimpleChaincode 模板简单链码实现\ntype SimpleChaincode struct {\n}\n\nfunc (s *SimpleChaincode) Init(stub shim.ChaincodeStubInterface) pb.Response {\n        // 初始化代码\n}\n\nfunc (s *SimpleChaincode) Invoke(stub shim.ChaincodeStubInterface) pb.Response {\n        // 调用代码\n}\n\n//NOTE - ccid 和端点信息参数很难在这里编码说明，可以通过多种标准方式指定\nfunc main() {\n       //ccid 设计用来安装链码实例 (使用“peer lifecycle chaincode install <package>” 命令) for instance\n        ccid := \"mycc:fcbf8724572d42e859a7dd9a7cd8e2efb84058292017df6e3d89178b64e6c831\"\n\n        server := &shim.ChaincodeServer{\n                        CCID: ccid,\n                        Address: \"myhost:9999\"\n                        CC: new(SimpleChaincode),\n                        TLSProps: shim.TLSProperties{\n                                Disabled: true,\n                        },\n                }\n        err := server.Start()\n        if err != nil {\n                fmt.Printf(\"Error starting Simple chaincode: %s\", err)\n        }\n}\n```\n\n将链码作为外部服务运行关键的是使用`shim.ChaincodeServer`.使用的新的链码服务`shim`API`shim.ChaincodeServer`属性描述如下:\n\n* **CCID**(string):CCID应该匹配`peer`节点上的链码包。`CCID`与被安装的链码关联，在使用`peer lifecycle chaincode install <package>`CLI命令返回。这可以在安装后使用`peer lifecycle chaincode queryinstalled`命令获得。\n* **Address**(string):链码服务的监听地址。\n* **CC**(Chaincode):处理初始化和调用的链码\n* **TLSProps**(TLSProperties):链码服务的TLS属性。\n* **KaOpts**(keepalive.ServerParameters):保持连接选项，默认为空\n\n## 部署链码\n\n当GO语言链码准备好部署后，可以通过[Packageing chaincode](https://hyperledger-fabric.readthedocs.io/en/latest/cc_service.html#packaging-chaincode)部分解释的内容对链码进行打包。并通过[chaincode lifecycle](https://hyperledger-fabric.readthedocs.io/en/latest/chaincode4noah.html#chaincode-lifecycle)部分内容对链码进行部署。\n\n## 将链码作为外部服务运行\n\n根据指定的[编写链码作为外部服务运行](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#编写链码作为外部服务运行)部分创建链码，并选择构建可运行的链码环境如`Kubernetes`或者直接在`peer`主机上运行。\n\n使用链码作为外部服务模板，将不再要求在每一个节点上安装链码。当链码端点在`peer`节点上部署并运行后，可以继续正常地实例化和调用链码。","source":"_posts/blog/fabric/链码作为外部服务.md","raw":"---\ntitle: Hyperledger Fabric链码作为外部服务\ndate: 2019-12-27 02:17:12\ntags: fabric\ncategories: fabric应用\n---\n\n# 链码作为外部服务\n\nFabric v2.0支持链码在Fabric环境外部署和执行。允许用户管理与节点保持独立的链码运行。这种方案激励了Fabric中的链码云部署，例如`Kubernetes`。代替了在每一个节点上面构建与运行链码。链码可以作为一个服务运行，它的生命周期将可以在Fabric环境外进行管理。这种措施利用Fabirc v2.0的外部构建和运行功能。其功能具有允许操作者通过程序构建，运行，发现链码对节点进行扩展。在读取本文内容之前，应该对[外部构建与扩展](https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/)较为熟悉。\n\n在外部构建功能可以使用之前，链码包内容要求指定的编程语言的源代码进行构建并作为链码二进制文件运行。新的外部构建和运行功能允许用户有选择地定制化构建过程。将链码作为外部服务运行。构建过程允许指定链码运行服务的端点信息。因此包内容可以简单地由外部链码运行服务端点信息和用于安全通信的TLS归档组成。TLS是可选的，但是除了简单的测试环境以外，强烈建议所有环境都使用TLS。\n\n接下来的部分将描述如何将链码配置为外部服务：\n\n* [打包链码](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#打包链码)\n* [配置节点对外部链码进行处理](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#配置节点对外部链码进行处理)\n* [外部构建和运行的简单脚本文件](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#外部构建和运行的脚本文件模板)\n* [编写链码作为外部服务运行](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#编写链码作为外部服务运行)\n* [部署链码](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#部署链码)\n* [将链码作为外部服务运行](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#将链码作为外部服务运行)\n\n\n## 打包链码\n\n通过Fabric V2.0版本的`chaincode lifecycle`，链码可以被打包并以`.tar.gz`格式进行安装。下面的`myccpackage.tgz`归档说明了要求的结构：\n```\ntar xvfz myccpackage.tgz\ncode.tar.gz\nmetadata.json\n```\n\n### `code.tar.gz`归档要求\n\n`code.tar.gz`归档必须包含链码端点的连接信息。该信息将在`/bin/release`步骤处打包进`connection.json`(见下面)。在这里直接将`connection.json`打包进`code.tar.gz`，所以`release`步骤可以直接从这里复制。\n\n* `address - `可以被`peer`节点访问的链码服务端点，必须指定以`:`格式。\n* `dial_timeout - `等待连接完成的间隔时间，字符串类型并需要指定单位，如`\"10s\",\"500ms\",\"1m\"`,默认为`\"3s\"`.\n* `tls_required - `是否使用`TLS`加密。如果为`false`则不要求使用下面四个属性\n* `client_auth_required - `如果为`true`则需要是定客户端权限认证的`key_path`,`cert_path`.默认为`false`.\n* `key_path - `秘钥文件的路径\n* `cert_path - `证书文件的路径\n* `root_cert_path - `根证书文件路径。\n\n例如：\n```\n{\n  \"address\": \"your.chaincode.host.com:9999\",\n  \"dial_timeout\": \"10s\",\n  \"tls_required\": true,\n  \"client_auth_required\": \"true\",\n  \"key_path\": \"path/rooted/in/release/directory/key.pem\",\n  \"cert_path\": \"path/rooted/in/release/directory/cert.pem\",\n  \"root_cert_path\": \"path/rooted/in/release/directory/rootcert.pem\"\n}\n```\n\n`TLS`文件可以放在`code.tar.gz`归档的任何地方，因为`.tar.gz`文件夹内的文件内容将会提供给外部链码构建脚本。`bin/release`脚本，将会将文件移动到合适的位置。\n\n### `metadata.json`文件要求\n\n当使用链码作为外部服务时，需要在`metadata.json`文件中设置`type`字段，为了指定使用的是外部服务，例如：\n```\n{\"path\":\"\",\"type\":\"external\",\"label\":\"mycc\"}\n```\n\n## 配置节点对外部链码进行处理\n这个过程和[外部构建与扩展](https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/)介绍的内容是相似的。利用这些脚本来定义外部链码信息。这些脚本位于`peer`节点的文件系统并且可以访问并处理`peer`节点处的`core.yaml`文件中`chaincode`部分定义的`externalBuilders`元素。\n\n### 创建`peer`节点上的外部构建器和运行器脚本\n\n 为了配置链码作为外部服务，必须使用以下脚本文件:\n\n* `detect - `检测`metadata.json`文件中`type`是否设置为`external`并接受链码包。\n* `build - `构建链码并将构建的归档放置在`BUILD_OUTPUT_DIR`位置。脚本提取`connection.json`文件中的链码端点信息并将`code.tar.gz`文件中的其他归档文件放置在指定位置。\n* `release - `拷贝被构建的归档(在`connection.sjon`文件中)到指定位置。\n\n注意到对于链码作为外部服务，没有要求外部构建器和运行器`bin/run`脚本。\n脚本文件要求存在`peer`节点的文件夹内:\n```\n    <peer的环境下完全正确的路径>\n    └── bin\n        ├── build\n        ├── detect\n        └── release\n```\n\n### 使`peer`节点的`core.yaml`文件包括`externalBuilder`\n\n最后，为了让`peer`节点能够使用外部构建器和运行器脚本，需要修改位于`peer`节点的`core.yaml`文件中的`chaincode`部分，使它包括`externalBuilder`配置元素。\n```\nexternalBuilders:\n     - name: myexternal\n       path: <peer的环境下完全正确的路径> #就是上面的那个路径\n```\n## 外部构建和运行的脚本文件模板\n为了帮助理解在链码作为外部服务时，每一个脚本需要包含哪些工作，这一部分包含`bin/detect,bin/build,bin/release`脚本示例。\n这些例子使用`jq`命令对`json`个数数据进行转换，可以通过运行`jq --version`检查是否安装该工具。否则，需要安装`jq`或者对脚本文件进行适当的修改。\n\n### **/bin/detect**\n`bin/detect`脚本的职责是决定是否使用`buildpack`对链码包进行构建和运行。对于链码作为外部服务，脚本需要检测`metadata.json`文件中的`type`是否被设置为`external`。`peer`节点通过两个参数调用该脚本:\n```\nbin/detect CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR\n```\n一个典型的`detect`脚本应该包含:\n```\n#!/bin/bash\n\nset -euo pipefail\n\nif [ \"$#\" -ne 2 ]; then\n    >&2 echo \"Expected 2 directories got $#\"\n    exit 2\nfi\n\n#检测`type`是否被设置为`external`\nif [ \"$(jq -r .type \"$2/metadata.json\")\" == \"external\" ]; then\n    exit 0\nfi\n\nexit 1\n```\n\n`metadata.json`文件应该包含以下关键点:\n```\n{\"path\":\"\",\"type\":\"external\",\"label\":\"mycc\"}\n```\n\n### **/bin/build**\n\n`bin/build`脚本的职责是构建，编译，以及转换链码包内容到可以被`release`和`run`使用的归档中。对于链码作为外部服务，该脚本拷贝`connection.json`文件到`BUILD_OUTPUT_DIR`.`peer`节点通过三个参数调用该脚本:\n```\nbin/build CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR BUILD_OUTPUT_DIR\n```\n一个典型的`build`脚本应该包含:\n```\n#!/bin/bash\n\nset -euo pipefail\n\nif [ \"$#\" -ne 3 ]; then\n    >&2 echo \"Expected 3 directories got $#\"\n    exit 1\nfi\n\nSOURCE=$1\nOUTPUT=$3\n\n#检查connection.json文件是否存在\nif [ ! -f \"$SOURCE/connection.json\" ]  ; then\n    >&2 echo \"$SOURCE/connection.json not found\"\n    exit 1\nfi\n\n#如果需要的话在这里做更多验证\n\n#简单拷贝端点信息到指定的输出位置\ncp $SOURCE/connection.json $OUTPUT/connection.json\n\nexit 0\n```\n### **/bin/release**\n`bin/release`脚本的职责是为`peer`节点提供链码元数据。对于链码作为外部服务，`bin/release`脚本作用是为`peer`提供放置在`RELEASE_OUTPUT_DIR`位置的`connection.json`文件。`peer`节点通过两个参数调用该脚本:\n```\nbin/release BUILD_OUTPUT_DIR RELEASE_OUTPUT_DIR\n```\n一个典型的`release`脚本应该包含:\n```\n#!/bin/bash\n\nset -euo pipefail\n\nset -x\n\nif [ \"$#\" -ne 2 ]; then\n    >&2 echo \"Expected 2 directories got $#\"\n    exit 2\nfi\n\nBLD=\"$1\"\nRELEASE=\"$2\"\n\n#外部链码期望归档被放置在\"$RELEASE\"/chaincode/server路径下\nif [ -f $BLD/connection.json ]; then\n   mkdir -p \"$RELEASE\"/chaincode/server\n   cp $BLD/connection.json \"$RELEASE\"/chaincode/server\n   exit 0\nfi\n\nexit 1\n```\n\n## 编写链码作为外部服务运行\n当前，将链码作为外部服务运行模板只支持GO语言链码shim.在Fabric v2.0，Go shim API添加了`ChaincodeServer`类型。开发者可以使用它创建链码服务。`Invoke`和`Query`API不受影响。开发者需要写`shim.ChaincodeServer`API，然后选择构建链码并在外部环境中运行。这里有一个简单的链码程序模板用来说明这种模式:\n```\npackage main\n\nimport (\n        \"fmt\"\n\n        \"github.com/hyperledger/fabric-chaincode-go/shim\"\n        pb \"github.com/hyperledger/fabric-protos-go/peer\"\n)\n\n// SimpleChaincode 模板简单链码实现\ntype SimpleChaincode struct {\n}\n\nfunc (s *SimpleChaincode) Init(stub shim.ChaincodeStubInterface) pb.Response {\n        // 初始化代码\n}\n\nfunc (s *SimpleChaincode) Invoke(stub shim.ChaincodeStubInterface) pb.Response {\n        // 调用代码\n}\n\n//NOTE - ccid 和端点信息参数很难在这里编码说明，可以通过多种标准方式指定\nfunc main() {\n       //ccid 设计用来安装链码实例 (使用“peer lifecycle chaincode install <package>” 命令) for instance\n        ccid := \"mycc:fcbf8724572d42e859a7dd9a7cd8e2efb84058292017df6e3d89178b64e6c831\"\n\n        server := &shim.ChaincodeServer{\n                        CCID: ccid,\n                        Address: \"myhost:9999\"\n                        CC: new(SimpleChaincode),\n                        TLSProps: shim.TLSProperties{\n                                Disabled: true,\n                        },\n                }\n        err := server.Start()\n        if err != nil {\n                fmt.Printf(\"Error starting Simple chaincode: %s\", err)\n        }\n}\n```\n\n将链码作为外部服务运行关键的是使用`shim.ChaincodeServer`.使用的新的链码服务`shim`API`shim.ChaincodeServer`属性描述如下:\n\n* **CCID**(string):CCID应该匹配`peer`节点上的链码包。`CCID`与被安装的链码关联，在使用`peer lifecycle chaincode install <package>`CLI命令返回。这可以在安装后使用`peer lifecycle chaincode queryinstalled`命令获得。\n* **Address**(string):链码服务的监听地址。\n* **CC**(Chaincode):处理初始化和调用的链码\n* **TLSProps**(TLSProperties):链码服务的TLS属性。\n* **KaOpts**(keepalive.ServerParameters):保持连接选项，默认为空\n\n## 部署链码\n\n当GO语言链码准备好部署后，可以通过[Packageing chaincode](https://hyperledger-fabric.readthedocs.io/en/latest/cc_service.html#packaging-chaincode)部分解释的内容对链码进行打包。并通过[chaincode lifecycle](https://hyperledger-fabric.readthedocs.io/en/latest/chaincode4noah.html#chaincode-lifecycle)部分内容对链码进行部署。\n\n## 将链码作为外部服务运行\n\n根据指定的[编写链码作为外部服务运行](https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#编写链码作为外部服务运行)部分创建链码，并选择构建可运行的链码环境如`Kubernetes`或者直接在`peer`主机上运行。\n\n使用链码作为外部服务模板，将不再要求在每一个节点上安装链码。当链码端点在`peer`节点上部署并运行后，可以继续正常地实例化和调用链码。","slug":"blog/fabric/链码作为外部服务","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyjx0048k0vqdrcg4m6u","content":"<h1 id=\"链码作为外部服务\"><a href=\"#链码作为外部服务\" class=\"headerlink\" title=\"链码作为外部服务\"></a>链码作为外部服务</h1><p>Fabric v2.0支持链码在Fabric环境外部署和执行。允许用户管理与节点保持独立的链码运行。这种方案激励了Fabric中的链码云部署，例如<code>Kubernetes</code>。代替了在每一个节点上面构建与运行链码。链码可以作为一个服务运行，它的生命周期将可以在Fabric环境外进行管理。这种措施利用Fabirc v2.0的外部构建和运行功能。其功能具有允许操作者通过程序构建，运行，发现链码对节点进行扩展。在读取本文内容之前，应该对<a href=\"https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/\" target=\"_blank\" rel=\"noopener\">外部构建与扩展</a>较为熟悉。</p>\n<p>在外部构建功能可以使用之前，链码包内容要求指定的编程语言的源代码进行构建并作为链码二进制文件运行。新的外部构建和运行功能允许用户有选择地定制化构建过程。将链码作为外部服务运行。构建过程允许指定链码运行服务的端点信息。因此包内容可以简单地由外部链码运行服务端点信息和用于安全通信的TLS归档组成。TLS是可选的，但是除了简单的测试环境以外，强烈建议所有环境都使用TLS。</p>\n<p>接下来的部分将描述如何将链码配置为外部服务：</p>\n<ul>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#打包链码\" target=\"_blank\" rel=\"noopener\">打包链码</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#配置节点对外部链码进行处理\" target=\"_blank\" rel=\"noopener\">配置节点对外部链码进行处理</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#外部构建和运行的脚本文件模板\" target=\"_blank\" rel=\"noopener\">外部构建和运行的简单脚本文件</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#编写链码作为外部服务运行\" target=\"_blank\" rel=\"noopener\">编写链码作为外部服务运行</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#部署链码\" target=\"_blank\" rel=\"noopener\">部署链码</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#将链码作为外部服务运行\" target=\"_blank\" rel=\"noopener\">将链码作为外部服务运行</a></li>\n</ul>\n<h2 id=\"打包链码\"><a href=\"#打包链码\" class=\"headerlink\" title=\"打包链码\"></a>打包链码</h2><p>通过Fabric V2.0版本的<code>chaincode lifecycle</code>，链码可以被打包并以<code>.tar.gz</code>格式进行安装。下面的<code>myccpackage.tgz</code>归档说明了要求的结构：</p>\n<pre><code>tar xvfz myccpackage.tgz\ncode.tar.gz\nmetadata.json</code></pre><h3 id=\"code-tar-gz归档要求\"><a href=\"#code-tar-gz归档要求\" class=\"headerlink\" title=\"code.tar.gz归档要求\"></a><code>code.tar.gz</code>归档要求</h3><p><code>code.tar.gz</code>归档必须包含链码端点的连接信息。该信息将在<code>/bin/release</code>步骤处打包进<code>connection.json</code>(见下面)。在这里直接将<code>connection.json</code>打包进<code>code.tar.gz</code>，所以<code>release</code>步骤可以直接从这里复制。</p>\n<ul>\n<li><code>address -</code>可以被<code>peer</code>节点访问的链码服务端点，必须指定以<code>:</code>格式。</li>\n<li><code>dial_timeout -</code>等待连接完成的间隔时间，字符串类型并需要指定单位，如<code>&quot;10s&quot;,&quot;500ms&quot;,&quot;1m&quot;</code>,默认为<code>&quot;3s&quot;</code>.</li>\n<li><code>tls_required -</code>是否使用<code>TLS</code>加密。如果为<code>false</code>则不要求使用下面四个属性</li>\n<li><code>client_auth_required -</code>如果为<code>true</code>则需要是定客户端权限认证的<code>key_path</code>,<code>cert_path</code>.默认为<code>false</code>.</li>\n<li><code>key_path -</code>秘钥文件的路径</li>\n<li><code>cert_path -</code>证书文件的路径</li>\n<li><code>root_cert_path -</code>根证书文件路径。</li>\n</ul>\n<p>例如：</p>\n<pre><code>{\n  &quot;address&quot;: &quot;your.chaincode.host.com:9999&quot;,\n  &quot;dial_timeout&quot;: &quot;10s&quot;,\n  &quot;tls_required&quot;: true,\n  &quot;client_auth_required&quot;: &quot;true&quot;,\n  &quot;key_path&quot;: &quot;path/rooted/in/release/directory/key.pem&quot;,\n  &quot;cert_path&quot;: &quot;path/rooted/in/release/directory/cert.pem&quot;,\n  &quot;root_cert_path&quot;: &quot;path/rooted/in/release/directory/rootcert.pem&quot;\n}</code></pre><p><code>TLS</code>文件可以放在<code>code.tar.gz</code>归档的任何地方，因为<code>.tar.gz</code>文件夹内的文件内容将会提供给外部链码构建脚本。<code>bin/release</code>脚本，将会将文件移动到合适的位置。</p>\n<h3 id=\"metadata-json文件要求\"><a href=\"#metadata-json文件要求\" class=\"headerlink\" title=\"metadata.json文件要求\"></a><code>metadata.json</code>文件要求</h3><p>当使用链码作为外部服务时，需要在<code>metadata.json</code>文件中设置<code>type</code>字段，为了指定使用的是外部服务，例如：</p>\n<pre><code>{&quot;path&quot;:&quot;&quot;,&quot;type&quot;:&quot;external&quot;,&quot;label&quot;:&quot;mycc&quot;}</code></pre><h2 id=\"配置节点对外部链码进行处理\"><a href=\"#配置节点对外部链码进行处理\" class=\"headerlink\" title=\"配置节点对外部链码进行处理\"></a>配置节点对外部链码进行处理</h2><p>这个过程和<a href=\"https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/\" target=\"_blank\" rel=\"noopener\">外部构建与扩展</a>介绍的内容是相似的。利用这些脚本来定义外部链码信息。这些脚本位于<code>peer</code>节点的文件系统并且可以访问并处理<code>peer</code>节点处的<code>core.yaml</code>文件中<code>chaincode</code>部分定义的<code>externalBuilders</code>元素。</p>\n<h3 id=\"创建peer节点上的外部构建器和运行器脚本\"><a href=\"#创建peer节点上的外部构建器和运行器脚本\" class=\"headerlink\" title=\"创建peer节点上的外部构建器和运行器脚本\"></a>创建<code>peer</code>节点上的外部构建器和运行器脚本</h3><p> 为了配置链码作为外部服务，必须使用以下脚本文件:</p>\n<ul>\n<li><code>detect -</code>检测<code>metadata.json</code>文件中<code>type</code>是否设置为<code>external</code>并接受链码包。</li>\n<li><code>build -</code>构建链码并将构建的归档放置在<code>BUILD_OUTPUT_DIR</code>位置。脚本提取<code>connection.json</code>文件中的链码端点信息并将<code>code.tar.gz</code>文件中的其他归档文件放置在指定位置。</li>\n<li><code>release -</code>拷贝被构建的归档(在<code>connection.sjon</code>文件中)到指定位置。</li>\n</ul>\n<p>注意到对于链码作为外部服务，没有要求外部构建器和运行器<code>bin/run</code>脚本。<br>脚本文件要求存在<code>peer</code>节点的文件夹内:</p>\n<pre><code>    &lt;peer的环境下完全正确的路径&gt;\n    └── bin\n        ├── build\n        ├── detect\n        └── release</code></pre><h3 id=\"使peer节点的core-yaml文件包括externalBuilder\"><a href=\"#使peer节点的core-yaml文件包括externalBuilder\" class=\"headerlink\" title=\"使peer节点的core.yaml文件包括externalBuilder\"></a>使<code>peer</code>节点的<code>core.yaml</code>文件包括<code>externalBuilder</code></h3><p>最后，为了让<code>peer</code>节点能够使用外部构建器和运行器脚本，需要修改位于<code>peer</code>节点的<code>core.yaml</code>文件中的<code>chaincode</code>部分，使它包括<code>externalBuilder</code>配置元素。</p>\n<pre><code>externalBuilders:\n     - name: myexternal\n       path: &lt;peer的环境下完全正确的路径&gt; #就是上面的那个路径</code></pre><h2 id=\"外部构建和运行的脚本文件模板\"><a href=\"#外部构建和运行的脚本文件模板\" class=\"headerlink\" title=\"外部构建和运行的脚本文件模板\"></a>外部构建和运行的脚本文件模板</h2><p>为了帮助理解在链码作为外部服务时，每一个脚本需要包含哪些工作，这一部分包含<code>bin/detect,bin/build,bin/release</code>脚本示例。<br>这些例子使用<code>jq</code>命令对<code>json</code>个数数据进行转换，可以通过运行<code>jq --version</code>检查是否安装该工具。否则，需要安装<code>jq</code>或者对脚本文件进行适当的修改。</p>\n<h3 id=\"bin-detect\"><a href=\"#bin-detect\" class=\"headerlink\" title=\"/bin/detect\"></a><strong>/bin/detect</strong></h3><p><code>bin/detect</code>脚本的职责是决定是否使用<code>buildpack</code>对链码包进行构建和运行。对于链码作为外部服务，脚本需要检测<code>metadata.json</code>文件中的<code>type</code>是否被设置为<code>external</code>。<code>peer</code>节点通过两个参数调用该脚本:</p>\n<pre><code>bin/detect CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR</code></pre><p>一个典型的<code>detect</code>脚本应该包含:</p>\n<pre><code>#!/bin/bash\n\nset -euo pipefail\n\nif [ &quot;$#&quot; -ne 2 ]; then\n    &gt;&amp;2 echo &quot;Expected 2 directories got $#&quot;\n    exit 2\nfi\n\n#检测`type`是否被设置为`external`\nif [ &quot;$(jq -r .type &quot;$2/metadata.json&quot;)&quot; == &quot;external&quot; ]; then\n    exit 0\nfi\n\nexit 1</code></pre><p><code>metadata.json</code>文件应该包含以下关键点:</p>\n<pre><code>{&quot;path&quot;:&quot;&quot;,&quot;type&quot;:&quot;external&quot;,&quot;label&quot;:&quot;mycc&quot;}</code></pre><h3 id=\"bin-build\"><a href=\"#bin-build\" class=\"headerlink\" title=\"/bin/build\"></a><strong>/bin/build</strong></h3><p><code>bin/build</code>脚本的职责是构建，编译，以及转换链码包内容到可以被<code>release</code>和<code>run</code>使用的归档中。对于链码作为外部服务，该脚本拷贝<code>connection.json</code>文件到<code>BUILD_OUTPUT_DIR</code>.<code>peer</code>节点通过三个参数调用该脚本:</p>\n<pre><code>bin/build CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR BUILD_OUTPUT_DIR</code></pre><p>一个典型的<code>build</code>脚本应该包含:</p>\n<pre><code>#!/bin/bash\n\nset -euo pipefail\n\nif [ &quot;$#&quot; -ne 3 ]; then\n    &gt;&amp;2 echo &quot;Expected 3 directories got $#&quot;\n    exit 1\nfi\n\nSOURCE=$1\nOUTPUT=$3\n\n#检查connection.json文件是否存在\nif [ ! -f &quot;$SOURCE/connection.json&quot; ]  ; then\n    &gt;&amp;2 echo &quot;$SOURCE/connection.json not found&quot;\n    exit 1\nfi\n\n#如果需要的话在这里做更多验证\n\n#简单拷贝端点信息到指定的输出位置\ncp $SOURCE/connection.json $OUTPUT/connection.json\n\nexit 0</code></pre><h3 id=\"bin-release\"><a href=\"#bin-release\" class=\"headerlink\" title=\"/bin/release\"></a><strong>/bin/release</strong></h3><p><code>bin/release</code>脚本的职责是为<code>peer</code>节点提供链码元数据。对于链码作为外部服务，<code>bin/release</code>脚本作用是为<code>peer</code>提供放置在<code>RELEASE_OUTPUT_DIR</code>位置的<code>connection.json</code>文件。<code>peer</code>节点通过两个参数调用该脚本:</p>\n<pre><code>bin/release BUILD_OUTPUT_DIR RELEASE_OUTPUT_DIR</code></pre><p>一个典型的<code>release</code>脚本应该包含:</p>\n<pre><code>#!/bin/bash\n\nset -euo pipefail\n\nset -x\n\nif [ &quot;$#&quot; -ne 2 ]; then\n    &gt;&amp;2 echo &quot;Expected 2 directories got $#&quot;\n    exit 2\nfi\n\nBLD=&quot;$1&quot;\nRELEASE=&quot;$2&quot;\n\n#外部链码期望归档被放置在&quot;$RELEASE&quot;/chaincode/server路径下\nif [ -f $BLD/connection.json ]; then\n   mkdir -p &quot;$RELEASE&quot;/chaincode/server\n   cp $BLD/connection.json &quot;$RELEASE&quot;/chaincode/server\n   exit 0\nfi\n\nexit 1</code></pre><h2 id=\"编写链码作为外部服务运行\"><a href=\"#编写链码作为外部服务运行\" class=\"headerlink\" title=\"编写链码作为外部服务运行\"></a>编写链码作为外部服务运行</h2><p>当前，将链码作为外部服务运行模板只支持GO语言链码shim.在Fabric v2.0，Go shim API添加了<code>ChaincodeServer</code>类型。开发者可以使用它创建链码服务。<code>Invoke</code>和<code>Query</code>API不受影响。开发者需要写<code>shim.ChaincodeServer</code>API，然后选择构建链码并在外部环境中运行。这里有一个简单的链码程序模板用来说明这种模式:</p>\n<pre><code>package main\n\nimport (\n        &quot;fmt&quot;\n\n        &quot;github.com/hyperledger/fabric-chaincode-go/shim&quot;\n        pb &quot;github.com/hyperledger/fabric-protos-go/peer&quot;\n)\n\n// SimpleChaincode 模板简单链码实现\ntype SimpleChaincode struct {\n}\n\nfunc (s *SimpleChaincode) Init(stub shim.ChaincodeStubInterface) pb.Response {\n        // 初始化代码\n}\n\nfunc (s *SimpleChaincode) Invoke(stub shim.ChaincodeStubInterface) pb.Response {\n        // 调用代码\n}\n\n//NOTE - ccid 和端点信息参数很难在这里编码说明，可以通过多种标准方式指定\nfunc main() {\n       //ccid 设计用来安装链码实例 (使用“peer lifecycle chaincode install &lt;package&gt;” 命令) for instance\n        ccid := &quot;mycc:fcbf8724572d42e859a7dd9a7cd8e2efb84058292017df6e3d89178b64e6c831&quot;\n\n        server := &amp;shim.ChaincodeServer{\n                        CCID: ccid,\n                        Address: &quot;myhost:9999&quot;\n                        CC: new(SimpleChaincode),\n                        TLSProps: shim.TLSProperties{\n                                Disabled: true,\n                        },\n                }\n        err := server.Start()\n        if err != nil {\n                fmt.Printf(&quot;Error starting Simple chaincode: %s&quot;, err)\n        }\n}</code></pre><p>将链码作为外部服务运行关键的是使用<code>shim.ChaincodeServer</code>.使用的新的链码服务<code>shim</code>API<code>shim.ChaincodeServer</code>属性描述如下:</p>\n<ul>\n<li><strong>CCID</strong>(string):CCID应该匹配<code>peer</code>节点上的链码包。<code>CCID</code>与被安装的链码关联，在使用<code>peer lifecycle chaincode install &lt;package&gt;</code>CLI命令返回。这可以在安装后使用<code>peer lifecycle chaincode queryinstalled</code>命令获得。</li>\n<li><strong>Address</strong>(string):链码服务的监听地址。</li>\n<li><strong>CC</strong>(Chaincode):处理初始化和调用的链码</li>\n<li><strong>TLSProps</strong>(TLSProperties):链码服务的TLS属性。</li>\n<li><strong>KaOpts</strong>(keepalive.ServerParameters):保持连接选项，默认为空</li>\n</ul>\n<h2 id=\"部署链码\"><a href=\"#部署链码\" class=\"headerlink\" title=\"部署链码\"></a>部署链码</h2><p>当GO语言链码准备好部署后，可以通过<a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/cc_service.html#packaging-chaincode\" target=\"_blank\" rel=\"noopener\">Packageing chaincode</a>部分解释的内容对链码进行打包。并通过<a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/chaincode4noah.html#chaincode-lifecycle\" target=\"_blank\" rel=\"noopener\">chaincode lifecycle</a>部分内容对链码进行部署。</p>\n<h2 id=\"将链码作为外部服务运行\"><a href=\"#将链码作为外部服务运行\" class=\"headerlink\" title=\"将链码作为外部服务运行\"></a>将链码作为外部服务运行</h2><p>根据指定的<a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#编写链码作为外部服务运行\" target=\"_blank\" rel=\"noopener\">编写链码作为外部服务运行</a>部分创建链码，并选择构建可运行的链码环境如<code>Kubernetes</code>或者直接在<code>peer</code>主机上运行。</p>\n<p>使用链码作为外部服务模板，将不再要求在每一个节点上安装链码。当链码端点在<code>peer</code>节点上部署并运行后，可以继续正常地实例化和调用链码。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"链码作为外部服务\"><a href=\"#链码作为外部服务\" class=\"headerlink\" title=\"链码作为外部服务\"></a>链码作为外部服务</h1><p>Fabric v2.0支持链码在Fabric环境外部署和执行。允许用户管理与节点保持独立的链码运行。这种方案激励了Fabric中的链码云部署，例如<code>Kubernetes</code>。代替了在每一个节点上面构建与运行链码。链码可以作为一个服务运行，它的生命周期将可以在Fabric环境外进行管理。这种措施利用Fabirc v2.0的外部构建和运行功能。其功能具有允许操作者通过程序构建，运行，发现链码对节点进行扩展。在读取本文内容之前，应该对<a href=\"https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/\" target=\"_blank\" rel=\"noopener\">外部构建与扩展</a>较为熟悉。</p>\n<p>在外部构建功能可以使用之前，链码包内容要求指定的编程语言的源代码进行构建并作为链码二进制文件运行。新的外部构建和运行功能允许用户有选择地定制化构建过程。将链码作为外部服务运行。构建过程允许指定链码运行服务的端点信息。因此包内容可以简单地由外部链码运行服务端点信息和用于安全通信的TLS归档组成。TLS是可选的，但是除了简单的测试环境以外，强烈建议所有环境都使用TLS。</p>\n<p>接下来的部分将描述如何将链码配置为外部服务：</p>\n<ul>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#打包链码\" target=\"_blank\" rel=\"noopener\">打包链码</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#配置节点对外部链码进行处理\" target=\"_blank\" rel=\"noopener\">配置节点对外部链码进行处理</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#外部构建和运行的脚本文件模板\" target=\"_blank\" rel=\"noopener\">外部构建和运行的简单脚本文件</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#编写链码作为外部服务运行\" target=\"_blank\" rel=\"noopener\">编写链码作为外部服务运行</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#部署链码\" target=\"_blank\" rel=\"noopener\">部署链码</a></li>\n<li><a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#将链码作为外部服务运行\" target=\"_blank\" rel=\"noopener\">将链码作为外部服务运行</a></li>\n</ul>\n<h2 id=\"打包链码\"><a href=\"#打包链码\" class=\"headerlink\" title=\"打包链码\"></a>打包链码</h2><p>通过Fabric V2.0版本的<code>chaincode lifecycle</code>，链码可以被打包并以<code>.tar.gz</code>格式进行安装。下面的<code>myccpackage.tgz</code>归档说明了要求的结构：</p>\n<pre><code>tar xvfz myccpackage.tgz\ncode.tar.gz\nmetadata.json</code></pre><h3 id=\"code-tar-gz归档要求\"><a href=\"#code-tar-gz归档要求\" class=\"headerlink\" title=\"code.tar.gz归档要求\"></a><code>code.tar.gz</code>归档要求</h3><p><code>code.tar.gz</code>归档必须包含链码端点的连接信息。该信息将在<code>/bin/release</code>步骤处打包进<code>connection.json</code>(见下面)。在这里直接将<code>connection.json</code>打包进<code>code.tar.gz</code>，所以<code>release</code>步骤可以直接从这里复制。</p>\n<ul>\n<li><code>address -</code>可以被<code>peer</code>节点访问的链码服务端点，必须指定以<code>:</code>格式。</li>\n<li><code>dial_timeout -</code>等待连接完成的间隔时间，字符串类型并需要指定单位，如<code>&quot;10s&quot;,&quot;500ms&quot;,&quot;1m&quot;</code>,默认为<code>&quot;3s&quot;</code>.</li>\n<li><code>tls_required -</code>是否使用<code>TLS</code>加密。如果为<code>false</code>则不要求使用下面四个属性</li>\n<li><code>client_auth_required -</code>如果为<code>true</code>则需要是定客户端权限认证的<code>key_path</code>,<code>cert_path</code>.默认为<code>false</code>.</li>\n<li><code>key_path -</code>秘钥文件的路径</li>\n<li><code>cert_path -</code>证书文件的路径</li>\n<li><code>root_cert_path -</code>根证书文件路径。</li>\n</ul>\n<p>例如：</p>\n<pre><code>{\n  &quot;address&quot;: &quot;your.chaincode.host.com:9999&quot;,\n  &quot;dial_timeout&quot;: &quot;10s&quot;,\n  &quot;tls_required&quot;: true,\n  &quot;client_auth_required&quot;: &quot;true&quot;,\n  &quot;key_path&quot;: &quot;path/rooted/in/release/directory/key.pem&quot;,\n  &quot;cert_path&quot;: &quot;path/rooted/in/release/directory/cert.pem&quot;,\n  &quot;root_cert_path&quot;: &quot;path/rooted/in/release/directory/rootcert.pem&quot;\n}</code></pre><p><code>TLS</code>文件可以放在<code>code.tar.gz</code>归档的任何地方，因为<code>.tar.gz</code>文件夹内的文件内容将会提供给外部链码构建脚本。<code>bin/release</code>脚本，将会将文件移动到合适的位置。</p>\n<h3 id=\"metadata-json文件要求\"><a href=\"#metadata-json文件要求\" class=\"headerlink\" title=\"metadata.json文件要求\"></a><code>metadata.json</code>文件要求</h3><p>当使用链码作为外部服务时，需要在<code>metadata.json</code>文件中设置<code>type</code>字段，为了指定使用的是外部服务，例如：</p>\n<pre><code>{&quot;path&quot;:&quot;&quot;,&quot;type&quot;:&quot;external&quot;,&quot;label&quot;:&quot;mycc&quot;}</code></pre><h2 id=\"配置节点对外部链码进行处理\"><a href=\"#配置节点对外部链码进行处理\" class=\"headerlink\" title=\"配置节点对外部链码进行处理\"></a>配置节点对外部链码进行处理</h2><p>这个过程和<a href=\"https://ifican.top/2019/12/24/blog/fabric/%E5%A4%96%E9%83%A8%E9%93%BE%E7%A0%81%E6%9E%84%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C/\" target=\"_blank\" rel=\"noopener\">外部构建与扩展</a>介绍的内容是相似的。利用这些脚本来定义外部链码信息。这些脚本位于<code>peer</code>节点的文件系统并且可以访问并处理<code>peer</code>节点处的<code>core.yaml</code>文件中<code>chaincode</code>部分定义的<code>externalBuilders</code>元素。</p>\n<h3 id=\"创建peer节点上的外部构建器和运行器脚本\"><a href=\"#创建peer节点上的外部构建器和运行器脚本\" class=\"headerlink\" title=\"创建peer节点上的外部构建器和运行器脚本\"></a>创建<code>peer</code>节点上的外部构建器和运行器脚本</h3><p> 为了配置链码作为外部服务，必须使用以下脚本文件:</p>\n<ul>\n<li><code>detect -</code>检测<code>metadata.json</code>文件中<code>type</code>是否设置为<code>external</code>并接受链码包。</li>\n<li><code>build -</code>构建链码并将构建的归档放置在<code>BUILD_OUTPUT_DIR</code>位置。脚本提取<code>connection.json</code>文件中的链码端点信息并将<code>code.tar.gz</code>文件中的其他归档文件放置在指定位置。</li>\n<li><code>release -</code>拷贝被构建的归档(在<code>connection.sjon</code>文件中)到指定位置。</li>\n</ul>\n<p>注意到对于链码作为外部服务，没有要求外部构建器和运行器<code>bin/run</code>脚本。<br>脚本文件要求存在<code>peer</code>节点的文件夹内:</p>\n<pre><code>    &lt;peer的环境下完全正确的路径&gt;\n    └── bin\n        ├── build\n        ├── detect\n        └── release</code></pre><h3 id=\"使peer节点的core-yaml文件包括externalBuilder\"><a href=\"#使peer节点的core-yaml文件包括externalBuilder\" class=\"headerlink\" title=\"使peer节点的core.yaml文件包括externalBuilder\"></a>使<code>peer</code>节点的<code>core.yaml</code>文件包括<code>externalBuilder</code></h3><p>最后，为了让<code>peer</code>节点能够使用外部构建器和运行器脚本，需要修改位于<code>peer</code>节点的<code>core.yaml</code>文件中的<code>chaincode</code>部分，使它包括<code>externalBuilder</code>配置元素。</p>\n<pre><code>externalBuilders:\n     - name: myexternal\n       path: &lt;peer的环境下完全正确的路径&gt; #就是上面的那个路径</code></pre><h2 id=\"外部构建和运行的脚本文件模板\"><a href=\"#外部构建和运行的脚本文件模板\" class=\"headerlink\" title=\"外部构建和运行的脚本文件模板\"></a>外部构建和运行的脚本文件模板</h2><p>为了帮助理解在链码作为外部服务时，每一个脚本需要包含哪些工作，这一部分包含<code>bin/detect,bin/build,bin/release</code>脚本示例。<br>这些例子使用<code>jq</code>命令对<code>json</code>个数数据进行转换，可以通过运行<code>jq --version</code>检查是否安装该工具。否则，需要安装<code>jq</code>或者对脚本文件进行适当的修改。</p>\n<h3 id=\"bin-detect\"><a href=\"#bin-detect\" class=\"headerlink\" title=\"/bin/detect\"></a><strong>/bin/detect</strong></h3><p><code>bin/detect</code>脚本的职责是决定是否使用<code>buildpack</code>对链码包进行构建和运行。对于链码作为外部服务，脚本需要检测<code>metadata.json</code>文件中的<code>type</code>是否被设置为<code>external</code>。<code>peer</code>节点通过两个参数调用该脚本:</p>\n<pre><code>bin/detect CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR</code></pre><p>一个典型的<code>detect</code>脚本应该包含:</p>\n<pre><code>#!/bin/bash\n\nset -euo pipefail\n\nif [ &quot;$#&quot; -ne 2 ]; then\n    &gt;&amp;2 echo &quot;Expected 2 directories got $#&quot;\n    exit 2\nfi\n\n#检测`type`是否被设置为`external`\nif [ &quot;$(jq -r .type &quot;$2/metadata.json&quot;)&quot; == &quot;external&quot; ]; then\n    exit 0\nfi\n\nexit 1</code></pre><p><code>metadata.json</code>文件应该包含以下关键点:</p>\n<pre><code>{&quot;path&quot;:&quot;&quot;,&quot;type&quot;:&quot;external&quot;,&quot;label&quot;:&quot;mycc&quot;}</code></pre><h3 id=\"bin-build\"><a href=\"#bin-build\" class=\"headerlink\" title=\"/bin/build\"></a><strong>/bin/build</strong></h3><p><code>bin/build</code>脚本的职责是构建，编译，以及转换链码包内容到可以被<code>release</code>和<code>run</code>使用的归档中。对于链码作为外部服务，该脚本拷贝<code>connection.json</code>文件到<code>BUILD_OUTPUT_DIR</code>.<code>peer</code>节点通过三个参数调用该脚本:</p>\n<pre><code>bin/build CHAINCODE_SOURCE_DIR CHAINCODE_METADATA_DIR BUILD_OUTPUT_DIR</code></pre><p>一个典型的<code>build</code>脚本应该包含:</p>\n<pre><code>#!/bin/bash\n\nset -euo pipefail\n\nif [ &quot;$#&quot; -ne 3 ]; then\n    &gt;&amp;2 echo &quot;Expected 3 directories got $#&quot;\n    exit 1\nfi\n\nSOURCE=$1\nOUTPUT=$3\n\n#检查connection.json文件是否存在\nif [ ! -f &quot;$SOURCE/connection.json&quot; ]  ; then\n    &gt;&amp;2 echo &quot;$SOURCE/connection.json not found&quot;\n    exit 1\nfi\n\n#如果需要的话在这里做更多验证\n\n#简单拷贝端点信息到指定的输出位置\ncp $SOURCE/connection.json $OUTPUT/connection.json\n\nexit 0</code></pre><h3 id=\"bin-release\"><a href=\"#bin-release\" class=\"headerlink\" title=\"/bin/release\"></a><strong>/bin/release</strong></h3><p><code>bin/release</code>脚本的职责是为<code>peer</code>节点提供链码元数据。对于链码作为外部服务，<code>bin/release</code>脚本作用是为<code>peer</code>提供放置在<code>RELEASE_OUTPUT_DIR</code>位置的<code>connection.json</code>文件。<code>peer</code>节点通过两个参数调用该脚本:</p>\n<pre><code>bin/release BUILD_OUTPUT_DIR RELEASE_OUTPUT_DIR</code></pre><p>一个典型的<code>release</code>脚本应该包含:</p>\n<pre><code>#!/bin/bash\n\nset -euo pipefail\n\nset -x\n\nif [ &quot;$#&quot; -ne 2 ]; then\n    &gt;&amp;2 echo &quot;Expected 2 directories got $#&quot;\n    exit 2\nfi\n\nBLD=&quot;$1&quot;\nRELEASE=&quot;$2&quot;\n\n#外部链码期望归档被放置在&quot;$RELEASE&quot;/chaincode/server路径下\nif [ -f $BLD/connection.json ]; then\n   mkdir -p &quot;$RELEASE&quot;/chaincode/server\n   cp $BLD/connection.json &quot;$RELEASE&quot;/chaincode/server\n   exit 0\nfi\n\nexit 1</code></pre><h2 id=\"编写链码作为外部服务运行\"><a href=\"#编写链码作为外部服务运行\" class=\"headerlink\" title=\"编写链码作为外部服务运行\"></a>编写链码作为外部服务运行</h2><p>当前，将链码作为外部服务运行模板只支持GO语言链码shim.在Fabric v2.0，Go shim API添加了<code>ChaincodeServer</code>类型。开发者可以使用它创建链码服务。<code>Invoke</code>和<code>Query</code>API不受影响。开发者需要写<code>shim.ChaincodeServer</code>API，然后选择构建链码并在外部环境中运行。这里有一个简单的链码程序模板用来说明这种模式:</p>\n<pre><code>package main\n\nimport (\n        &quot;fmt&quot;\n\n        &quot;github.com/hyperledger/fabric-chaincode-go/shim&quot;\n        pb &quot;github.com/hyperledger/fabric-protos-go/peer&quot;\n)\n\n// SimpleChaincode 模板简单链码实现\ntype SimpleChaincode struct {\n}\n\nfunc (s *SimpleChaincode) Init(stub shim.ChaincodeStubInterface) pb.Response {\n        // 初始化代码\n}\n\nfunc (s *SimpleChaincode) Invoke(stub shim.ChaincodeStubInterface) pb.Response {\n        // 调用代码\n}\n\n//NOTE - ccid 和端点信息参数很难在这里编码说明，可以通过多种标准方式指定\nfunc main() {\n       //ccid 设计用来安装链码实例 (使用“peer lifecycle chaincode install &lt;package&gt;” 命令) for instance\n        ccid := &quot;mycc:fcbf8724572d42e859a7dd9a7cd8e2efb84058292017df6e3d89178b64e6c831&quot;\n\n        server := &amp;shim.ChaincodeServer{\n                        CCID: ccid,\n                        Address: &quot;myhost:9999&quot;\n                        CC: new(SimpleChaincode),\n                        TLSProps: shim.TLSProperties{\n                                Disabled: true,\n                        },\n                }\n        err := server.Start()\n        if err != nil {\n                fmt.Printf(&quot;Error starting Simple chaincode: %s&quot;, err)\n        }\n}</code></pre><p>将链码作为外部服务运行关键的是使用<code>shim.ChaincodeServer</code>.使用的新的链码服务<code>shim</code>API<code>shim.ChaincodeServer</code>属性描述如下:</p>\n<ul>\n<li><strong>CCID</strong>(string):CCID应该匹配<code>peer</code>节点上的链码包。<code>CCID</code>与被安装的链码关联，在使用<code>peer lifecycle chaincode install &lt;package&gt;</code>CLI命令返回。这可以在安装后使用<code>peer lifecycle chaincode queryinstalled</code>命令获得。</li>\n<li><strong>Address</strong>(string):链码服务的监听地址。</li>\n<li><strong>CC</strong>(Chaincode):处理初始化和调用的链码</li>\n<li><strong>TLSProps</strong>(TLSProperties):链码服务的TLS属性。</li>\n<li><strong>KaOpts</strong>(keepalive.ServerParameters):保持连接选项，默认为空</li>\n</ul>\n<h2 id=\"部署链码\"><a href=\"#部署链码\" class=\"headerlink\" title=\"部署链码\"></a>部署链码</h2><p>当GO语言链码准备好部署后，可以通过<a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/cc_service.html#packaging-chaincode\" target=\"_blank\" rel=\"noopener\">Packageing chaincode</a>部分解释的内容对链码进行打包。并通过<a href=\"https://hyperledger-fabric.readthedocs.io/en/latest/chaincode4noah.html#chaincode-lifecycle\" target=\"_blank\" rel=\"noopener\">chaincode lifecycle</a>部分内容对链码进行部署。</p>\n<h2 id=\"将链码作为外部服务运行\"><a href=\"#将链码作为外部服务运行\" class=\"headerlink\" title=\"将链码作为外部服务运行\"></a>将链码作为外部服务运行</h2><p>根据指定的<a href=\"https://ifican.top/2019/12/27/blog/fabric/%E9%93%BE%E7%A0%81%E4%BD%9C%E4%B8%BA%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/#编写链码作为外部服务运行\" target=\"_blank\" rel=\"noopener\">编写链码作为外部服务运行</a>部分创建链码，并选择构建可运行的链码环境如<code>Kubernetes</code>或者直接在<code>peer</code>主机上运行。</p>\n<p>使用链码作为外部服务模板，将不再要求在每一个节点上安装链码。当链码端点在<code>peer</code>节点上部署并运行后，可以继续正常地实例化和调用链码。</p>\n"},{"title":"Hyperledger Fabric 最简单的方式测试你的链码","date":"2019-11-27T02:34:45.000Z","_content":"一直以来，写完链码进行测试都要先搭建一个Fabric环境，然后安装链码进行测试，实际上Fabric提供了最为简单的方式可以允许我们对编写的应用链码进行功能测试，不需要搭建一个完整的Fabeic环境。而且测试完直接停止网络也不会担心有残余的文件没有删除干净，以至于搭建正式环境的时候出现各种错误。\n进入正题好了，Fabric提供了一个开发模式，是专门用来对链码进行测试用的。\n\n**其实，这些内容在Fabric官方文档中都是有的，但是一般我们都忽略掉了，所以简单说一下步骤**\n官方文档地址：[点这里](https://github.com/hyperledger/fabric-samples/blob/master/chaincode-docker-devmode/README.rst)\n## 1.先决条件\n首先，也是需要一些先决条件，比如`Golang`环境，`Docker`容器,`docker-compose`工具，等等，这些不再说明，可以看[这里完成先决条件的安装](https://newonexd.github.io/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)。\n\n## 2.开始\n* * *\n完成准备工作后，我们需要将`Fabric-sample`文件夹从`Github`上`pull`下来，地址在[这里](https://github.com/hyperledger/fabric-samples),最简单的方式是直接下载压缩文件，然后到本地解压出来，但是推荐使用IDE工具通过`git`工具从`Github`上拉取下来，具体方法自行百度。\n完成之后，会有一个`fabric-sample`文件夹，将该文件夹放在`$GOPATH/src/github.com/hyperledger/`路径下，路径不存在自行创建。\n\n### 切换版本\n进入`fabric-samples`文件夹，执行以下命令，将Fabric版本切换至1.4，如果使用其他版本请下面部分下载二进制与Docker镜像的时候要对应。\n```\ngit checkout release-1.4\n```\n\n### 3.二进制文件以及Docker镜像\n* * *\n\n下载二进制文件是比较容易出错的地方，因为容易因为版本不匹配导致网络启动失败，所以在下载二进制文件的时候一定要注意使用的版本。\n```\ncurl -sS https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh -o ./scripts/bootstrap.sh\nchmod +x ./scripts/bootstrap.sh\n## ./scripts/bootstrap.sh [version] [ca version] [thirdparty_version]\n```\n这里需要输入三个版本号，第一个是Fabric的版本号，第二个是Ca的版本号(在这里我们用不到），第三个是第三方工具的版本号。\n我们之前是使用了1.4的Fabric，所以我们直接指定好版本就好了。\n\n* Fabric >> 1.4.3(只要前缀是1.4就可以)\n* CA    >>  1.4.3\n* ThirdParty >> 0.4.15\n\n完整的命令为:\n```\n#记得要在bootstrap.sh文件的上一级目录进行执行。\n./scripts/bootstrap.sh 1.4.3 1.4.3 0.4.15\n```\n或者直接将版本号在文件中修改：\n打开刚下载的`bootstrap.sh`文件，前面几行就是指定版本号的，自行修改就好，修改完直接使用命令进行下载就好了。\n```\n./scripts/bootstrap.sh\n```\n\n## 4.测试链码\n前面几部没有出现问题的话，到这里我们就可以对链码进行测试了，进入`fabric-sample/chaincode-docker-devmode`文件夹下,执行以下命令：\n```\ndocker-compose -f docker-compose-simple.yaml up\n```\n如果没有错误的话，我们的开发环境已经准备好了，接下来是对链码进行测试的步骤：\n\n1. 将编写的链码放到`fabric-sample/chaincode/`文件夹下\n```\n# 打开第二个终端执行：\ndocker exec -it chaincode sh\n```\n如果已经将链码放到`fabric-sample/chaincode/`文件夹内，执行以下命令应该可以看到自己的链码：\n```\nls\n```\n2. 编译链码,以官方的例子为例：\n```\ncd chaincode_example02/go\ngo build -o chaincode_example02\nCORE_PEER_ADDRESS=peer:7052 CORE_CHAINCODE_ID_NAME=mycc:0 ./chaincode_example02\n```\n\n3.**安装与实例化**：\n打开第三个终端执行：\n```\ndocker exec -it cli bash\n# 以下命令按照自己的链码内容自行修改\npeer chaincode install -p chaincodedev/chaincode/chaincode_example02/go -n mycc -v 0\npeer chaincode instantiate -n mycc -v 0 -c '{\"Args\":[\"init\",\"a\",\"100\",\"b\",\"200\"]}' -C myc\n```\n\n4 测试\n如果以上步骤没有报错的话，准备工作已经全部完成，剩下的就是测试自己的链码了。如果链码需要更新的话，只需要关闭网络：\n```\ndocker-compose -f docker-compose-simple.yaml down --volumes\n```\n重新启动网络并进行测试就好了。","source":"_posts/blog/fabric/链码测试.md","raw":"---\ntitle: Hyperledger Fabric 最简单的方式测试你的链码\ndate: 2019-11-27 10:34:45\ntags: fabric\ncategories: fabric应用\n---\n一直以来，写完链码进行测试都要先搭建一个Fabric环境，然后安装链码进行测试，实际上Fabric提供了最为简单的方式可以允许我们对编写的应用链码进行功能测试，不需要搭建一个完整的Fabeic环境。而且测试完直接停止网络也不会担心有残余的文件没有删除干净，以至于搭建正式环境的时候出现各种错误。\n进入正题好了，Fabric提供了一个开发模式，是专门用来对链码进行测试用的。\n\n**其实，这些内容在Fabric官方文档中都是有的，但是一般我们都忽略掉了，所以简单说一下步骤**\n官方文档地址：[点这里](https://github.com/hyperledger/fabric-samples/blob/master/chaincode-docker-devmode/README.rst)\n## 1.先决条件\n首先，也是需要一些先决条件，比如`Golang`环境，`Docker`容器,`docker-compose`工具，等等，这些不再说明，可以看[这里完成先决条件的安装](https://newonexd.github.io/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)。\n\n## 2.开始\n* * *\n完成准备工作后，我们需要将`Fabric-sample`文件夹从`Github`上`pull`下来，地址在[这里](https://github.com/hyperledger/fabric-samples),最简单的方式是直接下载压缩文件，然后到本地解压出来，但是推荐使用IDE工具通过`git`工具从`Github`上拉取下来，具体方法自行百度。\n完成之后，会有一个`fabric-sample`文件夹，将该文件夹放在`$GOPATH/src/github.com/hyperledger/`路径下，路径不存在自行创建。\n\n### 切换版本\n进入`fabric-samples`文件夹，执行以下命令，将Fabric版本切换至1.4，如果使用其他版本请下面部分下载二进制与Docker镜像的时候要对应。\n```\ngit checkout release-1.4\n```\n\n### 3.二进制文件以及Docker镜像\n* * *\n\n下载二进制文件是比较容易出错的地方，因为容易因为版本不匹配导致网络启动失败，所以在下载二进制文件的时候一定要注意使用的版本。\n```\ncurl -sS https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh -o ./scripts/bootstrap.sh\nchmod +x ./scripts/bootstrap.sh\n## ./scripts/bootstrap.sh [version] [ca version] [thirdparty_version]\n```\n这里需要输入三个版本号，第一个是Fabric的版本号，第二个是Ca的版本号(在这里我们用不到），第三个是第三方工具的版本号。\n我们之前是使用了1.4的Fabric，所以我们直接指定好版本就好了。\n\n* Fabric >> 1.4.3(只要前缀是1.4就可以)\n* CA    >>  1.4.3\n* ThirdParty >> 0.4.15\n\n完整的命令为:\n```\n#记得要在bootstrap.sh文件的上一级目录进行执行。\n./scripts/bootstrap.sh 1.4.3 1.4.3 0.4.15\n```\n或者直接将版本号在文件中修改：\n打开刚下载的`bootstrap.sh`文件，前面几行就是指定版本号的，自行修改就好，修改完直接使用命令进行下载就好了。\n```\n./scripts/bootstrap.sh\n```\n\n## 4.测试链码\n前面几部没有出现问题的话，到这里我们就可以对链码进行测试了，进入`fabric-sample/chaincode-docker-devmode`文件夹下,执行以下命令：\n```\ndocker-compose -f docker-compose-simple.yaml up\n```\n如果没有错误的话，我们的开发环境已经准备好了，接下来是对链码进行测试的步骤：\n\n1. 将编写的链码放到`fabric-sample/chaincode/`文件夹下\n```\n# 打开第二个终端执行：\ndocker exec -it chaincode sh\n```\n如果已经将链码放到`fabric-sample/chaincode/`文件夹内，执行以下命令应该可以看到自己的链码：\n```\nls\n```\n2. 编译链码,以官方的例子为例：\n```\ncd chaincode_example02/go\ngo build -o chaincode_example02\nCORE_PEER_ADDRESS=peer:7052 CORE_CHAINCODE_ID_NAME=mycc:0 ./chaincode_example02\n```\n\n3.**安装与实例化**：\n打开第三个终端执行：\n```\ndocker exec -it cli bash\n# 以下命令按照自己的链码内容自行修改\npeer chaincode install -p chaincodedev/chaincode/chaincode_example02/go -n mycc -v 0\npeer chaincode instantiate -n mycc -v 0 -c '{\"Args\":[\"init\",\"a\",\"100\",\"b\",\"200\"]}' -C myc\n```\n\n4 测试\n如果以上步骤没有报错的话，准备工作已经全部完成，剩下的就是测试自己的链码了。如果链码需要更新的话，只需要关闭网络：\n```\ndocker-compose -f docker-compose-simple.yaml down --volumes\n```\n重新启动网络并进行测试就好了。","slug":"blog/fabric/链码测试","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyk0004bk0vq7h9e3c91","content":"<p>一直以来，写完链码进行测试都要先搭建一个Fabric环境，然后安装链码进行测试，实际上Fabric提供了最为简单的方式可以允许我们对编写的应用链码进行功能测试，不需要搭建一个完整的Fabeic环境。而且测试完直接停止网络也不会担心有残余的文件没有删除干净，以至于搭建正式环境的时候出现各种错误。<br>进入正题好了，Fabric提供了一个开发模式，是专门用来对链码进行测试用的。</p>\n<p><strong>其实，这些内容在Fabric官方文档中都是有的，但是一般我们都忽略掉了，所以简单说一下步骤</strong><br>官方文档地址：<a href=\"https://github.com/hyperledger/fabric-samples/blob/master/chaincode-docker-devmode/README.rst\" target=\"_blank\" rel=\"noopener\">点这里</a></p>\n<h2 id=\"1-先决条件\"><a href=\"#1-先决条件\" class=\"headerlink\" title=\"1.先决条件\"></a>1.先决条件</h2><p>首先，也是需要一些先决条件，比如<code>Golang</code>环境，<code>Docker</code>容器,<code>docker-compose</code>工具，等等，这些不再说明，可以看<a href=\"https://newonexd.github.io/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\">这里完成先决条件的安装</a>。</p>\n<h2 id=\"2-开始\"><a href=\"#2-开始\" class=\"headerlink\" title=\"2.开始\"></a>2.开始</h2><hr>\n<p>完成准备工作后，我们需要将<code>Fabric-sample</code>文件夹从<code>Github</code>上<code>pull</code>下来，地址在<a href=\"https://github.com/hyperledger/fabric-samples\" target=\"_blank\" rel=\"noopener\">这里</a>,最简单的方式是直接下载压缩文件，然后到本地解压出来，但是推荐使用IDE工具通过<code>git</code>工具从<code>Github</code>上拉取下来，具体方法自行百度。<br>完成之后，会有一个<code>fabric-sample</code>文件夹，将该文件夹放在<code>$GOPATH/src/github.com/hyperledger/</code>路径下，路径不存在自行创建。</p>\n<h3 id=\"切换版本\"><a href=\"#切换版本\" class=\"headerlink\" title=\"切换版本\"></a>切换版本</h3><p>进入<code>fabric-samples</code>文件夹，执行以下命令，将Fabric版本切换至1.4，如果使用其他版本请下面部分下载二进制与Docker镜像的时候要对应。</p>\n<pre><code>git checkout release-1.4</code></pre><h3 id=\"3-二进制文件以及Docker镜像\"><a href=\"#3-二进制文件以及Docker镜像\" class=\"headerlink\" title=\"3.二进制文件以及Docker镜像\"></a>3.二进制文件以及Docker镜像</h3><hr>\n<p>下载二进制文件是比较容易出错的地方，因为容易因为版本不匹配导致网络启动失败，所以在下载二进制文件的时候一定要注意使用的版本。</p>\n<pre><code>curl -sS https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh -o ./scripts/bootstrap.sh\nchmod +x ./scripts/bootstrap.sh\n## ./scripts/bootstrap.sh [version] [ca version] [thirdparty_version]</code></pre><p>这里需要输入三个版本号，第一个是Fabric的版本号，第二个是Ca的版本号(在这里我们用不到），第三个是第三方工具的版本号。<br>我们之前是使用了1.4的Fabric，所以我们直接指定好版本就好了。</p>\n<ul>\n<li>Fabric &gt;&gt; 1.4.3(只要前缀是1.4就可以)</li>\n<li>CA    &gt;&gt;  1.4.3</li>\n<li>ThirdParty &gt;&gt; 0.4.15</li>\n</ul>\n<p>完整的命令为:</p>\n<pre><code>#记得要在bootstrap.sh文件的上一级目录进行执行。\n./scripts/bootstrap.sh 1.4.3 1.4.3 0.4.15</code></pre><p>或者直接将版本号在文件中修改：<br>打开刚下载的<code>bootstrap.sh</code>文件，前面几行就是指定版本号的，自行修改就好，修改完直接使用命令进行下载就好了。</p>\n<pre><code>./scripts/bootstrap.sh</code></pre><h2 id=\"4-测试链码\"><a href=\"#4-测试链码\" class=\"headerlink\" title=\"4.测试链码\"></a>4.测试链码</h2><p>前面几部没有出现问题的话，到这里我们就可以对链码进行测试了，进入<code>fabric-sample/chaincode-docker-devmode</code>文件夹下,执行以下命令：</p>\n<pre><code>docker-compose -f docker-compose-simple.yaml up</code></pre><p>如果没有错误的话，我们的开发环境已经准备好了，接下来是对链码进行测试的步骤：</p>\n<ol>\n<li>将编写的链码放到<code>fabric-sample/chaincode/</code>文件夹下<pre><code># 打开第二个终端执行：\ndocker exec -it chaincode sh</code></pre>如果已经将链码放到<code>fabric-sample/chaincode/</code>文件夹内，执行以下命令应该可以看到自己的链码：<pre><code>ls</code></pre></li>\n<li>编译链码,以官方的例子为例：<pre><code>cd chaincode_example02/go\ngo build -o chaincode_example02\nCORE_PEER_ADDRESS=peer:7052 CORE_CHAINCODE_ID_NAME=mycc:0 ./chaincode_example02</code></pre></li>\n</ol>\n<p>3.<strong>安装与实例化</strong>：<br>打开第三个终端执行：</p>\n<pre><code>docker exec -it cli bash\n# 以下命令按照自己的链码内容自行修改\npeer chaincode install -p chaincodedev/chaincode/chaincode_example02/go -n mycc -v 0\npeer chaincode instantiate -n mycc -v 0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;100&quot;,&quot;b&quot;,&quot;200&quot;]}&#39; -C myc</code></pre><p>4 测试<br>如果以上步骤没有报错的话，准备工作已经全部完成，剩下的就是测试自己的链码了。如果链码需要更新的话，只需要关闭网络：</p>\n<pre><code>docker-compose -f docker-compose-simple.yaml down --volumes</code></pre><p>重新启动网络并进行测试就好了。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>一直以来，写完链码进行测试都要先搭建一个Fabric环境，然后安装链码进行测试，实际上Fabric提供了最为简单的方式可以允许我们对编写的应用链码进行功能测试，不需要搭建一个完整的Fabeic环境。而且测试完直接停止网络也不会担心有残余的文件没有删除干净，以至于搭建正式环境的时候出现各种错误。<br>进入正题好了，Fabric提供了一个开发模式，是专门用来对链码进行测试用的。</p>\n<p><strong>其实，这些内容在Fabric官方文档中都是有的，但是一般我们都忽略掉了，所以简单说一下步骤</strong><br>官方文档地址：<a href=\"https://github.com/hyperledger/fabric-samples/blob/master/chaincode-docker-devmode/README.rst\" target=\"_blank\" rel=\"noopener\">点这里</a></p>\n<h2 id=\"1-先决条件\"><a href=\"#1-先决条件\" class=\"headerlink\" title=\"1.先决条件\"></a>1.先决条件</h2><p>首先，也是需要一些先决条件，比如<code>Golang</code>环境，<code>Docker</code>容器,<code>docker-compose</code>工具，等等，这些不再说明，可以看<a href=\"https://newonexd.github.io/2019/11/23/blog/fabric/Fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/\">这里完成先决条件的安装</a>。</p>\n<h2 id=\"2-开始\"><a href=\"#2-开始\" class=\"headerlink\" title=\"2.开始\"></a>2.开始</h2><hr>\n<p>完成准备工作后，我们需要将<code>Fabric-sample</code>文件夹从<code>Github</code>上<code>pull</code>下来，地址在<a href=\"https://github.com/hyperledger/fabric-samples\" target=\"_blank\" rel=\"noopener\">这里</a>,最简单的方式是直接下载压缩文件，然后到本地解压出来，但是推荐使用IDE工具通过<code>git</code>工具从<code>Github</code>上拉取下来，具体方法自行百度。<br>完成之后，会有一个<code>fabric-sample</code>文件夹，将该文件夹放在<code>$GOPATH/src/github.com/hyperledger/</code>路径下，路径不存在自行创建。</p>\n<h3 id=\"切换版本\"><a href=\"#切换版本\" class=\"headerlink\" title=\"切换版本\"></a>切换版本</h3><p>进入<code>fabric-samples</code>文件夹，执行以下命令，将Fabric版本切换至1.4，如果使用其他版本请下面部分下载二进制与Docker镜像的时候要对应。</p>\n<pre><code>git checkout release-1.4</code></pre><h3 id=\"3-二进制文件以及Docker镜像\"><a href=\"#3-二进制文件以及Docker镜像\" class=\"headerlink\" title=\"3.二进制文件以及Docker镜像\"></a>3.二进制文件以及Docker镜像</h3><hr>\n<p>下载二进制文件是比较容易出错的地方，因为容易因为版本不匹配导致网络启动失败，所以在下载二进制文件的时候一定要注意使用的版本。</p>\n<pre><code>curl -sS https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh -o ./scripts/bootstrap.sh\nchmod +x ./scripts/bootstrap.sh\n## ./scripts/bootstrap.sh [version] [ca version] [thirdparty_version]</code></pre><p>这里需要输入三个版本号，第一个是Fabric的版本号，第二个是Ca的版本号(在这里我们用不到），第三个是第三方工具的版本号。<br>我们之前是使用了1.4的Fabric，所以我们直接指定好版本就好了。</p>\n<ul>\n<li>Fabric &gt;&gt; 1.4.3(只要前缀是1.4就可以)</li>\n<li>CA    &gt;&gt;  1.4.3</li>\n<li>ThirdParty &gt;&gt; 0.4.15</li>\n</ul>\n<p>完整的命令为:</p>\n<pre><code>#记得要在bootstrap.sh文件的上一级目录进行执行。\n./scripts/bootstrap.sh 1.4.3 1.4.3 0.4.15</code></pre><p>或者直接将版本号在文件中修改：<br>打开刚下载的<code>bootstrap.sh</code>文件，前面几行就是指定版本号的，自行修改就好，修改完直接使用命令进行下载就好了。</p>\n<pre><code>./scripts/bootstrap.sh</code></pre><h2 id=\"4-测试链码\"><a href=\"#4-测试链码\" class=\"headerlink\" title=\"4.测试链码\"></a>4.测试链码</h2><p>前面几部没有出现问题的话，到这里我们就可以对链码进行测试了，进入<code>fabric-sample/chaincode-docker-devmode</code>文件夹下,执行以下命令：</p>\n<pre><code>docker-compose -f docker-compose-simple.yaml up</code></pre><p>如果没有错误的话，我们的开发环境已经准备好了，接下来是对链码进行测试的步骤：</p>\n<ol>\n<li>将编写的链码放到<code>fabric-sample/chaincode/</code>文件夹下<pre><code># 打开第二个终端执行：\ndocker exec -it chaincode sh</code></pre>如果已经将链码放到<code>fabric-sample/chaincode/</code>文件夹内，执行以下命令应该可以看到自己的链码：<pre><code>ls</code></pre></li>\n<li>编译链码,以官方的例子为例：<pre><code>cd chaincode_example02/go\ngo build -o chaincode_example02\nCORE_PEER_ADDRESS=peer:7052 CORE_CHAINCODE_ID_NAME=mycc:0 ./chaincode_example02</code></pre></li>\n</ol>\n<p>3.<strong>安装与实例化</strong>：<br>打开第三个终端执行：</p>\n<pre><code>docker exec -it cli bash\n# 以下命令按照自己的链码内容自行修改\npeer chaincode install -p chaincodedev/chaincode/chaincode_example02/go -n mycc -v 0\npeer chaincode instantiate -n mycc -v 0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;100&quot;,&quot;b&quot;,&quot;200&quot;]}&#39; -C myc</code></pre><p>4 测试<br>如果以上步骤没有报错的话，准备工作已经全部完成，剩下的就是测试自己的链码了。如果链码需要更新的话，只需要关闭网络：</p>\n<pre><code>docker-compose -f docker-compose-simple.yaml down --volumes</code></pre><p>重新启动网络并进行测试就好了。</p>\n"},{"title":"IPFS学习-DNS链接","date":"2019-12-18T06:31:30.000Z","_content":"# DNSLink\n## 什么是DNS链接\nDNS链接使用[DNS TXT](https://en.wikipedia.org/wiki/TXT_record)记录映射域名(如`ipfs.io`)到一个IPFS地址。因为你可以编辑自己的DNS记录，可以使他们总是指向最新版本的IPFS中的对象(如果修改了IPFS中的对象则IPFS中的对象地址也会改变)。由于DNS链接使用DNS记录，所以可以设计名字/路径/(子)域/任何容易分类，阅读和记的名字。\n一个DNS链接地址看起来像一个[IPNS]()地址，但是DNS链接使用域名代替了被哈希的公钥:\n```\n/ipns/myexampledomain.org\n```\n就像普通的IPFS地址，可以包含链接到其他的文件-或者是其他类型的IPFS支持的资源，像目录和链接：\n```\n/ipns/myexampledomain.org/media/\n```\n### 使用子域名发布\n虽然您可以根据需要将TXT记录发布到确切的域，但是使用称为`_dnslink`的特殊子域来发布DNSLink记录会更有利。这使您可以提高自动设置的安全性，或将对DNSLink记录的控制权委派给第三方，而不必放弃对原始DNS区域的完全控制权。\n例如，`docs.ipfs.io`没有含有TXT记录，但是页面仍然可以加载因为TXT记录在`_dnslink.docs.ipfs.io`中存在。如果查看`_dnslink.docs.ipfs.io`的DNS记录，可以看到以下DNSLink记录：\n```\n$ dig +noall +answer TXT _dnslink.docs.ipfs.io\n_dnslink.docs.ipfs.io.  34  IN  TXT \"dnslink=/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya\"\n```\n\n### 使用DNSLink解析\n当一个IPFS客户端或者节点尝试解析一个地址，将会寻找前缀为`dnslink=`的TXT记录。剩下的可以是`/ipfs/`链接或者是`/ipns/`，或者是链接到其他的DNSLink。\n```\ndnslink=/ipfs/<具体内容的CID>\n```\n例如，回到之前`_dnslink.docs.ipfs.io`的DNS记录继续了解DNS链接实体：\n```\n$ dig +noall +answer TXT _dnslink.docs.ipfs.io\n_dnslink.docs.ipfs.io.  34  IN  TXT \"dnslink=/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya\"\n```\n基于这个地址：\n```\n/ipns/docs.ipfs.io/introduction/\n```\n可以获取这个区块：\n```\n/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya/introduction/\n```","source":"_posts/blog/ipfs/IPFS学习-DNS链接.md","raw":"---\ntitle: IPFS学习-DNS链接\ndate: 2019-12-18 14:31:30\ntags: IPFS\ncategories: IPFS学习\n---\n# DNSLink\n## 什么是DNS链接\nDNS链接使用[DNS TXT](https://en.wikipedia.org/wiki/TXT_record)记录映射域名(如`ipfs.io`)到一个IPFS地址。因为你可以编辑自己的DNS记录，可以使他们总是指向最新版本的IPFS中的对象(如果修改了IPFS中的对象则IPFS中的对象地址也会改变)。由于DNS链接使用DNS记录，所以可以设计名字/路径/(子)域/任何容易分类，阅读和记的名字。\n一个DNS链接地址看起来像一个[IPNS]()地址，但是DNS链接使用域名代替了被哈希的公钥:\n```\n/ipns/myexampledomain.org\n```\n就像普通的IPFS地址，可以包含链接到其他的文件-或者是其他类型的IPFS支持的资源，像目录和链接：\n```\n/ipns/myexampledomain.org/media/\n```\n### 使用子域名发布\n虽然您可以根据需要将TXT记录发布到确切的域，但是使用称为`_dnslink`的特殊子域来发布DNSLink记录会更有利。这使您可以提高自动设置的安全性，或将对DNSLink记录的控制权委派给第三方，而不必放弃对原始DNS区域的完全控制权。\n例如，`docs.ipfs.io`没有含有TXT记录，但是页面仍然可以加载因为TXT记录在`_dnslink.docs.ipfs.io`中存在。如果查看`_dnslink.docs.ipfs.io`的DNS记录，可以看到以下DNSLink记录：\n```\n$ dig +noall +answer TXT _dnslink.docs.ipfs.io\n_dnslink.docs.ipfs.io.  34  IN  TXT \"dnslink=/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya\"\n```\n\n### 使用DNSLink解析\n当一个IPFS客户端或者节点尝试解析一个地址，将会寻找前缀为`dnslink=`的TXT记录。剩下的可以是`/ipfs/`链接或者是`/ipns/`，或者是链接到其他的DNSLink。\n```\ndnslink=/ipfs/<具体内容的CID>\n```\n例如，回到之前`_dnslink.docs.ipfs.io`的DNS记录继续了解DNS链接实体：\n```\n$ dig +noall +answer TXT _dnslink.docs.ipfs.io\n_dnslink.docs.ipfs.io.  34  IN  TXT \"dnslink=/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya\"\n```\n基于这个地址：\n```\n/ipns/docs.ipfs.io/introduction/\n```\n可以获取这个区块：\n```\n/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya/introduction/\n```","slug":"blog/ipfs/IPFS学习-DNS链接","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyk1004ek0vqbldndcjq","content":"<h1 id=\"DNSLink\"><a href=\"#DNSLink\" class=\"headerlink\" title=\"DNSLink\"></a>DNSLink</h1><h2 id=\"什么是DNS链接\"><a href=\"#什么是DNS链接\" class=\"headerlink\" title=\"什么是DNS链接\"></a>什么是DNS链接</h2><p>DNS链接使用<a href=\"https://en.wikipedia.org/wiki/TXT_record\" target=\"_blank\" rel=\"noopener\">DNS TXT</a>记录映射域名(如<code>ipfs.io</code>)到一个IPFS地址。因为你可以编辑自己的DNS记录，可以使他们总是指向最新版本的IPFS中的对象(如果修改了IPFS中的对象则IPFS中的对象地址也会改变)。由于DNS链接使用DNS记录，所以可以设计名字/路径/(子)域/任何容易分类，阅读和记的名字。<br>一个DNS链接地址看起来像一个<a href=\"\">IPNS</a>地址，但是DNS链接使用域名代替了被哈希的公钥:</p>\n<pre><code>/ipns/myexampledomain.org</code></pre><p>就像普通的IPFS地址，可以包含链接到其他的文件-或者是其他类型的IPFS支持的资源，像目录和链接：</p>\n<pre><code>/ipns/myexampledomain.org/media/</code></pre><h3 id=\"使用子域名发布\"><a href=\"#使用子域名发布\" class=\"headerlink\" title=\"使用子域名发布\"></a>使用子域名发布</h3><p>虽然您可以根据需要将TXT记录发布到确切的域，但是使用称为<code>_dnslink</code>的特殊子域来发布DNSLink记录会更有利。这使您可以提高自动设置的安全性，或将对DNSLink记录的控制权委派给第三方，而不必放弃对原始DNS区域的完全控制权。<br>例如，<code>docs.ipfs.io</code>没有含有TXT记录，但是页面仍然可以加载因为TXT记录在<code>_dnslink.docs.ipfs.io</code>中存在。如果查看<code>_dnslink.docs.ipfs.io</code>的DNS记录，可以看到以下DNSLink记录：</p>\n<pre><code>$ dig +noall +answer TXT _dnslink.docs.ipfs.io\n_dnslink.docs.ipfs.io.  34  IN  TXT &quot;dnslink=/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya&quot;</code></pre><h3 id=\"使用DNSLink解析\"><a href=\"#使用DNSLink解析\" class=\"headerlink\" title=\"使用DNSLink解析\"></a>使用DNSLink解析</h3><p>当一个IPFS客户端或者节点尝试解析一个地址，将会寻找前缀为<code>dnslink=</code>的TXT记录。剩下的可以是<code>/ipfs/</code>链接或者是<code>/ipns/</code>，或者是链接到其他的DNSLink。</p>\n<pre><code>dnslink=/ipfs/&lt;具体内容的CID&gt;</code></pre><p>例如，回到之前<code>_dnslink.docs.ipfs.io</code>的DNS记录继续了解DNS链接实体：</p>\n<pre><code>$ dig +noall +answer TXT _dnslink.docs.ipfs.io\n_dnslink.docs.ipfs.io.  34  IN  TXT &quot;dnslink=/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya&quot;</code></pre><p>基于这个地址：</p>\n<pre><code>/ipns/docs.ipfs.io/introduction/</code></pre><p>可以获取这个区块：</p>\n<pre><code>/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya/introduction/</code></pre>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"DNSLink\"><a href=\"#DNSLink\" class=\"headerlink\" title=\"DNSLink\"></a>DNSLink</h1><h2 id=\"什么是DNS链接\"><a href=\"#什么是DNS链接\" class=\"headerlink\" title=\"什么是DNS链接\"></a>什么是DNS链接</h2><p>DNS链接使用<a href=\"https://en.wikipedia.org/wiki/TXT_record\" target=\"_blank\" rel=\"noopener\">DNS TXT</a>记录映射域名(如<code>ipfs.io</code>)到一个IPFS地址。因为你可以编辑自己的DNS记录，可以使他们总是指向最新版本的IPFS中的对象(如果修改了IPFS中的对象则IPFS中的对象地址也会改变)。由于DNS链接使用DNS记录，所以可以设计名字/路径/(子)域/任何容易分类，阅读和记的名字。<br>一个DNS链接地址看起来像一个<a href=\"\">IPNS</a>地址，但是DNS链接使用域名代替了被哈希的公钥:</p>\n<pre><code>/ipns/myexampledomain.org</code></pre><p>就像普通的IPFS地址，可以包含链接到其他的文件-或者是其他类型的IPFS支持的资源，像目录和链接：</p>\n<pre><code>/ipns/myexampledomain.org/media/</code></pre><h3 id=\"使用子域名发布\"><a href=\"#使用子域名发布\" class=\"headerlink\" title=\"使用子域名发布\"></a>使用子域名发布</h3><p>虽然您可以根据需要将TXT记录发布到确切的域，但是使用称为<code>_dnslink</code>的特殊子域来发布DNSLink记录会更有利。这使您可以提高自动设置的安全性，或将对DNSLink记录的控制权委派给第三方，而不必放弃对原始DNS区域的完全控制权。<br>例如，<code>docs.ipfs.io</code>没有含有TXT记录，但是页面仍然可以加载因为TXT记录在<code>_dnslink.docs.ipfs.io</code>中存在。如果查看<code>_dnslink.docs.ipfs.io</code>的DNS记录，可以看到以下DNSLink记录：</p>\n<pre><code>$ dig +noall +answer TXT _dnslink.docs.ipfs.io\n_dnslink.docs.ipfs.io.  34  IN  TXT &quot;dnslink=/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya&quot;</code></pre><h3 id=\"使用DNSLink解析\"><a href=\"#使用DNSLink解析\" class=\"headerlink\" title=\"使用DNSLink解析\"></a>使用DNSLink解析</h3><p>当一个IPFS客户端或者节点尝试解析一个地址，将会寻找前缀为<code>dnslink=</code>的TXT记录。剩下的可以是<code>/ipfs/</code>链接或者是<code>/ipns/</code>，或者是链接到其他的DNSLink。</p>\n<pre><code>dnslink=/ipfs/&lt;具体内容的CID&gt;</code></pre><p>例如，回到之前<code>_dnslink.docs.ipfs.io</code>的DNS记录继续了解DNS链接实体：</p>\n<pre><code>$ dig +noall +answer TXT _dnslink.docs.ipfs.io\n_dnslink.docs.ipfs.io.  34  IN  TXT &quot;dnslink=/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya&quot;</code></pre><p>基于这个地址：</p>\n<pre><code>/ipns/docs.ipfs.io/introduction/</code></pre><p>可以获取这个区块：</p>\n<pre><code>/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya/introduction/</code></pre>"},{"title":"IPFS学习-IPNS","date":"2019-12-18T06:57:50.000Z","_content":"星际名称系统(IPNS)是一个创建个更新可变的链接到IPFS内容的系统，由于对象在IPFS中是内容寻址的，他们的内容变化将导致地址随之变化。对于多变的事物是有用的。但是很难获取某些内容的最新版本。\n\n在IPNS中名字是被哈希的公钥。它与一条记录相关联，该记录包含有关其链接的哈希的信息，该信息由相应的私钥签名。新的记录可以在任何时候被签名与发布。\n查看IPNS地址，使用了`/ipns/`前缀：\n```\n/ipns/QmSrPmbaUKA3ZodhzPWZnpFgcPMFWF4QsxXbkWfEptTBJd\n```\nIPNS不是在IPFS上创建可变地址的唯一方法。 您还可以使用[DNSLink]()（当前比IPNS快得多，并且还使用更易读的名称）。 其他社区成员正在探索使用区块链存储通用名称记录的方法。\n\n例如：\n假设您要在IPFS下发布您的网站。 您可以使用[Files API]()发布静态网站，然后获得一个可以链接到的CID。 但是，当您需要进行更改时，就会出现问题：您将获得一个新的CID，因为您现在拥有不同的内容。 而且，您不可能总是给别人新的地址。\n这是Name API派上用场的地方。 使用它，您可以创建一个稳定的IPNS地址，该地址指向您网站最新版本的CID。\n```\n//文件的地址\nconst addr = '/ipfs/QmbezGequPwcsWo8UL4wDF6a8hYwM1hmbzYv2mnKkEWaUp'\n\nipfs.name.publish(addr, function (err, res) {\n    // 接收到包含两个字段的资源：\n    //   - name: 被发布的内容的名字\n    //   - value: 名字指向的\"真实\"的地址\n    console.log(`https://gateway.ipfs.io/ipns/${res.name}`)\n})\n```\n用这种方式，可以使用相同的地址重新发布一个新的版本到网页，默认情况下，`ipfs.name.publish`将会使用节点ID。","source":"_posts/blog/ipfs/IPFS学习-IPNS.md","raw":"---\ntitle: IPFS学习-IPNS\ndate: 2019-12-18 14:57:50\ntags: IPFS\ncategories: IPFS学习\n---\n星际名称系统(IPNS)是一个创建个更新可变的链接到IPFS内容的系统，由于对象在IPFS中是内容寻址的，他们的内容变化将导致地址随之变化。对于多变的事物是有用的。但是很难获取某些内容的最新版本。\n\n在IPNS中名字是被哈希的公钥。它与一条记录相关联，该记录包含有关其链接的哈希的信息，该信息由相应的私钥签名。新的记录可以在任何时候被签名与发布。\n查看IPNS地址，使用了`/ipns/`前缀：\n```\n/ipns/QmSrPmbaUKA3ZodhzPWZnpFgcPMFWF4QsxXbkWfEptTBJd\n```\nIPNS不是在IPFS上创建可变地址的唯一方法。 您还可以使用[DNSLink]()（当前比IPNS快得多，并且还使用更易读的名称）。 其他社区成员正在探索使用区块链存储通用名称记录的方法。\n\n例如：\n假设您要在IPFS下发布您的网站。 您可以使用[Files API]()发布静态网站，然后获得一个可以链接到的CID。 但是，当您需要进行更改时，就会出现问题：您将获得一个新的CID，因为您现在拥有不同的内容。 而且，您不可能总是给别人新的地址。\n这是Name API派上用场的地方。 使用它，您可以创建一个稳定的IPNS地址，该地址指向您网站最新版本的CID。\n```\n//文件的地址\nconst addr = '/ipfs/QmbezGequPwcsWo8UL4wDF6a8hYwM1hmbzYv2mnKkEWaUp'\n\nipfs.name.publish(addr, function (err, res) {\n    // 接收到包含两个字段的资源：\n    //   - name: 被发布的内容的名字\n    //   - value: 名字指向的\"真实\"的地址\n    console.log(`https://gateway.ipfs.io/ipns/${res.name}`)\n})\n```\n用这种方式，可以使用相同的地址重新发布一个新的版本到网页，默认情况下，`ipfs.name.publish`将会使用节点ID。","slug":"blog/ipfs/IPFS学习-IPNS","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyk3004hk0vq733m0f3n","content":"<p>星际名称系统(IPNS)是一个创建个更新可变的链接到IPFS内容的系统，由于对象在IPFS中是内容寻址的，他们的内容变化将导致地址随之变化。对于多变的事物是有用的。但是很难获取某些内容的最新版本。</p>\n<p>在IPNS中名字是被哈希的公钥。它与一条记录相关联，该记录包含有关其链接的哈希的信息，该信息由相应的私钥签名。新的记录可以在任何时候被签名与发布。<br>查看IPNS地址，使用了<code>/ipns/</code>前缀：</p>\n<pre><code>/ipns/QmSrPmbaUKA3ZodhzPWZnpFgcPMFWF4QsxXbkWfEptTBJd</code></pre><p>IPNS不是在IPFS上创建可变地址的唯一方法。 您还可以使用<a href=\"\">DNSLink</a>（当前比IPNS快得多，并且还使用更易读的名称）。 其他社区成员正在探索使用区块链存储通用名称记录的方法。</p>\n<p>例如：<br>假设您要在IPFS下发布您的网站。 您可以使用<a href=\"\">Files API</a>发布静态网站，然后获得一个可以链接到的CID。 但是，当您需要进行更改时，就会出现问题：您将获得一个新的CID，因为您现在拥有不同的内容。 而且，您不可能总是给别人新的地址。<br>这是Name API派上用场的地方。 使用它，您可以创建一个稳定的IPNS地址，该地址指向您网站最新版本的CID。</p>\n<pre><code>//文件的地址\nconst addr = &#39;/ipfs/QmbezGequPwcsWo8UL4wDF6a8hYwM1hmbzYv2mnKkEWaUp&#39;\n\nipfs.name.publish(addr, function (err, res) {\n    // 接收到包含两个字段的资源：\n    //   - name: 被发布的内容的名字\n    //   - value: 名字指向的&quot;真实&quot;的地址\n    console.log(`https://gateway.ipfs.io/ipns/${res.name}`)\n})</code></pre><p>用这种方式，可以使用相同的地址重新发布一个新的版本到网页，默认情况下，<code>ipfs.name.publish</code>将会使用节点ID。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>星际名称系统(IPNS)是一个创建个更新可变的链接到IPFS内容的系统，由于对象在IPFS中是内容寻址的，他们的内容变化将导致地址随之变化。对于多变的事物是有用的。但是很难获取某些内容的最新版本。</p>\n<p>在IPNS中名字是被哈希的公钥。它与一条记录相关联，该记录包含有关其链接的哈希的信息，该信息由相应的私钥签名。新的记录可以在任何时候被签名与发布。<br>查看IPNS地址，使用了<code>/ipns/</code>前缀：</p>\n<pre><code>/ipns/QmSrPmbaUKA3ZodhzPWZnpFgcPMFWF4QsxXbkWfEptTBJd</code></pre><p>IPNS不是在IPFS上创建可变地址的唯一方法。 您还可以使用<a href=\"\">DNSLink</a>（当前比IPNS快得多，并且还使用更易读的名称）。 其他社区成员正在探索使用区块链存储通用名称记录的方法。</p>\n<p>例如：<br>假设您要在IPFS下发布您的网站。 您可以使用<a href=\"\">Files API</a>发布静态网站，然后获得一个可以链接到的CID。 但是，当您需要进行更改时，就会出现问题：您将获得一个新的CID，因为您现在拥有不同的内容。 而且，您不可能总是给别人新的地址。<br>这是Name API派上用场的地方。 使用它，您可以创建一个稳定的IPNS地址，该地址指向您网站最新版本的CID。</p>\n<pre><code>//文件的地址\nconst addr = &#39;/ipfs/QmbezGequPwcsWo8UL4wDF6a8hYwM1hmbzYv2mnKkEWaUp&#39;\n\nipfs.name.publish(addr, function (err, res) {\n    // 接收到包含两个字段的资源：\n    //   - name: 被发布的内容的名字\n    //   - value: 名字指向的&quot;真实&quot;的地址\n    console.log(`https://gateway.ipfs.io/ipns/${res.name}`)\n})</code></pre><p>用这种方式，可以使用相同的地址重新发布一个新的版本到网页，默认情况下，<code>ipfs.name.publish</code>将会使用节点ID。</p>\n"},{"title":"IPFS学习-内容标识符(CIDs)","date":"2019-12-18T03:14:57.000Z","_content":"# 内容标识符(CIDs)\n内容标识符也称为CID，是用于指向IPFS中材料的标签。 它不会指示内容的存储位置，但会根据内容本身形成一种地址。 CID简短，无论其基础内容的大小如何。\n\nCID基于内容的[加密哈希](http://localhost:1313/guides/concepts/hashes/)，意思是：\n\n* 任何不相同的内容将会产生不同的CID\n* 内容中相同的部分添加到两个不同的IPFS节点通过相同的设置应该产生相同的CID。\n\n## CID格式\n基于不同的编码或者是CID的版本使得CID具有不同的格式。多数存在的IPFS工具仍生成版本0的CID。但是`file`([MFS](http://localhost:1313/guides/concepts/mfs/))和`object`现在默认使用CID V1.\n\n### 版本0\n当IPFS初始设计的时候，使用base58多次哈希作为内容标识符(虽然简单，但是与新的CID相比缺乏灵活性。)CIDv0仍然是许多IPFS默认选项，所以IPFS版本应该支持v0。\n\n如果一个CID具有46字符并以`Qm`开头，说明是一个CIDv0\n\n### 版本1\nCID v1包含一些前导标识符，这些标识符明确说明了使用哪种表示形式以及内容哈希本身。 这些包括：\n\n* [multibase](https://github.com/multiformats/multibase)前缀，指定用于其余CID的编码.\n* CID版本标识符，指示这是哪个CID版本\n* 一个[多编解码器](https://github.com/multiformats/multicodec)标识符，指示目标内容的格式-帮助人们和软件在获取内容后知道如何解释该内容\n\n这些前导标识符还提供前向兼容性，支持在将来的CID版本中使用的不同格式。\n您可以使用CID的前几个字节来解释内容地址的其余部分，并知道从IPFS提取内容后如何对其进行解码。 有关更多详细信息，请查看[CID规范](https://github.com/ipld/cid)。 它包括[解码算法](https://github.com/ipld/cid/blob/ef1b2002394b15b1e6c26c30545fd485f2c4c138/README.md#decoding-algorithm)，并链接到用于解码CID的现有软件实现。\n\n## 探索CID\n是否想分解特定CID的多库，多编解码器或多哈希信息？ 您可以使用IPLD资源管理器中的[CID检查器](https://cid.ipfs.io/#QmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjQuPU)或[CID信息面板](https://explore.ipld.io/#/explore/QmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjQuPU)（两个链接都使用示例CID启动）来对不同格式的CID进行交互式细分。 还了解IPFS中[CID的未来](https://discuss.ipfs.io/t/who-decides-what-hashing-algorithms-ipfs-allows/6742)。","source":"_posts/blog/ipfs/IPFS学习-内容标识符CID.md","raw":"---\ntitle: IPFS学习-内容标识符(CIDs)\ndate: 2019-12-18 11:14:57\ntags: IPFS\ncategories: IPFS学习\n---\n# 内容标识符(CIDs)\n内容标识符也称为CID，是用于指向IPFS中材料的标签。 它不会指示内容的存储位置，但会根据内容本身形成一种地址。 CID简短，无论其基础内容的大小如何。\n\nCID基于内容的[加密哈希](http://localhost:1313/guides/concepts/hashes/)，意思是：\n\n* 任何不相同的内容将会产生不同的CID\n* 内容中相同的部分添加到两个不同的IPFS节点通过相同的设置应该产生相同的CID。\n\n## CID格式\n基于不同的编码或者是CID的版本使得CID具有不同的格式。多数存在的IPFS工具仍生成版本0的CID。但是`file`([MFS](http://localhost:1313/guides/concepts/mfs/))和`object`现在默认使用CID V1.\n\n### 版本0\n当IPFS初始设计的时候，使用base58多次哈希作为内容标识符(虽然简单，但是与新的CID相比缺乏灵活性。)CIDv0仍然是许多IPFS默认选项，所以IPFS版本应该支持v0。\n\n如果一个CID具有46字符并以`Qm`开头，说明是一个CIDv0\n\n### 版本1\nCID v1包含一些前导标识符，这些标识符明确说明了使用哪种表示形式以及内容哈希本身。 这些包括：\n\n* [multibase](https://github.com/multiformats/multibase)前缀，指定用于其余CID的编码.\n* CID版本标识符，指示这是哪个CID版本\n* 一个[多编解码器](https://github.com/multiformats/multicodec)标识符，指示目标内容的格式-帮助人们和软件在获取内容后知道如何解释该内容\n\n这些前导标识符还提供前向兼容性，支持在将来的CID版本中使用的不同格式。\n您可以使用CID的前几个字节来解释内容地址的其余部分，并知道从IPFS提取内容后如何对其进行解码。 有关更多详细信息，请查看[CID规范](https://github.com/ipld/cid)。 它包括[解码算法](https://github.com/ipld/cid/blob/ef1b2002394b15b1e6c26c30545fd485f2c4c138/README.md#decoding-algorithm)，并链接到用于解码CID的现有软件实现。\n\n## 探索CID\n是否想分解特定CID的多库，多编解码器或多哈希信息？ 您可以使用IPLD资源管理器中的[CID检查器](https://cid.ipfs.io/#QmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjQuPU)或[CID信息面板](https://explore.ipld.io/#/explore/QmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjQuPU)（两个链接都使用示例CID启动）来对不同格式的CID进行交互式细分。 还了解IPFS中[CID的未来](https://discuss.ipfs.io/t/who-decides-what-hashing-algorithms-ipfs-allows/6742)。","slug":"blog/ipfs/IPFS学习-内容标识符CID","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyk8004lk0vq0oc9hbbe","content":"<h1 id=\"内容标识符-CIDs\"><a href=\"#内容标识符-CIDs\" class=\"headerlink\" title=\"内容标识符(CIDs)\"></a>内容标识符(CIDs)</h1><p>内容标识符也称为CID，是用于指向IPFS中材料的标签。 它不会指示内容的存储位置，但会根据内容本身形成一种地址。 CID简短，无论其基础内容的大小如何。</p>\n<p>CID基于内容的<a href=\"http://localhost:1313/guides/concepts/hashes/\" target=\"_blank\" rel=\"noopener\">加密哈希</a>，意思是：</p>\n<ul>\n<li>任何不相同的内容将会产生不同的CID</li>\n<li>内容中相同的部分添加到两个不同的IPFS节点通过相同的设置应该产生相同的CID。</li>\n</ul>\n<h2 id=\"CID格式\"><a href=\"#CID格式\" class=\"headerlink\" title=\"CID格式\"></a>CID格式</h2><p>基于不同的编码或者是CID的版本使得CID具有不同的格式。多数存在的IPFS工具仍生成版本0的CID。但是<code>file</code>(<a href=\"http://localhost:1313/guides/concepts/mfs/\" target=\"_blank\" rel=\"noopener\">MFS</a>)和<code>object</code>现在默认使用CID V1.</p>\n<h3 id=\"版本0\"><a href=\"#版本0\" class=\"headerlink\" title=\"版本0\"></a>版本0</h3><p>当IPFS初始设计的时候，使用base58多次哈希作为内容标识符(虽然简单，但是与新的CID相比缺乏灵活性。)CIDv0仍然是许多IPFS默认选项，所以IPFS版本应该支持v0。</p>\n<p>如果一个CID具有46字符并以<code>Qm</code>开头，说明是一个CIDv0</p>\n<h3 id=\"版本1\"><a href=\"#版本1\" class=\"headerlink\" title=\"版本1\"></a>版本1</h3><p>CID v1包含一些前导标识符，这些标识符明确说明了使用哪种表示形式以及内容哈希本身。 这些包括：</p>\n<ul>\n<li><a href=\"https://github.com/multiformats/multibase\" target=\"_blank\" rel=\"noopener\">multibase</a>前缀，指定用于其余CID的编码.</li>\n<li>CID版本标识符，指示这是哪个CID版本</li>\n<li>一个<a href=\"https://github.com/multiformats/multicodec\" target=\"_blank\" rel=\"noopener\">多编解码器</a>标识符，指示目标内容的格式-帮助人们和软件在获取内容后知道如何解释该内容</li>\n</ul>\n<p>这些前导标识符还提供前向兼容性，支持在将来的CID版本中使用的不同格式。<br>您可以使用CID的前几个字节来解释内容地址的其余部分，并知道从IPFS提取内容后如何对其进行解码。 有关更多详细信息，请查看<a href=\"https://github.com/ipld/cid\" target=\"_blank\" rel=\"noopener\">CID规范</a>。 它包括<a href=\"https://github.com/ipld/cid/blob/ef1b2002394b15b1e6c26c30545fd485f2c4c138/README.md#decoding-algorithm\" target=\"_blank\" rel=\"noopener\">解码算法</a>，并链接到用于解码CID的现有软件实现。</p>\n<h2 id=\"探索CID\"><a href=\"#探索CID\" class=\"headerlink\" title=\"探索CID\"></a>探索CID</h2><p>是否想分解特定CID的多库，多编解码器或多哈希信息？ 您可以使用IPLD资源管理器中的<a href=\"https://cid.ipfs.io/#QmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjQuPU\" target=\"_blank\" rel=\"noopener\">CID检查器</a>或<a href=\"https://explore.ipld.io/#/explore/QmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjQuPU\" target=\"_blank\" rel=\"noopener\">CID信息面板</a>（两个链接都使用示例CID启动）来对不同格式的CID进行交互式细分。 还了解IPFS中<a href=\"https://discuss.ipfs.io/t/who-decides-what-hashing-algorithms-ipfs-allows/6742\" target=\"_blank\" rel=\"noopener\">CID的未来</a>。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"内容标识符-CIDs\"><a href=\"#内容标识符-CIDs\" class=\"headerlink\" title=\"内容标识符(CIDs)\"></a>内容标识符(CIDs)</h1><p>内容标识符也称为CID，是用于指向IPFS中材料的标签。 它不会指示内容的存储位置，但会根据内容本身形成一种地址。 CID简短，无论其基础内容的大小如何。</p>\n<p>CID基于内容的<a href=\"http://localhost:1313/guides/concepts/hashes/\" target=\"_blank\" rel=\"noopener\">加密哈希</a>，意思是：</p>\n<ul>\n<li>任何不相同的内容将会产生不同的CID</li>\n<li>内容中相同的部分添加到两个不同的IPFS节点通过相同的设置应该产生相同的CID。</li>\n</ul>\n<h2 id=\"CID格式\"><a href=\"#CID格式\" class=\"headerlink\" title=\"CID格式\"></a>CID格式</h2><p>基于不同的编码或者是CID的版本使得CID具有不同的格式。多数存在的IPFS工具仍生成版本0的CID。但是<code>file</code>(<a href=\"http://localhost:1313/guides/concepts/mfs/\" target=\"_blank\" rel=\"noopener\">MFS</a>)和<code>object</code>现在默认使用CID V1.</p>\n<h3 id=\"版本0\"><a href=\"#版本0\" class=\"headerlink\" title=\"版本0\"></a>版本0</h3><p>当IPFS初始设计的时候，使用base58多次哈希作为内容标识符(虽然简单，但是与新的CID相比缺乏灵活性。)CIDv0仍然是许多IPFS默认选项，所以IPFS版本应该支持v0。</p>\n<p>如果一个CID具有46字符并以<code>Qm</code>开头，说明是一个CIDv0</p>\n<h3 id=\"版本1\"><a href=\"#版本1\" class=\"headerlink\" title=\"版本1\"></a>版本1</h3><p>CID v1包含一些前导标识符，这些标识符明确说明了使用哪种表示形式以及内容哈希本身。 这些包括：</p>\n<ul>\n<li><a href=\"https://github.com/multiformats/multibase\" target=\"_blank\" rel=\"noopener\">multibase</a>前缀，指定用于其余CID的编码.</li>\n<li>CID版本标识符，指示这是哪个CID版本</li>\n<li>一个<a href=\"https://github.com/multiformats/multicodec\" target=\"_blank\" rel=\"noopener\">多编解码器</a>标识符，指示目标内容的格式-帮助人们和软件在获取内容后知道如何解释该内容</li>\n</ul>\n<p>这些前导标识符还提供前向兼容性，支持在将来的CID版本中使用的不同格式。<br>您可以使用CID的前几个字节来解释内容地址的其余部分，并知道从IPFS提取内容后如何对其进行解码。 有关更多详细信息，请查看<a href=\"https://github.com/ipld/cid\" target=\"_blank\" rel=\"noopener\">CID规范</a>。 它包括<a href=\"https://github.com/ipld/cid/blob/ef1b2002394b15b1e6c26c30545fd485f2c4c138/README.md#decoding-algorithm\" target=\"_blank\" rel=\"noopener\">解码算法</a>，并链接到用于解码CID的现有软件实现。</p>\n<h2 id=\"探索CID\"><a href=\"#探索CID\" class=\"headerlink\" title=\"探索CID\"></a>探索CID</h2><p>是否想分解特定CID的多库，多编解码器或多哈希信息？ 您可以使用IPLD资源管理器中的<a href=\"https://cid.ipfs.io/#QmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjQuPU\" target=\"_blank\" rel=\"noopener\">CID检查器</a>或<a href=\"https://explore.ipld.io/#/explore/QmY7Yh4UquoXHLPFo2XbhXkhBvFoPwmQUSa92pxnxjQuPU\" target=\"_blank\" rel=\"noopener\">CID信息面板</a>（两个链接都使用示例CID启动）来对不同格式的CID进行交互式细分。 还了解IPFS中<a href=\"https://discuss.ipfs.io/t/who-decides-what-hashing-algorithms-ipfs-allows/6742\" target=\"_blank\" rel=\"noopener\">CID的未来</a>。</p>\n"},{"title":"IPFS学习-哈希","date":"2019-12-18T06:57:39.000Z","_content":"# Hashes\n哈希函数是接受一些任意输入并返回固定长度值的函数。具体值取决于所使用的给定哈希算法，例如[SHA-1](https://en.wikipedia.org/wiki/SHA-1)(GIT在使用),[SHA-256](https://en.wikipedia.org/wiki/SHA-2),或者是[BLAKE2](https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE2),但是给予一个输入使用哈希算法总是返回相同的输出。\n例如：输入以下：\n```\nHello world\n```\n使用**SHA-1**则会输出：\n```\n0x7B502C3A1F48C8609AE212CDFB639DEE39673F5E\n```\n然而相同的输入使用**SHA-256**将会输出以下：\n```\n0x64EC88CA00B268E5BA1A35678A1B5316D212F4F366B2477232534A8AECA37F3C\n```\n第二个哈希值长度要大于第一个，这是因为SHA-1创建一个160比特的哈希值，而SHA-256创建一个256比特的哈希值。同样，前置的`0x`只是一个指示符，告诉我们以下的哈希表示为基数16（或十六进制）的数字。\n哈希可以用不同的基数表示（`base2`，`base16`，`base32`等）。 实际上，IPFS将此作为其[内容标识符](https://ifican.top/2019/12/18/blog/ipfs/IPFS%E5%AD%A6%E4%B9%A0-%E5%86%85%E5%AE%B9%E6%A0%87%E8%AF%86%E7%AC%A6CID/)的一部分，并使用[Multibase](https://github.com/multiformats/multibase)协议同时支持多种基本表示形式。\n例如，”Hello World\"的SHA-256哈希值使用`base32`表示为：\n```\nmtwirsqawjuoloq2gvtyug2tc3jbf5htm2zeo4rsknfiv3fdp46a\n```\n### 加密散列的特征\n加密散列带有非常重要的特性：\n\n* 确定性-相同的输入消息总是返回相同的输出哈希。\n* 不相关-消息中的微小变化应生成完全不同的哈希。\n* 唯一性-从两条不同的消息生成相同的哈希是不可行的。\n* 单向性-从其哈希值猜测或计算输入消息是不可行的。\n\n事实证明，这些功能还意味着我们可以使用加密哈希来识别任何数据：哈希对于我们从中计算出的数据是唯一的，并且它不会太长（哈希是固定长度的，因此SHA-256哈希是 1 GB的视频文件的大小仍然只有32个字节），因此通过网络发送它不会占用很多资源。\n\n这对于像IPFS这样的分布式系统至关重要，在该系统中，我们希望能够从许多地方存储和检索数据。 运行IPFS的计算机可以询问与之连接的所有对等方，是否有一个带有特定哈希值的文件，如果其中一个具有特定的哈希值，则他们将整个文件发回。 没有短而独特的标识符（例如密码哈希），就不可能实现。 这项技术称为“内容寻址”-因为内容本身是用来形成地址的，而不是用于存储其所在计算机和磁盘位置的信息。","source":"_posts/blog/ipfs/IPFS学习-哈希.md","raw":"---\ntitle: IPFS学习-哈希\ndate: 2019-12-18 14:57:39\ntags: IPFS\ncategories: IPFS学习\n---\n# Hashes\n哈希函数是接受一些任意输入并返回固定长度值的函数。具体值取决于所使用的给定哈希算法，例如[SHA-1](https://en.wikipedia.org/wiki/SHA-1)(GIT在使用),[SHA-256](https://en.wikipedia.org/wiki/SHA-2),或者是[BLAKE2](https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE2),但是给予一个输入使用哈希算法总是返回相同的输出。\n例如：输入以下：\n```\nHello world\n```\n使用**SHA-1**则会输出：\n```\n0x7B502C3A1F48C8609AE212CDFB639DEE39673F5E\n```\n然而相同的输入使用**SHA-256**将会输出以下：\n```\n0x64EC88CA00B268E5BA1A35678A1B5316D212F4F366B2477232534A8AECA37F3C\n```\n第二个哈希值长度要大于第一个，这是因为SHA-1创建一个160比特的哈希值，而SHA-256创建一个256比特的哈希值。同样，前置的`0x`只是一个指示符，告诉我们以下的哈希表示为基数16（或十六进制）的数字。\n哈希可以用不同的基数表示（`base2`，`base16`，`base32`等）。 实际上，IPFS将此作为其[内容标识符](https://ifican.top/2019/12/18/blog/ipfs/IPFS%E5%AD%A6%E4%B9%A0-%E5%86%85%E5%AE%B9%E6%A0%87%E8%AF%86%E7%AC%A6CID/)的一部分，并使用[Multibase](https://github.com/multiformats/multibase)协议同时支持多种基本表示形式。\n例如，”Hello World\"的SHA-256哈希值使用`base32`表示为：\n```\nmtwirsqawjuoloq2gvtyug2tc3jbf5htm2zeo4rsknfiv3fdp46a\n```\n### 加密散列的特征\n加密散列带有非常重要的特性：\n\n* 确定性-相同的输入消息总是返回相同的输出哈希。\n* 不相关-消息中的微小变化应生成完全不同的哈希。\n* 唯一性-从两条不同的消息生成相同的哈希是不可行的。\n* 单向性-从其哈希值猜测或计算输入消息是不可行的。\n\n事实证明，这些功能还意味着我们可以使用加密哈希来识别任何数据：哈希对于我们从中计算出的数据是唯一的，并且它不会太长（哈希是固定长度的，因此SHA-256哈希是 1 GB的视频文件的大小仍然只有32个字节），因此通过网络发送它不会占用很多资源。\n\n这对于像IPFS这样的分布式系统至关重要，在该系统中，我们希望能够从许多地方存储和检索数据。 运行IPFS的计算机可以询问与之连接的所有对等方，是否有一个带有特定哈希值的文件，如果其中一个具有特定的哈希值，则他们将整个文件发回。 没有短而独特的标识符（例如密码哈希），就不可能实现。 这项技术称为“内容寻址”-因为内容本身是用来形成地址的，而不是用于存储其所在计算机和磁盘位置的信息。","slug":"blog/ipfs/IPFS学习-哈希","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqykb004ok0vqeoir990m","content":"<h1 id=\"Hashes\"><a href=\"#Hashes\" class=\"headerlink\" title=\"Hashes\"></a>Hashes</h1><p>哈希函数是接受一些任意输入并返回固定长度值的函数。具体值取决于所使用的给定哈希算法，例如<a href=\"https://en.wikipedia.org/wiki/SHA-1\" target=\"_blank\" rel=\"noopener\">SHA-1</a>(GIT在使用),<a href=\"https://en.wikipedia.org/wiki/SHA-2\" target=\"_blank\" rel=\"noopener\">SHA-256</a>,或者是<a href=\"https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE2\" target=\"_blank\" rel=\"noopener\">BLAKE2</a>,但是给予一个输入使用哈希算法总是返回相同的输出。<br>例如：输入以下：</p>\n<pre><code>Hello world</code></pre><p>使用<strong>SHA-1</strong>则会输出：</p>\n<pre><code>0x7B502C3A1F48C8609AE212CDFB639DEE39673F5E</code></pre><p>然而相同的输入使用<strong>SHA-256</strong>将会输出以下：</p>\n<pre><code>0x64EC88CA00B268E5BA1A35678A1B5316D212F4F366B2477232534A8AECA37F3C</code></pre><p>第二个哈希值长度要大于第一个，这是因为SHA-1创建一个160比特的哈希值，而SHA-256创建一个256比特的哈希值。同样，前置的<code>0x</code>只是一个指示符，告诉我们以下的哈希表示为基数16（或十六进制）的数字。<br>哈希可以用不同的基数表示（<code>base2</code>，<code>base16</code>，<code>base32</code>等）。 实际上，IPFS将此作为其<a href=\"https://ifican.top/2019/12/18/blog/ipfs/IPFS%E5%AD%A6%E4%B9%A0-%E5%86%85%E5%AE%B9%E6%A0%87%E8%AF%86%E7%AC%A6CID/\" target=\"_blank\" rel=\"noopener\">内容标识符</a>的一部分，并使用<a href=\"https://github.com/multiformats/multibase\" target=\"_blank\" rel=\"noopener\">Multibase</a>协议同时支持多种基本表示形式。<br>例如，”Hello World”的SHA-256哈希值使用<code>base32</code>表示为：</p>\n<pre><code>mtwirsqawjuoloq2gvtyug2tc3jbf5htm2zeo4rsknfiv3fdp46a</code></pre><h3 id=\"加密散列的特征\"><a href=\"#加密散列的特征\" class=\"headerlink\" title=\"加密散列的特征\"></a>加密散列的特征</h3><p>加密散列带有非常重要的特性：</p>\n<ul>\n<li>确定性-相同的输入消息总是返回相同的输出哈希。</li>\n<li>不相关-消息中的微小变化应生成完全不同的哈希。</li>\n<li>唯一性-从两条不同的消息生成相同的哈希是不可行的。</li>\n<li>单向性-从其哈希值猜测或计算输入消息是不可行的。</li>\n</ul>\n<p>事实证明，这些功能还意味着我们可以使用加密哈希来识别任何数据：哈希对于我们从中计算出的数据是唯一的，并且它不会太长（哈希是固定长度的，因此SHA-256哈希是 1 GB的视频文件的大小仍然只有32个字节），因此通过网络发送它不会占用很多资源。</p>\n<p>这对于像IPFS这样的分布式系统至关重要，在该系统中，我们希望能够从许多地方存储和检索数据。 运行IPFS的计算机可以询问与之连接的所有对等方，是否有一个带有特定哈希值的文件，如果其中一个具有特定的哈希值，则他们将整个文件发回。 没有短而独特的标识符（例如密码哈希），就不可能实现。 这项技术称为“内容寻址”-因为内容本身是用来形成地址的，而不是用于存储其所在计算机和磁盘位置的信息。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Hashes\"><a href=\"#Hashes\" class=\"headerlink\" title=\"Hashes\"></a>Hashes</h1><p>哈希函数是接受一些任意输入并返回固定长度值的函数。具体值取决于所使用的给定哈希算法，例如<a href=\"https://en.wikipedia.org/wiki/SHA-1\" target=\"_blank\" rel=\"noopener\">SHA-1</a>(GIT在使用),<a href=\"https://en.wikipedia.org/wiki/SHA-2\" target=\"_blank\" rel=\"noopener\">SHA-256</a>,或者是<a href=\"https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE2\" target=\"_blank\" rel=\"noopener\">BLAKE2</a>,但是给予一个输入使用哈希算法总是返回相同的输出。<br>例如：输入以下：</p>\n<pre><code>Hello world</code></pre><p>使用<strong>SHA-1</strong>则会输出：</p>\n<pre><code>0x7B502C3A1F48C8609AE212CDFB639DEE39673F5E</code></pre><p>然而相同的输入使用<strong>SHA-256</strong>将会输出以下：</p>\n<pre><code>0x64EC88CA00B268E5BA1A35678A1B5316D212F4F366B2477232534A8AECA37F3C</code></pre><p>第二个哈希值长度要大于第一个，这是因为SHA-1创建一个160比特的哈希值，而SHA-256创建一个256比特的哈希值。同样，前置的<code>0x</code>只是一个指示符，告诉我们以下的哈希表示为基数16（或十六进制）的数字。<br>哈希可以用不同的基数表示（<code>base2</code>，<code>base16</code>，<code>base32</code>等）。 实际上，IPFS将此作为其<a href=\"https://ifican.top/2019/12/18/blog/ipfs/IPFS%E5%AD%A6%E4%B9%A0-%E5%86%85%E5%AE%B9%E6%A0%87%E8%AF%86%E7%AC%A6CID/\" target=\"_blank\" rel=\"noopener\">内容标识符</a>的一部分，并使用<a href=\"https://github.com/multiformats/multibase\" target=\"_blank\" rel=\"noopener\">Multibase</a>协议同时支持多种基本表示形式。<br>例如，”Hello World”的SHA-256哈希值使用<code>base32</code>表示为：</p>\n<pre><code>mtwirsqawjuoloq2gvtyug2tc3jbf5htm2zeo4rsknfiv3fdp46a</code></pre><h3 id=\"加密散列的特征\"><a href=\"#加密散列的特征\" class=\"headerlink\" title=\"加密散列的特征\"></a>加密散列的特征</h3><p>加密散列带有非常重要的特性：</p>\n<ul>\n<li>确定性-相同的输入消息总是返回相同的输出哈希。</li>\n<li>不相关-消息中的微小变化应生成完全不同的哈希。</li>\n<li>唯一性-从两条不同的消息生成相同的哈希是不可行的。</li>\n<li>单向性-从其哈希值猜测或计算输入消息是不可行的。</li>\n</ul>\n<p>事实证明，这些功能还意味着我们可以使用加密哈希来识别任何数据：哈希对于我们从中计算出的数据是唯一的，并且它不会太长（哈希是固定长度的，因此SHA-256哈希是 1 GB的视频文件的大小仍然只有32个字节），因此通过网络发送它不会占用很多资源。</p>\n<p>这对于像IPFS这样的分布式系统至关重要，在该系统中，我们希望能够从许多地方存储和检索数据。 运行IPFS的计算机可以询问与之连接的所有对等方，是否有一个带有特定哈希值的文件，如果其中一个具有特定的哈希值，则他们将整个文件发回。 没有短而独特的标识符（例如密码哈希），就不可能实现。 这项技术称为“内容寻址”-因为内容本身是用来形成地址的，而不是用于存储其所在计算机和磁盘位置的信息。</p>\n"},{"title":"IPFS学习-分布式哈希表DHT","date":"2019-12-18T02:54:50.000Z","_content":"## Distributed Hash Tables(DHT)\n[分布式哈希表](https://en.wikipedia.org/wiki/Distributed_hash_table)是一个分布式的键值对存储结构。在IPFS网络中，每一个节点都维护一个DHT的子集。当节点接受到一个请求。该节点要么直接回复，要么通过节点间传输直到找到可以回复该请求的节点。取决于实现方式，一个请求如果不能被第一个连接的节点回复\n\n* 进行节点间的转发，由最后一个节点联系收到请求的节点。\n* 进行节点间的转发，回复结果按照相同的路径转发回到原节点。\n* 由最优选择的节点对请求进行回复。\n\nIPFS使用这种策略。\nDHT的去中心化提供了相比于传统的键值对存储更好的优势。包括：\n\n* 扩展性。对长度为n的哈希请求只需要最多为log<sub>2</sub>n步即可解决。\n* 通过冗余进行错误容忍。即可能每一个节点都加入或离开DHT。另外，如果一个节点反应缓慢或者不可达，请求可以连接到其他节点。\n* 负载均衡，请求可以发送到任何节点，没有任何一个节点处理所有的请求。\n\n## DHT如何工作\n### Peer IDs\n每一个节点有有一个`peerID`，和DHT的键相同都是长度为n的哈希值。\n### Buckets\n由每一个节点维护的DHT的子集被称为”桶“，一个桶映射的哈希值和节点ID具有相同的前缀。最多m个比特位。有2<sup>m</sup>个桶，每个桶则映射2<sup>n-m</sup>个哈希值。\n例如，如果m=2^16，并且使用16进制数据，节点ID为`ABCDEF12345`，维护以`ABCD`为前缀的哈希值映射。桶内的哈希值则可能为`*ABCD*38E56,*ABCD*09CBA,*ABCD*17ABB`.\n### 节点列表\n节点之间保持连接到其他节点为了转发请求(当请求的哈希值不在当前节点的桶内)\n如果哈希值长度为n，一个节点将保持连接n-1个列表节点。\n\n* 第一个列表维护第一个比特值不同的节点ID的节点。\n* 第二个列表维护前一个比特值相同，第二个比特值不同的节点ID的节点。\n* 第三个列表维护前两个比特值相同，第三个比特值不同的节点ID的节点。\n* ...\n\n假设最高的是第m个列表，很难发现最多有m个比特值相同的节点ID的节点。“最接近”对等方的列表通常保持空白。此处的“最接近”定义为XOR距离，因此它们共享的前缀越长，它们就越接近。列表还具有最大的条目（k）-否则第一个列表将包含一半的网络，然后是网络的四分之一，依此类推。\n\n### DHT使用\n当节点接受到查询请求后，如果可以在自己的桶中找到答案则回复。否则联系最接近该节点的节点(IP+port,peerID,等等)回复。收到请求的节点尅将请求发送给最接近的节点。这个过程一直到可以回复请求的节点。一个哈希值长度为n的请求最多只需要log<sub>2</sub>n步，甚至是log2<sub>m</sub>n步。\n\n### 键和哈希值\n在IPFS的Kademili DHT，键使用SHA256哈希。[节点ID](https://docs.libp2p.io/concepts/peer-id/)使用由IPFS使用的网络库[libp2p](https://libp2p.io/)。\n使用DHT查看两种类型的对象时，都由SHA256进行散列：\n\n* 添加到IPFS的数据的[Content IDs](https://docs.ipfs.io/guides/concepts/cid/)。查找该值将给出具有该不变内容的对等方的peerID。 \n* [IPNS记录](https://docs.ipfs.io/guides/concepts/ipns/)。查找将给出与此IPNS地址关联的最后一个Content ID，从而启用可变内容的路由。\n\n所以，IPFS的DHT只是实现不可变与可变[内容路由](https://docs.libp2p.io/concepts/content-routing/)的一种方式.当前只是一种[实现](https://libp2p.io/implementations/#peer-routing).\n\n## 使用\n### 添加一条记录\n添加一个`blob`类型的数据到IPFS等同于广播它，由于DHT由内容路由实现。可以通过`ipfs add myData`自动打包数据挺添加内容ID和节点ID之间的映射到DHT。注意这里可能也被其他节点ID映射到该值，所以需要添加到列表中。如果提供的数据大于124KB，数据将会被打包成`blocks`，整个块将被映射。\n可以通过使用`ipfs.name.publish`发布一个IPNS记录。","source":"_posts/blog/ipfs/IPFS学习-分布式哈希表DHT.md","raw":"---\ntitle: IPFS学习-分布式哈希表DHT\ndate: 2019-12-18 10:54:50\ntags: IPFS\ncategories: IPFS学习\n---\n## Distributed Hash Tables(DHT)\n[分布式哈希表](https://en.wikipedia.org/wiki/Distributed_hash_table)是一个分布式的键值对存储结构。在IPFS网络中，每一个节点都维护一个DHT的子集。当节点接受到一个请求。该节点要么直接回复，要么通过节点间传输直到找到可以回复该请求的节点。取决于实现方式，一个请求如果不能被第一个连接的节点回复\n\n* 进行节点间的转发，由最后一个节点联系收到请求的节点。\n* 进行节点间的转发，回复结果按照相同的路径转发回到原节点。\n* 由最优选择的节点对请求进行回复。\n\nIPFS使用这种策略。\nDHT的去中心化提供了相比于传统的键值对存储更好的优势。包括：\n\n* 扩展性。对长度为n的哈希请求只需要最多为log<sub>2</sub>n步即可解决。\n* 通过冗余进行错误容忍。即可能每一个节点都加入或离开DHT。另外，如果一个节点反应缓慢或者不可达，请求可以连接到其他节点。\n* 负载均衡，请求可以发送到任何节点，没有任何一个节点处理所有的请求。\n\n## DHT如何工作\n### Peer IDs\n每一个节点有有一个`peerID`，和DHT的键相同都是长度为n的哈希值。\n### Buckets\n由每一个节点维护的DHT的子集被称为”桶“，一个桶映射的哈希值和节点ID具有相同的前缀。最多m个比特位。有2<sup>m</sup>个桶，每个桶则映射2<sup>n-m</sup>个哈希值。\n例如，如果m=2^16，并且使用16进制数据，节点ID为`ABCDEF12345`，维护以`ABCD`为前缀的哈希值映射。桶内的哈希值则可能为`*ABCD*38E56,*ABCD*09CBA,*ABCD*17ABB`.\n### 节点列表\n节点之间保持连接到其他节点为了转发请求(当请求的哈希值不在当前节点的桶内)\n如果哈希值长度为n，一个节点将保持连接n-1个列表节点。\n\n* 第一个列表维护第一个比特值不同的节点ID的节点。\n* 第二个列表维护前一个比特值相同，第二个比特值不同的节点ID的节点。\n* 第三个列表维护前两个比特值相同，第三个比特值不同的节点ID的节点。\n* ...\n\n假设最高的是第m个列表，很难发现最多有m个比特值相同的节点ID的节点。“最接近”对等方的列表通常保持空白。此处的“最接近”定义为XOR距离，因此它们共享的前缀越长，它们就越接近。列表还具有最大的条目（k）-否则第一个列表将包含一半的网络，然后是网络的四分之一，依此类推。\n\n### DHT使用\n当节点接受到查询请求后，如果可以在自己的桶中找到答案则回复。否则联系最接近该节点的节点(IP+port,peerID,等等)回复。收到请求的节点尅将请求发送给最接近的节点。这个过程一直到可以回复请求的节点。一个哈希值长度为n的请求最多只需要log<sub>2</sub>n步，甚至是log2<sub>m</sub>n步。\n\n### 键和哈希值\n在IPFS的Kademili DHT，键使用SHA256哈希。[节点ID](https://docs.libp2p.io/concepts/peer-id/)使用由IPFS使用的网络库[libp2p](https://libp2p.io/)。\n使用DHT查看两种类型的对象时，都由SHA256进行散列：\n\n* 添加到IPFS的数据的[Content IDs](https://docs.ipfs.io/guides/concepts/cid/)。查找该值将给出具有该不变内容的对等方的peerID。 \n* [IPNS记录](https://docs.ipfs.io/guides/concepts/ipns/)。查找将给出与此IPNS地址关联的最后一个Content ID，从而启用可变内容的路由。\n\n所以，IPFS的DHT只是实现不可变与可变[内容路由](https://docs.libp2p.io/concepts/content-routing/)的一种方式.当前只是一种[实现](https://libp2p.io/implementations/#peer-routing).\n\n## 使用\n### 添加一条记录\n添加一个`blob`类型的数据到IPFS等同于广播它，由于DHT由内容路由实现。可以通过`ipfs add myData`自动打包数据挺添加内容ID和节点ID之间的映射到DHT。注意这里可能也被其他节点ID映射到该值，所以需要添加到列表中。如果提供的数据大于124KB，数据将会被打包成`blocks`，整个块将被映射。\n可以通过使用`ipfs.name.publish`发布一个IPNS记录。","slug":"blog/ipfs/IPFS学习-分布式哈希表DHT","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyki004rk0vqcvj418ji","content":"<h2 id=\"Distributed-Hash-Tables-DHT\"><a href=\"#Distributed-Hash-Tables-DHT\" class=\"headerlink\" title=\"Distributed Hash Tables(DHT)\"></a>Distributed Hash Tables(DHT)</h2><p><a href=\"https://en.wikipedia.org/wiki/Distributed_hash_table\" target=\"_blank\" rel=\"noopener\">分布式哈希表</a>是一个分布式的键值对存储结构。在IPFS网络中，每一个节点都维护一个DHT的子集。当节点接受到一个请求。该节点要么直接回复，要么通过节点间传输直到找到可以回复该请求的节点。取决于实现方式，一个请求如果不能被第一个连接的节点回复</p>\n<ul>\n<li>进行节点间的转发，由最后一个节点联系收到请求的节点。</li>\n<li>进行节点间的转发，回复结果按照相同的路径转发回到原节点。</li>\n<li>由最优选择的节点对请求进行回复。</li>\n</ul>\n<p>IPFS使用这种策略。<br>DHT的去中心化提供了相比于传统的键值对存储更好的优势。包括：</p>\n<ul>\n<li>扩展性。对长度为n的哈希请求只需要最多为log<sub>2</sub>n步即可解决。</li>\n<li>通过冗余进行错误容忍。即可能每一个节点都加入或离开DHT。另外，如果一个节点反应缓慢或者不可达，请求可以连接到其他节点。</li>\n<li>负载均衡，请求可以发送到任何节点，没有任何一个节点处理所有的请求。</li>\n</ul>\n<h2 id=\"DHT如何工作\"><a href=\"#DHT如何工作\" class=\"headerlink\" title=\"DHT如何工作\"></a>DHT如何工作</h2><h3 id=\"Peer-IDs\"><a href=\"#Peer-IDs\" class=\"headerlink\" title=\"Peer IDs\"></a>Peer IDs</h3><p>每一个节点有有一个<code>peerID</code>，和DHT的键相同都是长度为n的哈希值。</p>\n<h3 id=\"Buckets\"><a href=\"#Buckets\" class=\"headerlink\" title=\"Buckets\"></a>Buckets</h3><p>由每一个节点维护的DHT的子集被称为”桶“，一个桶映射的哈希值和节点ID具有相同的前缀。最多m个比特位。有2<sup>m</sup>个桶，每个桶则映射2<sup>n-m</sup>个哈希值。<br>例如，如果m=2^16，并且使用16进制数据，节点ID为<code>ABCDEF12345</code>，维护以<code>ABCD</code>为前缀的哈希值映射。桶内的哈希值则可能为<code>*ABCD*38E56,*ABCD*09CBA,*ABCD*17ABB</code>.</p>\n<h3 id=\"节点列表\"><a href=\"#节点列表\" class=\"headerlink\" title=\"节点列表\"></a>节点列表</h3><p>节点之间保持连接到其他节点为了转发请求(当请求的哈希值不在当前节点的桶内)<br>如果哈希值长度为n，一个节点将保持连接n-1个列表节点。</p>\n<ul>\n<li>第一个列表维护第一个比特值不同的节点ID的节点。</li>\n<li>第二个列表维护前一个比特值相同，第二个比特值不同的节点ID的节点。</li>\n<li>第三个列表维护前两个比特值相同，第三个比特值不同的节点ID的节点。</li>\n<li>…</li>\n</ul>\n<p>假设最高的是第m个列表，很难发现最多有m个比特值相同的节点ID的节点。“最接近”对等方的列表通常保持空白。此处的“最接近”定义为XOR距离，因此它们共享的前缀越长，它们就越接近。列表还具有最大的条目（k）-否则第一个列表将包含一半的网络，然后是网络的四分之一，依此类推。</p>\n<h3 id=\"DHT使用\"><a href=\"#DHT使用\" class=\"headerlink\" title=\"DHT使用\"></a>DHT使用</h3><p>当节点接受到查询请求后，如果可以在自己的桶中找到答案则回复。否则联系最接近该节点的节点(IP+port,peerID,等等)回复。收到请求的节点尅将请求发送给最接近的节点。这个过程一直到可以回复请求的节点。一个哈希值长度为n的请求最多只需要log<sub>2</sub>n步，甚至是log2<sub>m</sub>n步。</p>\n<h3 id=\"键和哈希值\"><a href=\"#键和哈希值\" class=\"headerlink\" title=\"键和哈希值\"></a>键和哈希值</h3><p>在IPFS的Kademili DHT，键使用SHA256哈希。<a href=\"https://docs.libp2p.io/concepts/peer-id/\" target=\"_blank\" rel=\"noopener\">节点ID</a>使用由IPFS使用的网络库<a href=\"https://libp2p.io/\" target=\"_blank\" rel=\"noopener\">libp2p</a>。<br>使用DHT查看两种类型的对象时，都由SHA256进行散列：</p>\n<ul>\n<li>添加到IPFS的数据的<a href=\"https://docs.ipfs.io/guides/concepts/cid/\" target=\"_blank\" rel=\"noopener\">Content IDs</a>。查找该值将给出具有该不变内容的对等方的peerID。 </li>\n<li><a href=\"https://docs.ipfs.io/guides/concepts/ipns/\" target=\"_blank\" rel=\"noopener\">IPNS记录</a>。查找将给出与此IPNS地址关联的最后一个Content ID，从而启用可变内容的路由。</li>\n</ul>\n<p>所以，IPFS的DHT只是实现不可变与可变<a href=\"https://docs.libp2p.io/concepts/content-routing/\" target=\"_blank\" rel=\"noopener\">内容路由</a>的一种方式.当前只是一种<a href=\"https://libp2p.io/implementations/#peer-routing\" target=\"_blank\" rel=\"noopener\">实现</a>.</p>\n<h2 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h2><h3 id=\"添加一条记录\"><a href=\"#添加一条记录\" class=\"headerlink\" title=\"添加一条记录\"></a>添加一条记录</h3><p>添加一个<code>blob</code>类型的数据到IPFS等同于广播它，由于DHT由内容路由实现。可以通过<code>ipfs add myData</code>自动打包数据挺添加内容ID和节点ID之间的映射到DHT。注意这里可能也被其他节点ID映射到该值，所以需要添加到列表中。如果提供的数据大于124KB，数据将会被打包成<code>blocks</code>，整个块将被映射。<br>可以通过使用<code>ipfs.name.publish</code>发布一个IPNS记录。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Distributed-Hash-Tables-DHT\"><a href=\"#Distributed-Hash-Tables-DHT\" class=\"headerlink\" title=\"Distributed Hash Tables(DHT)\"></a>Distributed Hash Tables(DHT)</h2><p><a href=\"https://en.wikipedia.org/wiki/Distributed_hash_table\" target=\"_blank\" rel=\"noopener\">分布式哈希表</a>是一个分布式的键值对存储结构。在IPFS网络中，每一个节点都维护一个DHT的子集。当节点接受到一个请求。该节点要么直接回复，要么通过节点间传输直到找到可以回复该请求的节点。取决于实现方式，一个请求如果不能被第一个连接的节点回复</p>\n<ul>\n<li>进行节点间的转发，由最后一个节点联系收到请求的节点。</li>\n<li>进行节点间的转发，回复结果按照相同的路径转发回到原节点。</li>\n<li>由最优选择的节点对请求进行回复。</li>\n</ul>\n<p>IPFS使用这种策略。<br>DHT的去中心化提供了相比于传统的键值对存储更好的优势。包括：</p>\n<ul>\n<li>扩展性。对长度为n的哈希请求只需要最多为log<sub>2</sub>n步即可解决。</li>\n<li>通过冗余进行错误容忍。即可能每一个节点都加入或离开DHT。另外，如果一个节点反应缓慢或者不可达，请求可以连接到其他节点。</li>\n<li>负载均衡，请求可以发送到任何节点，没有任何一个节点处理所有的请求。</li>\n</ul>\n<h2 id=\"DHT如何工作\"><a href=\"#DHT如何工作\" class=\"headerlink\" title=\"DHT如何工作\"></a>DHT如何工作</h2><h3 id=\"Peer-IDs\"><a href=\"#Peer-IDs\" class=\"headerlink\" title=\"Peer IDs\"></a>Peer IDs</h3><p>每一个节点有有一个<code>peerID</code>，和DHT的键相同都是长度为n的哈希值。</p>\n<h3 id=\"Buckets\"><a href=\"#Buckets\" class=\"headerlink\" title=\"Buckets\"></a>Buckets</h3><p>由每一个节点维护的DHT的子集被称为”桶“，一个桶映射的哈希值和节点ID具有相同的前缀。最多m个比特位。有2<sup>m</sup>个桶，每个桶则映射2<sup>n-m</sup>个哈希值。<br>例如，如果m=2^16，并且使用16进制数据，节点ID为<code>ABCDEF12345</code>，维护以<code>ABCD</code>为前缀的哈希值映射。桶内的哈希值则可能为<code>*ABCD*38E56,*ABCD*09CBA,*ABCD*17ABB</code>.</p>\n<h3 id=\"节点列表\"><a href=\"#节点列表\" class=\"headerlink\" title=\"节点列表\"></a>节点列表</h3><p>节点之间保持连接到其他节点为了转发请求(当请求的哈希值不在当前节点的桶内)<br>如果哈希值长度为n，一个节点将保持连接n-1个列表节点。</p>\n<ul>\n<li>第一个列表维护第一个比特值不同的节点ID的节点。</li>\n<li>第二个列表维护前一个比特值相同，第二个比特值不同的节点ID的节点。</li>\n<li>第三个列表维护前两个比特值相同，第三个比特值不同的节点ID的节点。</li>\n<li>…</li>\n</ul>\n<p>假设最高的是第m个列表，很难发现最多有m个比特值相同的节点ID的节点。“最接近”对等方的列表通常保持空白。此处的“最接近”定义为XOR距离，因此它们共享的前缀越长，它们就越接近。列表还具有最大的条目（k）-否则第一个列表将包含一半的网络，然后是网络的四分之一，依此类推。</p>\n<h3 id=\"DHT使用\"><a href=\"#DHT使用\" class=\"headerlink\" title=\"DHT使用\"></a>DHT使用</h3><p>当节点接受到查询请求后，如果可以在自己的桶中找到答案则回复。否则联系最接近该节点的节点(IP+port,peerID,等等)回复。收到请求的节点尅将请求发送给最接近的节点。这个过程一直到可以回复请求的节点。一个哈希值长度为n的请求最多只需要log<sub>2</sub>n步，甚至是log2<sub>m</sub>n步。</p>\n<h3 id=\"键和哈希值\"><a href=\"#键和哈希值\" class=\"headerlink\" title=\"键和哈希值\"></a>键和哈希值</h3><p>在IPFS的Kademili DHT，键使用SHA256哈希。<a href=\"https://docs.libp2p.io/concepts/peer-id/\" target=\"_blank\" rel=\"noopener\">节点ID</a>使用由IPFS使用的网络库<a href=\"https://libp2p.io/\" target=\"_blank\" rel=\"noopener\">libp2p</a>。<br>使用DHT查看两种类型的对象时，都由SHA256进行散列：</p>\n<ul>\n<li>添加到IPFS的数据的<a href=\"https://docs.ipfs.io/guides/concepts/cid/\" target=\"_blank\" rel=\"noopener\">Content IDs</a>。查找该值将给出具有该不变内容的对等方的peerID。 </li>\n<li><a href=\"https://docs.ipfs.io/guides/concepts/ipns/\" target=\"_blank\" rel=\"noopener\">IPNS记录</a>。查找将给出与此IPNS地址关联的最后一个Content ID，从而启用可变内容的路由。</li>\n</ul>\n<p>所以，IPFS的DHT只是实现不可变与可变<a href=\"https://docs.libp2p.io/concepts/content-routing/\" target=\"_blank\" rel=\"noopener\">内容路由</a>的一种方式.当前只是一种<a href=\"https://libp2p.io/implementations/#peer-routing\" target=\"_blank\" rel=\"noopener\">实现</a>.</p>\n<h2 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h2><h3 id=\"添加一条记录\"><a href=\"#添加一条记录\" class=\"headerlink\" title=\"添加一条记录\"></a>添加一条记录</h3><p>添加一个<code>blob</code>类型的数据到IPFS等同于广播它，由于DHT由内容路由实现。可以通过<code>ipfs add myData</code>自动打包数据挺添加内容ID和节点ID之间的映射到DHT。注意这里可能也被其他节点ID映射到该值，所以需要添加到列表中。如果提供的数据大于124KB，数据将会被打包成<code>blocks</code>，整个块将被映射。<br>可以通过使用<code>ipfs.name.publish</code>发布一个IPNS记录。</p>\n"},{"title":"正则表达式","date":"2019-12-22T04:27:56.000Z","_content":"\n# 字符位置\n\n|符号表示|符号含义|示例|\n|---|---|---|\n|`^str`|需要查找的字符在行首|`^a` 查找以字符`a`开头|\n|`str$`|需要查找的字符在行尾|`a$` 查找以字符`a`结尾|\n\n# 字符表示\n\n|符号表示|符号含义|示例|\n|---|---|---|\n|`\\ `|转义符|将特殊字符变为普通字符如`\\^`|\n|`.`|说明一定有一个任意的字符|`a.b` 说明字符`a`与`b`之间一定存在一个字符|\n|`*`|说明存在零个或者多个前一个字符|`a*`说明字符`a`后边可能存在0个或多个字符`a` `.*`表示存在任意字符|\n|`+`|**扩展正则**说明存在一个或一个以上前一个字符|`a+`说明字符`a`后边可能存在1个或多个字符`a`|\n|`?`|**扩展正则**说明存在0个或一个前一个字符|`ab？`查找字符`ab`或者是`a`|\n|`|`|或|`a|b`:查找字符`a`或字符`b`|\n|`()`|**扩展正则**字符集合|`(ab|cd)`:查找字符串`ab`或者是`cd`|\n|`()+`|**扩展正则**多个重复字符集合|`(ab)+`:查找具有一个以上`ab`子字符串的字符串|\n|`[list]`|列出可能存在的字符|`a[bc]`查找字符`ab`或者是`ac`|\n|`[n1-n2]`|列出可能存在的字符区间|`[a-g]` 查找字符区间`a-g`中任意字符|\n|`[^list}`|列出不需要的字符即反向选择|`[^a]` 查找字符中不存在`a`的字符|\n|`\\{n,m\\}`|**连续**`n`到`m`个之前的字符|`a\\{2,3\\}`查找字符`aa`或者是`aaa`;</br> `a\\{2\\}`查找字符`aa`;</br>`a\\{2,\\}`查找连续2个字符`a`以上的字符如`aaa`，`aaaa`等;|\n\n# 字符替换\n\n|符号表示|符号含义|\n|---|---|\n|`[:digit:]`|代表数字`0-9`|\n|`[:alnum:]`|代表英文字符和数字:`a-z,A-Z,0-9`|\n|`[:lower:]`|代表小写字符:`a-z`|\n|`[:upper:]`|代表大写字符:`A-Z`|\n|`[:space:]`|代表空格，包括`[Tab]`|\n\n# 输出格式\n\n|符号表示|符号含义|\n|---|---|\n|`\\b`|回退键|\n|`\\f`|换页符|\n|`\\n`|换行符|\n|`\\r`|回车键|\n|`\\t`|`Tab`键|","source":"_posts/blog/linux/正则表达式.md","raw":"---\ntitle: 正则表达式\ndate: 2019-12-22 12:27:56\ntags: 正则表达式\n---\n\n# 字符位置\n\n|符号表示|符号含义|示例|\n|---|---|---|\n|`^str`|需要查找的字符在行首|`^a` 查找以字符`a`开头|\n|`str$`|需要查找的字符在行尾|`a$` 查找以字符`a`结尾|\n\n# 字符表示\n\n|符号表示|符号含义|示例|\n|---|---|---|\n|`\\ `|转义符|将特殊字符变为普通字符如`\\^`|\n|`.`|说明一定有一个任意的字符|`a.b` 说明字符`a`与`b`之间一定存在一个字符|\n|`*`|说明存在零个或者多个前一个字符|`a*`说明字符`a`后边可能存在0个或多个字符`a` `.*`表示存在任意字符|\n|`+`|**扩展正则**说明存在一个或一个以上前一个字符|`a+`说明字符`a`后边可能存在1个或多个字符`a`|\n|`?`|**扩展正则**说明存在0个或一个前一个字符|`ab？`查找字符`ab`或者是`a`|\n|`|`|或|`a|b`:查找字符`a`或字符`b`|\n|`()`|**扩展正则**字符集合|`(ab|cd)`:查找字符串`ab`或者是`cd`|\n|`()+`|**扩展正则**多个重复字符集合|`(ab)+`:查找具有一个以上`ab`子字符串的字符串|\n|`[list]`|列出可能存在的字符|`a[bc]`查找字符`ab`或者是`ac`|\n|`[n1-n2]`|列出可能存在的字符区间|`[a-g]` 查找字符区间`a-g`中任意字符|\n|`[^list}`|列出不需要的字符即反向选择|`[^a]` 查找字符中不存在`a`的字符|\n|`\\{n,m\\}`|**连续**`n`到`m`个之前的字符|`a\\{2,3\\}`查找字符`aa`或者是`aaa`;</br> `a\\{2\\}`查找字符`aa`;</br>`a\\{2,\\}`查找连续2个字符`a`以上的字符如`aaa`，`aaaa`等;|\n\n# 字符替换\n\n|符号表示|符号含义|\n|---|---|\n|`[:digit:]`|代表数字`0-9`|\n|`[:alnum:]`|代表英文字符和数字:`a-z,A-Z,0-9`|\n|`[:lower:]`|代表小写字符:`a-z`|\n|`[:upper:]`|代表大写字符:`A-Z`|\n|`[:space:]`|代表空格，包括`[Tab]`|\n\n# 输出格式\n\n|符号表示|符号含义|\n|---|---|\n|`\\b`|回退键|\n|`\\f`|换页符|\n|`\\n`|换行符|\n|`\\r`|回车键|\n|`\\t`|`Tab`键|","slug":"blog/linux/正则表达式","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqykl004vk0vq4es808it","content":"<h1 id=\"字符位置\"><a href=\"#字符位置\" class=\"headerlink\" title=\"字符位置\"></a>字符位置</h1><table>\n<thead>\n<tr>\n<th>符号表示</th>\n<th>符号含义</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>^str</code></td>\n<td>需要查找的字符在行首</td>\n<td><code>^a</code> 查找以字符<code>a</code>开头</td>\n</tr>\n<tr>\n<td><code>str$</code></td>\n<td>需要查找的字符在行尾</td>\n<td><code>a$</code> 查找以字符<code>a</code>结尾</td>\n</tr>\n</tbody></table>\n<h1 id=\"字符表示\"><a href=\"#字符表示\" class=\"headerlink\" title=\"字符表示\"></a>字符表示</h1><table>\n<thead>\n<tr>\n<th>符号表示</th>\n<th>符号含义</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>\\</code></td>\n<td>转义符</td>\n<td>将特殊字符变为普通字符如<code>\\^</code></td>\n</tr>\n<tr>\n<td><code>.</code></td>\n<td>说明一定有一个任意的字符</td>\n<td><code>a.b</code> 说明字符<code>a</code>与<code>b</code>之间一定存在一个字符</td>\n</tr>\n<tr>\n<td><code>*</code></td>\n<td>说明存在零个或者多个前一个字符</td>\n<td><code>a*</code>说明字符<code>a</code>后边可能存在0个或多个字符<code>a</code> <code>.*</code>表示存在任意字符</td>\n</tr>\n<tr>\n<td><code>+</code></td>\n<td><strong>扩展正则</strong>说明存在一个或一个以上前一个字符</td>\n<td><code>a+</code>说明字符<code>a</code>后边可能存在1个或多个字符<code>a</code></td>\n</tr>\n<tr>\n<td><code>?</code></td>\n<td><strong>扩展正则</strong>说明存在0个或一个前一个字符</td>\n<td><code>ab？</code>查找字符<code>ab</code>或者是<code>a</code></td>\n</tr>\n<tr>\n<td>`</td>\n<td>`</td>\n<td>或</td>\n</tr>\n<tr>\n<td><code>()</code></td>\n<td><strong>扩展正则</strong>字符集合</td>\n<td>`(ab</td>\n</tr>\n<tr>\n<td><code>()+</code></td>\n<td><strong>扩展正则</strong>多个重复字符集合</td>\n<td><code>(ab)+</code>:查找具有一个以上<code>ab</code>子字符串的字符串</td>\n</tr>\n<tr>\n<td><code>[list]</code></td>\n<td>列出可能存在的字符</td>\n<td><code>a[bc]</code>查找字符<code>ab</code>或者是<code>ac</code></td>\n</tr>\n<tr>\n<td><code>[n1-n2]</code></td>\n<td>列出可能存在的字符区间</td>\n<td><code>[a-g]</code> 查找字符区间<code>a-g</code>中任意字符</td>\n</tr>\n<tr>\n<td><code>[^list}</code></td>\n<td>列出不需要的字符即反向选择</td>\n<td><code>[^a]</code> 查找字符中不存在<code>a</code>的字符</td>\n</tr>\n<tr>\n<td><code>\\{n,m\\}</code></td>\n<td><strong>连续</strong><code>n</code>到<code>m</code>个之前的字符</td>\n<td><code>a\\{2,3\\}</code>查找字符<code>aa</code>或者是<code>aaa</code>;</br> <code>a\\{2\\}</code>查找字符<code>aa</code>;</br><code>a\\{2,\\}</code>查找连续2个字符<code>a</code>以上的字符如<code>aaa</code>，<code>aaaa</code>等;</td>\n</tr>\n</tbody></table>\n<h1 id=\"字符替换\"><a href=\"#字符替换\" class=\"headerlink\" title=\"字符替换\"></a>字符替换</h1><table>\n<thead>\n<tr>\n<th>符号表示</th>\n<th>符号含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>[:digit:]</code></td>\n<td>代表数字<code>0-9</code></td>\n</tr>\n<tr>\n<td><code>[:alnum:]</code></td>\n<td>代表英文字符和数字:<code>a-z,A-Z,0-9</code></td>\n</tr>\n<tr>\n<td><code>[:lower:]</code></td>\n<td>代表小写字符:<code>a-z</code></td>\n</tr>\n<tr>\n<td><code>[:upper:]</code></td>\n<td>代表大写字符:<code>A-Z</code></td>\n</tr>\n<tr>\n<td><code>[:space:]</code></td>\n<td>代表空格，包括<code>[Tab]</code></td>\n</tr>\n</tbody></table>\n<h1 id=\"输出格式\"><a href=\"#输出格式\" class=\"headerlink\" title=\"输出格式\"></a>输出格式</h1><table>\n<thead>\n<tr>\n<th>符号表示</th>\n<th>符号含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>\\b</code></td>\n<td>回退键</td>\n</tr>\n<tr>\n<td><code>\\f</code></td>\n<td>换页符</td>\n</tr>\n<tr>\n<td><code>\\n</code></td>\n<td>换行符</td>\n</tr>\n<tr>\n<td><code>\\r</code></td>\n<td>回车键</td>\n</tr>\n<tr>\n<td><code>\\t</code></td>\n<td><code>Tab</code>键</td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"字符位置\"><a href=\"#字符位置\" class=\"headerlink\" title=\"字符位置\"></a>字符位置</h1><table>\n<thead>\n<tr>\n<th>符号表示</th>\n<th>符号含义</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>^str</code></td>\n<td>需要查找的字符在行首</td>\n<td><code>^a</code> 查找以字符<code>a</code>开头</td>\n</tr>\n<tr>\n<td><code>str$</code></td>\n<td>需要查找的字符在行尾</td>\n<td><code>a$</code> 查找以字符<code>a</code>结尾</td>\n</tr>\n</tbody></table>\n<h1 id=\"字符表示\"><a href=\"#字符表示\" class=\"headerlink\" title=\"字符表示\"></a>字符表示</h1><table>\n<thead>\n<tr>\n<th>符号表示</th>\n<th>符号含义</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>\\</code></td>\n<td>转义符</td>\n<td>将特殊字符变为普通字符如<code>\\^</code></td>\n</tr>\n<tr>\n<td><code>.</code></td>\n<td>说明一定有一个任意的字符</td>\n<td><code>a.b</code> 说明字符<code>a</code>与<code>b</code>之间一定存在一个字符</td>\n</tr>\n<tr>\n<td><code>*</code></td>\n<td>说明存在零个或者多个前一个字符</td>\n<td><code>a*</code>说明字符<code>a</code>后边可能存在0个或多个字符<code>a</code> <code>.*</code>表示存在任意字符</td>\n</tr>\n<tr>\n<td><code>+</code></td>\n<td><strong>扩展正则</strong>说明存在一个或一个以上前一个字符</td>\n<td><code>a+</code>说明字符<code>a</code>后边可能存在1个或多个字符<code>a</code></td>\n</tr>\n<tr>\n<td><code>?</code></td>\n<td><strong>扩展正则</strong>说明存在0个或一个前一个字符</td>\n<td><code>ab？</code>查找字符<code>ab</code>或者是<code>a</code></td>\n</tr>\n<tr>\n<td>`</td>\n<td>`</td>\n<td>或</td>\n</tr>\n<tr>\n<td><code>()</code></td>\n<td><strong>扩展正则</strong>字符集合</td>\n<td>`(ab</td>\n</tr>\n<tr>\n<td><code>()+</code></td>\n<td><strong>扩展正则</strong>多个重复字符集合</td>\n<td><code>(ab)+</code>:查找具有一个以上<code>ab</code>子字符串的字符串</td>\n</tr>\n<tr>\n<td><code>[list]</code></td>\n<td>列出可能存在的字符</td>\n<td><code>a[bc]</code>查找字符<code>ab</code>或者是<code>ac</code></td>\n</tr>\n<tr>\n<td><code>[n1-n2]</code></td>\n<td>列出可能存在的字符区间</td>\n<td><code>[a-g]</code> 查找字符区间<code>a-g</code>中任意字符</td>\n</tr>\n<tr>\n<td><code>[^list}</code></td>\n<td>列出不需要的字符即反向选择</td>\n<td><code>[^a]</code> 查找字符中不存在<code>a</code>的字符</td>\n</tr>\n<tr>\n<td><code>\\{n,m\\}</code></td>\n<td><strong>连续</strong><code>n</code>到<code>m</code>个之前的字符</td>\n<td><code>a\\{2,3\\}</code>查找字符<code>aa</code>或者是<code>aaa</code>;</br> <code>a\\{2\\}</code>查找字符<code>aa</code>;</br><code>a\\{2,\\}</code>查找连续2个字符<code>a</code>以上的字符如<code>aaa</code>，<code>aaaa</code>等;</td>\n</tr>\n</tbody></table>\n<h1 id=\"字符替换\"><a href=\"#字符替换\" class=\"headerlink\" title=\"字符替换\"></a>字符替换</h1><table>\n<thead>\n<tr>\n<th>符号表示</th>\n<th>符号含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>[:digit:]</code></td>\n<td>代表数字<code>0-9</code></td>\n</tr>\n<tr>\n<td><code>[:alnum:]</code></td>\n<td>代表英文字符和数字:<code>a-z,A-Z,0-9</code></td>\n</tr>\n<tr>\n<td><code>[:lower:]</code></td>\n<td>代表小写字符:<code>a-z</code></td>\n</tr>\n<tr>\n<td><code>[:upper:]</code></td>\n<td>代表大写字符:<code>A-Z</code></td>\n</tr>\n<tr>\n<td><code>[:space:]</code></td>\n<td>代表空格，包括<code>[Tab]</code></td>\n</tr>\n</tbody></table>\n<h1 id=\"输出格式\"><a href=\"#输出格式\" class=\"headerlink\" title=\"输出格式\"></a>输出格式</h1><table>\n<thead>\n<tr>\n<th>符号表示</th>\n<th>符号含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>\\b</code></td>\n<td>回退键</td>\n</tr>\n<tr>\n<td><code>\\f</code></td>\n<td>换页符</td>\n</tr>\n<tr>\n<td><code>\\n</code></td>\n<td>换行符</td>\n</tr>\n<tr>\n<td><code>\\r</code></td>\n<td>回车键</td>\n</tr>\n<tr>\n<td><code>\\t</code></td>\n<td><code>Tab</code>键</td>\n</tr>\n</tbody></table>\n"},{"title":"CURL命令学习三","date":"2019-12-21T04:52:41.000Z","_content":"### `-I`\n\n只获取请求头\n\n### `-k --insecure`\n\n每次SSL连接curl都需要验证是否安全。`-k`参数表示如果不安全也可以继续操作。\n\n### `-4 --ipv4`\n\n告诉curl只使用ipv4地址\n\n### `-6 --ipv6`\n\n告诉curl只使用ipv6\n\n### `--keepalive-time <seconds>`\n\n设置时间保持心跳连接\n\n### `--no-keepalive`\n\n不设置心跳保持连接\n\n### `-l --list-only`\n\n(FTP)当列出FTP文件夹时，该选项强制只列出名字\n\n### `-: --next`\n\n表明curl可一次性发送多个请求。例子：\n```\ncurl www.exam1.com --next -d name=value www.exam2.com\n```\n\n### `-N --no-buffer`\n\n不使用输出流的缓冲区","source":"_posts/blog/other/CURL命令学习三.md","raw":"---\ntitle: CURL命令学习三\ndate: 2019-12-21 12:52:41\ntags: curl学习\ncategories: curl\n---\n### `-I`\n\n只获取请求头\n\n### `-k --insecure`\n\n每次SSL连接curl都需要验证是否安全。`-k`参数表示如果不安全也可以继续操作。\n\n### `-4 --ipv4`\n\n告诉curl只使用ipv4地址\n\n### `-6 --ipv6`\n\n告诉curl只使用ipv6\n\n### `--keepalive-time <seconds>`\n\n设置时间保持心跳连接\n\n### `--no-keepalive`\n\n不设置心跳保持连接\n\n### `-l --list-only`\n\n(FTP)当列出FTP文件夹时，该选项强制只列出名字\n\n### `-: --next`\n\n表明curl可一次性发送多个请求。例子：\n```\ncurl www.exam1.com --next -d name=value www.exam2.com\n```\n\n### `-N --no-buffer`\n\n不使用输出流的缓冲区","slug":"blog/other/CURL命令学习三","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqykm004yk0vqb28ldrqo","content":"<h3 id=\"I\"><a href=\"#I\" class=\"headerlink\" title=\"-I\"></a><code>-I</code></h3><p>只获取请求头</p>\n<h3 id=\"k-insecure\"><a href=\"#k-insecure\" class=\"headerlink\" title=\"-k --insecure\"></a><code>-k --insecure</code></h3><p>每次SSL连接curl都需要验证是否安全。<code>-k</code>参数表示如果不安全也可以继续操作。</p>\n<h3 id=\"4-ipv4\"><a href=\"#4-ipv4\" class=\"headerlink\" title=\"-4 --ipv4\"></a><code>-4 --ipv4</code></h3><p>告诉curl只使用ipv4地址</p>\n<h3 id=\"6-ipv6\"><a href=\"#6-ipv6\" class=\"headerlink\" title=\"-6 --ipv6\"></a><code>-6 --ipv6</code></h3><p>告诉curl只使用ipv6</p>\n<h3 id=\"keepalive-time-lt-seconds-gt\"><a href=\"#keepalive-time-lt-seconds-gt\" class=\"headerlink\" title=\"--keepalive-time &lt;seconds&gt;\"></a><code>--keepalive-time &lt;seconds&gt;</code></h3><p>设置时间保持心跳连接</p>\n<h3 id=\"no-keepalive\"><a href=\"#no-keepalive\" class=\"headerlink\" title=\"--no-keepalive\"></a><code>--no-keepalive</code></h3><p>不设置心跳保持连接</p>\n<h3 id=\"l-list-only\"><a href=\"#l-list-only\" class=\"headerlink\" title=\"-l --list-only\"></a><code>-l --list-only</code></h3><p>(FTP)当列出FTP文件夹时，该选项强制只列出名字</p>\n<h3 id=\"next\"><a href=\"#next\" class=\"headerlink\" title=\"-: --next\"></a><code>-: --next</code></h3><p>表明curl可一次性发送多个请求。例子：</p>\n<pre><code>curl www.exam1.com --next -d name=value www.exam2.com</code></pre><h3 id=\"N-no-buffer\"><a href=\"#N-no-buffer\" class=\"headerlink\" title=\"-N --no-buffer\"></a><code>-N --no-buffer</code></h3><p>不使用输出流的缓冲区</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"I\"><a href=\"#I\" class=\"headerlink\" title=\"-I\"></a><code>-I</code></h3><p>只获取请求头</p>\n<h3 id=\"k-insecure\"><a href=\"#k-insecure\" class=\"headerlink\" title=\"-k --insecure\"></a><code>-k --insecure</code></h3><p>每次SSL连接curl都需要验证是否安全。<code>-k</code>参数表示如果不安全也可以继续操作。</p>\n<h3 id=\"4-ipv4\"><a href=\"#4-ipv4\" class=\"headerlink\" title=\"-4 --ipv4\"></a><code>-4 --ipv4</code></h3><p>告诉curl只使用ipv4地址</p>\n<h3 id=\"6-ipv6\"><a href=\"#6-ipv6\" class=\"headerlink\" title=\"-6 --ipv6\"></a><code>-6 --ipv6</code></h3><p>告诉curl只使用ipv6</p>\n<h3 id=\"keepalive-time-lt-seconds-gt\"><a href=\"#keepalive-time-lt-seconds-gt\" class=\"headerlink\" title=\"--keepalive-time &lt;seconds&gt;\"></a><code>--keepalive-time &lt;seconds&gt;</code></h3><p>设置时间保持心跳连接</p>\n<h3 id=\"no-keepalive\"><a href=\"#no-keepalive\" class=\"headerlink\" title=\"--no-keepalive\"></a><code>--no-keepalive</code></h3><p>不设置心跳保持连接</p>\n<h3 id=\"l-list-only\"><a href=\"#l-list-only\" class=\"headerlink\" title=\"-l --list-only\"></a><code>-l --list-only</code></h3><p>(FTP)当列出FTP文件夹时，该选项强制只列出名字</p>\n<h3 id=\"next\"><a href=\"#next\" class=\"headerlink\" title=\"-: --next\"></a><code>-: --next</code></h3><p>表明curl可一次性发送多个请求。例子：</p>\n<pre><code>curl www.exam1.com --next -d name=value www.exam2.com</code></pre><h3 id=\"N-no-buffer\"><a href=\"#N-no-buffer\" class=\"headerlink\" title=\"-N --no-buffer\"></a><code>-N --no-buffer</code></h3><p>不使用输出流的缓冲区</p>\n"},{"title":"Linux命令-grep,sed,awk","date":"2019-12-22T06:59:39.000Z","_content":"# grep\n\n* * *\n\n(global search regular expression[RE] and print out the line)\n正则表达式全局搜索并将行打印出来\n\n* 在文件中查找包含字符串\"text\"的行\n\n```\ngrep text local_file\ngrep \"text\" local_file #另一种方式\ngrep \"text\" local_file1 local_file2 ...  #查找多个文件\n```\n\n* 在文件中查找**不**包含字符串\"text\"的行\n\n```\ngrep -v \"text\" local_file\n```\n\n* **忽略大小写**\n\n```\ngrep -i \"TeXt\" local_file\ngrep -y \"TeXt\" local_file\n```\n\n* **不显示错误信息**，常用于脚本文件中\n\n```\ngrep -s \"text\" local_file\n```\n\n* **只打印出匹配到的字符串**\n\n```\ngrep -o \"text\" local_file\n```\n\n* **统计文件中有多少行包含需要查找的字符串**\n\n```\ngrep -c \"text\" local_file\n```\n\n*  **不输出信息**，命令运行成功返回0.失败返回非0值，用于判断\n\n```\ngrep -q \"text\" local_file\n```\n\n* 匹配多个字符串,相当于逻辑中的或\n\n```\ngrep -e \"text1\" -e \"text2\" local_file\n```\n\n* **递归查找**，用于多级目录中的文件\n\n```\ngrep -r \"text\" . #在当前目录下进行查找\n```\n\n* 输出匹配需要查找字符串的行以及**之前**的行\n\n```\ngrep \"text\" -B 3 local_file #输出之前的3行\n```\n\n* 输出匹配需要查找字符串的行以及**之后**的行\n\n```\ngrep \"text\" -A 3 local_file #输出之后的3行\n```\n\n# sed\n\n* * *\n流编辑器，用来编辑一个或者多个文件，简化对文件的重复操作。在缓冲区内操作，除非特别指定，不对文件本身内容进行修改。\n\n## `-i`\n对文件本身进行修改\n## `-q`\n* 打印出第2行后退出`sed`\n\n```\nsed '2q' local_file\n```\n\n## 查找\n* 查找第2-5行数据\n\n```\nsed '2,5p' local_file\nsed -n '2,5p' local_file #并打印行号\n```\n\n* 查找包含字符串\"text\"的行与包含字符串\"file\"的行范围内的行\n\n```\nsed '/text/,/file/p' local_file\n```\n\n* 查找从第2行开始一直到以字符串\"text“开头的行之间的所有行\n\n```\nsed '2,/^text/p' local_file\n```\n\n## 添加\n* 在第2行后面一行添加字符串\"text\"\n\n```\nsed '2a text' local_file\n```\n\n* 在第二行前面一行添加字符串\"text\"\n\n```\nsed '2i text' local_file\n```\n\n* 在每一个单词前面加上字符\"a\":\n\n```\nsed 's/\\w\\+/a&/g'  # \\w\\+匹配每一个单词 &对应之前匹配的每一个单词\n```\n\n## 替换\n* 替换字符串`file`为`files`\n\n```\nsed 's/file/files/g' local_file #打印到控制台，不修改文件\nsed 's:file:file:g' local_file # /标记可以使用其他符号代替\nsed -i 's/file/files/g' local_file #修改文件本身内容，不打印到控制台\n```\n\n* 替换第2-5行为字符串\"text\"\n\n```\nsed '2,5c text' local_file\n```\n\n## 删除\n\n* 删除文件内的第2-5行\n\n```\nsed '/2,5/d' local_file\n```\n\n* 删除开头字符串为\"text\"的行\n\n```\nsed '/^text.*//g' local_file\nsed '/^text/'d local_file\n```\n\n* 删除最后一行\n\n```\nsed '$d' local_file\n```\n\n* 删除空白行\n\n```\nsed '/^$/d' local_file\n```\n# awk\n\n* * *\n\n* 打印每一行的第2，3列数据\n\n```\nawk '{print $2,$3}' local_file\n```","source":"_posts/blog/linux/Linux命令-grep_sed_awk.md","raw":"---\ntitle: Linux命令-grep,sed,awk\ndate: 2019-12-22 14:59:39\ntags: Linux\ncategories: Linux命令\n---\n# grep\n\n* * *\n\n(global search regular expression[RE] and print out the line)\n正则表达式全局搜索并将行打印出来\n\n* 在文件中查找包含字符串\"text\"的行\n\n```\ngrep text local_file\ngrep \"text\" local_file #另一种方式\ngrep \"text\" local_file1 local_file2 ...  #查找多个文件\n```\n\n* 在文件中查找**不**包含字符串\"text\"的行\n\n```\ngrep -v \"text\" local_file\n```\n\n* **忽略大小写**\n\n```\ngrep -i \"TeXt\" local_file\ngrep -y \"TeXt\" local_file\n```\n\n* **不显示错误信息**，常用于脚本文件中\n\n```\ngrep -s \"text\" local_file\n```\n\n* **只打印出匹配到的字符串**\n\n```\ngrep -o \"text\" local_file\n```\n\n* **统计文件中有多少行包含需要查找的字符串**\n\n```\ngrep -c \"text\" local_file\n```\n\n*  **不输出信息**，命令运行成功返回0.失败返回非0值，用于判断\n\n```\ngrep -q \"text\" local_file\n```\n\n* 匹配多个字符串,相当于逻辑中的或\n\n```\ngrep -e \"text1\" -e \"text2\" local_file\n```\n\n* **递归查找**，用于多级目录中的文件\n\n```\ngrep -r \"text\" . #在当前目录下进行查找\n```\n\n* 输出匹配需要查找字符串的行以及**之前**的行\n\n```\ngrep \"text\" -B 3 local_file #输出之前的3行\n```\n\n* 输出匹配需要查找字符串的行以及**之后**的行\n\n```\ngrep \"text\" -A 3 local_file #输出之后的3行\n```\n\n# sed\n\n* * *\n流编辑器，用来编辑一个或者多个文件，简化对文件的重复操作。在缓冲区内操作，除非特别指定，不对文件本身内容进行修改。\n\n## `-i`\n对文件本身进行修改\n## `-q`\n* 打印出第2行后退出`sed`\n\n```\nsed '2q' local_file\n```\n\n## 查找\n* 查找第2-5行数据\n\n```\nsed '2,5p' local_file\nsed -n '2,5p' local_file #并打印行号\n```\n\n* 查找包含字符串\"text\"的行与包含字符串\"file\"的行范围内的行\n\n```\nsed '/text/,/file/p' local_file\n```\n\n* 查找从第2行开始一直到以字符串\"text“开头的行之间的所有行\n\n```\nsed '2,/^text/p' local_file\n```\n\n## 添加\n* 在第2行后面一行添加字符串\"text\"\n\n```\nsed '2a text' local_file\n```\n\n* 在第二行前面一行添加字符串\"text\"\n\n```\nsed '2i text' local_file\n```\n\n* 在每一个单词前面加上字符\"a\":\n\n```\nsed 's/\\w\\+/a&/g'  # \\w\\+匹配每一个单词 &对应之前匹配的每一个单词\n```\n\n## 替换\n* 替换字符串`file`为`files`\n\n```\nsed 's/file/files/g' local_file #打印到控制台，不修改文件\nsed 's:file:file:g' local_file # /标记可以使用其他符号代替\nsed -i 's/file/files/g' local_file #修改文件本身内容，不打印到控制台\n```\n\n* 替换第2-5行为字符串\"text\"\n\n```\nsed '2,5c text' local_file\n```\n\n## 删除\n\n* 删除文件内的第2-5行\n\n```\nsed '/2,5/d' local_file\n```\n\n* 删除开头字符串为\"text\"的行\n\n```\nsed '/^text.*//g' local_file\nsed '/^text/'d local_file\n```\n\n* 删除最后一行\n\n```\nsed '$d' local_file\n```\n\n* 删除空白行\n\n```\nsed '/^$/d' local_file\n```\n# awk\n\n* * *\n\n* 打印每一行的第2，3列数据\n\n```\nawk '{print $2,$3}' local_file\n```","slug":"blog/linux/Linux命令-grep_sed_awk","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqykp0052k0vq0flq6ta1","content":"<h1 id=\"grep\"><a href=\"#grep\" class=\"headerlink\" title=\"grep\"></a>grep</h1><hr>\n<p>(global search regular expression[RE] and print out the line)<br>正则表达式全局搜索并将行打印出来</p>\n<ul>\n<li>在文件中查找包含字符串”text”的行</li>\n</ul>\n<pre><code>grep text local_file\ngrep &quot;text&quot; local_file #另一种方式\ngrep &quot;text&quot; local_file1 local_file2 ...  #查找多个文件</code></pre><ul>\n<li>在文件中查找<strong>不</strong>包含字符串”text”的行</li>\n</ul>\n<pre><code>grep -v &quot;text&quot; local_file</code></pre><ul>\n<li><strong>忽略大小写</strong></li>\n</ul>\n<pre><code>grep -i &quot;TeXt&quot; local_file\ngrep -y &quot;TeXt&quot; local_file</code></pre><ul>\n<li><strong>不显示错误信息</strong>，常用于脚本文件中</li>\n</ul>\n<pre><code>grep -s &quot;text&quot; local_file</code></pre><ul>\n<li><strong>只打印出匹配到的字符串</strong></li>\n</ul>\n<pre><code>grep -o &quot;text&quot; local_file</code></pre><ul>\n<li><strong>统计文件中有多少行包含需要查找的字符串</strong></li>\n</ul>\n<pre><code>grep -c &quot;text&quot; local_file</code></pre><ul>\n<li><strong>不输出信息</strong>，命令运行成功返回0.失败返回非0值，用于判断</li>\n</ul>\n<pre><code>grep -q &quot;text&quot; local_file</code></pre><ul>\n<li>匹配多个字符串,相当于逻辑中的或</li>\n</ul>\n<pre><code>grep -e &quot;text1&quot; -e &quot;text2&quot; local_file</code></pre><ul>\n<li><strong>递归查找</strong>，用于多级目录中的文件</li>\n</ul>\n<pre><code>grep -r &quot;text&quot; . #在当前目录下进行查找</code></pre><ul>\n<li>输出匹配需要查找字符串的行以及<strong>之前</strong>的行</li>\n</ul>\n<pre><code>grep &quot;text&quot; -B 3 local_file #输出之前的3行</code></pre><ul>\n<li>输出匹配需要查找字符串的行以及<strong>之后</strong>的行</li>\n</ul>\n<pre><code>grep &quot;text&quot; -A 3 local_file #输出之后的3行</code></pre><h1 id=\"sed\"><a href=\"#sed\" class=\"headerlink\" title=\"sed\"></a>sed</h1><hr>\n<p>流编辑器，用来编辑一个或者多个文件，简化对文件的重复操作。在缓冲区内操作，除非特别指定，不对文件本身内容进行修改。</p>\n<h2 id=\"i\"><a href=\"#i\" class=\"headerlink\" title=\"-i\"></a><code>-i</code></h2><p>对文件本身进行修改</p>\n<h2 id=\"q\"><a href=\"#q\" class=\"headerlink\" title=\"-q\"></a><code>-q</code></h2><ul>\n<li>打印出第2行后退出<code>sed</code></li>\n</ul>\n<pre><code>sed &#39;2q&#39; local_file</code></pre><h2 id=\"查找\"><a href=\"#查找\" class=\"headerlink\" title=\"查找\"></a>查找</h2><ul>\n<li>查找第2-5行数据</li>\n</ul>\n<pre><code>sed &#39;2,5p&#39; local_file\nsed -n &#39;2,5p&#39; local_file #并打印行号</code></pre><ul>\n<li>查找包含字符串”text”的行与包含字符串”file”的行范围内的行</li>\n</ul>\n<pre><code>sed &#39;/text/,/file/p&#39; local_file</code></pre><ul>\n<li>查找从第2行开始一直到以字符串”text“开头的行之间的所有行</li>\n</ul>\n<pre><code>sed &#39;2,/^text/p&#39; local_file</code></pre><h2 id=\"添加\"><a href=\"#添加\" class=\"headerlink\" title=\"添加\"></a>添加</h2><ul>\n<li>在第2行后面一行添加字符串”text”</li>\n</ul>\n<pre><code>sed &#39;2a text&#39; local_file</code></pre><ul>\n<li>在第二行前面一行添加字符串”text”</li>\n</ul>\n<pre><code>sed &#39;2i text&#39; local_file</code></pre><ul>\n<li>在每一个单词前面加上字符”a”:</li>\n</ul>\n<pre><code>sed &#39;s/\\w\\+/a&amp;/g&#39;  # \\w\\+匹配每一个单词 &amp;对应之前匹配的每一个单词</code></pre><h2 id=\"替换\"><a href=\"#替换\" class=\"headerlink\" title=\"替换\"></a>替换</h2><ul>\n<li>替换字符串<code>file</code>为<code>files</code></li>\n</ul>\n<pre><code>sed &#39;s/file/files/g&#39; local_file #打印到控制台，不修改文件\nsed &#39;s:file:file:g&#39; local_file # /标记可以使用其他符号代替\nsed -i &#39;s/file/files/g&#39; local_file #修改文件本身内容，不打印到控制台</code></pre><ul>\n<li>替换第2-5行为字符串”text”</li>\n</ul>\n<pre><code>sed &#39;2,5c text&#39; local_file</code></pre><h2 id=\"删除\"><a href=\"#删除\" class=\"headerlink\" title=\"删除\"></a>删除</h2><ul>\n<li>删除文件内的第2-5行</li>\n</ul>\n<pre><code>sed &#39;/2,5/d&#39; local_file</code></pre><ul>\n<li>删除开头字符串为”text”的行</li>\n</ul>\n<pre><code>sed &#39;/^text.*//g&#39; local_file\nsed &#39;/^text/&#39;d local_file</code></pre><ul>\n<li>删除最后一行</li>\n</ul>\n<pre><code>sed &#39;$d&#39; local_file</code></pre><ul>\n<li>删除空白行</li>\n</ul>\n<pre><code>sed &#39;/^$/d&#39; local_file</code></pre><h1 id=\"awk\"><a href=\"#awk\" class=\"headerlink\" title=\"awk\"></a>awk</h1><hr>\n<ul>\n<li>打印每一行的第2，3列数据</li>\n</ul>\n<pre><code>awk &#39;{print $2,$3}&#39; local_file</code></pre>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"grep\"><a href=\"#grep\" class=\"headerlink\" title=\"grep\"></a>grep</h1><hr>\n<p>(global search regular expression[RE] and print out the line)<br>正则表达式全局搜索并将行打印出来</p>\n<ul>\n<li>在文件中查找包含字符串”text”的行</li>\n</ul>\n<pre><code>grep text local_file\ngrep &quot;text&quot; local_file #另一种方式\ngrep &quot;text&quot; local_file1 local_file2 ...  #查找多个文件</code></pre><ul>\n<li>在文件中查找<strong>不</strong>包含字符串”text”的行</li>\n</ul>\n<pre><code>grep -v &quot;text&quot; local_file</code></pre><ul>\n<li><strong>忽略大小写</strong></li>\n</ul>\n<pre><code>grep -i &quot;TeXt&quot; local_file\ngrep -y &quot;TeXt&quot; local_file</code></pre><ul>\n<li><strong>不显示错误信息</strong>，常用于脚本文件中</li>\n</ul>\n<pre><code>grep -s &quot;text&quot; local_file</code></pre><ul>\n<li><strong>只打印出匹配到的字符串</strong></li>\n</ul>\n<pre><code>grep -o &quot;text&quot; local_file</code></pre><ul>\n<li><strong>统计文件中有多少行包含需要查找的字符串</strong></li>\n</ul>\n<pre><code>grep -c &quot;text&quot; local_file</code></pre><ul>\n<li><strong>不输出信息</strong>，命令运行成功返回0.失败返回非0值，用于判断</li>\n</ul>\n<pre><code>grep -q &quot;text&quot; local_file</code></pre><ul>\n<li>匹配多个字符串,相当于逻辑中的或</li>\n</ul>\n<pre><code>grep -e &quot;text1&quot; -e &quot;text2&quot; local_file</code></pre><ul>\n<li><strong>递归查找</strong>，用于多级目录中的文件</li>\n</ul>\n<pre><code>grep -r &quot;text&quot; . #在当前目录下进行查找</code></pre><ul>\n<li>输出匹配需要查找字符串的行以及<strong>之前</strong>的行</li>\n</ul>\n<pre><code>grep &quot;text&quot; -B 3 local_file #输出之前的3行</code></pre><ul>\n<li>输出匹配需要查找字符串的行以及<strong>之后</strong>的行</li>\n</ul>\n<pre><code>grep &quot;text&quot; -A 3 local_file #输出之后的3行</code></pre><h1 id=\"sed\"><a href=\"#sed\" class=\"headerlink\" title=\"sed\"></a>sed</h1><hr>\n<p>流编辑器，用来编辑一个或者多个文件，简化对文件的重复操作。在缓冲区内操作，除非特别指定，不对文件本身内容进行修改。</p>\n<h2 id=\"i\"><a href=\"#i\" class=\"headerlink\" title=\"-i\"></a><code>-i</code></h2><p>对文件本身进行修改</p>\n<h2 id=\"q\"><a href=\"#q\" class=\"headerlink\" title=\"-q\"></a><code>-q</code></h2><ul>\n<li>打印出第2行后退出<code>sed</code></li>\n</ul>\n<pre><code>sed &#39;2q&#39; local_file</code></pre><h2 id=\"查找\"><a href=\"#查找\" class=\"headerlink\" title=\"查找\"></a>查找</h2><ul>\n<li>查找第2-5行数据</li>\n</ul>\n<pre><code>sed &#39;2,5p&#39; local_file\nsed -n &#39;2,5p&#39; local_file #并打印行号</code></pre><ul>\n<li>查找包含字符串”text”的行与包含字符串”file”的行范围内的行</li>\n</ul>\n<pre><code>sed &#39;/text/,/file/p&#39; local_file</code></pre><ul>\n<li>查找从第2行开始一直到以字符串”text“开头的行之间的所有行</li>\n</ul>\n<pre><code>sed &#39;2,/^text/p&#39; local_file</code></pre><h2 id=\"添加\"><a href=\"#添加\" class=\"headerlink\" title=\"添加\"></a>添加</h2><ul>\n<li>在第2行后面一行添加字符串”text”</li>\n</ul>\n<pre><code>sed &#39;2a text&#39; local_file</code></pre><ul>\n<li>在第二行前面一行添加字符串”text”</li>\n</ul>\n<pre><code>sed &#39;2i text&#39; local_file</code></pre><ul>\n<li>在每一个单词前面加上字符”a”:</li>\n</ul>\n<pre><code>sed &#39;s/\\w\\+/a&amp;/g&#39;  # \\w\\+匹配每一个单词 &amp;对应之前匹配的每一个单词</code></pre><h2 id=\"替换\"><a href=\"#替换\" class=\"headerlink\" title=\"替换\"></a>替换</h2><ul>\n<li>替换字符串<code>file</code>为<code>files</code></li>\n</ul>\n<pre><code>sed &#39;s/file/files/g&#39; local_file #打印到控制台，不修改文件\nsed &#39;s:file:file:g&#39; local_file # /标记可以使用其他符号代替\nsed -i &#39;s/file/files/g&#39; local_file #修改文件本身内容，不打印到控制台</code></pre><ul>\n<li>替换第2-5行为字符串”text”</li>\n</ul>\n<pre><code>sed &#39;2,5c text&#39; local_file</code></pre><h2 id=\"删除\"><a href=\"#删除\" class=\"headerlink\" title=\"删除\"></a>删除</h2><ul>\n<li>删除文件内的第2-5行</li>\n</ul>\n<pre><code>sed &#39;/2,5/d&#39; local_file</code></pre><ul>\n<li>删除开头字符串为”text”的行</li>\n</ul>\n<pre><code>sed &#39;/^text.*//g&#39; local_file\nsed &#39;/^text/&#39;d local_file</code></pre><ul>\n<li>删除最后一行</li>\n</ul>\n<pre><code>sed &#39;$d&#39; local_file</code></pre><ul>\n<li>删除空白行</li>\n</ul>\n<pre><code>sed &#39;/^$/d&#39; local_file</code></pre><h1 id=\"awk\"><a href=\"#awk\" class=\"headerlink\" title=\"awk\"></a>awk</h1><hr>\n<ul>\n<li>打印每一行的第2，3列数据</li>\n</ul>\n<pre><code>awk &#39;{print $2,$3}&#39; local_file</code></pre>"},{"title":"CURL命令学习一","date":"2019-12-17T02:17:40.000Z","_content":"每天学习一点点。。。。\n\n* 直接获取页面数据：\n\n```\ncurl http://www.xxx.com/[可以指定具体的路径获取某个文件]\n```\n\n### 用户名(密码):\n\n```\ncurl -u username http://www.xxx.com\ncurl -u username:pwsswd http://www.xxx.com\ncurl http://name:passwd@xxx.domain/filepath/\n```\n\n### 下载页面数据：\n\n```\n#以`demo.html`文件保存\ncurl -o demo.html http://www.xxx.com/\n```\n\n* 下载某个页面数据保存到本地并以源页面名称为默认命名(可以指定多个页面)：\n\n```\ncurl -O http://www.xxx.com/index.html/. [-O http://www.xxx2.com/html/]\n```\n\n* 代理 \n\n```\ncurl -x proxy:port http://www.xxx.com/\n#如果代理需要名字和密码，用-U指定(-u)指定页面需要的用户名密码\ncurl -U user:passwd -x proxy:port http://www.xxx.com/\n```\n\n* 获取部分数据\n\n```\n#获取前100比特数据\ncurl -r 0-99 http://www.xxx.com/\n#获取最后100比特数据\ncurl -r -100 http://www.xxx.com/\n```\n\n### 上传文件\n\n```\n#上传所有文件或者是从输入上传\ncurl -T - ftp://ftp.upload.com/myfile\n#上传文件到远程服务器并使用本地文件名\ncurl -T uploadfile ftp://ftp.upload.com/\n#上传文件并添加到远程文件中\ncurl -T uploadfile -a ftp://ftp.upload.com/\n```\n\n### 打印日志信息\n\n```\ncurl -v http://www.xxx.com\n#获取更多信息\ncurl --trace http://www.xxx.com\n```\n\n### POST方法\n\n```\ncurl -d \"name=value&name1=value1\" http://www.xxx.com/\n-F 从文件中读取\ncurl -F \"coolfiles=@fill.gif;type=image/gif,fil2.txt,fil3.html\" http://www.xxx.com/\ncurl -F ”file=@coottext.txt“ -F \"name=value\" -F \"name=value1 value2 ...\" htttp://www.xxx.com/\ncurl -F \"pict=@dog.gif,cat.gif\" http://www.xxx.com/\n```\n\n### Agent\n\n```\ncurl -A 'Mozilla/3.0 (Win95; I)' http://www.xxx.com/\n```\n\n### Cookies\n\n```\ncurl -b \"name=value\" http://www.xxx.com\ncurl -c cookies.txt http://www.xxx.com\n#read write\ncurl -b cookies.txt -c cookies.txt http://www.xxx.com\n```\n\n### 额外的头部信息\n\n```\ncurl -H \"X-you-and-me: yes\" http://www.xxx.com\n```\n\n### FTP 防火墙 \n\n```\n#使用192.168.0.10作为IP地址\ncurl -P 192.168.0.10 ftp.download.com\n```\n\n### HTTPS\n\n```\ncurl -E /path/to/cert.pem:password https://www.xxx.com\n```\n\n### 文件续传\n\n```\n#download\ncurl -C - -o file ftp://ftp.server.com/path/file\n#upload\ncurl -C - -T file ftp://ftp.server.com/path/file\n```\n\n### -L\n如果页面内容移动到另一个页面比如返回状态码30X，则向新的页面发送请求\n\n### -s\n静默模式，没有输出\n\n### -S\n当使用`-s`时，输出错误信息。","source":"_posts/blog/other/CURL命令学习一.md","raw":"---\ntitle: CURL命令学习一\ndate: 2019-12-17 10:17:40\ntags: curl学习\ncategories: curl\n---\n每天学习一点点。。。。\n\n* 直接获取页面数据：\n\n```\ncurl http://www.xxx.com/[可以指定具体的路径获取某个文件]\n```\n\n### 用户名(密码):\n\n```\ncurl -u username http://www.xxx.com\ncurl -u username:pwsswd http://www.xxx.com\ncurl http://name:passwd@xxx.domain/filepath/\n```\n\n### 下载页面数据：\n\n```\n#以`demo.html`文件保存\ncurl -o demo.html http://www.xxx.com/\n```\n\n* 下载某个页面数据保存到本地并以源页面名称为默认命名(可以指定多个页面)：\n\n```\ncurl -O http://www.xxx.com/index.html/. [-O http://www.xxx2.com/html/]\n```\n\n* 代理 \n\n```\ncurl -x proxy:port http://www.xxx.com/\n#如果代理需要名字和密码，用-U指定(-u)指定页面需要的用户名密码\ncurl -U user:passwd -x proxy:port http://www.xxx.com/\n```\n\n* 获取部分数据\n\n```\n#获取前100比特数据\ncurl -r 0-99 http://www.xxx.com/\n#获取最后100比特数据\ncurl -r -100 http://www.xxx.com/\n```\n\n### 上传文件\n\n```\n#上传所有文件或者是从输入上传\ncurl -T - ftp://ftp.upload.com/myfile\n#上传文件到远程服务器并使用本地文件名\ncurl -T uploadfile ftp://ftp.upload.com/\n#上传文件并添加到远程文件中\ncurl -T uploadfile -a ftp://ftp.upload.com/\n```\n\n### 打印日志信息\n\n```\ncurl -v http://www.xxx.com\n#获取更多信息\ncurl --trace http://www.xxx.com\n```\n\n### POST方法\n\n```\ncurl -d \"name=value&name1=value1\" http://www.xxx.com/\n-F 从文件中读取\ncurl -F \"coolfiles=@fill.gif;type=image/gif,fil2.txt,fil3.html\" http://www.xxx.com/\ncurl -F ”file=@coottext.txt“ -F \"name=value\" -F \"name=value1 value2 ...\" htttp://www.xxx.com/\ncurl -F \"pict=@dog.gif,cat.gif\" http://www.xxx.com/\n```\n\n### Agent\n\n```\ncurl -A 'Mozilla/3.0 (Win95; I)' http://www.xxx.com/\n```\n\n### Cookies\n\n```\ncurl -b \"name=value\" http://www.xxx.com\ncurl -c cookies.txt http://www.xxx.com\n#read write\ncurl -b cookies.txt -c cookies.txt http://www.xxx.com\n```\n\n### 额外的头部信息\n\n```\ncurl -H \"X-you-and-me: yes\" http://www.xxx.com\n```\n\n### FTP 防火墙 \n\n```\n#使用192.168.0.10作为IP地址\ncurl -P 192.168.0.10 ftp.download.com\n```\n\n### HTTPS\n\n```\ncurl -E /path/to/cert.pem:password https://www.xxx.com\n```\n\n### 文件续传\n\n```\n#download\ncurl -C - -o file ftp://ftp.server.com/path/file\n#upload\ncurl -C - -T file ftp://ftp.server.com/path/file\n```\n\n### -L\n如果页面内容移动到另一个页面比如返回状态码30X，则向新的页面发送请求\n\n### -s\n静默模式，没有输出\n\n### -S\n当使用`-s`时，输出错误信息。","slug":"blog/other/CURL命令学习一","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqykr0055k0vq66edb5ac","content":"<p>每天学习一点点。。。。</p>\n<ul>\n<li>直接获取页面数据：</li>\n</ul>\n<pre><code>curl http://www.xxx.com/[可以指定具体的路径获取某个文件]</code></pre><h3 id=\"用户名-密码\"><a href=\"#用户名-密码\" class=\"headerlink\" title=\"用户名(密码):\"></a>用户名(密码):</h3><pre><code>curl -u username http://www.xxx.com\ncurl -u username:pwsswd http://www.xxx.com\ncurl http://name:passwd@xxx.domain/filepath/</code></pre><h3 id=\"下载页面数据：\"><a href=\"#下载页面数据：\" class=\"headerlink\" title=\"下载页面数据：\"></a>下载页面数据：</h3><pre><code>#以`demo.html`文件保存\ncurl -o demo.html http://www.xxx.com/</code></pre><ul>\n<li>下载某个页面数据保存到本地并以源页面名称为默认命名(可以指定多个页面)：</li>\n</ul>\n<pre><code>curl -O http://www.xxx.com/index.html/. [-O http://www.xxx2.com/html/]</code></pre><ul>\n<li>代理 </li>\n</ul>\n<pre><code>curl -x proxy:port http://www.xxx.com/\n#如果代理需要名字和密码，用-U指定(-u)指定页面需要的用户名密码\ncurl -U user:passwd -x proxy:port http://www.xxx.com/</code></pre><ul>\n<li>获取部分数据</li>\n</ul>\n<pre><code>#获取前100比特数据\ncurl -r 0-99 http://www.xxx.com/\n#获取最后100比特数据\ncurl -r -100 http://www.xxx.com/</code></pre><h3 id=\"上传文件\"><a href=\"#上传文件\" class=\"headerlink\" title=\"上传文件\"></a>上传文件</h3><pre><code>#上传所有文件或者是从输入上传\ncurl -T - ftp://ftp.upload.com/myfile\n#上传文件到远程服务器并使用本地文件名\ncurl -T uploadfile ftp://ftp.upload.com/\n#上传文件并添加到远程文件中\ncurl -T uploadfile -a ftp://ftp.upload.com/</code></pre><h3 id=\"打印日志信息\"><a href=\"#打印日志信息\" class=\"headerlink\" title=\"打印日志信息\"></a>打印日志信息</h3><pre><code>curl -v http://www.xxx.com\n#获取更多信息\ncurl --trace http://www.xxx.com</code></pre><h3 id=\"POST方法\"><a href=\"#POST方法\" class=\"headerlink\" title=\"POST方法\"></a>POST方法</h3><pre><code>curl -d &quot;name=value&amp;name1=value1&quot; http://www.xxx.com/\n-F 从文件中读取\ncurl -F &quot;coolfiles=@fill.gif;type=image/gif,fil2.txt,fil3.html&quot; http://www.xxx.com/\ncurl -F ”file=@coottext.txt“ -F &quot;name=value&quot; -F &quot;name=value1 value2 ...&quot; htttp://www.xxx.com/\ncurl -F &quot;pict=@dog.gif,cat.gif&quot; http://www.xxx.com/</code></pre><h3 id=\"Agent\"><a href=\"#Agent\" class=\"headerlink\" title=\"Agent\"></a>Agent</h3><pre><code>curl -A &#39;Mozilla/3.0 (Win95; I)&#39; http://www.xxx.com/</code></pre><h3 id=\"Cookies\"><a href=\"#Cookies\" class=\"headerlink\" title=\"Cookies\"></a>Cookies</h3><pre><code>curl -b &quot;name=value&quot; http://www.xxx.com\ncurl -c cookies.txt http://www.xxx.com\n#read write\ncurl -b cookies.txt -c cookies.txt http://www.xxx.com</code></pre><h3 id=\"额外的头部信息\"><a href=\"#额外的头部信息\" class=\"headerlink\" title=\"额外的头部信息\"></a>额外的头部信息</h3><pre><code>curl -H &quot;X-you-and-me: yes&quot; http://www.xxx.com</code></pre><h3 id=\"FTP-防火墙\"><a href=\"#FTP-防火墙\" class=\"headerlink\" title=\"FTP 防火墙\"></a>FTP 防火墙</h3><pre><code>#使用192.168.0.10作为IP地址\ncurl -P 192.168.0.10 ftp.download.com</code></pre><h3 id=\"HTTPS\"><a href=\"#HTTPS\" class=\"headerlink\" title=\"HTTPS\"></a>HTTPS</h3><pre><code>curl -E /path/to/cert.pem:password https://www.xxx.com</code></pre><h3 id=\"文件续传\"><a href=\"#文件续传\" class=\"headerlink\" title=\"文件续传\"></a>文件续传</h3><pre><code>#download\ncurl -C - -o file ftp://ftp.server.com/path/file\n#upload\ncurl -C - -T file ftp://ftp.server.com/path/file</code></pre><h3 id=\"L\"><a href=\"#L\" class=\"headerlink\" title=\"-L\"></a>-L</h3><p>如果页面内容移动到另一个页面比如返回状态码30X，则向新的页面发送请求</p>\n<h3 id=\"s\"><a href=\"#s\" class=\"headerlink\" title=\"-s\"></a>-s</h3><p>静默模式，没有输出</p>\n<h3 id=\"S\"><a href=\"#S\" class=\"headerlink\" title=\"-S\"></a>-S</h3><p>当使用<code>-s</code>时，输出错误信息。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>每天学习一点点。。。。</p>\n<ul>\n<li>直接获取页面数据：</li>\n</ul>\n<pre><code>curl http://www.xxx.com/[可以指定具体的路径获取某个文件]</code></pre><h3 id=\"用户名-密码\"><a href=\"#用户名-密码\" class=\"headerlink\" title=\"用户名(密码):\"></a>用户名(密码):</h3><pre><code>curl -u username http://www.xxx.com\ncurl -u username:pwsswd http://www.xxx.com\ncurl http://name:passwd@xxx.domain/filepath/</code></pre><h3 id=\"下载页面数据：\"><a href=\"#下载页面数据：\" class=\"headerlink\" title=\"下载页面数据：\"></a>下载页面数据：</h3><pre><code>#以`demo.html`文件保存\ncurl -o demo.html http://www.xxx.com/</code></pre><ul>\n<li>下载某个页面数据保存到本地并以源页面名称为默认命名(可以指定多个页面)：</li>\n</ul>\n<pre><code>curl -O http://www.xxx.com/index.html/. [-O http://www.xxx2.com/html/]</code></pre><ul>\n<li>代理 </li>\n</ul>\n<pre><code>curl -x proxy:port http://www.xxx.com/\n#如果代理需要名字和密码，用-U指定(-u)指定页面需要的用户名密码\ncurl -U user:passwd -x proxy:port http://www.xxx.com/</code></pre><ul>\n<li>获取部分数据</li>\n</ul>\n<pre><code>#获取前100比特数据\ncurl -r 0-99 http://www.xxx.com/\n#获取最后100比特数据\ncurl -r -100 http://www.xxx.com/</code></pre><h3 id=\"上传文件\"><a href=\"#上传文件\" class=\"headerlink\" title=\"上传文件\"></a>上传文件</h3><pre><code>#上传所有文件或者是从输入上传\ncurl -T - ftp://ftp.upload.com/myfile\n#上传文件到远程服务器并使用本地文件名\ncurl -T uploadfile ftp://ftp.upload.com/\n#上传文件并添加到远程文件中\ncurl -T uploadfile -a ftp://ftp.upload.com/</code></pre><h3 id=\"打印日志信息\"><a href=\"#打印日志信息\" class=\"headerlink\" title=\"打印日志信息\"></a>打印日志信息</h3><pre><code>curl -v http://www.xxx.com\n#获取更多信息\ncurl --trace http://www.xxx.com</code></pre><h3 id=\"POST方法\"><a href=\"#POST方法\" class=\"headerlink\" title=\"POST方法\"></a>POST方法</h3><pre><code>curl -d &quot;name=value&amp;name1=value1&quot; http://www.xxx.com/\n-F 从文件中读取\ncurl -F &quot;coolfiles=@fill.gif;type=image/gif,fil2.txt,fil3.html&quot; http://www.xxx.com/\ncurl -F ”file=@coottext.txt“ -F &quot;name=value&quot; -F &quot;name=value1 value2 ...&quot; htttp://www.xxx.com/\ncurl -F &quot;pict=@dog.gif,cat.gif&quot; http://www.xxx.com/</code></pre><h3 id=\"Agent\"><a href=\"#Agent\" class=\"headerlink\" title=\"Agent\"></a>Agent</h3><pre><code>curl -A &#39;Mozilla/3.0 (Win95; I)&#39; http://www.xxx.com/</code></pre><h3 id=\"Cookies\"><a href=\"#Cookies\" class=\"headerlink\" title=\"Cookies\"></a>Cookies</h3><pre><code>curl -b &quot;name=value&quot; http://www.xxx.com\ncurl -c cookies.txt http://www.xxx.com\n#read write\ncurl -b cookies.txt -c cookies.txt http://www.xxx.com</code></pre><h3 id=\"额外的头部信息\"><a href=\"#额外的头部信息\" class=\"headerlink\" title=\"额外的头部信息\"></a>额外的头部信息</h3><pre><code>curl -H &quot;X-you-and-me: yes&quot; http://www.xxx.com</code></pre><h3 id=\"FTP-防火墙\"><a href=\"#FTP-防火墙\" class=\"headerlink\" title=\"FTP 防火墙\"></a>FTP 防火墙</h3><pre><code>#使用192.168.0.10作为IP地址\ncurl -P 192.168.0.10 ftp.download.com</code></pre><h3 id=\"HTTPS\"><a href=\"#HTTPS\" class=\"headerlink\" title=\"HTTPS\"></a>HTTPS</h3><pre><code>curl -E /path/to/cert.pem:password https://www.xxx.com</code></pre><h3 id=\"文件续传\"><a href=\"#文件续传\" class=\"headerlink\" title=\"文件续传\"></a>文件续传</h3><pre><code>#download\ncurl -C - -o file ftp://ftp.server.com/path/file\n#upload\ncurl -C - -T file ftp://ftp.server.com/path/file</code></pre><h3 id=\"L\"><a href=\"#L\" class=\"headerlink\" title=\"-L\"></a>-L</h3><p>如果页面内容移动到另一个页面比如返回状态码30X，则向新的页面发送请求</p>\n<h3 id=\"s\"><a href=\"#s\" class=\"headerlink\" title=\"-s\"></a>-s</h3><p>静默模式，没有输出</p>\n<h3 id=\"S\"><a href=\"#S\" class=\"headerlink\" title=\"-S\"></a>-S</h3><p>当使用<code>-s</code>时，输出错误信息。</p>\n"},{"title":"CURL命令学习二","date":"2019-12-19T12:52:41.000Z","_content":"### `-a, --append`\n\n用于上传文件时，如果服务器上该文件不存在则创建，如果存在则追加到源文件。\n\n### `-K, --config <file>`\n\n指定从某个文件读取`curl`参数。如果指定`-`为文件名则从输入读取参数。如：`-K --config -`\n\n### `--connect-timeout <seconds>`\n\n指定连接超时时间，若指定多个时间则采用最后一个。\n\n### `-C --continue-at <offset>`\n\n从给予的偏移量继续文件传输，用于断点续传，如果使用`-C -`则表明由`curl`自动获取从哪里开始继续传输。\n\n### `-c --cookie-jar <filename>`\n\n指定将`cookie`写入的文件，如果指定文件名为`-`，则将`cookie`写入输出。\n\n### `-b --cookie <data|filename>`\n\n将数据添加到`Cookie header`中传输到`HTTP`服务器。数据格式应该为`name1=value1;name2=value2`。如果文件名为`-`，则从输入读取数据。\n`-b --cookie`只用于输入`cookie`，并不会写`cookie`信息到本地，所以需要和`-c --cookie-jar`同时使用。\n\n### `--create-dirs`\n\n当使用`-o --output`选项时，`curl`将会创建必要的文件夹分层结构。如果`--output`文件名使用不存在的文件夹或者需要分层的文件夹存在，则没有文件夹被创建。\n\n### `-d --data <data>`\n\n通过POST请求发送具体的数据到HTTP服务器。\n\n### `-f --fail`\n\n当curl请求出现服务器错误时不打印错误信息，通常用于脚本中。只返回错误码22\n\n### `-F --form <name=content>`\n\n与`-d`相似，想服务器发送数据，`-F`是以表单形式\n\n* 发送文件：`curl -F \"name=@file.txt\" http://www.xxx.html`\n* 指定`Content-Type`：`curl -F \"web=@index.html;type=text/html\" http://www.xxx.html`\n\n### `-i`\n\n在输出中包含HTTP响应头信息。\n\n### `-X`\n\n指定具体的请求方法如`GET,POST...`","source":"_posts/blog/other/CURL学习二.md","raw":"---\ntitle: CURL命令学习二\ndate: 2019-12-19 20:52:41\ntags: curl学习\ncategories: curl\n---\n### `-a, --append`\n\n用于上传文件时，如果服务器上该文件不存在则创建，如果存在则追加到源文件。\n\n### `-K, --config <file>`\n\n指定从某个文件读取`curl`参数。如果指定`-`为文件名则从输入读取参数。如：`-K --config -`\n\n### `--connect-timeout <seconds>`\n\n指定连接超时时间，若指定多个时间则采用最后一个。\n\n### `-C --continue-at <offset>`\n\n从给予的偏移量继续文件传输，用于断点续传，如果使用`-C -`则表明由`curl`自动获取从哪里开始继续传输。\n\n### `-c --cookie-jar <filename>`\n\n指定将`cookie`写入的文件，如果指定文件名为`-`，则将`cookie`写入输出。\n\n### `-b --cookie <data|filename>`\n\n将数据添加到`Cookie header`中传输到`HTTP`服务器。数据格式应该为`name1=value1;name2=value2`。如果文件名为`-`，则从输入读取数据。\n`-b --cookie`只用于输入`cookie`，并不会写`cookie`信息到本地，所以需要和`-c --cookie-jar`同时使用。\n\n### `--create-dirs`\n\n当使用`-o --output`选项时，`curl`将会创建必要的文件夹分层结构。如果`--output`文件名使用不存在的文件夹或者需要分层的文件夹存在，则没有文件夹被创建。\n\n### `-d --data <data>`\n\n通过POST请求发送具体的数据到HTTP服务器。\n\n### `-f --fail`\n\n当curl请求出现服务器错误时不打印错误信息，通常用于脚本中。只返回错误码22\n\n### `-F --form <name=content>`\n\n与`-d`相似，想服务器发送数据，`-F`是以表单形式\n\n* 发送文件：`curl -F \"name=@file.txt\" http://www.xxx.html`\n* 指定`Content-Type`：`curl -F \"web=@index.html;type=text/html\" http://www.xxx.html`\n\n### `-i`\n\n在输出中包含HTTP响应头信息。\n\n### `-X`\n\n指定具体的请求方法如`GET,POST...`","slug":"blog/other/CURL学习二","published":1,"updated":"2020-05-11T03:44:37.674Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqyku0059k0vqcy086528","content":"<h3 id=\"a-append\"><a href=\"#a-append\" class=\"headerlink\" title=\"-a, --append\"></a><code>-a, --append</code></h3><p>用于上传文件时，如果服务器上该文件不存在则创建，如果存在则追加到源文件。</p>\n<h3 id=\"K-config-lt-file-gt\"><a href=\"#K-config-lt-file-gt\" class=\"headerlink\" title=\"-K, --config &lt;file&gt;\"></a><code>-K, --config &lt;file&gt;</code></h3><p>指定从某个文件读取<code>curl</code>参数。如果指定<code>-</code>为文件名则从输入读取参数。如：<code>-K --config -</code></p>\n<h3 id=\"connect-timeout-lt-seconds-gt\"><a href=\"#connect-timeout-lt-seconds-gt\" class=\"headerlink\" title=\"--connect-timeout &lt;seconds&gt;\"></a><code>--connect-timeout &lt;seconds&gt;</code></h3><p>指定连接超时时间，若指定多个时间则采用最后一个。</p>\n<h3 id=\"C-continue-at-lt-offset-gt\"><a href=\"#C-continue-at-lt-offset-gt\" class=\"headerlink\" title=\"-C --continue-at &lt;offset&gt;\"></a><code>-C --continue-at &lt;offset&gt;</code></h3><p>从给予的偏移量继续文件传输，用于断点续传，如果使用<code>-C -</code>则表明由<code>curl</code>自动获取从哪里开始继续传输。</p>\n<h3 id=\"c-cookie-jar-lt-filename-gt\"><a href=\"#c-cookie-jar-lt-filename-gt\" class=\"headerlink\" title=\"-c --cookie-jar &lt;filename&gt;\"></a><code>-c --cookie-jar &lt;filename&gt;</code></h3><p>指定将<code>cookie</code>写入的文件，如果指定文件名为<code>-</code>，则将<code>cookie</code>写入输出。</p>\n<h3 id=\"b-cookie-lt-data-filename-gt\"><a href=\"#b-cookie-lt-data-filename-gt\" class=\"headerlink\" title=\"-b --cookie &lt;data|filename&gt;\"></a><code>-b --cookie &lt;data|filename&gt;</code></h3><p>将数据添加到<code>Cookie header</code>中传输到<code>HTTP</code>服务器。数据格式应该为<code>name1=value1;name2=value2</code>。如果文件名为<code>-</code>，则从输入读取数据。<br><code>-b --cookie</code>只用于输入<code>cookie</code>，并不会写<code>cookie</code>信息到本地，所以需要和<code>-c --cookie-jar</code>同时使用。</p>\n<h3 id=\"create-dirs\"><a href=\"#create-dirs\" class=\"headerlink\" title=\"--create-dirs\"></a><code>--create-dirs</code></h3><p>当使用<code>-o --output</code>选项时，<code>curl</code>将会创建必要的文件夹分层结构。如果<code>--output</code>文件名使用不存在的文件夹或者需要分层的文件夹存在，则没有文件夹被创建。</p>\n<h3 id=\"d-data-lt-data-gt\"><a href=\"#d-data-lt-data-gt\" class=\"headerlink\" title=\"-d --data &lt;data&gt;\"></a><code>-d --data &lt;data&gt;</code></h3><p>通过POST请求发送具体的数据到HTTP服务器。</p>\n<h3 id=\"f-fail\"><a href=\"#f-fail\" class=\"headerlink\" title=\"-f --fail\"></a><code>-f --fail</code></h3><p>当curl请求出现服务器错误时不打印错误信息，通常用于脚本中。只返回错误码22</p>\n<h3 id=\"F-form-lt-name-content-gt\"><a href=\"#F-form-lt-name-content-gt\" class=\"headerlink\" title=\"-F --form &lt;name=content&gt;\"></a><code>-F --form &lt;name=content&gt;</code></h3><p>与<code>-d</code>相似，想服务器发送数据，<code>-F</code>是以表单形式</p>\n<ul>\n<li>发送文件：<code>curl -F &quot;name=@file.txt&quot; http://www.xxx.html</code></li>\n<li>指定<code>Content-Type</code>：<code>curl -F &quot;web=@index.html;type=text/html&quot; http://www.xxx.html</code></li>\n</ul>\n<h3 id=\"i\"><a href=\"#i\" class=\"headerlink\" title=\"-i\"></a><code>-i</code></h3><p>在输出中包含HTTP响应头信息。</p>\n<h3 id=\"X\"><a href=\"#X\" class=\"headerlink\" title=\"-X\"></a><code>-X</code></h3><p>指定具体的请求方法如<code>GET,POST...</code></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"a-append\"><a href=\"#a-append\" class=\"headerlink\" title=\"-a, --append\"></a><code>-a, --append</code></h3><p>用于上传文件时，如果服务器上该文件不存在则创建，如果存在则追加到源文件。</p>\n<h3 id=\"K-config-lt-file-gt\"><a href=\"#K-config-lt-file-gt\" class=\"headerlink\" title=\"-K, --config &lt;file&gt;\"></a><code>-K, --config &lt;file&gt;</code></h3><p>指定从某个文件读取<code>curl</code>参数。如果指定<code>-</code>为文件名则从输入读取参数。如：<code>-K --config -</code></p>\n<h3 id=\"connect-timeout-lt-seconds-gt\"><a href=\"#connect-timeout-lt-seconds-gt\" class=\"headerlink\" title=\"--connect-timeout &lt;seconds&gt;\"></a><code>--connect-timeout &lt;seconds&gt;</code></h3><p>指定连接超时时间，若指定多个时间则采用最后一个。</p>\n<h3 id=\"C-continue-at-lt-offset-gt\"><a href=\"#C-continue-at-lt-offset-gt\" class=\"headerlink\" title=\"-C --continue-at &lt;offset&gt;\"></a><code>-C --continue-at &lt;offset&gt;</code></h3><p>从给予的偏移量继续文件传输，用于断点续传，如果使用<code>-C -</code>则表明由<code>curl</code>自动获取从哪里开始继续传输。</p>\n<h3 id=\"c-cookie-jar-lt-filename-gt\"><a href=\"#c-cookie-jar-lt-filename-gt\" class=\"headerlink\" title=\"-c --cookie-jar &lt;filename&gt;\"></a><code>-c --cookie-jar &lt;filename&gt;</code></h3><p>指定将<code>cookie</code>写入的文件，如果指定文件名为<code>-</code>，则将<code>cookie</code>写入输出。</p>\n<h3 id=\"b-cookie-lt-data-filename-gt\"><a href=\"#b-cookie-lt-data-filename-gt\" class=\"headerlink\" title=\"-b --cookie &lt;data|filename&gt;\"></a><code>-b --cookie &lt;data|filename&gt;</code></h3><p>将数据添加到<code>Cookie header</code>中传输到<code>HTTP</code>服务器。数据格式应该为<code>name1=value1;name2=value2</code>。如果文件名为<code>-</code>，则从输入读取数据。<br><code>-b --cookie</code>只用于输入<code>cookie</code>，并不会写<code>cookie</code>信息到本地，所以需要和<code>-c --cookie-jar</code>同时使用。</p>\n<h3 id=\"create-dirs\"><a href=\"#create-dirs\" class=\"headerlink\" title=\"--create-dirs\"></a><code>--create-dirs</code></h3><p>当使用<code>-o --output</code>选项时，<code>curl</code>将会创建必要的文件夹分层结构。如果<code>--output</code>文件名使用不存在的文件夹或者需要分层的文件夹存在，则没有文件夹被创建。</p>\n<h3 id=\"d-data-lt-data-gt\"><a href=\"#d-data-lt-data-gt\" class=\"headerlink\" title=\"-d --data &lt;data&gt;\"></a><code>-d --data &lt;data&gt;</code></h3><p>通过POST请求发送具体的数据到HTTP服务器。</p>\n<h3 id=\"f-fail\"><a href=\"#f-fail\" class=\"headerlink\" title=\"-f --fail\"></a><code>-f --fail</code></h3><p>当curl请求出现服务器错误时不打印错误信息，通常用于脚本中。只返回错误码22</p>\n<h3 id=\"F-form-lt-name-content-gt\"><a href=\"#F-form-lt-name-content-gt\" class=\"headerlink\" title=\"-F --form &lt;name=content&gt;\"></a><code>-F --form &lt;name=content&gt;</code></h3><p>与<code>-d</code>相似，想服务器发送数据，<code>-F</code>是以表单形式</p>\n<ul>\n<li>发送文件：<code>curl -F &quot;name=@file.txt&quot; http://www.xxx.html</code></li>\n<li>指定<code>Content-Type</code>：<code>curl -F &quot;web=@index.html;type=text/html&quot; http://www.xxx.html</code></li>\n</ul>\n<h3 id=\"i\"><a href=\"#i\" class=\"headerlink\" title=\"-i\"></a><code>-i</code></h3><p>在输出中包含HTTP响应头信息。</p>\n<h3 id=\"X\"><a href=\"#X\" class=\"headerlink\" title=\"-X\"></a><code>-X</code></h3><p>指定具体的请求方法如<code>GET,POST...</code></p>\n"},{"title":"布隆过滤器(Bloom Filter)","date":"2020-05-11T04:52:41.000Z","_content":"\n\n布隆过滤器(Bloom Filter)是一种基于Hash的高效查找数据结构，它能够快速答复“某个元素是否存在”的问题。布隆过滤器只能用于添加元素与查询元素，不能够用于删除元素。\n\n在布隆过滤器之前，使用的是基于Hash的快速查找算法。Hash可以将一个元素进行哈希，然后根据哈希值映射到数组的某一个位置。并且根据Hash算法的优劣，不同元素映射到相同位置的可能性不同。但是如果基于Hash的快速查找算法的数组大小被限制在一定的范围内，那么发生哈希冲突的概率将会变大。并且数组范围越小，冲突概率将越大。因此布隆过滤器采用了使用多个hash函数进行运算来提高空间利用率。\n\n## Bloom过滤器原理\n布隆过滤器是由一个可变长度为*N*的二进制数组与一组数量可变*M*的哈希函数构成。其中，哈希函数为确定性函数，所有哈希函数的输出值都在*1~N*之间，与二进制数组相对应。因此，每一个元素使用布隆过滤器的哈希函数进行运算都将会得到相同的结果。\n\n![图](/img/blog/bloom/1.png)\n\n\n### 插入一个元素\n假设我们需要插入一个元素到布隆过滤器中，我们需要使用不同的哈希函数进行运算生成不同的哈希值，并且根据生成的哈希值将二进制数组对应的``Bit``位置为1.例如插入字符串``\"Bloom\"``到过滤器中，使用三种哈希函数进行计算所得到的哈希值分别为1,3,7，那么布隆过滤器的二进制数组则会变为:\n![图](/img/blog/bloom/2.png)\n\n\n假设我们插入第二个字符串``\"Filter\"``到过滤器中，同样，我们使用相同的哈希函数进行运算，假设哈希值分别为2,4,7，那么二进制数组则会变为:\n![图](/img/blog/bloom/3.png)\n\n\n\n因为在插入第一个字符串时，哈希值为7的``Bit``位置已经被置为1，因此不需要更改，只需要将``Bit``位为2,4置为1即可。\n\n\n### 查询元素\n假设需要查询某个元素是否存在，只需要使用相同的哈希函数进行运算，然后与二进制数组进行Bit值匹配即可。比如，我们需要查询字符串``\"hash\"``是否存在，使用之前的哈希函数进行运算，假设输出的哈希值为4,5,7，由于``Bit``位为5的位置仍然为0，所以对于字符串``\"hash\"``并不存在。\n![图](/img/blog/bloom/4.png)\n\n\n\n但是如果运算的哈希值为2,3,7，我们也只能说该字符串有可能存在。因为随着存储的数组越多，将会有越多的``Bit``位被置为1，即使某个字符串没有存储，但是有可能该字符串的哈希值与其他被存储的数据哈希值重复，仍然可能误判为该字符串存在。\n![图](/img/blog/bloom/5.png)\n\n\n因此，对于查询某个元素，只能判定某个元素**一定不存在**或者**有可能存在**，并不能判定某个元素**一定存在**。\n\n\n### 选择合适的数组长度与哈希函数数量\n因此需要设置合适的数组长度与哈希函数数量。\n* 数组越短则更容易所有的位置被置为1，那么可能查询任何值都会被判断可能存在，过滤的效率将大大降低。\n* 数组越长则会增加过滤效率，但是过长则会耗费大量空间。\n\n哈希函数数量也会影响过滤效率.\n* 哈希函数越多则二进制位置1的次数越多，效率也会变低\n* 但是数量过少的话误判率将会变高。\n\n可以通过以下公式计算合适的数组长度与哈希函数数量:\n![图](/img/blog/bloom/6.png)\n\n\n其中k 为哈希函数个数，m 为布隆过滤器长度，n 为插入的元素个数，p 为误报率。\n\n### Hash与布隆过滤器\n实际上，无论是\tHash，还是布隆过滤器，基本思想是一致的，\n* 都是基于内容的编址。\n* Hash 函数存在冲突，布隆过滤器也存在冲突。\n* 都可能误报，但绝对不会漏报。\n","source":"_posts/blog/other/bloom.md","raw":"---\ntitle: 布隆过滤器(Bloom Filter)\ndate: 2020-05-11 12:52:41\ntags: bloom\ncategories: other\n---\n\n\n布隆过滤器(Bloom Filter)是一种基于Hash的高效查找数据结构，它能够快速答复“某个元素是否存在”的问题。布隆过滤器只能用于添加元素与查询元素，不能够用于删除元素。\n\n在布隆过滤器之前，使用的是基于Hash的快速查找算法。Hash可以将一个元素进行哈希，然后根据哈希值映射到数组的某一个位置。并且根据Hash算法的优劣，不同元素映射到相同位置的可能性不同。但是如果基于Hash的快速查找算法的数组大小被限制在一定的范围内，那么发生哈希冲突的概率将会变大。并且数组范围越小，冲突概率将越大。因此布隆过滤器采用了使用多个hash函数进行运算来提高空间利用率。\n\n## Bloom过滤器原理\n布隆过滤器是由一个可变长度为*N*的二进制数组与一组数量可变*M*的哈希函数构成。其中，哈希函数为确定性函数，所有哈希函数的输出值都在*1~N*之间，与二进制数组相对应。因此，每一个元素使用布隆过滤器的哈希函数进行运算都将会得到相同的结果。\n\n![图](/img/blog/bloom/1.png)\n\n\n### 插入一个元素\n假设我们需要插入一个元素到布隆过滤器中，我们需要使用不同的哈希函数进行运算生成不同的哈希值，并且根据生成的哈希值将二进制数组对应的``Bit``位置为1.例如插入字符串``\"Bloom\"``到过滤器中，使用三种哈希函数进行计算所得到的哈希值分别为1,3,7，那么布隆过滤器的二进制数组则会变为:\n![图](/img/blog/bloom/2.png)\n\n\n假设我们插入第二个字符串``\"Filter\"``到过滤器中，同样，我们使用相同的哈希函数进行运算，假设哈希值分别为2,4,7，那么二进制数组则会变为:\n![图](/img/blog/bloom/3.png)\n\n\n\n因为在插入第一个字符串时，哈希值为7的``Bit``位置已经被置为1，因此不需要更改，只需要将``Bit``位为2,4置为1即可。\n\n\n### 查询元素\n假设需要查询某个元素是否存在，只需要使用相同的哈希函数进行运算，然后与二进制数组进行Bit值匹配即可。比如，我们需要查询字符串``\"hash\"``是否存在，使用之前的哈希函数进行运算，假设输出的哈希值为4,5,7，由于``Bit``位为5的位置仍然为0，所以对于字符串``\"hash\"``并不存在。\n![图](/img/blog/bloom/4.png)\n\n\n\n但是如果运算的哈希值为2,3,7，我们也只能说该字符串有可能存在。因为随着存储的数组越多，将会有越多的``Bit``位被置为1，即使某个字符串没有存储，但是有可能该字符串的哈希值与其他被存储的数据哈希值重复，仍然可能误判为该字符串存在。\n![图](/img/blog/bloom/5.png)\n\n\n因此，对于查询某个元素，只能判定某个元素**一定不存在**或者**有可能存在**，并不能判定某个元素**一定存在**。\n\n\n### 选择合适的数组长度与哈希函数数量\n因此需要设置合适的数组长度与哈希函数数量。\n* 数组越短则更容易所有的位置被置为1，那么可能查询任何值都会被判断可能存在，过滤的效率将大大降低。\n* 数组越长则会增加过滤效率，但是过长则会耗费大量空间。\n\n哈希函数数量也会影响过滤效率.\n* 哈希函数越多则二进制位置1的次数越多，效率也会变低\n* 但是数量过少的话误判率将会变高。\n\n可以通过以下公式计算合适的数组长度与哈希函数数量:\n![图](/img/blog/bloom/6.png)\n\n\n其中k 为哈希函数个数，m 为布隆过滤器长度，n 为插入的元素个数，p 为误报率。\n\n### Hash与布隆过滤器\n实际上，无论是\tHash，还是布隆过滤器，基本思想是一致的，\n* 都是基于内容的编址。\n* Hash 函数存在冲突，布隆过滤器也存在冲突。\n* 都可能误报，但绝对不会漏报。\n","slug":"blog/other/bloom","published":1,"updated":"2020-05-11T03:54:00.567Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckadkqykz005ck0vq34br5he5","content":"<p>布隆过滤器(Bloom Filter)是一种基于Hash的高效查找数据结构，它能够快速答复“某个元素是否存在”的问题。布隆过滤器只能用于添加元素与查询元素，不能够用于删除元素。</p>\n<p>在布隆过滤器之前，使用的是基于Hash的快速查找算法。Hash可以将一个元素进行哈希，然后根据哈希值映射到数组的某一个位置。并且根据Hash算法的优劣，不同元素映射到相同位置的可能性不同。但是如果基于Hash的快速查找算法的数组大小被限制在一定的范围内，那么发生哈希冲突的概率将会变大。并且数组范围越小，冲突概率将越大。因此布隆过滤器采用了使用多个hash函数进行运算来提高空间利用率。</p>\n<h2 id=\"Bloom过滤器原理\"><a href=\"#Bloom过滤器原理\" class=\"headerlink\" title=\"Bloom过滤器原理\"></a>Bloom过滤器原理</h2><p>布隆过滤器是由一个可变长度为<em>N</em>的二进制数组与一组数量可变<em>M</em>的哈希函数构成。其中，哈希函数为确定性函数，所有哈希函数的输出值都在<em>1~N</em>之间，与二进制数组相对应。因此，每一个元素使用布隆过滤器的哈希函数进行运算都将会得到相同的结果。</p>\n<p><img src=\"/img/blog/bloom/1.png\" srcset=\"undefined\" alt=\"图\"></p>\n<h3 id=\"插入一个元素\"><a href=\"#插入一个元素\" class=\"headerlink\" title=\"插入一个元素\"></a>插入一个元素</h3><p>假设我们需要插入一个元素到布隆过滤器中，我们需要使用不同的哈希函数进行运算生成不同的哈希值，并且根据生成的哈希值将二进制数组对应的<code>Bit</code>位置为1.例如插入字符串<code>&quot;Bloom&quot;</code>到过滤器中，使用三种哈希函数进行计算所得到的哈希值分别为1,3,7，那么布隆过滤器的二进制数组则会变为:<br><img src=\"/img/blog/bloom/2.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>假设我们插入第二个字符串<code>&quot;Filter&quot;</code>到过滤器中，同样，我们使用相同的哈希函数进行运算，假设哈希值分别为2,4,7，那么二进制数组则会变为:<br><img src=\"/img/blog/bloom/3.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>因为在插入第一个字符串时，哈希值为7的<code>Bit</code>位置已经被置为1，因此不需要更改，只需要将<code>Bit</code>位为2,4置为1即可。</p>\n<h3 id=\"查询元素\"><a href=\"#查询元素\" class=\"headerlink\" title=\"查询元素\"></a>查询元素</h3><p>假设需要查询某个元素是否存在，只需要使用相同的哈希函数进行运算，然后与二进制数组进行Bit值匹配即可。比如，我们需要查询字符串<code>&quot;hash&quot;</code>是否存在，使用之前的哈希函数进行运算，假设输出的哈希值为4,5,7，由于<code>Bit</code>位为5的位置仍然为0，所以对于字符串<code>&quot;hash&quot;</code>并不存在。<br><img src=\"/img/blog/bloom/4.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>但是如果运算的哈希值为2,3,7，我们也只能说该字符串有可能存在。因为随着存储的数组越多，将会有越多的<code>Bit</code>位被置为1，即使某个字符串没有存储，但是有可能该字符串的哈希值与其他被存储的数据哈希值重复，仍然可能误判为该字符串存在。<br><img src=\"/img/blog/bloom/5.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>因此，对于查询某个元素，只能判定某个元素<strong>一定不存在</strong>或者<strong>有可能存在</strong>，并不能判定某个元素<strong>一定存在</strong>。</p>\n<h3 id=\"选择合适的数组长度与哈希函数数量\"><a href=\"#选择合适的数组长度与哈希函数数量\" class=\"headerlink\" title=\"选择合适的数组长度与哈希函数数量\"></a>选择合适的数组长度与哈希函数数量</h3><p>因此需要设置合适的数组长度与哈希函数数量。</p>\n<ul>\n<li>数组越短则更容易所有的位置被置为1，那么可能查询任何值都会被判断可能存在，过滤的效率将大大降低。</li>\n<li>数组越长则会增加过滤效率，但是过长则会耗费大量空间。</li>\n</ul>\n<p>哈希函数数量也会影响过滤效率.</p>\n<ul>\n<li>哈希函数越多则二进制位置1的次数越多，效率也会变低</li>\n<li>但是数量过少的话误判率将会变高。</li>\n</ul>\n<p>可以通过以下公式计算合适的数组长度与哈希函数数量:<br><img src=\"/img/blog/bloom/6.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>其中k 为哈希函数个数，m 为布隆过滤器长度，n 为插入的元素个数，p 为误报率。</p>\n<h3 id=\"Hash与布隆过滤器\"><a href=\"#Hash与布隆过滤器\" class=\"headerlink\" title=\"Hash与布隆过滤器\"></a>Hash与布隆过滤器</h3><p>实际上，无论是    Hash，还是布隆过滤器，基本思想是一致的，</p>\n<ul>\n<li>都是基于内容的编址。</li>\n<li>Hash 函数存在冲突，布隆过滤器也存在冲突。</li>\n<li>都可能误报，但绝对不会漏报。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>布隆过滤器(Bloom Filter)是一种基于Hash的高效查找数据结构，它能够快速答复“某个元素是否存在”的问题。布隆过滤器只能用于添加元素与查询元素，不能够用于删除元素。</p>\n<p>在布隆过滤器之前，使用的是基于Hash的快速查找算法。Hash可以将一个元素进行哈希，然后根据哈希值映射到数组的某一个位置。并且根据Hash算法的优劣，不同元素映射到相同位置的可能性不同。但是如果基于Hash的快速查找算法的数组大小被限制在一定的范围内，那么发生哈希冲突的概率将会变大。并且数组范围越小，冲突概率将越大。因此布隆过滤器采用了使用多个hash函数进行运算来提高空间利用率。</p>\n<h2 id=\"Bloom过滤器原理\"><a href=\"#Bloom过滤器原理\" class=\"headerlink\" title=\"Bloom过滤器原理\"></a>Bloom过滤器原理</h2><p>布隆过滤器是由一个可变长度为<em>N</em>的二进制数组与一组数量可变<em>M</em>的哈希函数构成。其中，哈希函数为确定性函数，所有哈希函数的输出值都在<em>1~N</em>之间，与二进制数组相对应。因此，每一个元素使用布隆过滤器的哈希函数进行运算都将会得到相同的结果。</p>\n<p><img src=\"/img/blog/bloom/1.png\" srcset=\"undefined\" alt=\"图\"></p>\n<h3 id=\"插入一个元素\"><a href=\"#插入一个元素\" class=\"headerlink\" title=\"插入一个元素\"></a>插入一个元素</h3><p>假设我们需要插入一个元素到布隆过滤器中，我们需要使用不同的哈希函数进行运算生成不同的哈希值，并且根据生成的哈希值将二进制数组对应的<code>Bit</code>位置为1.例如插入字符串<code>&quot;Bloom&quot;</code>到过滤器中，使用三种哈希函数进行计算所得到的哈希值分别为1,3,7，那么布隆过滤器的二进制数组则会变为:<br><img src=\"/img/blog/bloom/2.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>假设我们插入第二个字符串<code>&quot;Filter&quot;</code>到过滤器中，同样，我们使用相同的哈希函数进行运算，假设哈希值分别为2,4,7，那么二进制数组则会变为:<br><img src=\"/img/blog/bloom/3.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>因为在插入第一个字符串时，哈希值为7的<code>Bit</code>位置已经被置为1，因此不需要更改，只需要将<code>Bit</code>位为2,4置为1即可。</p>\n<h3 id=\"查询元素\"><a href=\"#查询元素\" class=\"headerlink\" title=\"查询元素\"></a>查询元素</h3><p>假设需要查询某个元素是否存在，只需要使用相同的哈希函数进行运算，然后与二进制数组进行Bit值匹配即可。比如，我们需要查询字符串<code>&quot;hash&quot;</code>是否存在，使用之前的哈希函数进行运算，假设输出的哈希值为4,5,7，由于<code>Bit</code>位为5的位置仍然为0，所以对于字符串<code>&quot;hash&quot;</code>并不存在。<br><img src=\"/img/blog/bloom/4.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>但是如果运算的哈希值为2,3,7，我们也只能说该字符串有可能存在。因为随着存储的数组越多，将会有越多的<code>Bit</code>位被置为1，即使某个字符串没有存储，但是有可能该字符串的哈希值与其他被存储的数据哈希值重复，仍然可能误判为该字符串存在。<br><img src=\"/img/blog/bloom/5.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>因此，对于查询某个元素，只能判定某个元素<strong>一定不存在</strong>或者<strong>有可能存在</strong>，并不能判定某个元素<strong>一定存在</strong>。</p>\n<h3 id=\"选择合适的数组长度与哈希函数数量\"><a href=\"#选择合适的数组长度与哈希函数数量\" class=\"headerlink\" title=\"选择合适的数组长度与哈希函数数量\"></a>选择合适的数组长度与哈希函数数量</h3><p>因此需要设置合适的数组长度与哈希函数数量。</p>\n<ul>\n<li>数组越短则更容易所有的位置被置为1，那么可能查询任何值都会被判断可能存在，过滤的效率将大大降低。</li>\n<li>数组越长则会增加过滤效率，但是过长则会耗费大量空间。</li>\n</ul>\n<p>哈希函数数量也会影响过滤效率.</p>\n<ul>\n<li>哈希函数越多则二进制位置1的次数越多，效率也会变低</li>\n<li>但是数量过少的话误判率将会变高。</li>\n</ul>\n<p>可以通过以下公式计算合适的数组长度与哈希函数数量:<br><img src=\"/img/blog/bloom/6.png\" srcset=\"undefined\" alt=\"图\"></p>\n<p>其中k 为哈希函数个数，m 为布隆过滤器长度，n 为插入的元素个数，p 为误报率。</p>\n<h3 id=\"Hash与布隆过滤器\"><a href=\"#Hash与布隆过滤器\" class=\"headerlink\" title=\"Hash与布隆过滤器\"></a>Hash与布隆过滤器</h3><p>实际上，无论是    Hash，还是布隆过滤器，基本思想是一致的，</p>\n<ul>\n<li>都是基于内容的编址。</li>\n<li>Hash 函数存在冲突，布隆过滤器也存在冲突。</li>\n<li>都可能误报，但绝对不会漏报。</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ckadkqyfg000bk0vq5drm0kai","category_id":"ckadkqyfa0007k0vqeh4n49n7","_id":"ckadkqyg0000kk0vqgoacdeso"},{"post_id":"ckadkqyf10004k0vqhbxi3m61","category_id":"ckadkqyfa0007k0vqeh4n49n7","_id":"ckadkqyg6000mk0vqg2r26akq"},{"post_id":"ckadkqyfk000ek0vqagvq6cch","category_id":"ckadkqyfa0007k0vqeh4n49n7","_id":"ckadkqyg8000pk0vqgn0i6vmm"},{"post_id":"ckadkqyfn000gk0vq4fb083tc","category_id":"ckadkqyfa0007k0vqeh4n49n7","_id":"ckadkqygj000sk0vq1vqa3qa9"},{"post_id":"ckadkqyfc0009k0vqb1vmdp2n","category_id":"ckadkqyfa0007k0vqeh4n49n7","_id":"ckadkqygn000wk0vqgei2fqp0"},{"post_id":"ckadkqyfw000jk0vq8gsyhvkh","category_id":"ckadkqyfa0007k0vqeh4n49n7","_id":"ckadkqygp000yk0vq100ufma8"},{"post_id":"ckadkqyg3000lk0vq3t69a895","category_id":"ckadkqyfa0007k0vqeh4n49n7","_id":"ckadkqygx0012k0vq525p3squ"},{"post_id":"ckadkqygh000rk0vq07hhep8h","category_id":"ckadkqyfa0007k0vqeh4n49n7","_id":"ckadkqygz0015k0vqbf82f98s"},{"post_id":"ckadkqyg7000ok0vq50de9frc","category_id":"ckadkqygk000uk0vq517yfwnk","_id":"ckadkqyh00019k0vqflxkhlub"},{"post_id":"ckadkqygy0014k0vqb6x85zp3","category_id":"ckadkqygx0011k0vq78zi3t9p","_id":"ckadkqyh3001ek0vq5dgc3hmb"},{"post_id":"ckadkqygl000vk0vqakilajuk","category_id":"ckadkqygx0011k0vq78zi3t9p","_id":"ckadkqyh5001ik0vqb6r09cs9"},{"post_id":"ckadkqygz0018k0vq2prmfo1h","category_id":"ckadkqygx0011k0vq78zi3t9p","_id":"ckadkqyhd001mk0vq0vpqdv7v"},{"post_id":"ckadkqygo000xk0vqdfztgyfz","category_id":"ckadkqygx0011k0vq78zi3t9p","_id":"ckadkqyhh001pk0vq7mfx9m3t"},{"post_id":"ckadkqygv0010k0vqbjii90p4","category_id":"ckadkqygx0011k0vq78zi3t9p","_id":"ckadkqyhk001sk0vqaijm535q"},{"post_id":"ckadkqyhg001ok0vq6uxxdwcd","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyhm001zk0vqc6omgn7z"},{"post_id":"ckadkqyh1001bk0vq0b9x6ag2","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyhv0022k0vq8rg7f18m"},{"post_id":"ckadkqyhi001rk0vqc0kv07qv","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyhw0025k0vqc1hi71vx"},{"post_id":"ckadkqyhk001vk0vq74u3eokf","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyhy0029k0vq1pfhgzgd"},{"post_id":"ckadkqyh2001dk0vqaarz98db","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyi0002dk0vqeuvjd827"},{"post_id":"ckadkqyhm001yk0vq2mjieeoo","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyi1002gk0vqefns5bu0"},{"post_id":"ckadkqyht0021k0vqhmnyczck","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyi3002kk0vq5bw40d62"},{"post_id":"ckadkqyh4001hk0vq09t6g8d2","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyi8002nk0vq9d7b2o1m"},{"post_id":"ckadkqyhv0024k0vq96kydcsx","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyib002qk0vqhnqd2tvi"},{"post_id":"ckadkqyhy0028k0vq1pic7vbe","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyig002uk0vqgqo93omd"},{"post_id":"ckadkqyh9001lk0vq6v2u3rja","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyiq002wk0vqavrm93cm"},{"post_id":"ckadkqyhz002ck0vq4kwe83ny","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyiu0030k0vqhn15c712"},{"post_id":"ckadkqyi0002fk0vq5edg3qm4","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyiw0033k0vqe4iya6kl"},{"post_id":"ckadkqyi2002jk0vqce2e2cea","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyiz0038k0vq3bfn1jg4"},{"post_id":"ckadkqyi6002mk0vq2j723006","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyj0003ak0vq1wbjcjwf"},{"post_id":"ckadkqyi8002pk0vq1wqgbeyd","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyj4003fk0vqbf60drfi"},{"post_id":"ckadkqyid002tk0vq3tvwc4ro","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyj6003hk0vq38xxhzzm"},{"post_id":"ckadkqyij002vk0vq4vef4acb","category_id":"ckadkqyhe001nk0vqauecaub5","_id":"ckadkqyjb003mk0vq1bhfdaqu"},{"post_id":"ckadkqyiz0039k0vq336s5w58","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyje003ok0vq5f7o9igk"},{"post_id":"ckadkqyit002zk0vq2cha6e4e","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyjo003tk0vqe8uv87ty"},{"post_id":"ckadkqyj1003dk0vq6c0a6gm3","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyjr003vk0vq494d7e98"},{"post_id":"ckadkqyiv0032k0vq0rt3b5v7","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyjs003zk0vq528p6tyr"},{"post_id":"ckadkqyj8003kk0vqdr0rgkbm","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyju0042k0vqh04cgj5g"},{"post_id":"ckadkqyix0037k0vq0oqkeyqj","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyjv0045k0vqemv42hbu"},{"post_id":"ckadkqyjp003uk0vqbl1t19kj","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyjy0049k0vq6t6t27yy"},{"post_id":"ckadkqyj5003gk0vq1a44gghl","category_id":"ckadkqyjo003sk0vq0p359xfp","_id":"ckadkqyk0004ck0vq31u277ci"},{"post_id":"ckadkqyjs003yk0vq038d3uci","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyk2004fk0vqcy6nae5y"},{"post_id":"ckadkqyjt0041k0vq64wo4q43","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyk4004ik0vq0c7320ts"},{"post_id":"ckadkqyjd003nk0vq437qfjbu","category_id":"ckadkqyjt0040k0vqfxvl8k7g","_id":"ckadkqyka004mk0vq6gru6nwb"},{"post_id":"ckadkqyjv0044k0vqcrllci97","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyke004pk0vqenmeak6j"},{"post_id":"ckadkqyjx0048k0vqdrcg4m6u","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqykk004tk0vq9jpg9ixz"},{"post_id":"ckadkqyjg003rk0vq69gf1uc5","category_id":"ckadkqyjo003sk0vq0p359xfp","_id":"ckadkqykm004wk0vqeioo7g63"},{"post_id":"ckadkqyk0004bk0vq7h9e3c91","category_id":"ckadkqyix0035k0vqaw5dcznr","_id":"ckadkqyko0050k0vq7zr63f5g"},{"post_id":"ckadkqykb004ok0vqeoir990m","category_id":"ckadkqyk5004kk0vq54jr5mf3","_id":"ckadkqykq0053k0vq74ic5guc"},{"post_id":"ckadkqyk1004ek0vqbldndcjq","category_id":"ckadkqyk5004kk0vq54jr5mf3","_id":"ckadkqykt0057k0vq5aswdqu8"},{"post_id":"ckadkqyki004rk0vqcvj418ji","category_id":"ckadkqyk5004kk0vq54jr5mf3","_id":"ckadkqykv005ak0vqbf7x1n1w"},{"post_id":"ckadkqyk3004hk0vq733m0f3n","category_id":"ckadkqyk5004kk0vq54jr5mf3","_id":"ckadkqyl0005dk0vqgc0g80qw"},{"post_id":"ckadkqyk8004lk0vq0oc9hbbe","category_id":"ckadkqyk5004kk0vq54jr5mf3","_id":"ckadkqyl1005gk0vqb0muej6n"},{"post_id":"ckadkqyku0059k0vqcy086528","category_id":"ckadkqykt0056k0vq72t22qex","_id":"ckadkqyl2005ik0vq8gt1657z"},{"post_id":"ckadkqykm004yk0vqb28ldrqo","category_id":"ckadkqykt0056k0vq72t22qex","_id":"ckadkqyl3005lk0vq87uw3bxk"},{"post_id":"ckadkqykp0052k0vq0flq6ta1","category_id":"ckadkqyl1005ek0vqcubm4tm4","_id":"ckadkqyl4005pk0vq23aab0z8"},{"post_id":"ckadkqykr0055k0vq66edb5ac","category_id":"ckadkqykt0056k0vq72t22qex","_id":"ckadkqyl5005sk0vq56hmgdca"},{"post_id":"ckadkqykz005ck0vq34br5he5","category_id":"ckadkqyl4005ok0vq6w5o3xte","_id":"ckadkqyl7005vk0vq4l89b9eu"}],"PostTag":[{"post_id":"ckadkqye80000k0vqfhxyeyrb","tag_id":"ckadkqyeq0002k0vqhz78adle","_id":"ckadkqyfb0008k0vq265kbde6"},{"post_id":"ckadkqyf60005k0vq08xyc017","tag_id":"ckadkqyeq0002k0vqhz78adle","_id":"ckadkqyfg000ak0vqhgnvcawk"},{"post_id":"ckadkqyem0001k0vq90rvgwhy","tag_id":"ckadkqyeq0002k0vqhz78adle","_id":"ckadkqyfj000dk0vqecht4l07"},{"post_id":"ckadkqyeu0003k0vqhcaf6aaa","tag_id":"ckadkqyeq0002k0vqhz78adle","_id":"ckadkqyfw000ik0vq82hu024q"},{"post_id":"ckadkqyf10004k0vqhbxi3m61","tag_id":"ckadkqyfr000hk0vq81z4bee6","_id":"ckadkqyg8000qk0vq1jva5q36"},{"post_id":"ckadkqyfc0009k0vqb1vmdp2n","tag_id":"ckadkqyg6000nk0vq59de1sb0","_id":"ckadkqygx0013k0vq1wye3ba6"},{"post_id":"ckadkqyfc0009k0vqb1vmdp2n","tag_id":"ckadkqyfr000hk0vq81z4bee6","_id":"ckadkqygz0016k0vqcnyd3vcs"},{"post_id":"ckadkqyfg000bk0vq5drm0kai","tag_id":"ckadkqygu000zk0vq9zzq3iyl","_id":"ckadkqyh4001gk0vqfjv891vy"},{"post_id":"ckadkqyfg000bk0vq5drm0kai","tag_id":"ckadkqyfr000hk0vq81z4bee6","_id":"ckadkqyh6001jk0vq57re55vg"},{"post_id":"ckadkqyfk000ek0vqagvq6cch","tag_id":"ckadkqygu000zk0vq9zzq3iyl","_id":"ckadkqyhk001uk0vq7xfmfhrt"},{"post_id":"ckadkqyfk000ek0vqagvq6cch","tag_id":"ckadkqyfr000hk0vq81z4bee6","_id":"ckadkqyhl001wk0vqd9i0e8wj"},{"post_id":"ckadkqyfn000gk0vq4fb083tc","tag_id":"ckadkqyg6000nk0vq59de1sb0","_id":"ckadkqyhx0027k0vqarox4x89"},{"post_id":"ckadkqyfn000gk0vq4fb083tc","tag_id":"ckadkqyfr000hk0vq81z4bee6","_id":"ckadkqyhz002ak0vq54g70sck"},{"post_id":"ckadkqyfw000jk0vq8gsyhvkh","tag_id":"ckadkqygu000zk0vq9zzq3iyl","_id":"ckadkqyi0002ek0vq4wd5ggpl"},{"post_id":"ckadkqyfw000jk0vq8gsyhvkh","tag_id":"ckadkqyfr000hk0vq81z4bee6","_id":"ckadkqyi2002hk0vqhqctf9j5"},{"post_id":"ckadkqyg3000lk0vq3t69a895","tag_id":"ckadkqygu000zk0vq9zzq3iyl","_id":"ckadkqyi6002lk0vq6xpz9aac"},{"post_id":"ckadkqyg3000lk0vq3t69a895","tag_id":"ckadkqyfr000hk0vq81z4bee6","_id":"ckadkqyi8002ok0vq6rg6bwyf"},{"post_id":"ckadkqyg7000ok0vq50de9frc","tag_id":"ckadkqyi2002ik0vq2l3v8s3r","_id":"ckadkqyic002sk0vq1ndq5lhl"},{"post_id":"ckadkqygh000rk0vq07hhep8h","tag_id":"ckadkqygu000zk0vq9zzq3iyl","_id":"ckadkqyit002yk0vqf3lx212x"},{"post_id":"ckadkqygh000rk0vq07hhep8h","tag_id":"ckadkqyfr000hk0vq81z4bee6","_id":"ckadkqyiv0031k0vqhl1102ia"},{"post_id":"ckadkqygl000vk0vqakilajuk","tag_id":"ckadkqyi2002ik0vq2l3v8s3r","_id":"ckadkqyix0036k0vq4m7w0uly"},{"post_id":"ckadkqygo000xk0vqdfztgyfz","tag_id":"ckadkqyi2002ik0vq2l3v8s3r","_id":"ckadkqyj1003ck0vq76t9e3yu"},{"post_id":"ckadkqygv0010k0vqbjii90p4","tag_id":"ckadkqyi2002ik0vq2l3v8s3r","_id":"ckadkqyj8003jk0vqees7hhoj"},{"post_id":"ckadkqygy0014k0vqb6x85zp3","tag_id":"ckadkqyi2002ik0vq2l3v8s3r","_id":"ckadkqyjg003qk0vq1sgic2t2"},{"post_id":"ckadkqygz0018k0vq2prmfo1h","tag_id":"ckadkqyi2002ik0vq2l3v8s3r","_id":"ckadkqyjs003xk0vqhj1n5tmm"},{"post_id":"ckadkqyh1001bk0vq0b9x6ag2","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyjv0046k0vq08ou682r"},{"post_id":"ckadkqyh2001dk0vqaarz98db","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyk0004dk0vq6mjtfq5d"},{"post_id":"ckadkqyh4001hk0vq09t6g8d2","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyk4004jk0vqabiwd8hb"},{"post_id":"ckadkqyh9001lk0vq6v2u3rja","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqykf004qk0vqffg44c5n"},{"post_id":"ckadkqyhg001ok0vq6uxxdwcd","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqykm004xk0vq0peb9ywm"},{"post_id":"ckadkqyhi001rk0vqc0kv07qv","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqykr0054k0vq4yko0lky"},{"post_id":"ckadkqyhk001vk0vq74u3eokf","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqykv005bk0vq4acq8ye1"},{"post_id":"ckadkqyhm001yk0vq2mjieeoo","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyl1005hk0vqdiju50cn"},{"post_id":"ckadkqyht0021k0vqhmnyczck","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyl3005mk0vq3nlg2goh"},{"post_id":"ckadkqyhv0024k0vq96kydcsx","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyl5005qk0vqc2kvcyk6"},{"post_id":"ckadkqyhy0028k0vq1pic7vbe","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyl6005tk0vqfz0ta97n"},{"post_id":"ckadkqyhz002ck0vq4kwe83ny","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyl7005wk0vqejah8zly"},{"post_id":"ckadkqyi0002fk0vq5edg3qm4","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyl8005yk0vqhixochxd"},{"post_id":"ckadkqyi2002jk0vqce2e2cea","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyl90060k0vq28jh52a7"},{"post_id":"ckadkqyi6002mk0vq2j723006","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyla0062k0vq7cps9eqi"},{"post_id":"ckadkqyi8002pk0vq1wqgbeyd","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqylb0064k0vqca0zf02y"},{"post_id":"ckadkqyid002tk0vq3tvwc4ro","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqylc0066k0vq3m2j6c7j"},{"post_id":"ckadkqyij002vk0vq4vef4acb","tag_id":"ckadkqyjr003wk0vq96qphecy","_id":"ckadkqyle0068k0vqdk0deeev"},{"post_id":"ckadkqyit002zk0vq2cha6e4e","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqyle006ak0vqd8upgkni"},{"post_id":"ckadkqyiv0032k0vq0rt3b5v7","tag_id":"ckadkqyle0069k0vq8esx6e9d","_id":"ckadkqylf006dk0vqb5r8e23b"},{"post_id":"ckadkqyiv0032k0vq0rt3b5v7","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqylg006ek0vqcxo83hix"},{"post_id":"ckadkqyix0037k0vq0oqkeyqj","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqylg006gk0vqe1f50hhz"},{"post_id":"ckadkqyiz0039k0vq336s5w58","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqyli006ik0vq9w570lka"},{"post_id":"ckadkqyj1003dk0vq6c0a6gm3","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqylj006kk0vqhd2cdut7"},{"post_id":"ckadkqyj5003gk0vq1a44gghl","tag_id":"ckadkqyle0069k0vq8esx6e9d","_id":"ckadkqylk006nk0vq7izn95pn"},{"post_id":"ckadkqyj5003gk0vq1a44gghl","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqylk006ok0vq17om7uti"},{"post_id":"ckadkqyj8003kk0vqdr0rgkbm","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqyll006qk0vq561165p3"},{"post_id":"ckadkqyjd003nk0vq437qfjbu","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqyll006sk0vq7tek5uwl"},{"post_id":"ckadkqyjg003rk0vq69gf1uc5","tag_id":"ckadkqyle0069k0vq8esx6e9d","_id":"ckadkqylm006uk0vq6ppx2ly9"},{"post_id":"ckadkqyjp003uk0vqbl1t19kj","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqylm006wk0vq06ehf8c1"},{"post_id":"ckadkqyjs003yk0vq038d3uci","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqyln006yk0vq45az7xwt"},{"post_id":"ckadkqyjt0041k0vq64wo4q43","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqylo0070k0vqdbrv59hg"},{"post_id":"ckadkqyjv0044k0vqcrllci97","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqylo0072k0vq0oog0evl"},{"post_id":"ckadkqyjx0048k0vqdrcg4m6u","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqylp0074k0vq3q9u4brb"},{"post_id":"ckadkqyk0004bk0vq7h9e3c91","tag_id":"ckadkqyld0067k0vqergy80hx","_id":"ckadkqylp0076k0vq5ecmc44d"},{"post_id":"ckadkqyk1004ek0vqbldndcjq","tag_id":"ckadkqylp0075k0vq3suhguf5","_id":"ckadkqylq0078k0vq79ya8azf"},{"post_id":"ckadkqyk3004hk0vq733m0f3n","tag_id":"ckadkqylp0075k0vq3suhguf5","_id":"ckadkqylq007ak0vqaui68n3t"},{"post_id":"ckadkqyk8004lk0vq0oc9hbbe","tag_id":"ckadkqylp0075k0vq3suhguf5","_id":"ckadkqylr007ck0vq86twfzhi"},{"post_id":"ckadkqykb004ok0vqeoir990m","tag_id":"ckadkqylp0075k0vq3suhguf5","_id":"ckadkqylt007ek0vqhbg78nvi"},{"post_id":"ckadkqyki004rk0vqcvj418ji","tag_id":"ckadkqylp0075k0vq3suhguf5","_id":"ckadkqylu007gk0vq1x851thh"},{"post_id":"ckadkqykl004vk0vq4es808it","tag_id":"ckadkqylt007fk0vq5imp8aiv","_id":"ckadkqylu007ik0vq35ucbqak"},{"post_id":"ckadkqykm004yk0vqb28ldrqo","tag_id":"ckadkqylu007hk0vq2ae09u6p","_id":"ckadkqylv007kk0vqb3mc930g"},{"post_id":"ckadkqykp0052k0vq0flq6ta1","tag_id":"ckadkqylv007jk0vqgol3h5d3","_id":"ckadkqylw007mk0vq35tz705f"},{"post_id":"ckadkqykr0055k0vq66edb5ac","tag_id":"ckadkqylu007hk0vq2ae09u6p","_id":"ckadkqylx007ok0vq4i0gebjo"},{"post_id":"ckadkqyku0059k0vqcy086528","tag_id":"ckadkqylu007hk0vq2ae09u6p","_id":"ckadkqyly007qk0vq5w6o4ztv"},{"post_id":"ckadkqykz005ck0vq34br5he5","tag_id":"ckadkqyly007pk0vq3ct1c6xk","_id":"ckadkqyly007rk0vqgpmgeh0x"}],"Tag":[{"name":"blockchain","_id":"ckadkqyeq0002k0vqhz78adle"},{"name":"algorithm","_id":"ckadkqyfr000hk0vq81z4bee6"},{"name":"Pbft","_id":"ckadkqyg6000nk0vq59de1sb0"},{"name":"Raft","_id":"ckadkqygu000zk0vq9zzq3iyl"},{"name":"CouchDb","_id":"ckadkqyi2002ik0vq2l3v8s3r"},{"name":"etcd","_id":"ckadkqyjr003wk0vq96qphecy"},{"name":"fabric","_id":"ckadkqyld0067k0vqergy80hx"},{"name":"fabric-ca","_id":"ckadkqyle0069k0vq8esx6e9d"},{"name":"IPFS","_id":"ckadkqylp0075k0vq3suhguf5"},{"name":"正则表达式","_id":"ckadkqylt007fk0vq5imp8aiv"},{"name":"curl学习","_id":"ckadkqylu007hk0vq2ae09u6p"},{"name":"Linux","_id":"ckadkqylv007jk0vqgol3h5d3"},{"name":"bloom","_id":"ckadkqyly007pk0vq3ct1c6xk"}]}}